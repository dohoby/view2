{"./":{"url":"./","title":"Introduction","keywords":"","body":"Java实用笔记 Zother 2 Interview Architecture Base 系统架构基础 Bigdata Algo 大数据基础算法 Hbase H Base Hdfs HDFS Concurrent Design 高并发系统设计 Flow Control 高并发下的流量控制 Index Design Recommended 推荐系统 Seckill 秒杀系统 Tiny URL 短链接 Distributed Cache 分布式缓存 Consensus 分布式一致性和共识协议 Dubbo Dubbo Kafka Kafka Lock 分布式锁 Mq MQ Rpc RPC Transaction 分布式事务 Zk Zookeeper Basic Algo Hash 哈希 Kmp KMP Mst 最小生成树算法 Path 最短路径算法 Search 查找算法 Skip List 跳跃表 Sort 排序算法 Tree 树 算法 Cryptology 密码学 Database Mysql Architecture MySQL 架构 Innodb Concurrent InnoDB 并发控制 Index InnoDB 索引 Transaction InnoDB 事务 Index Sharding MySQL 集群 Index Redis Redis Sql SQL Net Http HTTP Https HTTPS Ip IP Protocol 网络协议 Tcp TCP Websocket Websocket 网络分层 Os Arch 计算机体系结构 Concurrency 进程管理 Device 设备管理 Disk 磁盘与文件 Io I O Memory 内存管理 Questions Index 操作系统基础 Index Fromwork Mybatis Cache Mybatis 缓存机制 Proxy Mybatis 动态代理 Question 面试题 Index Netty Netty Spring Aop AOP Design Partten 设计模式 Ioc IOC Spring 基本 Java Annotation 注解 Collection Block Queue Blocking Queue Concurrenthashmap Concurrent Hashmap Hash Map 集合框架 Concurrent AQS Atomic Atomic Integer Count Down Latch Count Down Latch Interrupt 线程中断 Synchronized Synchronized Thread Java线程 Threadlocal Thread Local Volatile Volatile Index Exception Java 异常 Generics Java 泛型 Jvm Architecture JVM 架构 Classloader 类加载器 Dispatcher Java 分派机制 Gc 垃圾回收 Jvm Object Lifecycle 对象的生命周期 Runtime Area 运行时数据区 String Constant Pool String 常量池 Index Nio NIO Object Object Oop 面向对象基础 Operator 运算符 Proxy Java 代理 Question Java面试题 Serilaser 序列化 String Builder String Builder Leetcode Add Two Numbers 两数相加 All One 全 O(1) 的数据结构 Check Inclusion 字符串的排列 Detect Cycle 环形链表 II Find Circle Num 朋友圈 Find Kth Largest 数组中的第K个最大元素 Find Length Of LCIS Index Get Intersection Node Index Length Of Longest Substring Index Longest Common Prefix Index Longest Consecutive Index Lowest Common Ancestor Index LRU Cache Index Max Area Of Island Index Max Profit Index Max Profit 2 Index Max Sub Array Index Merge K Lists Index Merge Ragen Index Merge Two Lists Index Min Stack Index My Sqrt Index Restore Ip Addresses Index Reverse List Index Reverse Words Index Salary Index Search Rote Index Simplify Path Index Sort List Index String Multiply Index Three Sum Index Trap Index Valid Utf 8 Index Zigzag Level Order Index Offer Add Index BST Link Convert Index BST Kth Node Index Clone Link Index Count Of Sorted Array Index Cut Rope Index Duplicate Index Entry Node Of Loop Index Fibonacci Index Find Minimum In Rotated Sorted Array Index Find Continuous Sequence Index Find First Common Node Index Find Greatest Sum Of Sub Array Index Find Kth To Tail Index Find Numbers With Sum Index Find Nums Appear Once Index Find Path Index First Not Repeating Char Index Get Least Numbers Index Get Least Numbers Solution Index Get Next 二叉树的下一个结点 Get Number Of K Index Get Ugly Number Index Has Path Index Has Subtree Index Inverse Pairs Index Is Continuous Index Is Numeric Index Is Pop Order Index Is Symmetrical Index Last Remaining Index Left Rotate String Index Longest No Repeat Sub String Index Max Gift 礼物的最大值 Max In Windows Index Max Profit Index Merge Sort Link Index Min Stack Index Mirror Tree Index More Than Half Num Index Moving Count Index N Of Number Serialize Index Number Of One Index Number Of One Between One And N Index O 1 Delete Node Index Pattern Match Index Permutation Index Power Index Print Link From Tail Index Print From Top To Bottom Index Print Matrix Index Print Min Number Index Printn Index Re Construct Binary Tree Index Re Order Array Index Replay Space Index Reverse Sentence Index Revert Link Index Search A 2 D Matrix Index Serialize Tree Index Singleton Index Stream Mid Index Sum Index Sum Of N Dice Index Translate Num To Str Index Tree Depth Index Two Stack Fifo Index Verify Squence Of BST Index Index Zother 3 Java Interview Business Index Data Structures And Algorithms Algorithms Data Structures Db Cache Cache Basic Db Basic Db Index Index Mongodb Mysql Redis Sharding Design Pattern Index Distributed Index Java Basic Basic Collections Index Io Jvm Multithread Oop Java Web Index Java Web Basic Mybatis Spring Mic Service Index Mq Basic Index Network Server Index Netty Network Nginx Tomcat Security Performance Index Performance Security Software Engineering Uml System Index Zother 4 Easy Job Algorithm Sort 对一致性哈希算法的深入研究 九种内部排序算法的Java实现及其性能测试 Database DATABASE My SQL PASS Document Core Article Framework Netty Nginx Rabbitmq Redis Spark Streaming Spark Spring Aop Spring Ioc Websocket Git Java Java序列化 Java泛型 Java注解之注解处理器 Java注解入门 Java移位符 Java类加载器 Json JVM Maven基础 Spring Struts URL URI 一致性Hash算法 单例 字符编码 Network OS Linux QA Game Test Questions Java Summary Tools Idea 占用端口 Zother 6 Java Guide Database Redis Redis Collection Redis(1)——5种基本数据结构——5种基本数据结构.md) Redis(10)——Redis数据类型、编码、数据结构的关系——Redis数据类型、编码、数据结构的关系.md) Redis(2)——跳跃表——跳跃表.md) Redis(3)——分布式锁深入探究——分布式锁深入探究.md) Redis(5)——亿级数据过滤和布隆过滤器——亿级数据过滤和布隆过滤器.md) Redis(6)——GeoHash查找附近的人——GeoHash查找附近的人.md) Redis(7)——持久化——持久化.md) Redis(8)——发布订阅与Stream——发布订阅与Stream.md) Redis(9)——集群入门实践教程——集群入门实践教程.md) Reids(4)——神奇的HyperLoglog解决统计问题——神奇的HyperLoglog解决统计问题.md) Redis持久化 redis集群以及应用场景 Redlock分布式锁 如何做可靠的分布式锁，Redlock真的可行么 My SQL Index My SQL MySQL高性能优化规范建议 一千行MySQL命令 一条sql语句在mysql中如何执行的 事务隔离级别(图文详解).md) 关于数据库存储时间的一点思考 数据库索引 数据库连接池 阿里巴巴开发手册数据库部分的一些最佳实践 Data Structures Algorithms Data Structure Bloom Filter Backtracking N Queens 公司真题 几道常见的子符串算法题 几道常见的链表算法题 剑指offer部分编程题 数据结构 算法学习资源推荐 Essential Content For Interview BAT Jreal Interview Experience 2019 Alipay Pinduoduo Toutiao 2020 Zijietiaodong 5面阿里,终获offer Bingo Interview 蚂蚁金服实习生面经总结(已拿口头offer).md) Preparing For Interview Interview Prepare Java Interview Library Java Programmer Need Know 应届生面试最爱问的几道Java基础问题 程序员的简历之道 美团面试常见问题总结 面试官-你有什么问题要问我 Real Interview Experience Analysis Alibaba 1 手把手教你用Markdown写一份高质量的简历 简历模板 面试必备之乐观锁与悲观锁 Github Trending 2018 12 2019 1 2019 12 2019 2 2019 3 2019 4 2019 5 2019 6 Java Github Trending Java Basic Arrays Collections Common Methods Final Static This Super Reflection 用好Java中的枚举真的没有那么简单 Collection Array List Grow Array List Hash Map Java集合框架常见面试题 Linked List Java Programming Problem A Thread Safe Implementation Of Lru Cache Java程序设计题 Jdk New Features New Features From Jdk 8 To Jdk 14 Jvm [加餐]大白话带你认识JVM GC调优参数 Java内存区域 JDK监控和故障处理工具总结 jvm 知识点汇总 JVM垃圾回收 最重要的JVM参数指南 类加载器 类加载过程 类文件结构 Multithread AQS Atomic Java Concurrency Advanced Common Interview Questions Java Concurrency Basics Common Interview Questions Summary java线程池学习总结 Synchronized Thread Local 创建线程的几种方式总结 多线程学习指南 并发容器总结 What S New In JDK 8 Java8foreach指南 Java 8 Tutorial Java8教程推荐 BIO NIO AIO J2EE基础知识 JAD反编译tricks Java IO与NIO Java Naming Conventions Java基础知识 Java疑难点 Java编程规范 多线程系列 手把手教你定位常见Java性能问题 Network HTTPS中的TLS 干货：计算机网络知识总结 计算机网络 Operating System Basis Linux Shell Questions Java Big Data Java Learning Path And Methods Java Learning Website Blog Java Training 4 Month System Design Authority Certification Basis Of Authority Certification JWT Advantages And Disadvantages Sso Data Communication Dubbo Kafka Inverview Kafka入门看这一篇就够了 Message Queue Rabbitmq Rocket MQ Questions Rocket MQ Why Use Rpc Framework Mybatis Mybatis Interview Spring Spring Annotations Spring Design Patterns Spring Transaction Spring Spring Bean Spring Interview Questions Spring MVC Principle Zoo Keeper Plus Zoo Keeper ZooKeeper数据模型和常见命令 Micro Service Api Gateway Intro API网关 Limit Request Spring Cloud 分布式id生成方案总结 Website Architecture 8 张图读懂大型网站技术架构 关于大型网站系统架构你不得不懂的10个问题 分布式 如何设计一个高可用系统？要考虑哪些地方？ Restful Api 设计模式 Tools Github Github Star Ranking Docker Image Docker Git 阿里云服务器使用经验 javaguide面试突击版 Update History 公众号历史文章汇总 "},"zother2-interview/architecture/base/":{"url":"zother2-interview/architecture/base/","title":"系统架构基础","keywords":"","body":"系统架构基础 分布式系统的最大难点，就是各个节点的状态如何同步。CAP 定理是这方面的基本定理，也是理解分布式系统的起点。 1998年，加州大学的计算机科学家 Eric Brewer 提出，分布式系统有三个指标。 Consistency Availability Partition tolerance 它们的第一个字母分别是 C、A、P。Eric Brewer 说，这三个指标不可能同时做到。这个结论就叫做 CAP 定理。 CAP 它指出对于一个分布式计算系统来说，不可能同时满足以下三点： 一致性（Consistency） （等同于所有节点访问同一份最新的数据副本） 可用性（Availability）（每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据） 分区容错性（Partition tolerance）（以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。） 数据一致性模型 一些分布式系统通过复制数据来提高系统的可靠性和容错性，并且将数据的不同的副本存放在不同的机器，由于维护数据副本的一致性代价高，因此许多系统采用弱一致性来提高性能，一些不同的一致性模型也相继被提出。 强一致性： 要求无论更新操作实在哪一个副本执行，之后所有的读操作都要能获得最新的数据。 弱一致性：用户读到某一操作对系统特定数据的更新需要一段时间，我们称这段时间为“不一致性窗口”。 最终一致性：是弱一致性的一种特例，保证用户最终能够读取到某操作对系统特定数据的更新。 一致性解决方案 分布式事务：两段提交 分布式锁 MQ 消息持久化 重试 幂等 Paxos 算法 服务可用性 可用性，意思是只要收到用户的请求，服务器就必须给出回应。 高可用解决方案 负载均衡： 降级：当服务器压力剧增的情况下，根据当前业务情况及流量对一些服务和页面有策略的降级，以此释放服务器资源以保证核心任务的正常运行。 熔断：对于目标服务的请求和调用大量超时或失败，这时应该熔断该服务的所有调用，并且对于后续调用应直接返回，从而快速释放资源，确保在目标服务不可用的这段时间内，所有对它的调用都是立即返回，不会阻塞的。再等到目标服务好转后进行接口恢复。 流量控制： 异地多活： 熔断是减少由于下游服务故障对自己的影响；而降级则是在整个系统的角度上，考虑业务整体流量，保护核心业务稳定。 分区容错性 大多数分布式系统都分布在多个子网络。每个子网络就叫做一个区（partition）。分区容错的意思是，区间通信可能失败。比如，一台服务器放在中国，另一台服务器放在美国，这就是两个区，它们之间可能无法通信。 般来说，分区容错无法避免，因此可以认为 CAP 的 P 总是成立。CAP 定理告诉我们，剩下的 C 和 A 无法同时做到。 "},"zother2-interview/architecture/bigdata/algo/":{"url":"zother2-interview/architecture/bigdata/algo/","title":"大数据基础算法","keywords":"","body":"Bloom filter 适用范围 实现数据字典，进行数据的判重，或者集合求交集 基本原理 　　对于原理来说很简单，Bit-Map + K个独立 Hash 函数。将 Hash 函数对应的值的位数组置1，查找时如果发现所有 Hash 函数对应位都是1说明存在。很明显这个过程并不保证查找的结果是 100% 正确的。同时也不支持删除一个已经插入的关键字，因为该关键字对应的位会牵动到其他的关键字。所以一个简单的改进就是 Counting Bloom filter，用一个 Counter 数组代替位数组，就可以支持删除了。 对于元素个数：n，错误率（假阳率）：P。我们可以计算： Bit-Map 大小：m\\ge -\\frac{n\\times ln^P}{(ln^2)^2} Hash 函数个数：k=log_2^\\frac{1}{P} 参数在线计算工具：Bloom Filter Calculator 举个例子我们假设 P=0.01，n=4000，则此时 m 应大概是 9.5\\times n=38000 bit，k=7 注意这里 m 与 n 的单位不同，m是 bit 为单位，而n则是以元素个数为单位(准确的说是不同元素的个数)。通常单个元素的长度都是有很多 bit 的。所以使用bloom filter内存上通常都是节省的。 扩展 　　Bloom filter 将集合中的元素映射到位数组中，用 k 个映射位是否全1表示元素在不在这个集合中。Counting bloom filter（CBF）将 位数组中的每一位扩展为一个 Counter，从而支持了元素的删除操作。Spectral Bloom Filter（SBF）将其与集合元素的出现次数关联。SBF采用 Counter 中的最小值来近似表示元素的出现频率。 问题实例 给你A,B两个文件，各存放50亿条URL，每条URL占用64字节，内存限制是4G，让你找出A,B文件共同的URL。如果是三个乃至n个文件呢？ 答：根据这个问题我们来计算下内存的占用，4G=2^32大概是40亿*8大概是 340亿，n=50亿，如果按出错率 0.01 算需要的大概是 475 亿个bit。现在可用的是 340 亿，相差并不多，这样可能会使出错率上升些。另外如果这些url ip是一一对应的，就可以转换成ip，则大大简单了。 Hashing 适用范围 快速查找，删除的基本数据结构，通常需要总数据量可以放入内存 基本原理 　- hash函数选择，针对字符串，整数，排列，具体相应的hash方法。 碰撞处理，一种是open hashing，也称为拉链法；另一种就是closed hashing，也称开地址法。 扩展 　　d-left hashing中 的 d 是多个的意思，我们先简化这个问题，看一看 2-left hashing。2-left hashing指的是将一个哈希表分成长度相等的两半，分别叫做 T1 和 T2 ，给 T1 和 T2 分别配备一个哈希函数， h1 和 h2 。在存储一个新的key时，同时用两个哈希函数进行计算，得出两个地址 h1[key]和 h2[key]。这时需要检查 T1 中的 h1[key] 位置和 T2 中的 h2[key] 位置，哪一个位置已经存储的（有碰撞的）key比较多，然后 将新key存储在负载少的位置。如果两边一样多，比如两个位置都为空或者都存储了一个key，就把新key存储在左边的T1子表中，2-left也由此而来。在查找一个key时，必须进行两次hash，同时查找两个位置。 但是这种方法只能使用在静态集合上，一旦集合发生变化，就需要进行重新计算。 问题实例： 海量日志数据，提取出某日访问百度次数最多的那个IP。 答：IP的数目还是有限的，最多2^32个，所以可以考虑使用hash将 ip 直接存入内存，然后进行统计。 bit-map 适用范围 可进行数据的快速查找，判重，删除，一般来说数据范围是int的10倍以下 基本原理 使用bit数组来表示某些元素是否存在，比如8位电话号码。例如bit数组 001101001001101001 代表实际数组 [2,3,5,8] 。新加入一个元素，只需要将已有的bit数组和新加入的数字做按位或 or 计算。bitmap中 1 的数量就是集合的基数值。 bitmap 有一个很明显的优势是可以轻松合并多个统计结果，只需要对多个结果求异或就可以。也可以大大减少存储内存，可以做个简单的计算，如果要统计1亿个数据的基数值，大约需要内存： 100000000/8/1024/1024 \\approx≈ 12M 如果用 32bit 的 int 代表每个统计数据，大约需要内存：32*100000000/8/1024/1024 \\approx≈ 381M bitmap 对于内存的节约量是显而易见的，但还是不够。统计一个对象的基数值需要12M，如果统计 10000 个对象，就需要将近 120G 了，同样不能广泛用于大数据场景。 扩展 bloom filter可以看做是对bit-map的扩展 问题实例 已知某个文件内包含一些电话号码，每个号码为8位数字，统计不同号码的个数。 答：8位最多99 999 999，大概需要99m个bit，大概10几m字节的内存即可。 2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。 答：将bit-map扩展一下，用2bit表示一个数即可，0表示未出现，1表示出现一次，2表示出现2次及以上。或者我们不用2bit来进行表示，我们用两个bit-map即可模拟实现这个2bit-map。 HyperLogLog 算法演示 上面我们计算过用 bitmap 存储 1亿 个统计数据大概需要 12M 内存；而在 HLL 中，只需要不到 1K 内存就能做到；redis中实现的 HyperLogLog ，只需要 12K 内存，在标准误差 0.81% 的前提下，能够统计 2^{64} 个数据。首先容我感叹一下数学的强大和魅力，那么概率算法是怎样做到如此节省内存的，又是怎样控制误差的呢？ HLL中实际存储的是一个长度为 m 的大数组 S ，将待统计的数据集合划分成 m 组，每组根据算法记录一个统计值存入数组中。数组的大小 m 由算法实现方自己确定，redis中这个数组的大小是 16834，m 越大，基数统计的误差越小，但需要的内存空间也越大。 通过hash函数计算输入值对应的比特串 比特串的低 t(t=log_2^m)t 位对应的数字用来找到数组 S 中对应的位置 i t+1 位开始找到第一个 1 出现的位置 k，将 k 记入数组 S_i 位置 基于数组 S 记录的所有数据的统计值，计算整体的基数值，计算公式可以简单表示为：\\hat{n}=f(S) 原理 举一个我们最熟悉的抛硬币例子，出现正反面的概率都是 1/2 ，一直抛硬币直到出现正面，记录下投掷次数 k ，将这种抛硬币多次直到出现正面的过程记为一次伯努利过程，对于 n 次伯努利过程，我们会得到nn个出现正面的投掷次数值 k1, k_2 …… k_n，其中最大值记为 k{max}，那么可以得到下面结论： n 次伯努利过程的投掷次数都不大于 k_{max} n 次伯努利过程，至少有一次投掷次数等于 k{max} ​​ 回到基数统计的问题，我们需要统计一组数据中不重复元素的个数，集合中每个元素的经过hash函数后可以表示成 0 和 1 构成的二进制数串，一个二进制串可以类比为一次抛硬币实验，1 是抛到正面，0 是反面。二进制串中从低位开始第一个 1 出现的位置可以理解为抛硬币试验中第一次出现正面的抛掷次数 k ，那么基于上面的结论，我们可以通过多次抛硬币实验的最大抛到正面的次数来预估总共进行了多少次实验，同样可以可以通过第一个 1 出现位置的最大值 k{max} 来预估总共有多少个不同的数字（整体基数）。 这种通过局部信息预估整体数据流特性的方法似乎有些超出我们的基本认知，需要用概率和统计的方法才能推导和验证这种关联关系。HyperLogLog 核心在于观察集合中每个数字对应的比特串，通过统计和记录比特串中最大的出现 1 的位置来估计集合整体的基数，可以大大减少内存耗费。 Simhash SimHash算法可计算文本间的相似度，实现文本去重。 首先，对原始内容分词，并且计算每个词的权重； 对每个词哈希成一个整数，并且把这个整数对应的二进制序列中的 0 变成 -1，1 还是 1，得到一个 1 和 -1 组成的向量； 把每个词哈希后的向量乘以词的权重，得到一个新的加权向量； 把每个词的加权向量相加，得到一个最终向量，这个向量中每个元素有正有负； 把最终这个向量中元素为正的替换成 1，为负的替换成 0，这个向量变成一个二进制位序列，也就是最终变成了一个整数。 Simhash 为每一个内容生成一个整数指纹，其中的关键是把每个词哈希成一个整数，这一步常常采用 Jenkins 算法。这里简单示意的整数只有 8 个二进制位，实际上可能需要 64 个二进制位的整数，甚至范围更大。 得到每个内容的 Simhash 指纹后，可以两两计算 汉明距离，比较二进制位不同个数，其实就是计算两个指纹的异或，异或结果中如果包含 3 个以下的 1，则认为两条内容重复。 堆 适用范围 海量数据前n大，并且n比较小，堆可以放入内存 基本原理 最大堆求前n小，最小堆求前n大。适合大数据量，求前n小，n的大小比较小的情况，这样可以扫描一遍即可得到所有的前n元素，效率很高。 扩展 双堆，一个最大堆与一个最小堆结合，可以用来维护中位数。 问题实例： 100w个数中找最大的前100个数。 答：用一个100个元素大小的最小堆即可。 双层桶划分 适用范围 第k大，中位数，不重复或重复的数字 基本原理 因为元素范围很大，不能利用直接寻址表，所以通过多次划分，逐步确定范围，然后最后在一个可以接受的范围内进行。可以通过多次缩小，双层只是一个例子。 其实本质上就是【分而治之】的思想，重在分的技巧上！ 扩展 当有时候需要用一个小范围的数据来构造一个大数据，也是可以利用这种思想，相比之下不同的，只是其中的逆过程。 问题实例 2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。 答：有点像鸽巢原理，整数个数为 2^32 ,也就是，我们可以将这 2^32 个数，划分为 2^8 个区域(比如用单个文件代表一个区域)，然后将数据分离到不同的区域，然后不同的区域在利用 bitmap 就可以直接解决了。也就是说只要有足够的磁盘空间，就可以很方便的解决。 5亿个 int 找它们的中位数。 答一：这个例子比上面那个更明显。首先我们将 int 划分为 2^16个 区域，然后读取数据统计落到各个区域里的数的个数，之后我们根据统计结果就可以判断中位数落到那个区域，同时知道这个区域中的第几大数刚好是中位数。然后第二次扫描我们只统计落在这个区域中的那些数就可以了。 答二：实际上，如果不是 int 是 int64，我们可以经过3次这样的划分即可降低到可以接受的程度。即可以先将 int64 分成 2^24 个区域，然后确定区域的第几大数，在将该区域分成 2^20 个子区域，然后确定是子区域的第几大数，然后子区域里的数的个数只有 2^20 ，就可以直接利用 direct addr table 进行统计了。 数据库索引 适用范围 大数据量的增删改查 基本原理 利用数据的设计实现方法，对海量数据的增删改查进行处理。 倒排索引(Inverted index) 适用范围 搜索引擎，关键字查询 基本原理 用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。 以英文为例，下面是要被索引的文本： T0 = \"it is what it is\" T1 = \"what is it\" T2 = \"it is a banana\" 我们就能得到下面的反向文件索引： \"a\": {2} \"banana\": {2} \"is\": {0, 1, 2} \"it\": {0, 1, 2} \"what\": {0, 1} 检索的条件\"what\",\"is\"和\"it\"将对应集合的交集。 正向索引开发出来用来存储每个文档的单词的列表。正向索引的查询往往满足每个文档有序频繁的全文查询和每个单词在校验文档中的验证这样的查询。在正向索引中，文档占据了中心的位置，每个文档指向了一个它所包含的索引项的序列。也就是说文档指向了它包含的那些单词，而反向索引则是单词指向了包含它的文档，很容易看到这个反向的关系。 扩展 问题实例 文档检索系统，查询那些文件包含了某单词，比如常见的学术论文的关键字搜索。 K 路归并 适用范围 大数据的排序，去重。 基本原理 一般来说外排序分为两个步骤：预处理和合并排序。 按照内存大小，将大文件分成若干长度为 l 的子文件（l 应小于内存的可使用容量），然后将各个子文件依次读入内存，使用适当的内部排序算法对其进行排序（排好序的子文件统称为“归并段”或者“顺段”），将排好序的归并段重新写入外存，为下一个子文件排序腾出内存空间； 对得到的顺段进行合并，直至得到整个有序的文件为止。 问题实例 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16个字节，内存限制大小是1M。返回频数最高的100个词。 败/胜者树 对于外部排序算法来说，其直接影响算法效率的因素为读写外存的次数，即次数越多，算法效率越低。若想提高算法的效率，即减少算法运行过程中读写外存的次数，可以增加 k 路平衡归并中的 k 值。但是如果毫无限度地增加 k 值，虽然会减少读写外存数据的次数，但会增加内部归并的时间，得不偿失。 为了避免在增加 k 值的过程中影响内部归并的效率，在进行 k-路归并时可以使用 败者树 来实现，该方法在增加 k 值时不会影响其内部归并的效率。 基本原理 败/胜者树实际上是保存部分数据的比较结果，以减少新增数据的比较次数，以减少IO。对于无序表 {49，38，65，97，76，13，27，49} 创建的完全二叉树如图所示，构建此树的目的是选出无序表中的最小值。 这是一棵 胜者树 。因为树中每个非终端结点（除叶子结点之外的其它结点）中的值都表示的是左右孩子相比较后的较小值（谁最小即为胜者）。例如叶子结点 49 和 38 相对比，由于 38 更小，所以其双亲结点中的值保留的是胜者 38。然后用 38 去继续同上层去比较，一直比较到树的根结点。 胜者树和败者树的区别就是：胜者树中的非终端结点中存储的是胜利的一方；而败者树中的非终端结点存储的是失败的一方。而在比较过程中，都是拿胜者去比较。 对于 10 个临时文件，采用 5-路平衡归并时，若每次从 5 个文件中想得到一个最小值就需要比较 4 次。而采用败者树，只需要进行 2 次比较。以上仅仅是得到一个最小值记录，如要得到整个临时文件，其耗费的时间就会相差很大。 置换选择排序 除了增加 k-路归并排序中的 k 值来提高外部排序效率的方法，还有另外一条路可走，即减少初始归并段的个数：置换选择排序 是为在较小的空间内，得到更大的有序段。相比于按照内存容量大小对初始文件进行等分，大大减少了初始归并段的数量，从而提高了外部排序的整体效率。 通过置换选择排序算法得到的初始归并段，其长度并不会受内存容量的限制，且通过证明得知使用该方法所获得的归并段的平均长度为内存工作区大小的两倍。 例如已知初始文件中总共有 24 个记录，假设内存工作区最多可容纳 6 个记录，按照之前的选择排序算法最少也只能分为 4 个初始归并段。而如果使用置换—选择排序，可以实现将 24 个记录分为 3 个初始归并段： 置换—选择排序算法的具体操作过程为： 首先从初始文件中输入 6 个记录到内存工作区中； 从内存工作区中选出关键字最小的记录，将其记为 MINIMAX 记录； 然后将 MINIMAX 记录输出到归并段文件中； 此时内存工作区中还剩余 5 个记录，若初始文件不为空，则从初始文件中输入下一个记录到内存工作区中； 从内存工作区中的所有比 MINIMAX 值大的记录中选出值最小的关键字的记录，作为新的 MINIMAX 记录； 重复过程 3—5，直至在内存工作区中选不出新的 MINIMAX 记录为止，由此就得到了一个初始归并段； 重复 2—6，直至内存工作为空，由此就可以得到全部的初始归并段。 最佳归并树 无论是通过等分还是置换-选择排序得到的归并段，如何设置它们的归并顺序，可以使得对外存的访问次数降到最低？现有通过置换选择排序算法所得到的 9 个初始归并段，其长度分别为：9，30，12，18，3，17，2，6，24。在对其采用 3-路平衡归并的方式时可能出现如图所示的情况： 图中的叶子结点表示初始归并段，各自包含记录的长度用结点的权重来表示；非终端结点表示归并后的临时文件。 假设在进行平衡归并时，操作每个记录都需要单独进行一次对外存的读写，那么图中的归并过程需要对外存进行读或者写的次数为： $$(9+30+12+18+3+17+2+6+24)\\times 2 \\times 2 = 484$$ 图中涉及到了两次归并，对外存的读和写各进行 2 次 从计算结果上看，对于图中的 3 叉树来讲，其操作外存的次数恰好是树的带权路径长度的 2 倍。所以，对于如何减少访问外存的次数的问题，就等同于考虑如何使 k 路归并所构成的 k 叉树的带权路径长度最短。 若想使树的带权路径长度最短，就是构造赫夫曼树。若对上述 9 个初始归并段构造一棵赫夫曼树作为归并树，如图所示： 归并过程中需要对外存进行IO的次数为： $$(2\\times3+3\\times3+6\\times3+9\\times2+12\\times2+17\\times2+18\\times2+24\\times2+30)\\times2=446$$ 通过以构建赫夫曼树的方式构建归并树，使其对读写外存的次数降至最低（k-路平衡归并，需要选取合适的 k 值，构建赫夫曼树作为归并树），所以称此归并树为 最佳归并树 。 Trie树 适用范围 数据量大，重复多，但是数据种类小可以放入内存 字符串的快速检索 字符串排序 最长公共前缀 自动匹配前缀显示后缀 基本原理 Trie树的创建要考虑的是父节点如何保存孩子节点，主要有链表和数组两种方式 链表：空间占用少，查找效率低 数组：空间占用多，查找效率高 Hash： 扩展 压缩Trie Patricia Tree 基数树（也叫基数特里树或压缩前缀树）是一种数据结构，是一种更节省空间的Trie（前缀树），其中作为唯一子节点的每个节点都与其父节点合并，边既可以表示为元素序列又可以表示为单个元素。 基数树的查找方式也与常规树不同（常规的树查找一开始就对整个键进行比较，直到不相同为止），基数树查找时节点时，对于节点上的键都按块进行逐块比较，其中该节点中块的长度是基数r； 当 r 为2时，基数树为二进制的（即该节点的键的长度为1比特位），能最大程度地减小树的深度来最小化稀疏性（最大限度地合并键中没有分叉的节点）。 当 r≥4 且为2的整数次幂时，基数树是 r 元基数树，能以潜在的稀疏性为代价降低基数树的深度。 后缀Trie 压缩的后缀字典树 查找某个字符串s1是否在另外一个字符串s2中 指定字符串s1在字符串s2中重复的次数 两个字符串S1，S2的最长公共部分 最长回文串 双数组字典树 Double-Array Trie 双数组Trie (Double-Array Trie)结构由日本人JUN-ICHI AOE于1989年提出的，是 Trie结构的压缩形式，仅用两个线性数组来表示Trie树，该结构有效结合了数字搜索树(Digital Search Tree)检索时间高效的特点和链式表示的Trie空间结构紧凑的特点。双数组Trie的本质是一个确定有限状态自动机（DFA），每个节点代表自动机的一个状态，根据变量不同，进行状态转移，当到达结束状态或无法转移时，完成一次查询操作。在双数组所有键中包含的字符之间的联系都是通过简单的数学加法运算表示，不仅提高了检索速度，而且省去了链式结构中使用的大量指针，节省了存储空间。 DAT的生成如上说的只能对 排序 好的单词进行构建 双数组Trie树归根结底还是属于Trie树，所以免不了有一颗树的构造过程。不过这棵树并没有保存下来，而是边构造树边维护双数组以表示整棵树。 //i 为状态值 c为当前字符 parent(i) 为i的前置状态 base[i]=base[parent(i)+c] check[i]=base[parent(i)] base[i]问题实例 有10个文件，每个文件1G，每个文件的每一行都存放的是用户的query，每个文件的query都可能重复。要你按照query的频度排序。 1000万字符串，其中有些是相同的(重复),需要把重复的全部去掉，保留没有重复的字符串。请问怎么设计和实现？ 寻找热门查询：查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个，每个不超过255字节。 map-reduce 适用范围 数据量大，但是数据种类小可以放入内存 基本原理及要点 将数据交给不同的机器去处理，数据划分，结果归约。 问题实例 The canonical example application of MapReduce is a process to count the appearances ofeach different word in a set of documents: 海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。 一共有N个机器，每个机器上有N个数。每个机器最多存O(N)个数并对它们操作。如何找到N^2个数的中数(median)？ 经典问题分析 如何从大量的 URL 中找出相同的 URL？ 给定 a、b 两个文件，各存放 50 亿个 URL，每个 URL 各占 64B，内存限制是 4G。请找出 a、b 两个文件共同的 URL。 解答思路： 分而治之，进行哈希取余； 对每个子文件进行 HashSet 统计。 如何从大量数据中找出高频词？ 有一个 1GB 大小的文件，文件里每一行是一个词，每个词的大小不超过 16B，内存大小限制是 1MB，要求返回频数最高的 100 个词(Top 100)。 解答思路： 分而治之，进行哈希取余； 使用 HashMap 统计频数； 求解最大的 TopN 个，用小顶堆；求解最小的 TopN 个，用大顶堆。 如何在大量的数据中找出不重复的整数？ 在 2.5 亿个整数中找出不重复的整数。注意：内存不足以容纳这 2.5 亿个整数。 解题思路：位图法 如何在大量的数据中判断一个数是否存在？ 给定 40 亿个不重复的没排过序的 unsigned int 型整数，然后再给定一个数，如何快速判断这个数是否在这 40 亿个整数当中？ 解题思路：位图法 如何查询最热门的查询串？ 搜索引擎会通过日志文件把用户每次检索使用的所有查询串都记录下来，每个查询串的长度不超过 255 字节。 假设目前有 1000w 个记录（这些查询串的重复度比较高，虽然总数是 1000w，但如果除去重复后，则不超过 300w 个）。请统计最热门的 10 个查询串，要求使用的内存不能超过 1G。（一个查询串的重复度越高，说明查询它的用户越多，也就越热门。） 解题思路：HashMap、前缀树+小顶堆 如何统计不同电话号码的个数？ 已知某个文件内包含一些电话号码，每个号码为 8 位数字，统计不同号码的个数。 解题思路：位图法（将电话号码作为整型） 如何从 5 亿个数中找出中位数？ 从 5 亿个数中找出中位数。数据排序后，位置在最中间的数就是中位数。当样本数为奇数时，中位数为 第 (N+1)/2 个数；当样本数为偶数时，中位数为 第 N/2 个数与第 1+N/2 个数的均值。 解题思路：双堆法、分治法 如何按照 query 的频度排序？ 有 10 个文件，每个文件大小为 1G，每个文件的每一行存放的都是用户的 query，每个文件的 query 都可能重复。要求按照 query 的频度排序。 解题思路：外排序 如何找出排名前 500 的数？ 有 20 个数组，每个数组有 500 个元素，并且有序排列。如何在这 20*500 个数中找出前 500 的数？ 解题思路：败者树 参考链接 360 搜索的百亿级网页搜索引擎架构实现 最佳归并树 海量数据处理 "},"zother2-interview/architecture/bigdata/hbase/":{"url":"zother2-interview/architecture/bigdata/hbase/","title":"H Base","keywords":"","body":"HBase 概述 HBase 是一个高可靠、高性能、面向列、可伸缩 的分布式数据库，是谷歌 BigTable 的开源实现，主要用来存储非结构化和半结构化的松散数据。HBase 的目标是处理非常庞大的表，可以通过水平扩展的方式，利用廉价计算机集群处理超过10亿行数据和百万列元素组成的数据表。 BigTable HBase 文件存储系统 GFS HDFS 数据处理 MapReduce Hadoop MapReduce 协同服务管理 Chubby Zookeeper Hadoop 已经有 HDFS 为什么还需要 HBase？这里就需要说到 HDFS 的特性： HDFS 本身是高延迟的数据处理机制，无法提供大规模数据的实时查询； HDFS 是面向批量访问模式，不是随机访问模式 HDFS 与传统的关系型数据库的对比： 数据类型：HBase 中所有的数据都是未经解释的字符串（byte 数组） 数据操作：HBase 操作不存在复杂的表与表之间的关系，只有简单的插入、查询、删除、清空 存储模式：HBase 基于列存储，每个列族都由几个文件保存，不同列族的文件分离 数据索引：HBase 只有一个索引——行键 数据维护：HBase 中的更新操作均不会覆盖旧数据，而是生成一个新的版本 可伸缩性：HBase 可以实现灵活的水平扩展 HBase 数据模型 HBase 是一个稀疏、多维度、排序的映射表。HBase 中可以存储若干个列族，一个列族可包含任意个列，同一个列族里面的数据存储在一起。列族支持动态扩展，可以轻松的增加一个列族或者列，无需预先定义列。 表：HBase 采用表来组织数据，表由行和列组成，列划分为若干个列族 行：每个 HBase 表都由若干行组成，每个行由行键（raw key）来标示 列族：HBase 表被分组成许多 “列族”（Column Family） 的集合，它是基本的访问控制单元 列限定符：列族里的数据通过列限定符来定位 单元格：在 HBase 表中，通过行、列族和列限定符能确定一个 “单元格”（Cell） ，单元格中的数据没有数据类型，被视为 byte[] 时间戳：每个单元格都保存着同一份数据的多个版本，这些版本采用时间戳进行索引 HBase 中可通过行键、列族、列限定符和时间戳，唯一确定一个数据，因此可以被视为一个思维坐标，即：[行键，列族，列限定符，时间戳]。 在 HBase 的逻辑上，数据分布如下所示： 但在物理上的结构有所不同： HBase 实现原理 HBase 包含三个主要的功能组件： 客户端： Master 主服务器：管理和维护 HBase 表的分区信息，维护 Region 服务器列表，分配 Region ，负载均衡 Region 服务器：负责存储和维护分配给自己的 Region ，处理客户端的读写请求 数据写入过程 数据读取过程 HBase 运行机制 HBase 应用方案 参考链接 HBase － 数据写入流程解析 HBase原理－数据读取流程解析 "},"zother2-interview/architecture/bigdata/hdfs/":{"url":"zother2-interview/architecture/bigdata/hdfs/","title":"HDFS","keywords":"","body":"HDFS 相关概念 HDFS要实现有以下优势： 兼容廉价硬件设备 流数据读写 大数据集 简单的文件模型 强大的跨平台兼容性 HDFS 在满足上述优势的同时，也不可避免的有一些自身的局限性，主要包括以下几个方面： 不适合低延时的数据访问 无法高效的存储大量小文件 不支持多用户写入及任意修改文件 块 HDFS 同计算机系统一样，有一个存储块的概念，默认大小为 64M，一个文件被分成多个块存储。块的大小远远大于计算机文件系统，可以减小寻址开销。其优势有： 支持大规模文件存储：以块为单位进行存储，一个大文件可被分割为多个块，并分发到不同节点，因此单个节点的存储容量不会限制存储文件的上限 简化系统设计：首先简化了存储管理，因为文件块大小固定，这样很容易计算节点可存储文件块的数量；其次，方便了元数据管理，元数据不与文件块存储，可由其他系统负责管理元数据。 适合数据备份：每个文件块都可冗余存储到多个节点，大大提高了系统的容错性和可用性。 名称节点（NameNode）和数据节点（DataNode） NameNode DataNode 存储元数据 存储文件内容 元数据存储在内存 文件内容存储在磁盘 保存文件、block、 DataNode 之间的映射关系 维护block、本地文件系统的映射关系 名称节点（NameNode）数据结构 在 HDFS 中，名称节点复杂管理分布式文件系统的命名空间（NameSpace），保存两大核心数据结构：FSImage 和 EditLog。名称节点中还记录了每个文件中各个块所在的数据节点的位置信息。 FSImage 文件用于维护文件系统树以及文件树中所有文件和文件夹的元数据。FSImage 文件中包含文件系统所有目录和文件 inode 的序列化形式，每个 inode 是一个文件或目录的元数据表示，包含：文件复制等级、修改和访问时间、访问权限、块大小以及组成文件的块。 FSImage 中并没有记录块存储在哪个数据节点。而是由名称节点把这些映射保存在内存中，当数据节点加入 HDFS 集群时，数据节点会报告自己所包含的数据块给名称节点，并定期执行告知，以确保名称节点的映射关系是正确的。 EditLog 记录了所有针对文件的创建、删除、重命名等操作。 NameNode 将对文件系统的改动追加保存到本地文件系统上的一个日志文件（EditLog）。当一个 NameNode 启动时，它首先从一个映像文件（FSImage）中读取 HDFS 的状态，接着应用日志文件中的 EditLog 操作。然后它将新的HDFS状态写入（FSImage）中，并使用一个空的 EditLog 文件开始正常操作。因为NameNode只有在启动阶段才合并 FSImage 和 EditLog ，所以久而久之日志文件可能会变得非常庞大，特别是对大型的集群。日志文件太大的另一个副作用是下一次 NameNode 启动会花很长时间。 Secondary NameNode 定期合并 FSImage 和 EditLog 日志，将 EditLog 日志文件大小控制在一个限度下。 名称节点的启动 名称节点启动后，会将 FSImage 文件中的内容加载到内存，之后再执行 EditLog 文件中的各项操作，使得内存中的元数据与实际数据一致。 一旦内存中管理文件系统元数据的映射，则会创建一个新的 FSImage 文件和一个空白的 EditLog 文件。 名称节点启动后， HDFS 中的更新操作会重新写入 EditLog，因为 FSImage 文件一般很大，所以不直接向 FSImage 文件中写入数据，但是 EditLog 每次启动后都是空白的。每次执行完写操作，且在向客户端发送成功消息之前，EditLog 文件都需要同步更新。 数据节点 数据节点是 HDFS 的工作节点，负责数据的存储和读取，会根据客户端或者名称节点的调度来进行数据的存储和检索，并定期向名称节点发送自己所存储的块信息。每个数据节点中的数据都会被保存在各自节点的本地 Linux 文件系统中。 HDFS 体系结构 HDFS 采取主从模型，一个 HDFS 集群包含一个 NameNode 和若干个 DataNode 。名称节点作为中心服务器，负责管理文件系统的命名空间及客户端对文件的访问。 命名空间 HDFS 的命名空间包含块映射、以及相关属性，存储在 FSImage 中。在 HDFS 1.0 体系结构中，整个 HDFS 集群中只有一个命名空间，并且只有唯一一个名称节点，该节点对这个命名空间进行管理。在 HDFS 2.0 中提出了 联邦 （Federation） 的概念。 Federation Federation 的设计就是为了解决 HDFS 1.0 中的单一 NameNode 的问题，采用 Federation 的最主要原因是设计实现简单。 block pool 存储在 DataNode 上，并通过 BPOfferService 提供服务 Federation 的核心思想是将一个大的 namespace 划分多个子 namespace ，并且每个 namespace 分别由单独的 NameNode 负责，这些 NameNode 之间互相独立，不会影响，不需要做任何协调工作（其实跟拆集群有一些相似），集群的所有 DataNode 会被多个 NameNode 共享。 其中，每个子 namespace 和 DataNode 之间会由数据块管理层作为中介建立映射关系，数据块管理层由若干 数据块池（Pool） 构成，每个数据块只会唯一属于某个固定的数据块池，而一个子 namespace 可以对应多个数据块池。每个 DataNode 需要向集群中所有的 NameNode 注册，且周期性地向所有 NameNode 发送心跳和块报告，并执行来自所有 NameNode 的命令。 一个 block pool 由属于同一个 namespace 的数据块组成，每个 DataNode 可能会存储集群中所有 block pool 的数据块； 每个 block pool 内部自治，也就是说各自管理各自的 block，不会与其他 block pool 交流，如果一个 NameNode 挂掉了，不会影响其他 NameNode; 某个 NameNode 上的 namespace 和它对应的 block pool 一起被称为 namespace volume，它是管理的基本单位。当一个 NameNode/namespace 被删除后，其所有 DataNode 上对应的 block pool 也会被删除，当集群升级时，每个 namespace volume 可以作为一个基本单元进行升级。 HA 在 Hadoop 1.0 时代，Hadoop 的两大核心组件 HDFS NameNode 和 JobTracker 都存在着单点问题，这其中以 NameNode 的单点问题尤为严重。因为 NameNode 保存了整个 HDFS 的元数据信息，一旦 NameNode 挂掉，整个 HDFS 就无法访问，同时 Hadoop 生态系统中依赖于 HDFS 的各个组件，并且重新启动 NameNode 和进行数据恢复的过程也会比较耗时。这些问题在给 Hadoop 的使用者带来困扰的同时，也极大地限制了 Hadoop 的使用场景，使得 Hadoop 在很长的时间内仅能用作离线存储和离线计算，无法应用到对可用性和数据一致性要求很高的在线应用场景中。 所幸的是，在 Hadoop2.0 中，HDFS NameNode 和 YARN ResourceManger 的单点问题都得到了解决，经过多个版本的迭代和发展，目前已经能用于生产环境。 NameNode 的高可用架构主要分为下面几个部分： Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务。 主备切换控制器 ZKFailoverController：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换，当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换。 Zookeeper 集群：为主备切换控制器提供主备选举支持。 共享存储系统：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和 备份 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。 DataNode 节点：除了通过共享存储系统共享 HDFS 的元数据信息之外，主 NameNode 和备 NameNode 还需要共享 HDFS 的数据块和 DataNode 之间的映射关系。DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。 NameNode 主备切换主要由 ZKFailoverController、HealthMonitor 和 ActiveStandbyElector 这 3 个组件来协同实现： HDFS 脑裂问题 在实际中，NameNode 可能会出现这种情况，NameNode 在垃圾回收（GC）时，可能会在长时间内整个系统无响应，因此，也就无法向 ZK 写入心跳信息，这样的话可能会导致临时节点掉线，备 NameNode 会切换到 Active 状态。这种情况，可能会导致整个集群会有同时有两个 NameNode，这就是脑裂问题。 脑裂问题的解决方案是隔离（Fencing），主要是在以下三处采用隔离措施： 第三方共享存储：任一时刻，只有一个 NN 可以写入 DataNode：需要保证只有一个 NN 发出与管理数据副本有关的删除命令 Client：需要保证同一时刻只有一个 NN 能够对 Client 的请求发出正确的响应 共享存储 上述 HA 方案还有一个明显缺点，那就是第三方存储节点有可能失效，目前社区已经把由 Clouderea 公司实现的基于 QJM 的方案作为默认的共享存储实现。QJM（Quorum Journal Manager）本质上是利用 Paxos 协议来实现的，QJM 在 2F+1 个 JournalNode 上存储 NameNode 的 EditLog ，每次写入操作都通过 Paxos 保证写入的一致性，它最多可以允许有 F 个 JournalNode 节点同时故障。 Active NameNode 首先把 EditLog 提交到 JournalNode 集群，然后 Standby NameNode 再从 JournalNode 集群定时同步 EditLog 。还有一点需要注意的是，在 2.0 中不再有 SNN 这个角色了，NameNode 在启动后，会先加载 FSImage 文件和共享目录上的 EditLog Segment 文件，之后 NameNode 会启动 EditLogTailer 线程和 StandbyCheckpointer 线程，正式进入 Standby 模式，其中： EditLogTailer 线程的作用是定时从 JournalNode 集群上同步 EditLog StandbyCheckpointer 线程的作用其实是为了替代 Hadoop 1.x 版本之中的 Secondary NameNode 的功能，StandbyCheckpointer 线程会在 Standby NameNode 节点上定期进行 Checkpoint，将 Checkpoint 之后的 FSImage 文件上传到 Active NameNode 节点 通信协议 客户端与 NameNode 通过 TCP 连接，并使用客户端协议进行通信 NameNode 和 DataNode 之间使用数据节点协议通信 客户端与 DataNode 之间通过 RPC 进行交互 局限性 命名空间的限制（Hadoop 2.0 已解决） 性能瓶颈：受限于单节点吞吐量 隔离问题：单个 NameSpace 无法对不同应用程序进行隔离（Hadoop 2.0 已解决） 可用性：单个 NameNode 故障无法快速切换（Hadoop 2.0 已解决） HDFS 存储原理 冗余数据保存 HDFS 采用多副本方式对数据进行冗余存储，通常一个数据库的多个副本会被分布到不同的数据节点。这样多副本具有以下几个优点： 加速数据传输 容易检查数据错误 保证数据可靠性 数据存取策略 HDFS 使用的是传统的分级文件体系，因此，用户可以像使用普通文件系统一样，创建、删除目录和文件，在目录之间移动、重命名文件，但不支持修改。 存储类型 DISK：普通磁盘 SSD：SSD盘 RAM_DISK：内存盘 ARCHIVE：归档/压缩，不是实际的磁盘类型，而是数据被压缩存储。 存储策略 存储策略允许不同的文件存储在不同的存储类型上。目前有以下策略： Hot：存储和计算都热。 如果是热快，那么复制的目标也是DISK（普通的磁盘）。 Cold：用于有限计算的存储。 数据不再使用，或者需要归档的数据被移动到冷存储。如果数据块是冷的，则复制使用ARCHIVE. Warm：半冷半热。warm块的复制内容，部分放置在DISK，其它的在ARCHIVE. All_SSD：所有数据存储在SSD. One_SSD：一个复制在SSD，其它的在DISK. Lazy_Persist：只针对只有一个复制的数据块，它们被放在RAM_DISK,之后会被写入DISK。 当创建文件/目录的时候，并未为它们设定了存储策略。 但可以通过hdfs storagepolicies 命令来管理。文件/路径的存储策略按照如下规则解析： 如果有设定特定的策略，那么就是那个策略 如果没有设定，就返回上级目录的存储策略。如果是没有策略的根目录，那么返回默认的存储策略（Hot)。 存储时 DataNode 选择 默认情况下，Hadoop 机架感知是没有启用的，需要在NameNode机器的hadoop-site.xml 里配置一个选项。 当没有配置机架信息时，所有的机器 Hadoop 都默认在同一个默认的机架下，名为 /default-rack，这种情况下，任何一台 DataNode 机器，不管物理上是否属于同一个机架，都会被认为是在同一个机架下，此时，就很容易出现之前提到的增添机架间网络负载的情况。在没有机架信息的情况下， NameNode 默认将所有的 slaves 机器全部默认为在 /default-rack 下，此时写 block 时，三个 DataNode 机器的选择完全是随机的。 当配置了机架感知信息以后，hadoop在选择三个datanode时，就会进行相应的判断。 数据读取策略 HDFS 提供 API 可以确定一个数据节点所属机架ID，客户端也可以调用 API 获取自己的机架ID 从名称节点获取数据块不同副本的存放位置列表，列表中包含副本所在的数据节点 客户端调用 API 获取数据节点所属的机架ID，如果与客户端机架ID 相同则优先选择，否则就随机读取 数据错误与恢复 NameNode 出错 HDFS 有备份机制，定时将 FSImage 和 EditLog 备份到 SecondaryNameNode 。当 NameNode 出错，就可以根据 SecondaryNameNode 中的数据进行恢复。 DataNode 出错 当 DataNode 发生网络问题或机器故障时，这些 DataNode 会被标记为 “宕机”，节点上的数据均被标记为 “不可读”， NameNode 不会再对它发送任何数据请求。这时，由于部分 DataNode 不可用，会导致一些数据块的副本数量小于冗余因子。在这种情况下，NameNode 会启动冗余复制，为数据块生成新的副本。 数据出错 当文件被创建时，客户端都会对每个文件块进行信息摘要，并将其存储在同路径的隐藏文件里。客户端在读取数据时，会首先获取信息摘要，并对数据块进行 MD5 和 SHA1 校验。如果数据错误，则会向 NameNode 报告错误，NameNode 定时检查并重新复制该数据块。 HDFS 数据读写过程 写数据过程 具体过程如下： Client 调用 DistributedFileSystem 对象的 create 方法，创建一个文件输出流（FSDataOutputStream）对象； 通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，在 HDFS 的 Namespace 中创建一个文件条目（Entry），此时该条目没有任何的 Block，NameNode 会返回该数据每个块需要拷贝的 DataNode 地址信息； 通过 FSDataOutputStream 对象，开始向 DataNode 写入数据，数据首先被写入 FSDataOutputStream 对象内部的数据队列中，数据队列由 DataStreamer 使用，它通过选择合适的 DataNode 列表来存储副本，从而要求 NameNode 分配新的 block； DataStreamer 将数据包以流式传输的方式传输到分配的第一个 DataNode 中，该数据流将数据包存储到第一个 DataNode 中并将其转发到第二个 DataNode 中，接着第二个 DataNode 节点会将数据包转发到第三个 DataNode 节点； DataNode 确认数据传输完成，最后由第一个 DataNode 通知 client 数据写入成功； 完成向文件写入数据，Client 在文件输出流（FSDataOutputStream）对象上调用 close 方法，完成文件写入； 调用 DistributedFileSystem 对象的 complete 方法，通知 NameNode 文件写入成功，NameNode 会将相关结果记录到 editlog 中。 注意：client运行 write 操作后，写完的 block 才是可见的，正在写的 block 对 client 是不可见的，仅仅有调用 sync 方法。client才确保该文件的写操作已经全部完毕。当 client 调用 close 方法时，会默认调用 sync 方法。是否须要手动调用取决你依据程序须要在数据健壮性和吞吐率之间的权衡。 读数据过程 Client 通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，获取文件 block 位置信息； NameNode 返回存储的每个块的 DataNode 列表； Client 将连接到列表中最近的 DataNode； Client 开始从 DataNode 并行读取数据； 一旦 Client 获得了所有必须的 block，它就会将这些 block 组合起来形成一个文件。 参考 HDFS机架感知功能原理 - rack awareness HDFS数据存储与读写过程 HDFS 架构学习总结 Hadoop NameNode 高可用 (High Availability) 实现解析 "},"zother2-interview/architecture/concurrent/design/":{"url":"zother2-interview/architecture/concurrent/design/","title":"高并发系统设计","keywords":"","body":"高并发系统设计 总览 在高并发的情景下进行系统设计， 可以分为以下 6 点： 系统拆分 熔断 降级 缓存 MQ 分库分表 读写分离 ElasticSearch 系统拆分 将一个系统拆分为多个子系统，用 RPC 来搞。然后每个系统连一个数据库，这样本来就一个库，现在多个数据库，不也可以扛高并发么。 缓存 大部分的高并发场景，都是读多写少，那你完全可以在数据库和缓存里都写一份，然后读的时候大量走缓存不就得了。毕竟 Redis 轻轻松松单机几万的并发。所以你可以考虑考虑你的项目里，那些承载主要请求的读场景，怎么用缓存来抗高并发。 MQ 可能你还是会出现高并发写的场景，比如说一个业务操作里要频繁搞数据库几十次，增删改增删改。那高并发绝对搞挂你的系统，你要是用 Redis 来承载写那肯定不行，人家是缓存，数据随时就被 LRU 了，数据格式还无比简单，没有事务支持。所以该用 MySQL 还得用 MySQL 啊。那你咋办？用 MQ 吧，大量的写请求灌入 MQ 里，后边系统消费后慢慢写，控制在 MySQL 承载范围之内。所以你得考虑考虑你的项目里，那些承载复杂写业务逻辑的场景里，如何用 MQ 来异步写，提升并发性。 分库分表 分库分表，可能到了最后数据库层面还是免不了抗高并发的要求，好吧，那么就将一个数据库拆分为多个库，多个库来扛更高的并发；然后将一个表拆分为多个表，每个表的数据量保持少一点，提高 SQL 跑的性能。 读写分离 读写分离，这个就是说大部分时候数据库可能也是读多写少，没必要所有请求都集中在一个库上吧，可以搞个主从架构，主库写入，从库读取，搞一个读写分离。读流量太多的时候，还可以加更多的从库。 ElasticSearch ES 是分布式的，可以随便扩容，分布式天然就可以支撑高并发，因为动不动就可以扩容加机器来扛更高的并发。那么一些比较简单的查询、统计类的操作，可以考虑用 ES 来承载，还有一些全文搜索类的操作，也可以考虑用 ES 来承载。 参考链接 如何设计一个高并发系统？ "},"zother2-interview/architecture/concurrent/flow-control/":{"url":"zother2-interview/architecture/concurrent/flow-control/","title":"高并发下的流量控制","keywords":"","body":"高并发下的流量控制 这个时候如果不做任何保护措施，服务器就会承受很大的处理压力，请求量很高，服务器负载也很高，并且当请求超过服务器承载极限的时候，系统就会崩溃，导致所有人都不能访问。 为了应用服务的高可用，一个常用的办法是对大流量的请求（秒杀/抢购）进行限流，拦截掉大部分请求，只允许一部分请求真正进入后端服务器，这样就可以防止大量请求造成系统压力过大导致的系统崩溃，从而保护服务正常可用。 令牌桶(Token Bucket)、漏桶(leaky bucket)和 计数器 算法是最常用的三种限流的算法。 限流算法 计数器 计数器限流算法也是比较常用的，主要用来限制总并发数。比如限流 qps 为 100 ，算法的实现思路就是从第一个请求进来开始计时，在接下去的 1s 内，每来一个请求，就把计数加 1 ，如果累加的数字达到了 100 ，那么后续的请求就会被全部拒绝。等到 1s 结束后，把计数恢复成 0 ，重新开始计数。 这种实现方式有一个弊端：如果我在单位时间 1s 内的前 10ms ，已经通过了 100 个请求，那后面的 990ms ，只能眼巴巴的把请求拒绝，这种现象称为 突刺现象。 漏桶 为了消除 突刺现象，可以采用漏桶算法实现限流，漏桶算法这个名字就很形象，算法内部有一个容器，类似生活用到的漏斗，当请求进来时，相当于水倒入漏斗，然后从下端小口慢慢匀速的流出。不管上面流量多大，下面流出的速度始终保持不变。 不管服务调用方多么不稳定，通过漏桶算法进行限流，每 10 毫秒处理一次请求。因为处理的速度是固定的，请求进来的速度是未知的，可能突然进来很多请求，没来得及处理的请求就先放在桶里，既然是个桶，肯定是有容量上限，如果桶满了，那么新进来的请求就丢弃。 在算法实现方面，可以 准备一个队列，用来保存请求，另外通过一个线程池定期从队列中获取请求并执行，可以一次性获取多个并发执行。 这种算法，在使用过后也存在弊端：无法应对短时间的突发流量，同时它的优点也是可以平滑网络上的突发流量，请求可以被整形成稳定的流量。 令牌桶 从某种意义上讲，令牌桶算法是对漏桶算法的一种改进，桶算法能够限制请求调用的速率，而令牌桶算法能够在限制调用的平均速率的同时还允许一定程度的突发调用。 在令牌桶算法中，存在一个桶，用来存放固定数量的令牌。算法中存在一种机制，以一定的速率往桶中放令牌。每次请求调用需要先获取令牌，只有拿到令牌，才有机会继续执行，否则选择选择等待可用的令牌、或者直接拒绝。 放令牌这个动作是持续不断的进行，如果桶中令牌数达到上限，就丢弃令牌，所以就存在这种情况，桶中一直有大量的可用令牌，这时进来的请求就可以直接拿到令牌执行，比如设置 qps 为 100 ，那么限流器初始化完成一秒后，桶中就已经有 100 个令牌了，这时服务还没完全启动好，等启动完成对外提供服务时，该限流器可以抵挡瞬时的 100 个请求。所以，只有桶中没有令牌时，请求才会进行等待，最后相当于以一定的速率执行。 实现思路：可以 准备一个队列，用来保存令牌，另外通过一个线程池定期生成令牌放到队列中，每来一个请求，就从队列中获取一个令牌，并继续执行。 漏桶 VS 令牌桶：两者主要区别在于“漏桶算法”能够强行限制数据的传输速率，而“令牌桶算法”在能够限制数据的平均传输速率外，还允许某种程度的突发传输。在“令牌桶算法”中，只要令牌桶中存在令牌，那么就允许突发地传输数据直到达到用户配置的门限，所以它适合于具有突发特性的流量。 集群限流 Redis 请求窗口 采用redis 的计时和计数方式,在规定的时间窗口期,允许通过的最大请求数量 比如为了限制某个资源被每个用户或者商户的访问次数，5s 只能访问 2 次，或者一天只能调用 1000 次，这种需求，单机限流是无法实现的，这时就需要通过集群限流进行实现。 如何实现？为了控制访问次数，肯定需要一个计数器，而且这个计数器只能保存在第三方服务，比如redis。 大概思路：每次有相关操作的时候，就向 redis 服务器发送一个 incr 命令，比如需要限制某个用户访问 /index 接口的次数，只需要拼接用户 id 和接口名生成 redis 的 key ，每次该用户访问此接口时，只需要对这个 key 执行 incr 命令，在这个 key 带上过期时间，就可以实现指定时间的访问频率。 Nginx 限流 Nginx按请求速率限速模块使用的是漏桶算法，即能够强行保证请求的实时处理速度不会超过设置的阈值。 Nginx官方版本限制IP的连接和并发分别有两个模块： limit_req_zone 用来限制单位时间内的请求数，即速率限制,采用的漏桶算法 \"leaky bucket\"。 limit_req_conn 用来限制同一时间连接数，即并发限制。 "},"zother2-interview/architecture/concurrent/_index.html":{"url":"zother2-interview/architecture/concurrent/_index.html","title":"Index","keywords":"","body":""},"zother2-interview/architecture/design/recommended/":{"url":"zother2-interview/architecture/design/recommended/","title":"推荐系统","keywords":"","body":"推荐系统 根据用户的历史信息和行为，向用户推荐他感兴趣的内容 解决的问题 信息过载 挖掘长尾 提升用户体验 概念 CTR: Click-Through-Rate 点击通过率 召回： 精排： 冷启动： 基于行为的协同过滤： 基于内容的相似性推荐： 三个环节 在召回阶段，首先筛选出和用户直接相关或间接相关的物品，将原始数据从万、百万、亿级别缩小到万、千级别；在排序阶段，通常使用二分类算法来预测用户对物品的喜好程度（或者是点击率），然后将物品按照喜好程序从大到小依次排列，筛选出用户最有可能喜欢的物品，这里又将召回数据从万、千级别缩小到千、百级别；最后在调整阶段，需要过滤掉重复推荐的、已经购买或阅读的、已经下线的物品，当召回和排序结果不足时，需要使用热门物品进行补充，最后合并物品基础信息，将包含完整信息的物品推荐列表返回给客户端。 召回： 排序： 调整： 数据去重 https://www.narsi.me/2018/03/14/bloom-filter/ 去重一般涉及到两方面内容： 1、内容源去重，这个在新闻等信息流推荐中很常用，很多相近的文章讲的都是同一件事情； 2、给用户推荐的内容去重，即不给用户推荐重复的内容； 2就是题主关注的点。 内容去重常采用BloomFilter算法 我现在是这么做的，适用于用户量不大的情况，当前DAU为100W1、spark streaming接收前端pingback日志，提取数据；2、使用redis cluster的map结构，以用户ID为key，以item列表的json为value，更新；3、下次查询的时候，取出这个item列表，过滤掉；其中spark streaming的处理时间为10秒钟一个窗口；另外，每天会清空这个用户3天前的观看历史，也就是3天前的item会仍然再次推送；我们是类似电商，所以没买过的会过一段时间再推； 参考 文章推荐系统 | 一、推荐流程设计 CTR展现打散去重的设计 个性化推荐系统从0到1 "},"zother2-interview/architecture/design/seckill/":{"url":"zother2-interview/architecture/design/seckill/","title":"秒杀系统","keywords":"","body":"问题场景 在进行系统设计的过程中，首先问题场景的特点。秒杀系统是十分典型的高并发场景，其特点也十分显著：高并发、低库存、高瞬时流量。再者分析整个系统的输入输出，即大概的 API 网关拥有的功能：查（用户查询商品信息）、改（用户购买商品）。将系统的特点和功能分析完毕后，就可以根据这些信息进行系统设计。一个常规的秒杀系统从前到后，依次有： 前端页面 -> 代理服务 -> 后端服务 -> 数据库 根据这个流程，一般优化设计思路：将 请求拦截在系统上游，降低下游压力。在一个并发量大，实际需求小的系统中，应当尽量在前端拦截无效流量，降低下游服务器和数据库的压力，不然很可能造成数据库读写锁冲突，甚至导致死锁，最终请求超时。 整体优化手段包含：缓存、限流、削峰（MQ）、异步处理、降级、熔断、SET化、快速扩容 前端页面 资源静态化：将活动页面上的所有可以静态的元素全部静态化，尽量减少动态元素；通过CDN缓存静态资源，来抗峰值。 禁止重复提交：用户提交之后按钮置灰，禁止重复提交 URL动态化：防止恶意抓取 代理服务 利用负载均衡（例如 Nginx 等）使用多个服务器并发处理请求，减小服务器压力。 后端服务 用户限流：在某一时间段内只允许用户提交一次请求，比如可以采取 IP 限流 业务拆分 利用 MQ 削峰 利用缓存应对大量查询请求 利用缓存应对写请求（注意数据一致性、持久性问题）：缓存也是可以应对写请求的，可把数据库中的库存数据转移到 Redis 缓存中，所有减库存操作都在 Redis 中进行，然后再通过后台进程把Redis中的用户秒杀请求同步到数据库中。 数据库 多数据库：防止数据热点问题 优化 SQL 防止死锁 "},"zother2-interview/architecture/design/tinyURL/":{"url":"zother2-interview/architecture/design/tinyURL/","title":"短链接","keywords":"","body":"短链接 使用场景(Scenario) 微博和Twitter都有140字数的限制，如果分享一个长网址，很容易就超出限制，发布出去。短网址服务可以把一个长网址变成短网址，方便在社交网络上传播。 需求(Needs) 很显然，要尽可能的短。长度设计为多少才合适呢？ 短网址的长度 当前互联网上的网页总数大概是 45 亿，45亿 超过了 2^{32}，但远远小于64位整数的上限值，那么用一个64位整数足够了。微博的短网址服务用的是长度为 7 的字符串，这个字符串可以看做是 62 进制的数，那么最大能表示 62^7 个网址，远远大于 45 亿。所以长度为 7 就足够了。这个量级远远超过互联网上的 URL 总数了，绝对够用了。现代的 web 服务器（例如Apache, Nginx）大部分都区分 URL 里的大小写了，所以用大小写字母来区分不同的 URL 是没问题的。因此，正确答案：长度不超过7的字符串，由大小写字母加数字共 62 个字母组成。 一对一还是一对多映射？ 一个长网址，对应一个短网址，还是可以对应多个短网址？ 这也是个重大选择问题。一般而言，一个长网址，在不同的地点，不同的用户等情况下，生成的短网址应该不一样，这样，在后端数据库中，可以更好的进行数据分析。如果一个长网址与一个短网址一一对应，那么在数据库中，仅有一行数据，无法区分不同的来源，就无法做数据分析了。 以这个7位长度的短网址作为唯一ID，这个ID下可以挂各种信息，比如生成该网址的用户名，所在网站，HTTP头部的 User Agent等信息，收集了这些信息，才有可能在后面做大数据分析，挖掘数据的价值。短网址服务商的一大盈利来源就是这些数据。 正确答案：一对多 如何计算短网址 现在我们设定了短网址是一个长度为7的字符串，如何计算得到这个短网址呢？ 将 62^7 的 ID 进行分段，比如分为 N 段，前 K 位一致。那么剩下的位就可以通过 Redis 来进行生成，防止重复。 如何存储 如果存储短网址和长网址的对应关系？以短网址为 primary key, 长网址为value, 可以用传统的关系数据库存起来，例如MySQL,PostgreSQL，也可以用任意一个分布式 KV 数据库，例如Redis, LevelDB。 301还是302重定向 这也是一个有意思的问题。这个问题主要是考察你对301和302的理解，以及浏览器缓存机制的理解。 301是永久重定向，302是临时重定向。短地址一经生成就不会变化，所以用301是符合http语义的。但是如果用了301， Google，百度等搜索引擎，搜索的时候会直接展示真实地址，那我们就无法统计到短地址被点击的次数了，也无法收集用户的Cookie, User Agent 等信息，这些信息可以用来做很多有意思的大数据分析，也是短网址服务商的主要盈利来源。 所以，正确答案是302重定向。 可以抓包看看mrw.so的短网址是怎么做的，使用 Chrome 浏览器，访问这个URL http://mrw.so/4UD39p，是我事先发微博自动生成的短网址。来抓包看看返回的结果是啥，可见新浪微博用的就是302临时重定向。 "},"zother2-interview/architecture/distributed/cache/":{"url":"zother2-interview/architecture/distributed/cache/","title":"分布式缓存","keywords":"","body":"分布式缓存 高并发环境下，例如典型的淘宝双11秒杀，几分钟内上亿的用户涌入淘宝，这个时候如果访问不加拦截，让大量的读写请求涌向数据库，由于磁盘的处理速度与内存显然不在一个量级，服务器马上就要宕机。从减轻数据库的压力和提高系统响应速度两个角度来考虑，都会在数据库之前加一层缓存，访问压力越大的，在缓存之前就开始 CDN 拦截图片等访问请求。 并且由于最早的单台机器的内存资源以及承载能力有限，如果大量使用本地缓存，也会使相同的数据被不同的节点存储多份，对内存资源造成较大的浪费，因此，才催生出了分布式缓存。 应用场景 页面缓存：用来缓存Web 页面的内容片段,包括HTML、CSS 和图片等; 应用对象缓存：缓存系统作为ORM 框架的二级缓存对外提供服务,目的是减轻数据库的负载压力,加速应用访问;解决分布式Web部署的 session 同步问题，状态缓存.缓存包括Session 会话状态及应用横向扩展时的状态数据等,这类数据一般是难以恢复的,对可用性要求较高,多应用于高可用集群。 并行处理：通常涉及大量中间计算结果需要共享; 云计算领域提供分布式缓存服务 常见问题和挑战 缓存雪崩 缓存雪崩我们可以简单的理解为：由于原有缓存失效、新缓存未到之间(例如：我们设置缓存时采用了相同的过期时间，在同一时刻出现大面积的缓存过期)，所有原本应该访问缓存的请求都去查询数据库了，而对数据库CPU和内存造成巨大压力，严重的会造成数据库宕机。从而形成一系列连锁反应，造成整个系统崩溃。 缓存穿透 缓存穿透是指用户查询数据，在数据库没有，自然在缓存中也不会有。这样就导致用户查询的时候，在缓存中找不到，每次都要去数据库再查询一遍，然后返回空（相当于进行了两次无用的查询）。这样请求就绕过缓存直接查数据库，这也是经常提的缓存命中率问题。 缓存预热 缓存预热这个应该是一个比较常见的概念，相信很多小伙伴都应该可以很容易的理解，缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！ 缓存更新 除了缓存服务器自带的缓存失效策略之外，我们还可以根据具体的业务需求进行自定义的缓存淘汰，常见的策略有两种： 定时去清理过期的缓存； 当有用户请求过来时，再判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数据并更新缓存。 两者各有优劣，第一种的缺点是维护大量缓存的key是比较麻烦的，第二种的缺点就是每次用户请求过来都要判断缓存失效，逻辑相对比较复杂！具体用哪种方案，大家可以根据自己的应用场景来权衡。 缓存降级 当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务。系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级。 降级的最终目的是 保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）。 在进行降级之前要对系统进行梳理，看看系统是不是可以丢卒保帅；从而梳理出哪些必须誓死保护，哪些可降级；比如可以参考日志级别设置预案： 一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级； 警告：有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级，并发送告警； 错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级； 严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。 缓存与数据库不一致问题 首先，缓存由于其高并发和高性能的特性，已经在项目中被广泛使用。在读取缓存方面，大家没啥疑问，都是按照下图的流程来进行业务操作。 但是在更新缓存方面，对于更新完数据库，是更新缓存呢，还是删除缓存。又或者是先删除缓存，再更新数据库，其实大家存在很大的争议。 从理论上来说，给 缓存设置过期时间，是保证最终一致性的解决方案。这种方案下，我们可以对存入缓存的数据设置过期时间，所有的写操作以数据库为准，对缓存操作只是尽最大努力即可。也就是说如果数据库写成功，缓存更新失败，那么只要到达过期时间，则后面的读请求自然会从数据库中读取新值然后回填缓存。 先删除缓存，再更新数据库 该方案会导致不一致的原因是。同时有一个请求A进行更新操作，另一个请求B进行查询操作。那么会出现如下情形: 请求A进行写操作，删除缓存 请求B查询发现缓存不存在 请求B去数据库查询得到旧值 请求B将旧值写入缓存 请求A将新值写入数据库 上述情况就会导致不一致的情形出现。而且，如果不采用给缓存设置过期时间策略，该数据永远都是脏数据。 可以通过： 更新操作数据库后，再次更新缓存来实现 缓存设置过期时间，等待过期时间后，数据恢复 "},"zother2-interview/architecture/distributed/consensus/":{"url":"zother2-interview/architecture/distributed/consensus/","title":"分布式一致性和共识协议","keywords":"","body":"一致性 在分布式系统中，一致性(Consistency，早期也叫 Agreement)是指对于系统中的多个服务节点，给定一系列操作，在协议（往往通过某种共识算法）保障下，试图使得它们对处理结果达成某种程度的一致。 一致性并不代表结果正确与否，而是系统对外呈现的状态一致与否，例如，所有节点都达成失败状态也是一种一致。 分布式的挑战 在实际的计算机集群系统（看似强大的计算机系统，很多地方都比人类世界要脆弱的多）中，存在如下的问题： 节点之间的网络通讯是不可靠的，包括任意延迟和内容故障； 节点的处理可能是错误的，甚至节点自身随时可能宕机； 同步调用会让系统变得不具备可扩展性。 要解决这些挑战，愿意动脑筋的读者可能会很快想出一些不错的思路。为了简化理解，仍然以两个电影院一起卖票的例子。可能有如下的解决思路： 每次要卖一张票前打电话给另外一家电影院，确认下当前票数并没超售； 两家电影院提前约好，奇数小时内一家可以卖票，偶数小时内另外一家可以卖； 成立一个第三方的存票机构，票都放到他那里，每次卖票找他询问； 这些思路大致都是可行的。实际上，这些方法背后的思想，将可能引发不一致的并行操作进行串行化，就是现在计算机系统里处理分布式一致性问题的基础思路和唯一秘诀。只是因为计算机系统比较傻，需要考虑得更全面一些；而人们又希望计算机系统能工作的更快更稳定，所以算法需要设计得再精巧一些。 规范的说，理想的分布式系统一致性应该满足： 可终止性（Termination）：一致的结果在有限时间内能完成； 共识性（Consensus）：不同节点最终完成决策的结果应该相同； 合法性（Validity）：决策的结果必须是其它进程提出的提案。 第一点很容易理解，这是计算机系统可以被使用的前提。需要注意，在现实生活中这点并不是总能得到保障的，例如取款机有时候会是 服务中断 状态，电话有时候是 无法连通 的。 第二点看似容易，但是隐藏了一些潜在信息。算法考虑的是任意的情形，凡事一旦推广到任意情形，就往往有一些惊人的结果。例如现在就剩一张票了，中关村和西单的电影院也分别刚确认过这张票的存在，然后两个电影院同时来了一个顾客要买票，从各自观察看来，自己的顾客都是第一个到的……怎么能达成结果的共识呢？记住我们的唯一秘诀：核心在于需要把两件事情进行排序，而且这个顺序还得是大家都认可的。 第三点看似绕口，但是其实比较容易理解，即达成的结果必须是节点执行操作的结果。仍以卖票为例，如果两个影院各自卖出去一千张，那么达成的结果就是还剩八千张，决不能认为票售光了。 强一致性 线性一致性 线性一致性或称 原子一致性 或 严格一致性 指的是程序在执行的历史中在存在可线性化点P的执行模型，这意味着一个操作将在程序的调用和返回之间的某个点P起作用。这里“起作用”的意思是被系统中并发运行的所有其他线程所感知。要求如下： 写后读 这里写和读是两个操作，如果写操作在完成之后，读才开始，读要能读到最新的数据，而且保证以后也能读操作也都能读到这个最新的数据。 所有操作的时序与真实物理时间一致，要求即使不相关的两个操作，如果执行有先后顺序，线性一致性要求最终执行的结果也需要满足这个先后顺序。比如，操作序列(写A，读A，写B，读B)，那么不仅，读A，读B能读到最新A值和B值；而且要保证，如果读B读到最新值时，读A一定也能读到最新值，也就是需要保证执行时序与真实时序相同。 如果两个操作是并发的(比如读A没有结束时，写B开始了)，那么这个并发时序不确定，但从最终执行的结果来看，要确保所有线程(进程，节点)看到的执行序列是一致的。 顺序一致性 相比线性一致性，主要区别在于，对于物理上有先后顺序的操作，不保证这个时序。具体而言，对于单个线程，操作的顺序仍然要保留，对于多个线程(进程，节点)，执行的事件的先后顺序与物理时钟顺序不保证。但是要求，从执行结果来看，所有线程(进程，节点)看到的执行序列是一样的。 线性一致性和顺序一致性。 图1 是顺序一致性：从这两个进程的角度来看，顺序应该是这样的 write(y,2) -> read(x,0) -> write(x,4) -> read(y,2) ，每个进程内部的读写顺序都是合理的，但是这个顺序与全局时钟下看到的顺序并不一样，write(x,4) 先于 read(x,0) 执行，但是 read 却没有读到最新值。 图2 是线性一致性：每个读操作都读到了该变量的最新写的结果，同时 两个进程看到的操作顺序与全局时钟的顺序一样，都是 write(y,2) -> write(x,4) -> read(x,4) -> read(y,2)。 图3 不符合顺序一致性，更加不符合线性一致性，两个进程内部的顺序可能是：write(x,4) -> read(y,0) -> write(y,2) -> read(x,0)、或者：write(y,2) -> read(x,0) -> write(x,4) -> read(y,0) 显然两个顺序又不能同时被 P1、P2 满足，因此这个顺序是有冲突的，不满足顺序一致性。 因果一致性 因果一致性，被认为是比 顺序一致性 更弱的一致性，在因果一致性中，只对有因果关系的事件有顺序要求。 对于 P1 和 P2 的操作是没有先后关系的，因此谁先发生都是可以的。 从 P3 的视角来看，操作执行序列是 w(x,7) -> r(x,7) -> w(x,2) -> r(x,2) -> w(x,4) 从 P4 的视角来看，操作执行序列是 w(x,2) -> w(x,4) -> r(x,4) -> w(x,7) -> r(x,7) 但是不同进程看到的执行序列不一样，所以不符合顺序一致性。 带约束的一致性 绝对理想的 强一致性（Strong Consistency） 代价很大。除非不发生任何故障，所有节点之间的通信无需任何时间，这个时候其实就等价于一台机器了。实际上，越强的一致性要求往往意味着越弱的性能、越低的可用性。 强一致的系统往往比较难实现。很多时候，人们发现实际需求并没有那么强，可以适当放宽一致性要求，降低系统实现的难度。例如在一定约束下实现所谓 最终一致性（Eventual Consistency），即总会存在一个时刻（而不是立刻），系统达到一致的状态，这对于大部分的 Web 系统来说已经足够了。这一类弱化的一致性，被笼统称为 弱一致性（Weak Consistency）。 最终一致性 最终一致性也被称为 乐观复制(optimistic replication)，用户只能读到某次更新后的值，但系统保证数据将最终达到完全一致的状态，只是所需时间不能保障。这个达成一致所需要的时间，我们称为 窗口时间。 我们常见的 异步复制的主从架构实现的是最终一致性 。它的一个典型常见是用户读取异步从库时，可能读取到较旧的信息，因为该从库尚未完全与主库同步。注意，同步复制的主从架构会出现任一节点宕机导致的单点问题。 共识算法 共识算法解决的是对某个提案（Proposal），大家达成一致意见的过程。提案的含义在分布式系统中十分宽泛，如多个事件发生的顺序、某个键对应的值、谁是领导……等等，可以认为任何需要达成一致的信息都是一个提案。 实践中，一致性的结果往往还需要客户端的特殊支持，典型地通过访问足够多个服务节点来验证确保获取共识后结果。 拜占庭问题 拜占庭将军问题描述了一个如下的场景，有一组将军分别指挥一部分军队，每一个将军都不知道其它将军是否是可靠的，也不知道其他将军传递的信息是否可靠，但是它们需要通过投票选择是否要进攻或者撤退。 在这时，无论将军是否可靠，只要所有的将军达成了统一的方案，选择进攻或者撤退其实就是没有任何问题的。上述的情况不会对当前的战局有太多的影响，也不会造成损失，但是如果其中的一个将军告诉其中一部分将军选择进攻、另一部分选择撤退，就会出现非常严重的问题了。 由于将军的队伍中出了一个叛徒或者信息在传递的过程中被拦截，会导致一部分将军会选择进攻，剩下的一部分会选择撤退，它们都认为自己的选择是大多数人的选择，这时就出现了严重的不一致问题。 拜占庭将军问题是对分布式系统容错的最高要求，然而这不是日常工作中使用的大多数分布式系统中会面对的问题，我们遇到更多的还是节点故障宕机或者不响应等情况，这就大大简化了系统对容错的要求。 问题挑战 实际上，如果分布式系统中各个节点都能保证以十分强大的性能（瞬间响应、高吞吐）无故障的运行，则实现共识过程并不复杂，简单通过多播过程投票即可。 很可惜的是，现实中这样完美的系统并不存在，如响应请求往往存在时延、网络会发生中断、节点会发生故障、甚至存在恶意节点故意要破坏系统。 一般地，把故障（不响应）的情况称为 非拜占庭错误 ，恶意响应的情况称为 拜占庭错误（对应节点为拜占庭节点）。 常见算法 针对非拜占庭错误的情况，一般包括 Paxos、Raft 及其变种。 对于要能容忍拜占庭错误的情况，一般包括 PBFT 系列、 PoW 系列算法等。从概率角度，PBFT 系列算法是确定的，一旦达成共识就不可逆转；而 PoW 系列算法则是不确定的，随着时间推移，被推翻的概率越来越小。 理论界限 搞学术的人都喜欢对问题先确定一个界限，那么，这个问题的最坏界限在哪里呢？很不幸，一般情况下，分布式系统的共识问题无解。 当节点之间的通信网络自身不可靠情况下，很显然，无法确保实现共识。但好在，一个设计得当的网络可以在大概率上实现可靠的通信。然而，即便在网络通信可靠情况下，一个可扩展的分布式系统的共识问题的下限是无解。 这个结论，被称为 FLP 不可能性 原理，可以看做分布式领域的“测不准原理”。 FLP FLP 不可能定理是分布式系统领域最重要的定理之一，它给出了一个非常重要的结论：在网络可靠并且存在节点失效的异步模型系统中，不存在一个可以解决一致性问题的确定性算法。 这个定理其实也就是告诉我们不要浪费时间去为异步分布式系统设计在任意场景上都能够实现共识的算法，异步系统完全没有办法保证能在有限时间内达成一致。理解这一原理的一个不严谨的例子是： 三个人在不同房间，进行投票（投票结果是 0 或者 1）。三个人彼此可以通过电话进行沟通，但经常会有人时不时地睡着。比如某个时候，A 投票 0，B 投票 1，C 收到了两人的投票，然后 C 睡着了。A 和 B 则永远无法在有限时间内获知最终的结果。如果可以重新投票，则类似情形每次在取得结果前发生。 这岂不是意味着研究一致性问题压根没有意义吗？ 先别这么悲观，学术界做研究，考虑的是数学和物理意义上最极端的情形，很多时候现实生活要美好的多。Paxos算法的场景比FLP的系统模型还要松散，除了异步通信，Paxos允许消息丢失（通信不健壮），但Paxos却被认为是最牛的一致性算法，其作者 Lamport 也获得2014年的图灵奖，这又是为什么？ 其实仔细回忆Paxos论文会发现， Paxos 中存在活锁，理论上的活锁会导致 Paxos 算法无法满足 Termination 属性，也就不算一个正确的一致性算法。Lamport 在自己的论文中也提到 FLP结果表明，不存在完全满足一致性的异步算法... ，因此他建议通过 Leader 来代替 Paxos 中的 Proposer ，而 Leader 则通过随机或其他方式来选定（Paxos中假如随机过程会极大降低FLP发生的概率）。也就是说Paxos算法其实也不算理论上完全正确的，只是在工程实现中避免了一些理论上存在的问题。 科学告诉你什么是不可能的；工程则告诉你，付出一些代价，我可以把它变成可能。 一致性（Consistency）与共识（Consensus） 我们常说的 一致性（Consistency） 在分布式系统中指的是 副本（Replication） 问题中对于同一个数据的多个副本，其对外表现的数据一致性，如 线性一致性 、因果一致性、最终一致性等，都是用来描述副本问题中的一致性的。 而 共识（Consensus） 则不同，共识问题中所有的节点要最终达成共识，由于最终目标是所有节点都要达成一致，所以根本 不存在一致性强弱 之分。 只有当你使用像 Paxos 这样的共识算法作为解决副本问题的核心组件时，才会对外展现出不同的一致性级别。但是，即使是在这样的场景下，讨论一个共识算法的一致性也是不合适的，因为 整个副本系统最终的一致性并不单单取决于共识算法 ，Client 访问所遵循的规范也会有决定性的作用。比如说：即使副本系统使用 multi-paxos 在所有副本服务器上同步了日志序号，但如果 Client 被允许从非 Leader 节点获取数据，则整个副本系统仍然不是强一致的。 CAP 分布式计算系统不可能同时确保一致性（Consistency）、可用性（Availablity）和分区容忍性（Partition），设计中往往需要弱化对某个特性的保证。 一致性（Consistency - 线性一致性）：任何操作应该都是原子的，发生在后面的事件能看到前面事件发生导致的结果，注意这里指的是强一致性； 可用性（Availablity）：在有限时间内，任何非失败节点都能应答请求； 分区容忍性（Partition）：网络可能发生分区，即节点之间的通信不可保障。 比较直观地理解，当网络可能出现分区时候，系统是无法同时保证一致性和可用性的。要么，节点收到请求后因为没有得到其他人的确认就不应答，要么节点只能应答非一致的结果。 好在大部分时候网络被认为是可靠的，因此系统可以提供一致可靠的服务；当网络不可靠时，系统要么牺牲掉一致性（大部分时候都是如此），要么牺牲掉可用性。 既然 CAP 不可同时满足，则设计系统时候必然要弱化对某个特性的支持。 弱化一致性 对结果一致性不敏感的应用，可以允许在新版本上线后过一段时间才更新成功，期间不保证一致性。例如网站静态页面内容、实时性较弱的查询类数据库等，CouchDB、Cassandra 等为此设计。 弱化可用性 对结果一致性很敏感的应用，例如银行取款机，当系统故障时候会拒绝服务。MongoDB、Redis 等为此设计。Paxos、Raft 等算法，主要处理这种情况。 弱化分区容忍性 现实中，网络分区出现概率减小，但较难避免。某些关系型数据库、ZooKeeper 即为此设计。实践中，网络通过双通道等机制增强可靠性，达到高稳定的网络通信。 Paxos Paxos 其实是一类能够解决分布式一致性问题的协议，它能够让分布式网络中的节点在出现错误时仍然保持一致；Leslie Lamport 提出的 Paxos 可以在没有恶意节点的前提下保证系统中节点的一致性，也是第一个被证明完备的共识算法，目前的完备的共识算法包括 Raft 本质上都是 Paxos 的变种。 Basic Paxos Basic Paxos 是 Paxos 中最为基础的协议，每一个 Basic Paxos 的协议实例最终都会选择唯一一个结果；使用 Paxos 作为共识算法的分布式系统中，节点都会有三种身份，分别是 Proposer、Acceptor 和 Learner。 Paxos 的运行过程分为两个阶段，分别是准备阶段（Prepare）和接受阶段（Accept），当 Proposer 接收到来自客户端的请求时，就会进入如下流程： 在整个共识算法运行的过程中，Proposer 负责提出提案并向 Acceptor 分别发出两次 RPC 请求，Prepare 和 Accept；Acceptor 会根据其持有的信息 minProposal、acceptedProposal 和 acceptedValue 选择接受或者拒绝当前的提案，当某一个提案被过半数的 Acceptor 接受之后，我们就认为当前提案被整个集群接受了。 Multi-Paxos 由于大多数的分布式集群都需要接受一系列的值，如果使用 Basic Paxos 来处理数据流，那么就会导致非常明显的性能损失，而 Multi-Paxos 是前者的加强版，如果集群中的 Leader 是非常稳定的，那么我们往往不需要准备阶段的工作，这样就能够将 RPC 的数量减少一半。 上述图片中描述的就是稳定阶段 Multi-Paxos 的处理过程，S1 是整个集群的 Leader，当其他的服务器接收到来自客户端的请求时，都会将请求转发给 Leader 进行处理。 当然，Leader 角色的出现自然会带来另一个问题，也就是 Leader 究竟应该如何选举，在 Paxos Made Simple 一文中并没有给出 Multi-Paxos 的具体实现方法和细节，所以不同 Multi-Paxos 的实现上总有各种各样细微的差别。 Raft Raft 其实就是 Multi-Paxos 的一个变种，Raft 通过简化 Multi-Paxos 的模型，实现了一种更容易让人理解的共识算法，它们两者都能够对一系列连续的问题达成一致。 Raft 在 Multi-Paxos 的基础之上做了两个限制，首先是 Raft 中追加日志的操作必须是连续的，而 Multi-Paxos 中追加日志的操作是并发的，但是对于节点内部的状态机来说两者都是有序的，第二就是 Raft 对 Leader 选举的条件做了限制，只有拥有最新、最全日志的节点才能够当选 Leader，但是 Multi-Paxos 由于任意节点都可以写日志，所以在选择 Leader 上也没有什么限制，只是在选择 Leader 之后需要将 Leader 中的日志补全。 在 Raft 中，所有 Follower 的日志都是 Leader 的子集，而 Multi-Paxos 中的日志并不会做这个保证，由于 Raft 对日志追加的方式和选举过程进行了限制，所以在实现上会更加容易和简单。 从理论上来讲，支持并发日志追加的 Paxos 会比 Raft 有更优秀的性能，不过其理解和实现上还是比较复杂的，很多人都会说 Paxos 是科学，而 Raft 是工程，当作者需要去实现一个共识算法，会选择使用 Raft 和更简洁的实现，避免因为一些边界条件而带来的复杂问题。 Raft协议将一致性协议的核心内容分拆成为几个关键阶段，以简化流程，提高协议的可理解性。 Leader election Raft协议的每个副本都会处于三种状态之一： Leader：所有请求的处理者，Leader副本接受client的更新请求，本地处理后再同步至多个其他副本 Follower：请求的被动更新者，从Leader接受更新请求，然后写入本地日志文件 Candidate：如果 Follower 副本在一段时间内没有收到 Leader 副本的心跳，则判断 Leader 可能已经故障，此时启动选主过程，此时副本会变成 Candidate 状态，直到选主结束。 时间被分为很多连续的随机长度的 term ， term 有唯一的 id，每个 term 最多只有一个 Leader 。每个 term 一开始就进行选主： Follower 将自己维护的 current_term_id 加 1。 然后将自己的状态转成 Candidate 发送 RequestVoteRPC 消息(带上 current_term_id ) 给 其它所有 Server 本轮选举成功，当收到了 majority 的投票后，状态切成 Leader ，并且定期给其它的所有 Server 发心跳消息（不带 Log 的 AppendEntriesRPC ）以告诉对方自己是 current_term_id 所标识的 term 的 Leader 。term id 作为Logical clock，在每个 RPC 消息中都会带上，用于检测过期的消息。 当一个 Server 收到的 RPC 消息中的 rpc_term_id 比本地的 current_term_id 更大时，就更新 current_term_id 为 rpc_term_id ，并且如果当前 state 为 Leader 或者 candidate 时，将自己的状态切成 follower。 当 rpc_term_id 比本地的 current_term_id 更小，则拒绝这个RPC消息。 本轮选举失败，则没有任何一个 candidate 收到了 majority 的 vote 时，没有 Leader 被选出。这种情况下，每个 candidate 等待的投票的过程就超时了，接着 candidates 都会将本地的 current_term_id 再加1，再等待 150ms ~ 300ms 之后随机发起 RequestVoteRPC 进行新一轮的 Leader election，以避免再次选主失败。 Log Replication 当 Leader 被选出来后，就可以接受客户端发来的请求了，每个请求包含一条需要被 replicated state machines 执行的命令。 Leader 会把它作为一个 Log Entry append 到日志中，然后给其它的 Server 发 AppendEntriesRPC 请求。当 Leader 确定一个 Log Entry 被 safely replicated 了（大多数副本已经将该命令写入日志当中），就 apply 这条 Log Entry 到状态机中然后返回结果给客户端。如果某个 Follower 宕机了或者运行的很慢，或者网络丢包了，则会一直给这个 Follower 发 AppendEntriesRPC 直到日志一致。 当一条日志是 commited 时，Leader 才可以将它应用到状态机中。Raft 保证一条 commited 的 Log Entry 已经持久化了并且会被所有的节点执行。当一个新的 Leader 被选出来时，它的日志和其它的 Follower 的日志可能不一样，这个时候，就需要一个机制来保证日志的一致性。 因此，需要有一种机制来让 Leader 和 Follower 对 Log 达成一致， Leader 会为每个 Follower 维护一个 nextIndex ，表示 Leader 给各个 Follower 发送的下一条 Log Entry 在 Log 中的 index ，初始化为 Leader 的最后一条 Log Entry 的下一个位置。leader 给 Follower 发送 AppendEntriesRPC 消息，带着 (term_id, nextIndex-1)， term_id 即 nextIndex-1 这个槽位的 Log Entry 的term_id ，Follower 接收到 AppendEntriesRPC 后，会从自己的 Log 中找是不是存在这样的 Log Entry，如果不存在，就给 Leader 回复拒绝消息，然后 Leader 则将 nextIndex 减1，再重复，直到 AppendEntriesRPC 消息被接收。 Safety Raft 保证被选为新 Leader 的节点拥有所有已提交的 Log Entry。这个保证是在 RequestVoteRPC 阶段做的，candidate 在发送 RequestVoteRPC 时，会带上自己的最后一条日志记录的 term_id,index ，其他节点收到消息时，如果发现自己的日志比 RPC 请求中携带的更新，拒绝投票。日志比较的原则是，如果本地的最后一条 Log Entry 的 term id 更大，则更新，如果 term id 一样大，则 index 更大的更大。 Log Compaction 在实际的系统中，不能让日志无限增长，否则系统重启时需要花很长的时间进行回放，从而影响 availability 。Raft 采用对整个系统进行 snapshot 来处理， snapshot 之前的日志都可以丢弃。Snapshot 技术在 Chubby 和 ZooKeeper 系统中都有采用。 Raft使用的方案是：每个副本独立的对自己的系统状态进行 Snapshot ，并且只能对已经提交的日志记录（已经应用到状态机）进行snapshot。 POW 无论是 Paxos 还是 Raft 其实都只能解决非拜占庭将军容错的一致性问题，不能够应对分布式网络中出现的极端情况，但是这在传统的分布式系统都不是什么问题，无论是分布式数据库还是消息队列集群，它们内部的节点并不会故意的发送错误信息，在类似系统中，最常见的问题就是节点失去响应或者失效，所以它们在这种前提下是有效可行的，也是充分的。 工作量证明（POW，Proof-of-Work） 是一个用于阻止拒绝服务攻击和类似垃圾邮件等服务错误问题的协议，它在 1993 年被 Cynthia Dwork 和 Moni Naor 提出，它能够帮助分布式系统达到拜占庭容错。 工作量证明的关键特点就是，分布式系统中的请求服务的节点必须解决一个一般难度但是可行（feasible）的问题，但是验证问题答案的过程对于服务提供者来说却非常容易，也就是一个不容易解答但是容易验证的问题。 工作量证明的原理其实非常简单，比特币网络选择的谜题非常好的适应了工作量证明定义中的问题，比较难以寻找同时又易于证明，我们可以简单理解为工作量证明防止错误或者无效请求的原理就是增加客户端请求服务的工作量，而适合难度的谜题又能够保证合法的请求不会受到影响。 由于工作量证明需要消耗大量的算力，同时比特币大约 10min 才会产生一个区块，区块的大小也只有 1MB，仅仅能够包含 3、4000 笔交易，平均下来每秒只能够处理 5~7（个位数）笔交易，所以比特币网络的拥堵状况非常严重。 可靠性指标 很多领域一般都喜欢谈服务可靠性，用几个 9 来说事。这几个 9 其实是粗略代表了概率意义上系统能提供服务的可靠性指标，最初是电信领域提出的概念。 下表给出不同指标下，每年允许服务出现不可用时间的参考值。 指标 概率可靠性 每年允许不可用时间 典型场景 一个九 90% 1.2 个月 不可用 二个九 99% 3.6 天 普通单点 三个九 99.9% 8.6 小时 普通企业 四个九 99.99% 51.6 分钟 高可用 五个九 99.999% 5 分钟 电信级 六个九 99.9999% 31 秒 极高要求 七个九 99.99999% 3 秒 N/A 八个九 99.999999% 0.3 秒 N/A 九个九 99.9999999% 30 毫秒 N/A 一般来说，单点的服务器系统至少应能满足两个九；普通企业信息系统三个九就肯定足够了（大家可以统计下自己企业内因系统维护每年要停多少时间），系统能达到四个九已经是业界领先水平了（参考 AWS）。电信级的应用一般号称能达到五个九，这已经很厉害了，一年里面最多允许五分钟的服务停用。 那么，该如何提升可靠性呢？有两个思路：一是让系统中的单点变得更可靠；二是消灭单点。然而，依靠单点实现的可靠性毕竟是有限的，要想进一步的提升，那就只好消灭单点，通过主从、多活等模式让多个节点集体完成原先单点的工作。这可以从概率意义上改善服务的可靠性，也是分布式系统的一个重要用途。 参考链接 分布式一致性与共识算法 分布式一致性和分布式共识协议 被误用的一致性 FLP Impossibility 一致性问题 Raft协议详解 当数据库遇到分布式 "},"zother2-interview/architecture/distributed/dubbo/":{"url":"zother2-interview/architecture/distributed/dubbo/","title":"Dubbo","keywords":"","body":"Dubbo 领域模型 在 Dubbo 的核心领域模型中： Protocol 是服务域，它是 Invoker 暴露和引用的主功能入口，它负责 Invoker 的生命周期管理。 Invoker 是实体域，它是 Dubbo 的核心模型，其它模型都向它靠扰，或转换成它，它代表一个可执行体，可向它发起 invoke 调用，它有可能是一个本地的实现，也可能是一个远程的实现，也可能一个集群实现。 Invocation 是会话域，它持有调用过程中的变量，比如方法名，参数等。 基本设计原则 采用 Microkernel + Plugin 模式，Microkernel 只负责组装 Plugin，Dubbo 自身的功能也是通过扩展点实现的，也就是 Dubbo 的所有功能点都可被用户自定义扩展所替换。 采用 URL 作为配置信息的统一格式，所有扩展点都通过传递 URL 携带配置信息。 Dubbo 服务暴露过程 官方文档--服务导出 Dubbo 结构 第一层：service 层，接口层，给服务提供者和消费者来实现的 第二层：config 层，配置层，主要是对 dubbo 进行各种配置的 第三层：proxy 层，服务代理层，无论是 consumer 还是 provider，dubbo 都会给你生成代理，代理之间进行网络通信 第四层：registry 层，服务注册层，负责服务的注册与发现 第五层：cluster 层，集群层，封装多个服务提供者的路由以及负载均衡，将多个实例组合成一个服务 第六层：monitor 层，监控层，对 rpc 接口的调用次数和调用时间进行监控 第七层：protocal 层，远程调用层，封装 rpc 调用 第八层：exchange 层，信息交换层，封装请求响应模式，同步转异步 第九层：transport 层，网络传输层，抽象 mina 和 netty 为统一接口 第十层：serialize 层，数据序列化层 工作流程 第一步：provider 向注册中心去注册 第二步：consumer 从注册中心订阅服务，注册中心会通知 consumer 注册好的服务 第三步：consumer 调用 provider 第四步：consumer 和 provider 都异步通知监控中心 注册中心挂了可以继续通信吗？ 可以，因为刚开始初始化的时候，消费者会将提供者的地址等信息拉取到 本地缓存，所以注册中心挂了可以继续通信。 Dubbo 支持哪些序列化协议？说一下 Hessian 的数据结构？PB 知道吗？为什么 PB 的效率是最高的？ Dubbo 支持不同的通信协议 dubbo 协议：默认就是走 dubbo 协议，单一长连接，进行的是 NIO 异步通信，基于 hessian 作为序列化协议。使用的场景是：传输数据量小（每次请求在 100kb 以内），但是并发量很高。 rmi 协议：走 Java 二进制序列化，多个短连接，适合消费者和提供者数量差不多的情况，适用于文件的传输，一般较少用。 hessian 协议：走 hessian 序列化协议，多个短连接，适用于提供者数量比消费者数量还多的情况，适用于文件的传输，一般较少用。 http 协议：走 json 序列化 webservice：走 SOAP 文本序列化 Dubbo 支持的序列化协议 dubbo 支持 hession 、 Java 二进制序列化、json、SOAP 文本序列化多种序列化协议。但是 hessian 是其默认的序列化协议。 为什么 PB 的效率是最高的？ 其实 PB 之所以性能如此好，主要得益于两个： 它使用 proto 编译器，自动进行序列化和反序列化，速度非常快，应该比 XML 和 JSON 快上了 20~100 倍； 它的数据压缩效果好，就是说它序列化后的数据量体积小。因为体积小，传输起来带宽和速度上会有优化。 dubbo 负载均衡策略和集群容错策略都有哪些？动态代理策略呢？ dubbo 负载均衡策略 random loadbalance 默认情况下，dubbo 是 random load balance ，即 随机 调用实现负载均衡，可以对 provider 不同实例 设置不同的权重，会按照权重来负载均衡，权重越大分配流量越高，一般就用这个默认的就可以了。 roundrobin loadbalance 这个的话默认就是均匀地将流量打到各个机器上去，但是如果各个机器的性能不一样，容易导致性能差的机器负载过高。所以此时需要调整权重，让性能差的机器承载权重小一些，流量少一些。 leastactive loadbalance 这个就是自动感知一下，如果某个机器性能越差，那么接收的请求越少，越不活跃，此时就会给 不活跃的性能差的机器更少的请求。 consistanthash loadbalance 一致性 Hash 算法，相同参数的请求一定分发到一个 provider 上去， provider 挂掉的时候，会基于虚拟节点均匀分配剩余的流量，抖动不会太大。如果你需要的不是随机负载均衡，是要一类请求都到一个节点，那就走这个一致性 Hash 策略。 dubbo 集群容错策略 failover cluster 模式 失败自动切换，自动重试其他机器，默认就是这个，常见于读操作。（失败重试其它机器） failfast cluster模式 一次调用失败就立即失败，常见于写操作。（调用失败就立即失败） failsafe cluster 模式 出现异常时忽略掉，常用于不重要的接口调用，比如记录日志。 failback cluster 模式 失败了后台自动记录请求，然后定时重发，比较适合于写消息队列这种。 forking cluster 模式 并行调用 多个 provider ，只要一个成功就立即返回。 broadcacst cluster 逐个调用所有的 provider。 dubbo动态代理策略 默认使用 javassist 动态字节码生成，创建代理类。但是可以通过 spi 扩展机制配置自己的动态代理策略。 dubbo 的 spi 思想是什么？ spi ，简单来说，就是 service provider interface，说白了是什么意思呢，比如你有个接口，现在这个接口有 3 个实现类，那么在系统运行的时候对这个接口到底选择哪个实现类呢？这就需要 spi 了，需要根据指定的配置或者是默认的配置，去找到对应的实现类加载进来，然后用这个实现类的实例对象。 dubbo 也用了 spi 思想，不过没有用 jdk 的 spi 机制，是自己实现的一套 spi 机制。 Protocol protocol = ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension(); Protocol 接口，在系统运行的时候， dubbo 会判断一下应该选用这个 Protocol 接口的哪个实现类来实例化对象来使用。 它会去找一个你配置的 Protocol ，将你配置的 Protocol 实现类，加载到 jvm 中来，然后实例化对象，就用你的那个 Protocol 实现类就可以了。 与 Java SPI 对比 JDK 标准的 SPI 会一次性实例化扩展点所有实现，如果有扩展实现初始化很耗时，但如果没用上也加载，会很浪费资源。 增加了对扩展点 IoC 和 AOP 的支持，一个扩展点可以直接 setter 注入其它扩展点。 如果扩展点加载失败，连扩展点的名称都拿不到了。比如：JDK 标准的 ScriptEngine，通过 getName() 获取脚本类型的名称，但如果 RubyScriptEngine 因为所依赖的 jruby.jar 不存在，导致 RubyScriptEngine 类加载失败，这个失败原因被吃掉了，和 ruby 对应不起来，当用户执行 ruby 脚本时，会报不支持 ruby，而不是真正失败的原因。 如何基于 Dubbo 进行服务治理、服务降级、失败重试以及超时重试？ 服务治理 1. 调用链路自动生成 一个大型的分布式系统，或者说是用现在流行的微服务架构来说吧，分布式系统由大量的服务组成。那么这些服务之间互相是如何调用的？调用链路是啥？说实话，几乎到后面没人搞的清楚了，因为服务实在太多了，可能几百个甚至几千个服务。 那就需要基于 dubbo 做的分布式系统中，对各个服务之间的调用自动记录下来，然后自动将 各个服务之间的依赖关系和调用链路生成出来，做成一张图，显示出来，大家才可以看到对吧。 2. 服务访问压力以及时长统计 需要自动统计 各个接口和服务之间的调用次数以及访问延时，而且要分成两个级别。 一个级别是接口粒度，就是每个服务的每个接口每天被调用多少次，TP50/TP90/TP99，三个档次的请求延时分别是多少； 第二个级别是从源头入口开始，一个完整的请求链路经过几十个服务之后，完成一次请求，每天全链路走多少次，全链路请求延时的 TP50/TP90/TP99，分别是多少。 这些东西都搞定了之后，后面才可以来看当前系统的压力主要在哪里，如何来扩容和优化啊。 3. 其它 服务分层（避免循环依赖） 调用链路失败监控和报警 服务鉴权 每个服务的可用性的监控（接口调用成功率？几个 9？99.99%，99.9%，99%） 服务降级 比如说服务 A 调用服务 B，结果服务 B 挂掉了，服务 A 重试几次调用服务 B，还是不行，那么直接降级，走一个备用的逻辑，给用户返回响应。 举个栗子，我们有接口 HelloService。HelloServiceImpl 有该接口的具体实现。 public interface HelloService { void sayHello(); } public class HelloServiceImpl implements HelloService { public void sayHello() { System.out.println(\"hello world......\"); } } 我们调用接口失败的时候，可以通过 mock 统一返回 null 。 mock 的值也可以修改为 true，然后再跟接口同一个路径下实现一个 Mock 类，命名规则是 “接口名称+Mock” 后缀。然后在 Mock 类里实现自己的降级逻辑。 public class HelloServiceMock implements HelloService { public void sayHello() { // 降级逻辑 } } 失败重试和超时重试 所谓失败重试，就是 consumer 调用 provider 要是失败了，比如抛异常了，此时应该是可以重试的，或者调用超时了也可以重试。配置如下： 参考链接 advanced-java "},"zother2-interview/architecture/distributed/kafka/":{"url":"zother2-interview/architecture/distributed/kafka/","title":"Kafka","keywords":"","body":"Kafka 术语 Broker：Kafka 集群包含一个或多个服务器，这种服务器被称为 broker 。 Topic：每条发布到 Kafka 集群的消息都有一个类别，这个类别被称为 Topic。（物理上不同 Topic 的消息分开存储，逻辑上一个 Topic 的消息虽然保存于一个或多个 broker 上，但用户只需指定消息的 Topic 即可生产或消费数据而不必关心数据存于何处）。 Partition： Partition 是物理上的概念，每个 Topic 包含一个或多个 Partition 。 Producer：负责发布消息到 Kafka broker。 Consumer：消息消费者，向 Kafka broker 读取消息的客户端。 Consumer Group：每个 Consumer 属于一个特定的 Consumer Group（可为每个 Consumer 指定 group name，若不指定 group name 则属于默认的 group）。 拓扑结构 如上图所示，一个典型的 Kafka 集群中包含若干 Producer （可以是web前端产生的Page View，或者是服务器日志，系统CPU、Memory等），若干 broker （Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高），若干 Consumer Group ，以及一个 Zookeeper 集群。 Kafka 通过 Zookeeper 管理集群配置，选举 leader ，以及在 Consumer Group 发生变化时进行 rebalance。 Producer 使用 push 模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息。 Topic & Partition Topic 在逻辑上可以被认为是一个 queue ，每条消费都必须指定它的 Topic ，可以简单理解为必须指明把这条消息放进哪个 queue 里。为了使得 Kafka 的吞吐率可以线性提高，物理上把 Topic 分成一个或多个 Partition ，每个 Partition 在物理上对应一个文件夹，该文件夹下存储这个 Partition 的所有消息和索引文件。若创建 topic1 和 topic2 两个 topic ，且分别有 13 个和 19 个分区，则整个集群上会相应会生成共 32 个文件夹（本文所用集群共8个节点，此处 topic1 和 topic2 replication-factor 均为1）。 Partition 都是通过 顺序读写，所以效率很高 replication-factor 配置 partition 副本数。配置副本之后,每个 partition 都有一个唯一的 leader ，有 0 个或多个 follower 。所有的读写操作都在 leader 上完成，followers 从 leader 消费消息来复制 message，就跟普通的 consumer 消费消息一样。一般情况下 partition 的数量大于等于 broker 的数量，并且所有 partition 的 leader 均匀分布在 broker 上。 对于传统的 MQ 而言，一般会删除已经被消费的消息，而 Kafka 集群会保留所有的消息，无论其被消费与否。当然，因为磁盘限制，不可能永久保留所有数据（实际上也没必要），因此 Kafka 提供两种策略删除旧数据。一是基于时间，二是基于 Partition 文件大小。 Producer 消息路由 Producer 发送消息到 broker 时，会根据 Paritition 机制选择将其存储到哪一个 Partition 。如果 Partition 机制设置合理，所有消息可以均匀分布到不同的 Partition 里，这样就实现了负载均衡。如果一个 Topic 对应一个文件，那这个文件所在的机器I/O将会成为这个 Topic 的性能瓶颈，而有了 Partition 后，不同的消息可以并行写入不同 broker 的不同 Partition 里，极大的提高了吞吐率。 可以在 $KAFKA_HOME/config/server.properties 中通过配置项 num.partitions 来指定新建 Topic 的默认 Partition 数量，也可在创建 Topic 时通过参数指定，同时也可以在 Topic 创建之后通过 Kafka 提供的工具修改。 指定了 patition，则直接使用 未指定 patition 但指定 key，通过对 key 进行 hash 选出一个 patition patition 和 key 都未指定，使用轮询选出一个 patition Consumer Group 这是 Kafka 用来实现一个 Topic 消息的广播（发给所有的 Consumer ）和单播（发给某一个 Consumer ）的手段。一个 Topic 可以对应多个 Consumer Group 。如果需要实现广播，只要每个 Consumer 有一个独立的 Group 就可以了。要实现单播只要所有的 Consumer 在同一个 Group 里。用 Consumer Group 还可以将 Consumer 进行自由的分组而不需要多次发送消息到不同的 Topic 。 Consumer 个数与 Parition 数有什么关系？ topic 下的一个分区只能被同一个 consumer group 下的一个 consumer 线程来消费，但反之并不成立，即一个 consumer 线程可以消费多个分区的数据。比如 Kafka 提供的 ConsoleConsumer ，默认就只是一个线程来消费所有分区的数据。 即分区数决定了同组消费者个数的上限 所以，如果你的分区数是 N ，那么最好线程数也保持为 N ，这样通常能够达到最大的吞吐量。超过 N 的配置只是浪费系统资源，因为多出的线程不会被分配到任何分区。 如果消费线程大于 patition 数量，则有些线程将收不到消息 如果 patition 数量大于消费线程数，则有些线程多收到多个 patition 的消息 如果一个线程消费多个 patition，则无法保证你收到的消息的顺序，而一个 patition 内的消息是有序的 Push vs. Pull　　 作为一个消息系统，Kafka 遵循了传统的方式，选择由 Producer 向 broker push 消息并由 Consumer 从 broker pull 消息。事实上，push 模式和 pull 模式各有优劣。 Push模式 很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。push模式的目标是尽可能以最快速度传递消息，但是这样很容易造成 Consumer 来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。 Pull模式 可以根据Consumer的消费能力以适当的速率消费消息。 对于 Kafka 而言，Pull模式 更合适。Pull模式 可简化 broker 的设计，Consumer 可自主控制消费消息的速率，同时 Consumer 可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。 高可用性 Kafka 0.8 以前，是没有 HA 机制的，就是任何一个 broker 宕机了，那个 broker 上的 partition 就废了，没法写也没法读，没有什么高可用性可言。 比如说，我们假设创建了一个 topic ，指定其 partition 数量是 3 个，分别在三台机器上。但是，如果第二台机器宕机了，会导致这个 topic 的 1/3 的数据就丢了，因此这个是做不到高可用的。 Kafka 0.8 以后，提供了 HA 机制，就是 replica 副本机制。每个 partition 的数据都会同步到其它机器上，形成自己的多个 replica 副本。所有 replica 会选举一个 leader 出来，那么生产和消费都跟这个 leader 打交道，然后其他 replica 就是 follower 。写的时候， leader 会负责把数据同步到所有 follower 上去，读的时候就直接读 leader 上的数据即可。 Kafka 会均匀地将一个 partition 的所有 replica 分布在不同的机器上，这样才可以提高容错性。 这么搞，就有所谓的高可用性了，因为如果某个 broker 宕机了，没事儿，那个 broker 上面的 partition 在其他机器上都有副本的，如果这上面有某个 partition 的 leader ，那么此时会从 follower 中 重新选举 一个新的 leader 出来，大家继续读写那个新的 leader 即可。这就有所谓的高可用性了。 写数据 的时候，生产者就写 leader ，然后 leader 将数据落地写本地磁盘，接着其他 follower 自己主动从 leader 来 pull 数据。一旦所有 follower 同步好数据了，就会发送 ack 给 leader ， leader 收到所有 follower 的 ack 之后，就会返回写成功的消息给生产者。（当然，这只是其中一种模式，还可以适当调整这个行为） 消费 的时候，只会从 leader 去读，但是 只有当一个消息已经被所有 follower 都同步成功返回 ack 的时候，这个消息才会被消费者读到。 消息幂等性 Kafka 实际上有个 offset 的概念，就是每个消息写进去，都有一个 offset ，代表消息的序号，然后 consumer 消费了数据之后，每隔一段时间（定时定期），会把自己消费过的消息的 offset 提交一下，表示“我已经消费过了，下次我要是重启啥的，你就让我继续从上次消费到的 offset 来继续消费吧”。 但是凡事总有意外，比如我们之前生产经常遇到的，就是你有时候重启系统，看你怎么重启了，如果碰到点着急的，直接 kill 进程了，再重启。这会导致 consumer 有些消息处理了，但是没来得及提交 offset ，尴尬了。重启之后，少数消息会再次消费一次。 幂等性，通俗点说，一个数据，或者一个请求，给你重复来多次，你得确保对应的数据是不会改变的，不能出错。其实重复消费不可怕，可怕的是你没考虑到重复消费之后，怎么保证幂等性？其实还是得 结合业务来思考，这里给几个思路： 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。 比如你是写 Redis ，那没问题了，反正每次都是 set，天然幂等性。 比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。 比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。 消息丢失 消费端弄丢了数据 唯一可能导致消费者弄丢数据的情况，就是说，你消费到了这个消息，然后消费者那边 自动提交了 offset ，让 Kafka 以为你已经消费好了这个消息，但其实你才刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。 Kafka 会自动提交 offset ，那么只要 关闭自动提交 offset，在处理完之后自己手动提交 offset ，就可以保证数据不会丢。但是此时确实还是可能会有重复消费，比如你刚处理完，还没提交 offset ，结果自己挂了，此时肯定会重复消费一次，自己 保证幂等性 就好了。 Kafka 弄丢了数据 这块比较常见的一个场景，就是 Kafka 某个 broker 宕机，然后重新选举 partition 的 leader 。大家想想，要是此时其他的 follower 刚好还有些数据没有同步，结果此时 leader 挂了，然后选举某个 follower 成 leader 之后，不就少了一些数据？这就丢了一些数据啊。 此时一般是要求起码设置如下 4 个参数： 给 topic 设置 replication.factor 参数：这个值必须大于 1，要求每个 partition 必须有 至少 2 个副本。 在 Kafka 服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是 要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。 在 producer 端设置 acks=all：这个是要求每条数据，必须是写入所有 replica 之后，才能认为是写成功了。 在 producer 端设置 retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求 一旦写入失败，就无限重试，卡在这里了。 这样配置之后，至少在 Kafka broker 端就可以保证在 leader 所在 broker 发生故障，进行 leader 切换时，数据不会丢失。 生产者会不会弄丢数据？ 如果按照上述的思路设置了 acks=all，一定不会丢，要求是，你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者可以自动不断的重试，重试无限次。 消息的顺序性 比如说我们建了一个 topic ，有三个 partition 。生产者在写的时候，其实可以指定一个 key ，比如说我们指定了某个订单 id 作为 key ，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个 partition 中的数据一定是有顺序的。 消费者从 partition 中取出来数据的时候，也一定是有顺序的。到这里，顺序还是 ok 的，没有错乱。接着，我们在消费者里可能会搞 多个线程来并发处理消息。而多个线程并发跑的话，顺序可能就乱掉了。 解决方案： 一个 topic ，一个 partition ，一个 consumer ，内部单线程消费，单线程吞吐量太低，一般不会用这个。 写 N 个内存 queue ，具有相同 key 的数据都到同一个内存 queue ；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性。 Kafka 如何进行扩容的？ 假如集群有 3 个 broker，一共有 4 个 TP，每个 3 副本，均匀分布。现在要扩容一台机器，新 broker 加入集群后需要通过工具进行 TP 的迁移。一共迁移 3 个 TP 的副本到新 broker 上。等迁移结束之后，会重新进行 Leader balance。 从微观的角度看，TP 从一台 broker 迁移到另一个 broker 的流程是怎么样的呢？咱们来看下 TP3 第三个副本，从 broker1 迁移到 broker4 的过程，如下图所示，broker4 作为 TP3 的 follower，从 broker1 上最早的 offset 进行获取数据，直到赶上最新的 offset 为止，新副本被放入 ISR 中，并移除 broker1 上的副本，迁移过程完毕。 但在现有的扩容流程中存有如下问题：数据迁移从 TP3 的最初的 offset 开始拷贝数据，这会导致大量读磁盘，消耗大量的 I/O 资源，导致磁盘繁忙，从而造成 produce 操作延迟增长，产生抖动。所以整体迁移流程不够平滑。我们看下实际的监控到的数据。从中可以看到数据迁移中， broker1 上磁盘读量增大，磁盘 util 持续打满，produce 极其不稳定。 针对这个问题，我们回到 Kafka 迁移的流程上看，理论上 Kafka 是一个缓存系统，不需要永久存储数据，很有可能费了很多工作迁移过来的数据根本就不会被使用，甚至马上就会被删除了。从这个角度上看，那么迁移数据时，为什么一定要从 partition 最初 offset 开始迁移数据呢？细想想，好像不需要这样。 所以，解决这个问题的思路就比较简单了，在迁移 TP 时，直接从 partition 最新的 offset 开始数据迁移，但是要同步保持一段时间，主要是确保所有 consumer 都已经跟得上了。 Leader 选举过程 控制器的选举 在Kafka集群中会有一个或多个broker，其中有一个broker会被选举为控制器（Kafka Controller），它负责管理整个集群中所有分区和副本的状态等工作。比如当某个分区的 Leader 副本出现故障时，由控制器负责为该分区选举新的 Leader 副本。再比如当检测到某个分区的 ISR(In-Sync Replicas) 集合发生变化时，由控制器负责通知所有 broker 更新其元数据信息。 Kafka Controller 的选举是依赖 Zookeeper 来实现的，在 Kafka 集群中哪个 broker 能够成功创建 /controller 这个临时（EPHEMERAL）节点他就可以成为 Kafka Controller。Kafka Controller 的出现是处于性能考虑，当 Kafka 集群规模很大，partition 达到成千上万时，当 broker 宕机时，造成集群内大量的调整，会造成大量 Watch 事件被触发，Zookeeper负载会过重。 Controller 脑裂 kafka 中只有一个控制器 controller 负责分区的 leader 选举，同步 broker 的新增或删除消息，但有时由于网络问题，可能同时有两个 broker 认为自己是 controller ，这时候其他的 broker 就会发生脑裂。 解决方案：每当新的 controller 产生的时候就会在 ZK 中生成一个全新的、数值更大的 controller epoch 的标识，并同步给其他的 broker 进行保存，这样当第二个 controller 发送指令时，其他的 broker 就会自动忽略。 分区 Leader 的选举 分区 Leader 副本的选举由 Kafka Controller 负责具体实施。当创建分区（创建主题或增加分区都有创建分区的动作）或分区上线（比如分区中原先的 Leader 副本下线，此时分区需要选举一个新的 Leader 上线来对外提供服务）的时候都需要执行 Leader 的选举动作。 消费者相关的选举 组协调器 GroupCoordinator 需要为消费组内的消费者选举出一个消费组的 Leader，这个选举的算法也很简单，分两种情况分析。如果消费组内还没有 Leader，那么第一个加入消费组的消费者即为消费组的 Leader。如果某一时刻 Leader 消费者由于某些原因退出了消费组，那么会重新选举一个新的 Leader。 负载均衡 Producers 负载均衡 对于同一个 topic 的不同 partition，Kafka 会尽力将这些 partition 分布到不同的 broker 服务器上，这种均衡策略实际上是基于 ZooKeeper 实现的。在一个 broker 启动时，会首先完成 broker 的注册过程，并注册一些诸如 “有哪些可订阅的 topic” 之类的元数据信息。producers 启动后也要到 ZooKeeper 下注册，创建一个临时节点来监听 broker 服务器列表的变化。由于在 ZooKeeper 下 broker 创建的也是临时节点，当 brokers 发生变化时，producers 可以得到相关的通知，从改变自己的 broker list。其它的诸如 topic 的变化以及 broker 和 topic 的关系变化，也是通过 ZooKeeper 的这种 Watcher 监听实现的。 在生产中，必须指定 topic；但是对于 partition，有两种指定方式： 明确指定 partition(0-N)，则数据被发送到指定 partition； 设置为 RD_KAFKA_PARTITION_UA ，则 Kafka 会回调 partitioner 进行均衡选取， partitioner 方法需要自己实现。可以轮询或者传入 key 进行 hash。未实现则采用默认的随机方法 rd_kafka_msg_partitioner_random 随机选择。 Consumer 负载均衡 Kafka 保证同一 consumer group 中只有一个 consumer 可消费某条消息，实际上，Kafka 保证的是稳定状态下每一个 consumer 实例只会消费某一个或多个特定的数据，而某个 partition 的数据只会被某一个特定的 consumer 实例所消费。这样设计的劣势是无法让同一个 consumer group 里的 consumer 均匀消费数据，优势是每个 consumer 不用都跟大量的 broker 通信，减少通信开销，同时也降低了分配难度，实现也更简单。另外，因为同一个 partition 里的数据是有序的，这种设计可以保证每个 partition 里的数据也是有序被消费。 consumer 数量不等于 partition 数量 如果某 consumer group 中 consumer 数量少于 partition 数量，则至少有一个 consumer 会消费多个 partition 的数据；如果 consumer 的数量与 partition 数量相同，则正好一个 consumer 消费一个 partition 的数据，而如果 consumer 的数量多于 partition 的数量时，会有部分 consumer 无法消费该 topic 下任何一条消息。 借助 ZooKeeper 实现负载均衡 关于负载均衡，对于某些低级别的 API，consumer 消费时必须指定 topic 和 partition，这显然不是一种友好的均衡策略。基于高级别的 API，consumer 消费时只需制定 topic，借助 ZooKeeper 可以根据 partition 的数量和 consumer 的数量做到均衡的动态配置。 consumers 在启动时会到 ZooKeeper 下以自己的 conusmer-id 创建临时节点 /consumer/[group-id]/ids/[conusmer-id]，并对 /consumer/[group-id]/ids 注册监听事件，当消费者发生变化时，同一 group 的其余消费者会得到通知。当然，消费者还要监听 broker 列表的变化。kafka 通常会将 partition 进行排序后，根据消费者列表，进行轮流的分配。 参考 Kafka设计解析 Kafka的高可用 Kafka幂等性 Kafka消息丢失 快手万亿级别 Kafka 集群应用实践与技术演进之路 kafka的leader选举过程 "},"zother2-interview/architecture/distributed/lock/":{"url":"zother2-interview/architecture/distributed/lock/","title":"分布式锁","keywords":"","body":"分布式锁 Redis 锁 单节点 Redis 锁 锁的获取： SET resource_name my_random_value NX PX 30000 锁释放： if redis.call(\"get\",KEYS[1]) == ARGV[1] then return redis.call(\"del\",KEYS[1]) else return 0 end 缺陷 由于超时时间导致锁被多 Client 同时获取： C1 获取锁 A 成功，但由于 GC 等原因线程挂起，锁 A 过期 C2 获取锁 A 成功 C1 & C2 同时认为自己加锁成功 异步的主从复制 & Master 宕机，导致锁丢失： C1 获取锁 A 成功，Master 宕机，Slave 未同步到锁 A C2 获取锁 A 成功 C1 & C2 同时认为自己加锁成功 RedLock 为了解决 Redis 单点的问题。 Redis 的作者提出了 RedLock 的解决方案。方案非常的巧妙和简洁。 RedLock 的核心思想就是，同时使用多个 Redis Master 来冗余，且这些节点都是完全的独立的，也不需要对这些节点之间的数据进行同步。 假设我们有N个Redis节点，N应该是一个大于2的奇数。RedLock的实现步骤: 取得当前时间 使用单节点获取锁的方式，依次获取 N 个节点的 Redis 锁。 如果获取到的锁的数量大于 N/2+1 个，且获取的时间小于锁的有效时间(lock validity time)就认为获取到了一个有效的锁，锁自动释放时间就是最初的锁释放时间减去之前获取锁所消耗的时间。 如果获取锁的数量小于 N/2+1，或者在锁的有效时间(lock validity time)内没有获取到足够的锁，就认为获取锁失败，这个时候需要向所有节点发送释放锁的消息。 对于释放锁的实现就很简单了，向所有的 Redis 节点发起释放的操作，无论之前是否获取锁成功。 缺陷 RedLock中，为了防止死锁，锁是具有过期时间的。 如果 Client 1 在持有锁的时候，发生了一次很长时间的 FGC 超过了锁的过期时间。锁就被释放了。 这个时候 Client 2 又获得了一把锁，提交数据。 这个时候 Client 1 从 FGC 中苏醒过来了，又一次提交数据。 这种情况下，数据就发生了错误。RedLock 只是保证了锁的高可用性，并没有保证锁的正确性。 解决方案可以为锁增加一个自增标识，类似于 Kafka 脑裂的处理方式： 同时 RedLock 是严重依赖系统时钟的一致性。如果某个 Redis Master的系统时间发生了错误，造成了它持有的锁提前过期被释放。 每一个系统设计都有自己的侧重或者局限。工程也不是完美的。在现实中工程中不存在完美的解决方案。我们应当深入了解其中的原理，了解解决方案的优缺点。明白选用方案的局限性。是否可以接受方案的局限带来的后果。架构本来就是一门平衡的艺术。 实现基于数据库的乐观锁 提交数据更新之前，每个事务会先检查在该事务读取数据后，有没有其他事务又修改了该数据。如果其他事务有更新的话，正在提交的事务会进行回滚。 Connection conn = DriverManager.getConnection(url, user, password); conn.setAutoCommit(false); Statement stmt = conn.createStatement(); // step 1 int oldVersion = getOldVersion(stmt); // step 2 // 用这个数据库连接做其他的逻辑 // step 3 可用预编译语句 int i = stmt.executeUpdate( \"update optimistic_lock set version = \" + (oldVersion + 1) + \" where version = \" + oldVersion); // step 4 if (i > 0) { conn.commit(); // 更新成功表明数据没有被修改，提交事务。 } else { conn.rollback(); // 更新失败，数据被修改，回滚。 } 乐观锁的缺点： 会带来大数量的无效更新请求、事务回滚，给DB造成不必要的额外压力。 无法保证先到先得，后面的请求可能由于并发压力小了反而有可能处理成功。 基于 ZooKeeper 的分布式锁 基于 ZK 的特性，很容易得出使用 ZK 实现分布式锁的落地方案： 使用 ZK 的临时节点和有序节点，每个线程获取锁就是在 ZK 创建一个临时有序的节点，比如在 /lock/ 目录下。 创建节点成功后，获取 /lock 目录下的所有临时节点，再判断当前线程创建的节点是否是所有的节点的序号最小的节点。 如果当前线程创建的节点是所有节点序号最小的节点，则认为获取锁成功。 如果当前线程创建的节点不是所有节点序号最小的节点，则对节点序号的 前一个节点 添加一个事件监听。 缺陷 羊群效应：当一个节点变化时，会触发大量的 watches 事件，导致集群响应变慢。每个节点尽量少的 watches，这里就只注册 前一个节点 的监听 ZK 集群的读写吞吐量不高 网络抖动可能导致 Session 离线，锁被释放 参考链接 Redis实现分布式锁 Redis RedLock 完美的分布式锁么？ "},"zother2-interview/architecture/distributed/mq/":{"url":"zother2-interview/architecture/distributed/mq/","title":"MQ","keywords":"","body":"MQ 消息队列技术(Message Queue) 是分布式应用间交换信息的一种技术。消息队列可驻留在内存或磁盘上, 队列存储消息直到它们被应用程序读走。通过消息队列，应用程序可独立地执行 ———— 它们不需要知道彼此的位置、或在继续执行前不需要等待接收程序接收此消息。在分布式计算环境中，为了集成分布式应用，开发者需要对异构网络环境下的分布式应用提供有效的通信手段。 MQ使用场景 异步通信：有些业务不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。 解耦：降低工程间的强依赖程度，针对异构系统进行适配。在项目启动之初来预测将来项目会碰到什么需求，是极其困难的。通过消息系统在处理过程中间插入了一个隐含的、基于数据的接口层，两边的处理过程都要实现这一接口，当应用发生变化时，可以独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束 冗余：有些情况下，处理数据的过程会失败。除非数据被持久化，否则将造成丢失。消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的\"插入-获取-删除\"范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。 扩展性：因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。不需要改变代码、不需要调节参数。便于分布式扩容 过载保护：在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量无法提取预知；如果以为了能处理这类瞬间峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃 可恢复性：系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 顺序保证：在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。 缓冲：在任何重要的系统中，都会有需要不同的处理时间的元素。消息队列通过一个缓冲层来帮助任务最高效率的执行，该缓冲有助于控制和优化数据流经过系统的速度。以调节系统响应时间。 数据流处理：分布式系统产生的海量数据流，如：业务日志、监控数据、用户行为等，针对这些数据流进行实时或批量采集汇总，然后进行大数据分析是当前互联网的必备技术，通过消息队列完成此类数据收集是最好的选择 MQ缺点 系统可用性降低：系统引入的外部依赖越多，越容易挂掉。本来你就是 A 系统调用 BCD 三个系统的接口就好了， ABCD 四个系统好好的，没啥问题，你偏加个 MQ 进来，万一 MQ 挂了咋整，MQ 一挂，整套系统崩溃的，你不就完了？如何保证消息队列的高可用。 系统复杂度提高：硬生生加个 MQ 进来，你怎么保证消息没有重复消费？怎么处理消息丢失的情况？怎么保证消息传递的顺序性？头大头大，问题一大堆，痛苦不已。 一致性问题： A 系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是 BCD 三个系统那里， BD 两个系统写库成功了，结果 C 系统写库失败了，咋整？你这数据就不一致了。 MQ常用协议 AMQP协议 AMQP即Advanced Message Queuing Protocol,一个提供统一消息服务的应用层标准高级消息队列协议,是应用层协议的一个开放标准,为面向消息的中间件设计。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件不同产品，不同开发语言等条件的限制。 优点：可靠、通用 MQTT协议 MQTT（Message Queuing Telemetry Transport，消息队列遥测传输）是IBM开发的一个即时通讯协议，有可能成为物联网的重要组成部分。该协议支持所有平台，几乎可以把所有联网物品和外部连接起来，被用来当做传感器和致动器（比如通过Twitter让房屋联网）的通信协议。 优点：格式简洁、占用带宽小、移动端通信、PUSH、嵌入式系统 STOMP协议 STOMP（Streaming Text Orientated Message Protocol）是流文本定向消息协议，是一种为MOM(Message Oriented Middleware，面向消息的中间件)设计的简单文本协议。STOMP提供一个可互操作的连接格式，允许客户端与任意STOMP消息代理（Broker）进行交互。 优点：命令模式（非topic/queue模式） XMPP协议 XMPP（可扩展消息处理现场协议，Extensible Messaging and Presence Protocol）是基于可扩展标记语言（XML）的协议，多用于即时消息（IM）以及在线现场探测。适用于服务器之间的准即时操作。核心是基于XML流传输，这个协议可能最终允许因特网用户向因特网上的其他任何人发送即时消息，即使其操作系统和浏览器不同。 优点：通用公开、兼容性强、可扩展、安全性高，但XML编码格式占用带宽大 其他基于TCP/IP自定义的协议：有些特殊框架（如：redis、kafka、zeroMq等）根据自身需要未严格遵循MQ规范，而是基于TCP\\IP自行封装了一套协议，通过网络socket接口进行传输，实现了MQ的功能。 MQ的通讯模式 点对点通讯：点对点方式是最为传统和常见的通讯方式，它支持一对一、一对多、多对多、多对一等多种配置方式，支持树状、网状等多种拓扑结构。 多点广播：MQ适用于不同类型的应用。其中重要的，也是正在发展中的是\"多点广播\"应用，即能够将消息发送到多个目标站点(Destination List)。可以使用一条MQ指令将单一消息发送到多个目标站点，并确保为每一站点可靠地提供信息。MQ不仅提供了多点广播的功能，而且还拥有智能消息分发功能，在将一条消息发送到同一系统上的多个用户时，MQ将消息的一个复制版本和该系统上接收者的名单发送到目标MQ系统。目标MQ系统在本地复制这些消息，并将它们发送到名单上的队列，从而尽可能减少网络的传输量。 发布/订阅(Publish/Subscribe)模式：发布/订阅功能使消息的分发可以突破目的队列地理指向的限制，使消息按照特定的主题甚至内容进行分发，用户或应用程序可以根据主题或内容接收到所需要的消息。发布/订阅功能使得发送者和接收者之间的耦合关系变得更为松散，发送者不必关心接收者的目的地址，而接收者也不必关心消息的发送地址，而只是根据消息的主题进行消息的收发。在MQ家族产品中，MQ Event Broker是专门用于使用发布/订阅技术进行数据通讯的产品，它支持基于队列和直接基于TCP/IP两种方式的发布和订阅。 集群(Cluster)：为了简化点对点通讯模式中的系统配置，MQ提供 Cluster 的解决方案。集群类似于一个 域(Domain) ，集群内部的队列管理器之间通讯时，不需要两两之间建立消息通道，而是采用 Cluster 通道与其它成员通讯，从而大大简化了系统配置。此外，集群中的队列管理器之间能够自动进行负载均衡，当某一队列管理器出现故障时，其它队列管理器可以接管它的工作，从而大大提高系统的高可靠性 消息投递保证 At most once：消息可能会丢，但绝不会重复投递 At least one：消息绝不会丢，但可能会重复投递 Exactly once：每条消息肯定会被投递一次且仅投递一次，很多时候这是用户所想要的。 参考链接 消息队列面试场景 "},"zother2-interview/architecture/distributed/rpc/":{"url":"zother2-interview/architecture/distributed/rpc/","title":"RPC","keywords":"","body":"RPC 远程过程调用（英语：Remote Procedure Call，缩写为 RPC）是一个计算机通信协议。该协议允许运行于一台计算机的程序调用另一台计算机的子程序，而程序员无需额外地为这个交互作用编程。 应用发展流程 单一应用架构 当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。此时，用于简化增删改查工作量的数据访问框架(ORM)是关键。 垂直应用架构 当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用拆成互不相干的几个应用，以提升效率。此时，用于加速前端页面开发的Web框架(MVC)是关键。 分布式服务架构 当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。此时，用于提高业务复用及整合的分布式服务框架(RPC)是关键。 流动计算架构 当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。此时，用于提高机器利用率的资源调度和治理中心(SOA)是关键。 "},"zother2-interview/architecture/distributed/transaction/":{"url":"zother2-interview/architecture/distributed/transaction/","title":"分布式事务","keywords":"","body":"分布式事务 分布式事务的实现主要有以下 5 种方案： XA 方案 TCC 方案 可靠消息最终一致性方案 最大努力通知方案 2PC/XA方案 所谓的 XA 方案，即：两阶段提交，有一个事务管理器的概念，负责协调多个数据库（资源管理器）的事务，事务管理器先问问各个数据库你准备好了吗？如果每个数据库都回复 ok，那么就正式提交事务，在各个数据库上执行操作；如果任何其中一个数据库回答不 ok，那么就回滚事务。 这种分布式事务方案，比较适合单块应用里，跨多个库的分布式事务，而且因为严重依赖于数据库层面来搞定复杂的事务，效率很低，绝对不适合高并发的场景。 一般来说某个系统内部如果出现跨多个库的这么一个操作，是不合规的。如果你要操作别人的服务的库，你必须是通过调用别的服务的接口来实现，绝对不允许交叉访问别人的数据库。 TCC强一致性方案 TCC 的全称是：Try、Confirm、Cancel。 Try 阶段：这个阶段说的是对各个服务的资源做检测以及对资源进行 锁定或者预留。 Confirm 阶段：这个阶段说的是在各个服务中执行实际的操作。 Cancel 阶段：如果任何一个服务的业务方法执行出错，那么这里就需要 进行补偿，就是执行已经执行成功的业务逻辑的回滚操作。（把那些执行成功的回滚） 这种方案说实话几乎很少人使用，但是也有使用的场景。因为这个 事务回滚实际上是严重依赖于你自己写代码来回滚和补偿 了，会造成补偿代码巨大，非常之恶心。 可靠消息最终一致性方案 基于 MQ 来实现事务。比如阿里的 RocketMQ 就支持消息事务。大概的意思就是： A 系统先发送一个 prepared 消息到 MQ，如果这个 prepared 消息发送失败那么就直接取消操作别执行了； 如果这个消息发送成功过了，那么接着执行本地事务，如果成功就告诉 MQ 发送确认消息，如果失败就告诉 MQ 回滚消息； 如果发送了确认消息，那么此时 B 系统会接收到确认消息，然后执行本地的事务； mq 会自动定时轮询所有 prepared 消息回调你的接口，问你，这个消息是不是本地事务处理失败了，所有没发送确认的消息，是继续重试还是回滚？一般来说这里你就可以查下数据库看之前本地事务是否执行，如果回滚了，那么这里也回滚吧。这个就是避免可能本地事务执行成功了，而确认消息却发送失败了。 这个方案里，要是系统 B 的事务失败了咋办？重试咯，自动不断重试直到成功，如果实在是不行，要么就是针对重要的资金类业务进行回滚，比如 B 系统本地回滚后，想办法通知系统 A 也回滚；或者是发送报警由人工来手工回滚和补偿。 这个还是比较合适的，目前国内互联网公司大都是这么玩儿的，要不你举用 RocketMQ 支持的，要不你就自己基于类似 ActiveMQ？RabbitMQ？自己封装一套类似的逻辑出来，总之思路就是这样子的。 最大努力通知方案 这个方案的大致意思就是： 系统 A 本地事务执行完之后，发送个消息到 MQ； 这里会有个专门消费 MQ 的最大努力通知服务，这个服务会消费 MQ 然后写入数据库中记录下来，或者是放入个内存队列也可以，接着调用系统 B 的接口； 要是系统 B 执行成功就 ok 了；要是系统 B 执行失败了，那么最大努力通知服务就定时尝试重新调用系统 B，反复 N 次，最后还是不行就放弃。 "},"zother2-interview/architecture/distributed/zk/":{"url":"zother2-interview/architecture/distributed/zk/","title":"Zookeeper","keywords":"","body":"Zookeeper ZK 不是解决分布式问题的银弹 分布式应用 分布式应用可以在给定时间（同时）在网络中的多个系统上运行，通过协调它们以快速有效的方式完成特定任务。通常来说，对于复杂而耗时的任务，非分布式应用（运行在单个系统中）需要几个小时才能完成，而分布式应用通过使用所有系统涉及的计算能力可以在几分钟内完成。 通过将分布式应用配置为在更多系统上运行，可以进一步减少完成任务的时间。分布式应用正在运行的一组系统称为 集群，而在集群中运行的每台机器被称为 节点。 分布式应用的优点 可靠性：单个或几个系统的故障不会使整个系统出现故障。 可扩展性：可以在需要时增加性能，通过添加更多机器，在应用程序配置中进行微小的更改，而不会有停机时间。 透明性：隐藏系统的复杂性，并将其显示为单个实体/应用程序。 分布式应用的挑战 竞争条件：两个或多个机器尝试执行特定任务，实际上只需在任意给定时间由单个机器完成。例如，共享资源只能在任意给定时间由单个机器修改。 死锁：两个或多个操作等待彼此无限期完成。 不一致：数据的部分失败。 ZooKeeper基础 Apache ZooKeeper是由集群（节点组）使用的一种服务，用于在自身之间协调，并通过稳健的同步技术维护共享数据。ZooKeeper本身是一个分布式应用程序，为写入分布式应用程序提供服务。 ZooKeeper 的好处： 简单的分布式协调过程 同步：服务器进程之间的相互排斥和协作。 有序性 序列化：根据特定规则对数据进行编码(Jute)。 可靠性 原子性：数据转移完全成功或完全失败，但没有事务是部分的。 架构 一个 ZooKeeper 集群通常由一组机器组成，一般 3 台以上就可以组成一个可用的 ZooKeeper 集群了。组成 ZooKeeper 集群的每台机器都会在内存中维护当前的服务器状态，并且每台机器之间都会互相保持通信。 ZooKeeper 本身就是一个 复制和分布式 应用程序，其目的作为服务运行，类似于我们运行 DNS 或任何其他集中式服务的方式。 ZK 集群 半数以上存活 即可用 ZooKeeper 的客户端程序会选择和集群中的任意一台服务器创建一个 TCP 连接，而且一旦客户端和服务器断开连接，客户端就会自动连接到集群中的其他服务器。 部分 描述 Client（客户端） 客户端是我们的分布式应用集群中的一个节点，从服务器访问信息。对于特定的时间间隔，每个客户端向服务器发送消息以使服务器知道客户端是活跃的。类似地，当客户端连接时，服务器发送确认码。如果连接的服务器没有响应，客户端会自动将消息重定向到另一个服务器。 Server（服务器） 服务器，我们的ZooKeeper总体中的一个节点，为客户端提供所有的服务。向客户端发送确认码以告知服务器是活跃的。 ZooKeeper Service ZooKeeper服务器组。形成 Service 所需的最小节点数为3。 Leader 服务器节点，如果任何连接的节点失败，则执行自动恢复。Leader在服务启动时被选举。 Follower 用于接受客户端请求并向客户端返回结果，在选主过程中参与投票 Observer 接受客户端连接，将写请求转发给leader，但 observer 不参与 投票过程，只同步 leader 的状态， observer 的目的是为了扩展系统，提高读取速度 数据模型 到znode是一个标准的文件系统，层次结构很像一棵树。 需要注意的一些要点如下： 根节点有一个名为 /zoo 的子节点，它又有三个 znode 。 ZooKeeper 树中的每个 znode 都由一个路径标识，路径元素由/分隔。 这些节点被称为数据寄存器，因为它们可以存储数据。 因此，一个 znode 可以有子节点以及与之相关的数据。 这与文件系统可以把文件作为路径很类似。 znode 中的数据通常以字节格式存储，每个 znode 中的最大数据大小不超过1 MB。 ZooKeeper 是为协调而设计的，几乎所有形式的协调数据都比较小， 因此，对数据大小的限制是强制的。 与文件系统中的文件一样， znode 维护一个 stat 结构，其中包含数据更改的 版本号 以及随更改相关的时间戳而更改的 访问控制列表（ACL）。 只要 znode 的数据发生变化，版本号就会增加。 ZooKeeper 使用版本号以及相关的时间戳来验证它的核心内缓存。 znode 版本号还允许客户端通过 ZooKeeper API 更新或删除特定的 znode。 如果指定的版本号与 znode 的当前版本不匹配，则操作失败。 但是，执行 znode 更新或删除操作时，可以通过指定 0 作为版本号来覆盖。 Znode persistent：即使在创建该特定znode的客户端断开连接后，持久节点仍然存在。默认情况下，除非另有说明，否则所有znode都是持久的。 ephemeral：客户端活跃时，临时节点就是有效的。当客户端与 ZooKeeper 集合断开连接时，临时节点会自动删除。因此，只有临时节点不允许有子节点。如果临时节点被删除，则下一个合适的节点将填充其位置。临时节点在 leader 选举中起着重要作用。 sequential：顺序节点可以是持久的或临时的。当一个新的 znode 被创建为一个顺序节点时，ZooKeeper 通过将 10位 的序列号附加到原始名称来设置 znode 的路径。例如，如果将具有路径 /myapp 的znode创建为顺序节点，则ZooKeeper会将路径更改为 /myapp0000000001 ，并将下一个序列号设置为0000000002。如果两个顺序节点是同时创建的，那么 ZooKeeper 不会对每个znode使用相同的数字。顺序节点在锁定和同步中起重要作用。 Sessions 会话对于 ZooKeeper 的操作非常重要。会话中的请求按 FIFO 顺序执行。一旦客户端连接到服务器，将建立会话并向客户端分配 会话ID 。 客户端 以特定的时间间隔发送心跳 以保持会话有效。如果 ZooKeeper 集合在超过服务器开启时指定的期间（会话超时）都没有从客户端接收到心跳，则它会判定客户端死机。 会话超时通常以毫秒为单位。当会话由于任何原因结束时，在该会话期间创建的临时节点也会被删除。 Watcher ZooKeeper 的设计是一种可伸缩的、健壮的集中式服务。在客户端访问此类服务时，常见的设计模式是通过轮询或拉式（pull）模型。当在大型和复杂的分布式系统中实现时，拉模型常常会受到可伸缩性问题的影响。为了解决这个问题，ZooKeeper设计了一种机制，客户端可以从 ZooKeeper 服务中获取通知。客户端接收到这个消息通知之后，需要主动到服务端获取最新的数据。 客户可以使用 ZooKeeper 服务注册与 znode 相关的任何更改。 这种注册被称为在 ZooKeeper 术语中的 znode 上设置 watch。 监视允许客户以任何方式更改 znode 时收到通知。 Watcher 是一次性操作，这意味着它只触发一个通知。 要继续接收通知，客户必须在收到每个事件通知后重新注册一个监视。 监视触发： 对 znode 数据的任何更改，例如使用 setData 操作将新数据写入 znode 的数据字段时。 对 znode 的子节点的任何更改。 例如，一个 znode 的子节点被删除。 正在创建或删除的 znode ，如果将新的 znode 添加到路径中或现有的 znode 被删除，则可能发生这种情况。 同样，ZooKeeper 针对监视和通知声明以下保证： ZooKeeper 确保监视始终以先进先出（FIFO）方式排序，并且通知总是按顺序发送 在对同一个 znode 进行任何其他更改之前，监视会将通知发送给客户端 监视事件的顺序是按照 ZooKeeper 服务的更新顺序排列的 Zookeeper 工作流程 一旦 ZooKeeper 集合启动，它将等待客户端连接。客户端将连接到 ZooKeeper 集合中的一个节点。它可以是 leader 或 follower 节点。一旦客户端被连接，节点将向特定客户端分配 会话ID 并向该客户端发送确认。如果客户端没有收到确认，它将尝试连接 ZooKeeper 集合中的另一个节点。 一旦连接到节点，客户端将以有规律的间隔向节点发送 心跳，以确保连接不会丢失。 如果客户端想要读取特定的znode，它将会向具有znode路径的节点发送读取请求，并且节点通过从其自己的数据库获取来返回所请求的znode。为此，在ZooKeeper集合中读取速度很快。 如果客户端想要将数据存储在ZooKeeper集合中，则会将 znode 路径和数据发送到服务器。连接的服务器将该请求转发给 leader，然后leader将向所有的follower重新发出写入请求。如果只有大部分节点成功响应，而写入请求成功，则成功返回代码将被发送到客户端。 否则，写入请求失败。绝大多数节点被称为 Quorum 。 ZooKeeper Service 节点数量的影响 如果我们有 单个节点，则当该节点故障时，ZooKeeper Service 将故障。即“单点故障\"，不建议在生产环境中使用。 如果我们有 两个节点 而一个节点故障，我们没有占多数，ZooKeeper Service 故障，因为两个中的一个不是多数。 如果我们有 三个节点 而一个节点故障，那么我们有大多数，因此，这是 最低要求。ZooKeeper集合在实际生产环境中必须至少有三个节点。 如果我们有 四个节点 而两个节点故障，它将再次故障。类似于有三个节点，额外节点不用于任何目的，因此，最好添加奇数的节点，例如 3，5，7。 我们知道写入过程比 ZooKeeper 集合中的读取过程要耗时，因为 所有节点都需要在数据库中写入相同的数据。因此，对于平衡的环境拥有较少数量（例如3，5，7）的节点比拥有大量的节点要好。 ZAB 协议 ZAB 协议是为分布式协调服务 ZooKeeper 专门设计的一种支持崩溃恢复的原子广播协议。在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。 读取时：客户端连接 zk 的任一节点，节点直接拿出自己对应的数据返回，这时该节点扮演 Observer 角色； 写入时：客户端的任一提交都会由 Leader 去广播给所有的节点，有半数以上的节点写入成功即视为写入成功； ZAB 的所有动作都是节点们通过协议同步的。在 ZAB 协议的事务编号 Zxid 设计中， Zxid 是一个 64 位的数字，其中低 32 位是一个简单的单调递增的计数器，针对客户端每一个事务请求，计数器加 1；而高 32 位则代表 Leader 周期 epoch 的编号，每个当选产生一个新的 Leader 服务器，就会从这个 Leader 服务器上取出其本地日志中最大事务的 ZXID ，并从中读取 epoch 值，然后加 1，以此作为新的 epoch，并将低 32 位从 0 开始计数。 epoch 可以理解为当前集群所处的年代或者周期，每个 leader 就像皇帝，都有自己的年号，所以每次改朝换代，leader 变更之后，都会在前一个年代的基础上加 1。这样就算旧的 leader 崩溃恢复之后，也没有人听他的了，因为 follower 只听从当前年代的 leader 的命令。 ZAB 协议有两种模式，崩溃恢复（选主+数据同步）和消息广播（事务操作）。任何时候都需要保证只有一个主进程负责进行事务操作，而如果主进程崩溃了，就需要迅速选举出一个新的主进程。主进程的选举机制与事务操作机制是紧密相关的。 消息广播 在 zookeeper 集群中，数据副本的传递策略就是采用消息广播模式。 zookeeper 中数据副本的同步方式与二段提交相似。二段提交要求协调者必须等到所有的参与者全部反馈 ACK 确认消息后，再发送 commit 消息。要求所有的参与者要么全部成功，要么全部失败，因此二段提交会产生严重的阻塞问题。 Zab 协议中 Leader 等待半数以上的Follower成功反馈即可，不需要收到全部Follower反馈。消息广播具体步骤： 客户端发起一个写操作请求。 Leader 服务器将客户端的请求转化为事务 Proposal 提案，同时为每个 Proposal 分配一个全局的ID，即 zxid。 Leader 服务器为每个 Follower 服务器分配一个单独的队列，然后将需要广播的 Proposal 依次放到队列中取，并且根据 FIFO 策略进行消息发送。 Follower 接收到 Proposal 后，会首先将其以事务日志的方式写入本地磁盘中，写入成功后向 Leader 反馈一个 Ack 响应消息。 Leader 接收到超过半数以上 Follower 的 Ack 响应消息后，即认为消息发送成功，可以发送 commit 消息。 Leader 向所有 Follower 广播 commit 消息，同时自身也会完成事务提交。Follower 接收到 commit 消息后，会将上一条事务提交。 崩溃恢复 数据同步 主从架构下，leader 崩溃，为了保证数据一致性，会在选出新leader后进入恢复阶段，新 leader 具有所有已经提交的提议，因此它会保证让 followers 同步已经提交的提议，丢弃未提交的提议（以 leader 的记录为准） 选举 下面任何一种情况，都会触发 Leader 选举： 启动时，集群服务器刚启动 运行时，Leader 崩溃 服务器的状态流转： Leader 选举过程，本质就是 广播优先级消息 的过程，选出 数据最新的服务节点，选出优先级最高的服务节点，基本步骤： 各个服务器节点，广播自己的优先级标识 (sid，zxid) 服务器节点收到其他广播消息后，跟自己的优先级（zxid）对比，自己优先级低，则变更当前节点投票的优先级(sid，zxid) ，并广播变更后的结果 当任意一个服务器节点收到的投票数，超过了法定数量(quorum)，则，升级为 Leader，并广播结果。 由于网络延时，节点得不到足够多广播信息时，会做出错误的投票判断，纠正过程更耗时 选举过程中，服务器节点会等待一定时间，再广播投票信息，时间间隔一般设定为 200 ms 上面 Leader 选举，采取事件触发 Push 方式 广播消息，称为 快速 Leader 选举，因为之前的 Leader 选举，采用 Pull 方式，每隔 1s 拉取一次。 应用场景 发布订阅 通过 Zookeeper 进行数据的发布与订阅其实可以说是它提供的最基本功能，它能够允许多个客户端同时订阅某一个节点的变更并在变更发生时执行我们预先设置好的回调函数，在运行时改变服务的配置和行为 命名服务 除了实现服务配置数据的发布与订阅功能，Zookeeper 还能帮助分布式系统实现命名服务，在每一个分布式系统中，客户端应用都有根据指定名字获取资源、服务器地址的需求，在这时就要求整个集群中的全部服务有着唯一的名字。 在大型分布式系统中，有两件事情非常常见，一是不同服务之间的可能拥有相同的名字，另一个是同一个服务可能会在集群中部署很多的节点，Zookeeper 就可以通过文件系统和顺序节点解决这两个问题。 协调分布式事务 Zookeeper 的另一个作用就是担任分布式事务中的协调者角色，在之前介绍 分布式事务 的文章中我们曾经介绍过分布式事务本质上都是通过 2PC 来实现的，在两阶段提交中就需要一个协调者负责协调分布式事务的执行。 所有的事务参与者会向当前节点中写入提交或者终止，一旦当前的节点改变了事务的状态，其他节点就会得到通知，如果出现一个写入终止的节点，所有的节点就会回滚对分布式事务进行回滚。 分布式锁 在数据库中，锁的概念其实是非常重要的，常见的关系型数据库就会对排他锁和共享锁进行支持，而 Zookeeper 提供的 API 也可以让我们非常简单的实现分布式锁。 作为分布式协调服务，Zookeeper 的应用场景非常广泛，不仅能够用于服务配置的下发、命名服务、协调分布式事务以及分布式锁，还能够用来实现微服务治理中的服务注册以及发现等功能，这些其实都源于 Zookeeper 能够提供高可用的分布式协调服务，能够为客户端提供分布式一致性的支持。 ZooKeeper 的缺陷 zookeeper 不是为高可用性设计的 由于要跨机房容灾，很多系统实际上是需要跨机房部署的。出于性价比的考虑我们通常会让多个机房同时工作，而不会搭建N倍的冗余。也就是说单个机房肯定撑不住全流量（你能设想谷歌在全球只剩下一个机房在干活吗）。由于 zookeeper 集群只能有一个 master，因此一旦机房之间连接出现故障，zookeeper master 就只能照顾一个机房，其他机房运行的业务模块由于没有 master 都只能停掉。于是所有流量集中到有 master 的那个机房，于是系统 crash。 即使是在同一个机房里面，由于网段的不同，在调整机房交换机的时候偶尔也会发生网段隔离的情况。实际上机房每个月基本上都会发生短暂的网络隔离之类的子网段调整。在那个时刻 zookeeper 将处于不可用状态。如果整个业务系统基于 zookeeper （比如要求每个业务请求都先去 zookeeper 获取业务系统的master地址），则系统的可用性将非常脆弱。 由于 zookeeper 对于网络隔离的极度敏感，导致 zookeeper 对于网络的任何风吹草动都会做出激烈反应。这使得 zookeeper 的‘不可用’时间比较多，我们不能让 zookeeper 的‘不可用’，变成系统的不可用。 zookeeper 的选举过程速度很慢 这是一个很难从理论分析上看到的弱点，但是你一旦遇到就会痛不欲生。 前面我们已经说过，网络实际上常常是会出现隔离等不完整状态的，而 zookeeper 对那种情况非常敏感。一旦出现网络隔离， zookeeper 就要发起选举流程。 zookeeper 的选举流程通常耗时 30 到 120 秒，期间 zookeeper 由于没有master，都是不可用的。 对于网络里面偶尔出现的，比如半秒一秒的网络隔离，zookeeper 会由于选举过程，而把不可用时间放大几十倍。 zookeeper 的性能是有限的 典型的 zookeeper 的 tps(transaction peer secondes) 大概是一万多，无法覆盖系统内部每天动辄几十亿次的调用。因此每次请求都去 zookeeper 获取业务系统 master 信息是不可能的。 因此 zookeeper 的 client 必须自己缓存业务系统的 master 地址。 因此 zookeeper 提供的‘强一致性’实际上是不可用的。如果我们需要强一致性，还需要其他机制来进行保障：比如用自动化脚本把业务系统的 old master 给 kill 掉，但是那会有很多陷阱。 zookeeper 无法进行有效的权限控制 zookeeper 的权限控制非常薄弱 在大型的复杂系统里面，使用 zookeeper 必须自己再额外的开发一套权限控制系统，通过那套权限控制系统再访问 zookeeper 额外的权限控制系统不但增加了系统复杂性和维护成本，而且降低了系统的总体性能 即使有了 zookeeper 也很难避免业务系统的数据不一致 前面已经讨论过了，由于 zookeeper 的性能限制，我们无法让每次系统内部调用都走 zookeeper ，因此总有某些时刻，业务系统会存在两个 master（业务系统 client 那边缓存的业务系统 master 信息是定时从 zookeeper 更新的，因此会有更新不同步的问题）。 如果要在业务系统 client 的 master 信息不一致的情况下，仍要保持系统的数据一致性的方法是 先 kill 掉老 master ，再在 zookeeper 上更新 master 信息。但是在是否要 kill current master 这个问题上，程序是无法完全自动决定的（因为网络隔离的时候zookeeper已经不可用了，自动脚本没有全局信息，不管怎么做都可能是错的，什么都不做也可能是错的。当网络故障的时候，只有运维人员才有全局信息，程序是无法接电话得知其他机房的情况的）。因此系统无法自动的保障数据一致性，必须要人工介入。而人工介入的典型时间是半个小时以上，我们不能让系统这么长时间不可用。因此我们必须在某个方向上进行妥协，最常见的妥协方式是放弃 ‘强一致性’，而接受‘最终一致性’。 如果我们需要人工介入才能保证‘可靠的强一致性’，那么 zookeeper 的价值就大打折扣。 Zookeeper 并不保证读取的是最新数据 ZooKeeper 并不保证在每个实例中，两个不同的客户端将具有相同的 ZooKeeper 数据的视图。由于诸如网络延迟的因素，一个客户端可以在另一客户端被通知该改变之前执行更新，考虑两个客户端A和B的场景。如果客户端A将 /a 的值从 0 设置为 1 ，客户端B读取 /a ，客户端 B 可以读取旧值 0，这取决于它连接到的服务器。如果客户端A 和客户端B 读取相同的值很重要，则客户端B应该在执行读取之前从 ZooKeeper API 方法调用 sync() 方法。 对于 Zookeeper 来说，它实现了A可用性（非高可用）、P分区容错性、C中的写入强一致性，丧失的是C中的读取一致性。 我们能做什么 我们或者选择人工介入的强一致性，或者选择程序自动化进行的弱一致性。需要进行取舍。 最终一致性甚至未必是程序来做的，有时候人工修正数据反而在灵活、可靠、低成本上有优势。这需要权衡。 不要迷信zookeeper，有时候不妨考虑一下主备数据库。数据库自带权限控制，用起来比zookeeper方便多了。 zookeeper 比较有价值的东西也许是内容变化的时候，可以阻塞回调的方式通知所有在线的 client 实时更新信息，但这个功能用处不大。 FAQ 这段时间来，也在和公司里的一些同学交流使用zk的心得，整理了一些常见的zookeeper问题。这个页面的目标是解答一些zk常见的使用问题，同时也让大家明确zk不能干什么。页面会一直更新。 客户端对 ServerList 的轮询机制是什么 随机，客户端在初始化( new ZooKeeper(String connectString, int sessionTimeout, Watcher watcher) )的过程中，将所有 Server 保存在一个 List 中，然后随机打散，形成一个环。之后从 0 号位开始一个一个使用。两个注意点： Server地址能够重复配置，这样能够弥补客户端无法设置Server权重的缺陷，但是也会加大风险。（比如: 192.168.1.1:2181,192.168.1.1:2181,192.168.1.2:2181). 如果客户端在进行 Server 切换过程中耗时过长，那么将会收到 SESSION_EXPIRED . 这也是上面第1点中的加大风险之处。 客户端如何正确处理 CONNECTIONLOSS (连接断开) 和 SESSIONEXPIRED (Session 过期)两类连接异常 在 ZooKeeper 中，服务器和客户端之间维持的是一个 长连接，在 SESSION_TIMEOUT 时间内，服务器会确定客户端是否正常连接(客户端会定时向服务器发送 heart_beat ),服务器重置下次 SESSION_TIMEOUT 时间。因此，在正常情况下， Session 一直有效，并且 zk 集群所有机器上都保存这个 Session 信息。在出现问题情况下，客户端与服务器之间连接断了（客户端所连接的那台zk机器挂了，或是其它原因的网络闪断），这个时候客户端会主动在地址列表（初始化的时候传入构造方法的那个参数 connectString ）中选择新的地址进行连接。 好了，上面基本就是服务器与客户端之间维持长连接的过程了。在这个过程中，用户可能会看到两类异常 CONNECTIONLOSS (连接断开) 和 SESSIONEXPIRED (Session 过期)。 CONNECTIONLOSS ：应用在进行操作A时，发生了 CONNECTIONLOSS ，此时用户不需要关心我的会话是否可用，应用所要做的就是等待客户端帮我们自动连接上新的 zk 机器，一旦成功连接上新的 zk 机器后，确认刚刚的操作A是否执行成功了。 SESSIONEXPIRED ：这个通常是zk客户端与服务器的连接断了，试图连接上新的 zk 机器，这个过程如果耗时过长，超过 SESSION_TIMEOUT 后还没有成功连接上服务器，那么服务器认为这个 session 已经结束了（服务器无法确认是因为其它异常原因还是客户端主动结束会话），开始清除和这个会话有关的信息，包括这个会话创建的临时节点和注册的 Watcher 。在这之后，客户端重新连接上了服务器在，但是很不幸，服务器会告诉客户端 SESSIONEXPIRED 。此时客户端要做的事情就看应用的复杂情况了，总之，要重新实例 zookeeper 对象，重新操作所有临时数据（包括临时节点和注册 Watcher ）。 一个客户端修改了某个节点的数据，其它客户端能够马上获取到这个最新数据吗 ZooKeeper 不能确保任何客户端能够获取（即 Read Request ）到一样的数据，除非客户端自己要求：方法是客户端在获取数据之前调用org.apache.zookeeper.AsyncCallback.VoidCallback, java.lang.Object) sync. 通常情况下（这里所说的通常情况满足：1. 对获取的数据是否是最新版本不敏感，2. 一个客户端修改了数据，其它客户端需要不需要立即能够获取最新），可以不关心这点。 在其它情况下，最清晰的场景是这样：ZK 客户端 A 对 /my_test 的内容从 v1->v2, 但是 ZK 客户端 B 对 /my_test 的内容获取，依然得到的是 v1. 请注意，这个是实际存在的现象，当然延时很短。解决的方法是客户端B先调用 sync(), 再调用 getData(). ZK为什么不提供一个永久性的Watcher注册机制 不支持用持久Watcher的原因很简单，ZK无法保证性能。 使用watch需要注意的几点 Watches 通知是一次性的，必须重复注册. 发生 CONNECTIONLOSS 之后，只要在 session_timeout 之内再次连接上（即不发生 SESSIONEXPIRED ），那么这个连接注册的 watches 依然在。 节点数据的版本变化会触发 NodeDataChanged ，注意，这里特意说明了是版本变化。存在这样的情况，只要成功执行了 setData()方法，无论内容是否和之前一致，都会触发 NodeDataChanged 。 对某个节点注册了 watch ，但是节点被删除了，那么注册在这个节点上的 watches 都会被移除。 同一个 zk 客户端对某一个节点注册相同的 watch ，只会收到一次通知。 Watcher 对象只会保存在客户端，不会传递到服务端。 我能否收到每次节点变化的通知 如果节点数据的更新频率很高的话，不能。 原因在于：当一次数据修改，通知客户端，客户端再次注册 watch ，在这个过程中，可能数据已经发生了许多次数据修改，因此，千万不要做这样的测试：\"数据被修改了n次，一定会收到n次通知\"来测试 server 是否正常工作。 能为临时节点创建子节点吗 不能。 是否可以拒绝单个IP对ZK的访问,操作 ZK 本身不提供这样的功能，它仅仅提供了对单个 IP 的连接数的限制。你可以通过修改 iptables 来实现对单个 ip 的限制。 在[getChildren(String path, boolean watch)]注册对节点子节点的变化，那么子节点的子节点变化能通知吗 不能 创建的临时节点什么时候会被删除，是连接一断就删除吗？延时是多少？ 连接断了之后，ZK 不会马上移除临时数据，只有当 SESSIONEXPIRED 之后，才会把这个会话建立的临时数据移除。因此，用户需要谨慎设置 Session_TimeOut zookeeper是否支持动态进行机器扩容？如果目前不支持，那么要如何扩容呢？ 3.4.3版本的zookeeper，还不支持这个功能，在3.5.0版本开始，支持动态加机器了。 ZooKeeper集群中个服务器之间是怎样通信的？ Leader服务器会和每一个 Follower/Observer 服务器都建立TCP连接，同时为每个 F/O 都创建一个叫做 LearnerHandler 的实体。LearnerHandler 主要负责 Leader 和 F/O 之间的网络通讯，包括数据同步，请求转发和 Proposal 提议的投票等。Leader 服务器保存了所有 F/O 的 LearnerHandler 。 zookeeper是否会自动进行日志清理？如果进行日志清理？ zk自己不会进行日志清理，需要运维人员进行日志清理 参考文档 ZooKeeper FAQ Apache ZooKeeper数据模型 Zookeeper并不保证读取的是最新数据 详解分布式协调服务 ZooKeeper ZooKeeper 架构 阿里巴巴为什么不用ZooKeeper 做服务发现？ ZooKeeper 技术内幕：Leader 选举 "},"zother2-interview/basic/algo/hash/":{"url":"zother2-interview/basic/algo/hash/","title":"哈希","keywords":"","body":"Hash 哈希表（Hash Table，也叫散列表），是根据关键码值 (Key-Value) 而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。哈希表的实现主要需要解决两个问题，哈希函数和冲突解决。 哈希函数 哈希函数也叫散列函数，它对不同的输出值得到一个固定长度的消息摘要。理想的哈希函数对于不同的输入应该产生不同的结构，同时散列结果应当具有同一性（输出值尽量均匀）和雪崩效应（微小的输入值变化使得输出值发生巨大的变化）。 冲突解决 开放地址法：以发生冲突的哈希地址为输入，通过某种哈希冲突函数得到一个新的空闲的哈希地址的方法。有以下几种方式： 线性探查法：从发生冲突的地址开始，依次探查下一个地址，直到找到一个空闲单元。 平方探查法：设冲突地址为d0，则探查序列为：d0+1^2,d0-1^2,d0+2^2... 拉链法：把所有的同义词用单链表链接起来。在这种方法下，哈希表每个单元中存放的不再是元素本身，而是相应同义词单链表的头指针。HashMap就是使用这种方法解决冲突的。 "},"zother2-interview/basic/algo/kmp/":{"url":"zother2-interview/basic/algo/kmp/","title":"KMP","keywords":"","body":"KMP算法 KMP算法解决的问题是字符匹配，这个算法把字符匹配的时间复杂度缩小到O(m+n),而空间复杂度也只有O(m),n是target的长度，m是pattern的长度。 部分匹配表（Next数组）：表的作用是 让算法无需多次匹配S中的任何字符。能够实现线性时间搜索的关键是 在不错过任何潜在匹配的情况下，我们\"预搜索\"这个模式串本身并将其译成一个包含所有可能失配的位置对应可以绕过最多无效字符的列表。 Next数组（前缀和前缀的比较）：t为模式串，j为下标 Next[0] = -1 Next[j] = MAX{ k | 0 i 0 1 2 3 4 5 6 t[i] A B C D A B D next[i] -1 0 0 0 0 1 2 NextVal数组：是一种优化后的Next数组，是为了解决类似aaaab这种模式串的匹配，减少重复的比较。 如果t[next[j]]=t[j]：nextval[j]=nextval[next[j]]，否则nextval[j]=next[j]。 i 0 1 2 3 4 5 6 t a b c a b a a next[j] -1 0 0 0 1 2 1 nextval[j] -1 0 0 -1 0 2 1 在上面的表格中，t[next[4]]=t[4]=b，所以nextval[4]=nextval[next[4]]=0 "},"zother2-interview/basic/algo/mst/":{"url":"zother2-interview/basic/algo/mst/","title":"最小生成树算法","keywords":"","body":"最小生成树算法 连通图：在无向图G中，若从顶点i到顶点j有路径，则称顶点i和顶点j是连通的。若图G中任意两个顶点都连通，则称G为连通图。 生成树：一个连通图的生成树是该连通图的一个极小连通子图，它含有全部顶点，但只有构成一个数的(n-1)条边。 最小生成树：对于一个带权连通无向图G中的不同生成树，各树的边上的 权值之和最小。构造最小生成树的准则有三条： 必须只使用该图中的边来构造最小生成树。 必须使用且仅使用(n-1)条边来连接图中的n个顶点。 不能使用产生回路的边。 Prim算法 假设G=(V,E)是一个具有n个顶点的带权连通无向图，T(U,TE)是G的最小生成树，其中U是T的顶点集，TE是T的边集，则由G构造从起始顶点v出发的最小生成树T的步骤为： 初始化U={v}，以v到其他顶点的所有边为候选边(U中所有点到其他顶点的边)。 重复以下步骤(n-1)次，使得其他(n-1)个顶点被加入到U中。 从候选边中挑选权值最小的边加入TE，设该边在V-U(这里是集合减)中的顶点是k，将k加入U中。 考察当前V-U中的所有顶点j，修改候选边，若边(k,j)的权值小于原来和顶点j关联的候选边，则用(k,j)取代后者作为候选边。 Kruskal算法 假设G=(V,E)是一个具有n个顶点的带权连通无向图，T(U,TE)是G的最小生成树，其中U是T的顶点集，TE是T的边集，则由G构造从起始顶点v出发的最小生成树T的步骤为： 置U的初始值等于V(即包含G中的全部顶点)，TE的初始值为空 将图G中的边按权值从小到大的顺序依次选取，若选取的边未使生成树T形成回路，则加入TE，否则放弃，知道TE中包含(n-1)条边为止。 "},"zother2-interview/basic/algo/path/":{"url":"zother2-interview/basic/algo/path/","title":"最短路径算法","keywords":"","body":"最短路径算法 Dijkstra —— 贪心算法 从一个顶点到其余顶点的最短路径 设G=(V,E)是一个带权有向图，把图中顶点集合V分成两组，第1组为已求出最短路径的顶点（用S表示，初始时S只有一个源点，以后每求得一条最短路径v,...k，就将k加到集合S中，直到全部顶点都加入S）。第2组为其余未确定最短路径的顶点集合（用U表示），按最短路径长度的递增次序把第2组的顶点加入S中。 步骤： 1. 初始时，S只包含源点，即`S={v}`，顶点v到自己的距离为0。U包含除v外的其他顶点，v到U中顶点i的距离为边上的权。 2. 从U中选取一个顶点u，顶点v到u的距离最小，然后把顶点u加入S中。 3. 以顶点u为新考虑的中间点，修改v到U中各个点的距离。 4. 重复以上步骤知道S包含所有顶点。 Floyd —— 动态规划 Floyd 算法是解决任意两点间的最短路径的一种算法，可以正确处理有向图或负权（但不可存在负权回路）的最短路径问题。该算法的时间复杂度为 O(N^{3})，空间复杂度为 O(N^{2}) 设 D_{i,j,k} 为从 i 到 j 的只以 (1..k) 集合中的节点为中间节点的最短路径的长度。 $$ D{i,j,k}=\\begin{cases} D{i,j,k-1} & 最短路径不经过 k\\ D{i,k,k-1}+D{k,j,k-1} & 最短路径经过 k \\end{cases} $$ 因此， D{i,j,k}=min(D{i,k,k-1}+D{k,j,k-1},D{i,j,k-1})。伪代码描述如下： // let dist be a |V| × |V| array of minimum distances initialized to ∞ (infinity) for each vertex v dist[v][v] ← 0 for each edge (u,v) dist[u][v] ← w(u,v) // the weight of the edge (u,v) for k from 1 to |V| for i from 1 to |V| for j from 1 to |V| if dist[i][j] > dist[i][k] + dist[k][j] dist[i][j] ← dist[i][k] + dist[k][j] end if "},"zother2-interview/basic/algo/search/":{"url":"zother2-interview/basic/algo/search/","title":"查找算法","keywords":"","body":"查找算法 ASL 由于查找算法的主要运算是关键字的比较，所以通常把查找过程中对关键字的平均比较次数（平均查找长度）作为衡量一个查找算法效率的标准。ASL= ∑(n,i=1) Pi*Ci，其中n为元素个数，Pi是查找第i个元素的概率，一般为Pi=1/n，Ci是找到第i个元素所需比较的次数。 顺序查找 原理是让关键字与队列中的数从最后一个开始逐个比较，直到找出与给定关键字相同的数为止，它的缺点是效率低下。时间复杂度o(n)。 折半查找 折半查找要求线性表是有序表。搜索过程从数组的中间元素开始，如果中间元素正好是要查找的元素，则搜索过程结束；如果某一特定元素大于或者小于中间元素，则在数组大于或小于中间元素的那一半中查找，而且跟开始一样从中间元素开始比较。如果在某一步骤数组为空，则代表找不到。这种搜索算法每一次比较都使搜索范围缩小一半。折半搜索每次把搜索区域减少一半，时间复杂度为O(log n)。 可以借助二叉判定树求得折半查找的平均查找长度：log2(n+1)-1。 折半查找在失败时所需比较的关键字个数不超过判定树的深度，n个元素的判定树的深度和n个元素的完全二叉树的深度相同log2(n)+1。 public int binarySearchStandard(int[] num, int target){ int start = 0; int end = num.length - 1; while(start > 1); if(num[mid] == target) return mid; else if(num[mid] > target){ end = mid - 1; //注意2 } else{ start = mid + 1; //注意3 } } return -1; } 如果是start 因为num[mid] > target, 所以如果有num[index] == target, index一定小于mid，能不能写成end = mid呢？举例来说：num = {1, 2, 5, 7, 9}; 如果写成end = mid，当循环到start = 0, end = 0时（即num[start] = 1, num[end] = 1时），mid将永远等于0，此时end也将永远等于0，陷入死循环。也就是说寻找target = -2时，程序将死循环。 因为num[mid] 分块查找 分块查找又称索引顺序查找，它是一种性能介于顺序查找和折半查找之间的查找方法。分块查找由于只要求索引表是有序的，对块内节点没有排序要求，因此特别适合于节点动态变化的情况。 "},"zother2-interview/basic/algo/skip_list/":{"url":"zother2-interview/basic/algo/skip_list/","title":"跳跃表","keywords":"","body":"跳跃表 跳跃列表是一种数据结构。它允许快速查询一个有序连续元素的数据链表。跳跃列表的平均查找和插入时间复杂度都是 O(log n) ，优于普通队列的 O(n)。 快速查询是通过维护一个多层次的链表，且每一层链表中的元素是前一层链表元素的子集。一开始时，算法在最稀疏的层次进行搜索，直至需要查找的元素在该层两个相邻的元素中间。这时，算法将跳转到下一个层次，重复刚才的搜索，直到找到需要查找的元素为止。跳过的元素的方法可以是 随机性选择 或 确定性选择，其中前者更为常见。 在查找目标元素时，从顶层列表、头元素起步。算法沿着每层链表搜索，直至找到一个大于或等于目标的元素，或者到达当前层列表末尾。如果该元素等于目标元素，则表明该元素已被找到；如果该元素大于目标元素或已到达链表末尾，则退回到当前层的上一个元素，然后转入下一层进行搜索。 跳跃列表不像平衡树等数据结构那样提供对最坏情况的性能保证：由于用来建造跳跃列表采用随机选取元素进入更高层的方法，在小概率情况下会生成一个不平衡的跳跃列表（最坏情况例如最底层仅有一个元素进入了更高层，此时跳跃列表的查找与普通列表一致）。但是在实际中它通常工作良好，随机化平衡方案也比平衡二叉查找树等数据结构中使用的确定性平衡方案容易实现。跳跃列表在并行计算中也很有用：插入可以在跳跃列表不同的部分并行地进行，而不用对数据结构进行全局的重新平衡。 跳跃表插入一个元素： 实现 因为跳跃列表中的元素可以在多个列表中，所以每个元素可以有多于一个指针。跳跃列表的插入和删除的实现与普通的链表操作类似，但高层元素必须在进行多个链表中进行插入或删除。 package io.github.hadyang.leetcode.algo; import lombok.Getter; import lombok.Setter; import java.util.Arrays; import java.util.Random; /** * @author haoyang.shi */ public class SkipList, V> { @Getter @Setter static final class Node, V> { private K key; private V value; private Node up, down, pre, next; Node(K key, V value) { this.key = key; this.value = value; } @Override public String toString() { return \"Node{\" + \"key=\" + key + \", value=\" + value + \", hashcode=\" + hashCode() + \", up=\" + (up == null ? \"null\" : up.hashCode()) + \", down=\" + (down == null ? \"null\" : down.hashCode()) + \", pre=\" + (pre == null ? \"null\" : pre.hashCode()) + \", next=\" + (next == null ? \"null\" : next.hashCode()) + '}'; } } private Node head;//k,v都是NULL private Integer levels = 0; private Integer length = 0; private Random random = new Random(System.currentTimeMillis()); public SkipList() { createNewLevel(); } public void put(K key, V value) { if (key == null || value == null) { return; } Node newNode = new Node<>(key, value); insertNode(newNode); } private void insertNode(Node newNode) { Node curNode = findNode(newNode.getKey()); if (curNode.getKey() == null) { insertNext(curNode, newNode); } else if (curNode.getKey().compareTo(newNode.getKey()) == 0) { //update curNode.setValue(newNode.getValue()); return; } else { insertNext(curNode, newNode); } int currentLevel = 1; Node oldTop = newNode; while (random.nextInt(100) newTop = new Node<>(newNode.getKey(), null); if (currentLevel >= levels) { createNewLevel(); } while (curNode.getPre() != null && curNode.getUp() == null) { curNode = curNode.getPre(); } if (curNode.getUp() == null) { continue; } curNode = curNode.getUp(); Node curNodeNext = curNode.getNext(); curNode.setNext(newTop); newTop.setPre(curNode); newTop.setDown(oldTop); oldTop.setUp(newTop); newTop.setNext(curNodeNext); oldTop = newTop; currentLevel++; } } private void createNewLevel() { Node newHead = new Node<>(null, null); if (this.head == null) { this.head = newHead; this.levels++; return; } this.head.setUp(newHead); newHead.setDown(this.head); this.head = newHead; this.levels++; } private void insertNext(Node curNode, Node newNode) { Node curNodeNext = curNode.getNext(); newNode.setNext(curNodeNext); if (curNodeNext != null) { curNodeNext.setPre(newNode); } curNode.setNext(newNode); newNode.setPre(curNode); this.length++; } public V get(K key) { Node node = findNode(key); if (key.equals(node.getKey())) { return node.getValue(); } return null; } private Node findNode(K key) { Node curNode = this.head; for (; ; ) { while (curNode.getNext() != null && curNode.getNext().getKey().compareTo(key) curI = this.head; String[][] strings = new String[levels][length + 1]; for (String[] string : strings) { Arrays.fill(string, \"0\"); } while (curI.getDown() != null) { curI = curI.getDown(); } System.out.println(\"levels:\" + levels + \"_\" + \"length:\" + length); int i = 0; while (curI != null) { Node curJ = curI; int j = levels - 1; while (curJ != null) { strings[j][i] = String.valueOf(curJ.getKey()); if (curJ.getUp() == null) { break; } curJ = curJ.getUp(); j--; } if (curI.getNext() == null) { break; } curI = curI.getNext(); i++; } for (String[] string : strings) { System.out.println(Arrays.toString(string)); } } public static void main(String[] args) { SkipList skipList = new SkipList<>(); skipList.put(2, \"B\"); skipList.put(1, \"A\"); skipList.put(3, \"C\"); skipList.print(); System.out.println(skipList.get(2)); } } "},"zother2-interview/basic/algo/sort/":{"url":"zother2-interview/basic/algo/sort/","title":"排序算法","keywords":"","body":"排序算法 常见排序算法 稳定排序： 冒泡排序 — O(n²) 插入排序 — O(n²) 桶排序 — O(n); 需要 O(k) 额外空间 归并排序 — O(nlogn); 需要 O(n) 额外空间 二叉排序树排序 — O(n log n) 期望时间; O(n²)最坏时间; 需要 O(n) 额外空间 基数排序 — O(n·k); 需要 O(n) 额外空间 不稳定排序 选择排序 — O(n²) 希尔排序 — O(nlogn) 堆排序 — O(nlogn) 快速排序 — O(nlogn) 期望时间, O(n²) 最坏情况; 对于大的、乱数串行一般相信是最快的已知排序 交换排序 冒泡排序 它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。冒泡排序总的平均时间复杂度为O(n^2)。冒泡排序是一种稳定排序算法。 比较相邻的元素。如果第一个比第二个大，就交换他们两个。 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。在这一点，最后的元素应该会是最大的数。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。 void bubble_sort(int a[], int n) { int i, j, temp; for (j = 0; j a[i + 1]) { temp = a[i]; a[i] = a[i + 1]; a[i + 1] = temp; } } } 快速排序 快速排序-百度百科 快速排序是一种 不稳定 的排序算法，平均时间复杂度为 O(nlogn)。快速排序使用分治法（Divide and conquer）策略来把一个序列（list）分为两个子序列（sub-lists）。 步骤为： 从数列中挑出一个元素，称为\"基准\"（pivot）， 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。 快排的时间花费主要在划分上，所以 最坏情况：时间复杂度为O(n^2)。因为最坏情况发生在每次划分过程产生的两个区间分别包含n-1个元素和1个元素的时候。 最好情况：每次划分选取的基准都是当前无序区的中值。如果每次划分过程产生的区间大小都为n/2，则快速排序法运行就快得多了。 public void sort(int[] arr, int low, int high) { int l = low; int h = high; int povit = arr[low]; while (l = povit) h--; if (l low) sort(arr, low, l - 1); if (h + 1 快排的优化 当待排序序列的长度分割到一定大小后，使用插入排序。 快排函数在函数尾部有两次递归操作，我们可以对其使用尾递归优化。优化后，可以缩减堆栈深度，由原来的O(n)缩减为O(logn)，将会提高性能。 从左、中、右三个数中取中间值。 插入排序 直接插入排序 插入排序的基本操作就是将一个数据插入到已经排好序的有序数据中，从而得到一个新的、个数加一的有序数据，算法适用于少量数据的排序，时间复杂度为O(n^2)。是稳定的排序方法。 插入算法把要排序的数组分成两部分：第一部分包含了这个数组的所有元素，但将最后一个元素除外（让数组多一个空间才有插入的位置），而第二部分就只包含这一个元素（即待插入元素）。在第一部分排序完成后，再将这个最后元素插入到已排好序的第一部分中。 void insert_sort(int* a, int len) { for (int i = 1; i = 0 && temp 希尔排序 也称缩小增量排序，是直接插入排序算法的一种更高效的改进版本。希尔排序是非稳定排序算法。 希尔排序是把记录按下标的一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止。 void shell_sort(int* a, int len) { int step = len / 2; int temp; while (step > 0) { for (int i = step; i = 0 && temp 选择排序 直接选择排序 首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。实际适用的场合非常罕见。 void selection_sort(int arr[], int len) { int i, j, min, temp; for (i = 0; i arr[j]) min = j; temp = arr[min]; arr[min] = arr[i]; arr[i] = temp; } } 堆排序 堆排序利用了大根堆（或小根堆）堆顶记录的关键字最大（或最小）这一特征，使得在当前无序区中选取最大（或最小）关键字的记录变得简单。 将数组分为有序区和无序区，在无序区中建立最大堆 将堆顶的数据与无序区末尾的数据交换 从后往前，直到所有数据排序完成 public void heapSort(int[] nums) { for (int i = nums.length - 1; i >= 0; i--) { maxHeap(nums, 0, i); swap(nums, 0, i); } } public void maxHeap(int[] heap, int start, int end) { if (start == end) { return; } int parent = start; int childLeft = start * 2 + 1; int childRight = childLeft + 1; if (childLeft heap[parent]) { swap(heap, parent, childLeft); } } if (childRight heap[parent]) { swap(heap, parent, childRight); } } } private void swap(int[] nums, int a, int b) { int t = nums[a]; nums[a] = nums[b]; nums[b] = t; } 归并排序 归并排序采用分治的思想： Divide：将n个元素平均划分为各含n/2个元素的子序列； Conquer：递归的解决俩个规模为n/2的子问题； Combine：合并俩个已排序的子序列。 性能：时间复杂度总是为O(NlogN)，空间复杂度也总为为O(N)，算法与初始序列无关，排序是稳定的。 public void mergeSort(int[] array, int start, int end, int[] temp) { if (start >= end) { return; } int mid = (start + end) / 2; mergeSort(array, start, mid, temp); mergeSort(array, mid + 1, end, temp); int f = start, s = mid + 1; int t = 0; while (f 桶排序 桶排序工作的原理是将 数组分到有限数量的桶 里。每个桶再个别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排序）。当要被排序的数组内的数值是均匀分配的时候，桶排序使用线性时间 O(n)。由于桶排序不是比较排序，他不受到 O(n\\log n) 下限的影响。 桶排序以下列程序进行： 设置一个定量的数组当作空桶子。 寻访序列，并且把项目一个一个放到对应的桶子去。 对每个不是空的桶子进行排序。 从不是空的桶子里把项目再放回原来的序列中。 private int indexFor(int a, int min, int step) { return (a - min) / step; } public void bucketSort(int[] arr) { int max = arr[0], min = arr[0]; for (int a : arr) { if (max a) min = a; } // 该值可根据实际情况选择 int bucketNum = max / 10 - min / 10 + 1; List buckList = new ArrayList>(); // create bucket for (int i = 1; i ()); } // push into the bucket for (int i = 0; i ) buckList.get(index)).add(arr[i]); } ArrayList bucket = null; int index = 0; for (int i = 0; i ) buckList.get(i); insertSort(bucket); for (int k : bucket) { arr[index++] = k; } } } // 把桶內元素插入排序 private void insertSort(List bucket) { for (int i = 1; i = 0 && bucket.get(j) > temp; j--) { bucket.set(j + 1, bucket.get(j)); } bucket.set(j + 1, temp); } } 基数排序 对于有d个关键字时，可以分别按关键字进行排序。有俩种方法： MSD：先从高位开始进行排序，在每个关键字上，可采用基数排序 LSD：先从低位开始进行排序，在每个关键字上，可采用桶排序 即通过每个数的每位数字的大小来比较 //找出最大数字的位数 int maxNum(int arr[], int len) { int _max = 0; for (int i = 0; i 拓扑排序 在有向图中找拓扑序列的过程，就是拓扑排序。拓扑序列常常用于判定图是否有环。 从有向图中选择一个入度为0的结点，输出它。 将这个结点以及该结点出发的所有边从图中删除。 重复前两步，直到没有入度为0的点。 如果所有点都被输出，即存在一个拓扑序列，则图没有环。 "},"zother2-interview/basic/algo/tree/":{"url":"zother2-interview/basic/algo/tree/","title":"树","keywords":"","body":"树 二叉树 L、D、R分别表示遍历左子树、访问根结点和遍历右子树 先序遍历：DLR 中序遍历：LDR 后序遍历：LRD 仅有前序和后序遍历，不能确定一个二叉树，必须有中序遍历的结果 二叉树的性质 性质1：在二叉树中第 i 层的结点数最多为 2^{i-1}（i ≥ 1） 性质2：高度为k的二叉树其结点总数最多为 2^{k}－1（k ≥ 1） 性质3：对任意的非空二叉树 T ，如果叶结点的个数为 n_0，而其度为 2 的结点数为 n_2，则：n_0 = n_2 + 1 满二叉树 深度为k，且有 2^k-1 个节点称之为 满二叉树； 性质4：第i层上的节点数为 2^{i-1}； 完全二叉树 深度为k，有n个节点的二叉树，当且仅当其每一个节点都与深度为k的满二叉树中，序号为1至n的节点对应时，称之为完全二叉树。 性质5：对于具有n个结点的完全二叉树的高度为 \\log_{2}^{n}+1 求完全二叉树的叶子结点个数： 二叉树的构造 //n 表示当前结点字符 Node* tree(vector data, int n) { Node* node; if (n >= data.size()) return NULL; if (data[n] == '#') return NULL; node = new Node; node->data = data[n]; node->left = tree(data, n + 1); node->right = tree(data, n + 2); return node; } 堆 堆通常是一个可以被看做一棵树的数组对象。堆的实现通过构造二叉堆（binary heap），实为二叉树的一种； 任意节点小于（或大于）它的所有后裔，最小元（或最大元）在堆的根上（堆序性）。 堆总是一棵完全树。即除了最底层，其他层的节点都被元素填满，且最底层尽可能地从左到右填入。 将根节点最大的堆叫做最大堆或大根堆，根节点最小的堆叫做最小堆或小根堆。常见的堆有二叉堆、斐波那契堆等。 通常堆是通过一维数组来实现的。在数组起始位置为1的情形中： 父节点i的左子节点在位置 2\\times i; 父节点i的右子节点在位置 2\\times i +1; 子节点i的父节点在位置 i\\div 2; 霍夫曼树 霍夫曼树又称最优二叉树，是一种带权路径长度最短的二叉树。所谓树的带权路径长度，就是树中所有的叶结点的权值乘上其到根结点的路径长度（若根结点为0层，叶结点到根结点的路径长度为叶结点的层数）。树的路径长度是从树根到每一结点的路径长度之和，记为 WPL=W1\\times L1+W2\\times L2+W3\\times L3+...+Wn\\times Ln，N个权值Wi（i=1,2,...n）构成一棵有N个叶结点的二叉树，相应的叶结点的路径长度为Li（i=1,2,...n）。可以证明霍夫曼树的WPL是最小的。 霍夫曼树构造 根据给定的n个权值(W1,W2...Wn)，使对应节点构成n个二叉树的森林T=(T1,T2...Tn)，其中每个二叉树Ti(1 中都有一个带权值为Wi的根节点，其左、右子树均为空。 在森林T中选取两个节点权值最小的子树，分别作为左、右子树构造一个新的二叉树，且置新的二叉树的根节点的权值为其左右子树上根节点权值之和。 在森林T中，用新得到的二叉树替代选取的两个二叉树。 重复2和3，直到T只包含一个树为止。这个数就是霍夫曼树。 定理：对于具有n个叶子节点的霍夫曼树，共有2n-1个节点。这是由于霍夫曼树只有度为0和度为2的结点，根据二叉树的性质 n0 = n2 + 1，因此度为2的结点个数为n-1个，总共有2n-1个节点。 霍夫曼编码 对于一个霍夫曼树，所有左链接取'0'、右链接取'1'。从树根至树叶依序记录所有字母的编码。 带权路径 结点的权：若将树中结点赋给一个有着某种含义的数值，则这个数值称为该结点的权。 结点的带权路径：从根结点到该结点之间的路径长度与该结点的权的乘积。 树的带权路径：所有叶子结点的带权路径长度之和，记为WPL。 二叉排序树 二叉查找树，也称二叉搜索树、有序二叉树，排序二叉树，是指一棵空树或者具有下列性质的二叉树： 任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 任意节点的左、右子树也分别为二叉查找树； 没有键值相等的节点。 二分查找的时间复杂度是O(log(n))，最坏情况下的时间复杂度是O(n)（相当于顺序查找） 平衡二叉树 平衡树是计算机科学中的一类改进的二叉查找树。一般的二叉查找树的查询复杂度是跟目标结点到树根的距离（即深度）有关，因此当结点的深度普遍较大时，查询的均摊复杂度会上升，为了更高效的查询，平衡树应运而生了。平衡指所有叶子的深度趋于平衡，更广义的是指在树上所有可能查找的均摊复杂度偏低。 AVL树 AVL树是最先发明的 自平衡二叉查找树。在AVL树中任何节点的两个子树的高度最大差别为一，所以它也被称为高度平衡树。 它的左子树和右子树都是平衡二叉树。 左子树和右子树的深度之差的绝对值不超过1。 增加和删除可能需要通过一次或多次树旋转来重新平衡这个树。 右旋：左结点转到根节点位置。 左旋：右节点转到根节点位置。 高度为k的AVL树，节点数N最多2^k -1，即满二叉树； 红黑树 红黑树是一种自平衡二叉查找树，每个节点都带有颜色属性的二叉查找树，颜色为红色或黑色。在二叉查找树强制一般要求以外，对于任何有效的红黑树我们增加了如下的额外要求： 节点是红色或黑色。 根是黑色。 所有叶子都是黑色（叶子是NIL节点）。 每个红色节点必须有两个黑色的子节点。（从每个叶子到根的所有路径上不能有两个连续的红色节点。） 从任一节点到其每个叶子的所有简单路径都包含相同数目的黑色节点。 如果一条路径上的顶点除了起点和终点可以相同外，其它顶点均不相同，则称此路径为一条简单路径；起点和终点相同的简单路径称为回路（或环）。 红黑树相对于AVL树来说，牺牲了部分平衡性以换取插入/删除操作时少量的旋转操作，整体来说性能要优于AVL树。 这些约束确保了红黑树的关键特性：从根到叶子的最长的可能路径不多于最短的可能路径的两倍长。结果是这个树大致上是平衡的。因为操作比如插入、删除和查找某个值的最坏情况时间都要求与树的高度成比例，这个在高度上的理论上限 允许红黑树在最坏情况下都是高效的，而不同于普通的二叉查找树。 在很多树数据结构的表示中，一个节点有可能只有一个子节点，而叶子节点包含数据。用这种范例表示红黑树是可能的，但是这会改变一些性质并使算法复杂。为此，本文中我们使用\"nil叶子\"或\"空（null）叶子\"，如上图所示，它不包含数据而只充当树在此结束的指示。这些节点在绘图中经常被省略，导致了这些树好像同上述原则相矛盾，而实际上不是这样。与此有关的结论是所有节点都有两个子节点，尽管其中的一个或两个可能是空叶子。 因为每一个红黑树也是一个特化的二叉查找树，因此红黑树上的只读操作与普通二叉查找树上的只读操作相同。然而，在红黑树上进行插入操作和删除操作会导致不再符合红黑树的性质。恢复红黑树的性质需要少量（O(log n)）的颜色变更（实际是非常快速的）和不超过三次树旋转（对于插入操作是两次）。虽然插入和删除很复杂，但操作时间仍可以保持为O(log n)次。 B树 B树是一种自平衡的树，能够保持数据有序。这种数据结构能够让查找数据、顺序访问、插入数据及删除的动作，复杂度均为 O(n)。总的来说，B树是一个泛化的二叉查找树，一个节点可以拥有两个以上的子节点。但其与自平衡二叉查找树不同，B树更适合大数据块的存储系统，例如：磁盘。 在B树中，内部（非叶子）节点可以拥有可变数量的子节点（数量范围预先定义好）。当数据被插入或从一个节点中移除，它的子节点数量发生变化。为了维持在预先设定的数量范围内，内部节点可能会被 合并 或者 分离。因为子节点数量有一定的允许范围，所以B树不需要像其他自平衡查找树那样频繁地重新保持平衡，但是由于节点 没有被完全填充，可能浪费了一些空间。子节点数量的上界和下界依特定的实现而设置。例如，在一个2-3 B树（通常简称2-3树），每一个内部节点只能有 2 或 3 个子节点。 根据 Knuth 的定义，一个 m 阶的B树是一个有以下属性的树： 每一个节点最多有 m 个子节点 每一个非叶子节点（除根节点）最少有 m\\div 2 个子节点 如果根节点不是叶子节点，那么它至少有两个子节点 有 k 个子节点的非叶子节点拥有 k − 1 个键 所有的叶子节点都在同一层 每一个内部节点的键将节点的子树分开。例如，如果一个内部节点有 3 个子节点（子树），那么它就必须有两个键： a1 和 a2 。左边子树的所有值都必须小于 a1 ，中间子树的所有值都必须在 a1 和a2 之间，右边子树的所有值都必须大于 a2 。 B树内的节点可分为三类： 内部节点：内部节点是除叶子节点和根节点之外的所有节点。它们通常被表示为一组有序的元素和指向子节点的指针。 根节点：根节点拥有的子节点数量的上限和内部节点相同，但是没有下限。 叶子节点：叶子节点对元素的数量有相同的限制，但是没有子节点，也没有指向子节点的指针。 B树的查找 在B树中的查找给定关键字的方法 类似于二叉排序树上的查找，不同的是在每个节点上确定向下查找的路径不一定是二路的，而是n+1路的。因为节点内的关键字序列key[1..n]有序，故既可以使用顺序查找，也可以使用二分查找。在一棵B树上查找关键字为k的方法为：将k与根节点中的key[i]进行比较： 若k=key[i]，则查找成功； 若k 若key[i] 若k>key[n]，则沿着指针ptr[n]所指的子树继续查找。 B树的插入 将关键字k插入到B树的过程分两步完成： 利用B树的查找算法查找出该关键字的插入节点(注意B树的插入节点一定属于最低非叶子节点层)。 判断该节点是否还有空位，即判断该节点是否满足n B树的删除 首先查找B树中需删除的元素，如果该元素在B树中存在，则将该元素在其结点中进行删除；如果删除该元素后，首先判断该元素是否有左右孩子结点，如果有，则上移孩子结点中的某相近元素到父节点中，然后是移动之后的情况；如果没有，直接删除后，然后是移动之后的情况。 删除元素，移动相应元素之后，如果某结点中元素数目（即关键字数）小于Min(m/2)-1，则需要看其某相邻兄弟结点是否丰满，如果丰满，则向父节点借一个元素来满足条件；如果其相邻兄弟都刚脱贫，即借了之后其结点数目小于Min(m/2)-1，则该结点与其相邻的某一兄弟结点进行“合并”成一个结点， B+树 B+ 树是 B 树的变体，也是一种多路搜索树。m阶的 B+ 树和 B 树的主要差异如下： 在B+树中，具有n个关键字的节点含有n个子树，即每个关键字对应一个子树，而在B树中，具有n个关键字的节点含有(n+1)个子树。 在B+树中，每个节点(除根节点外)中的关键字个数n的取值范围是[m/2] B+树中所有叶子节点包含了全部关键字，即其他非叶子节点中的关键字包含在叶子节点中，而在B树中，关键字是不重复的。 B+树中所有非叶子节点仅起到索引的作用，即节点中每个索引项值含有对应子树的最大关键字和指向该子树的指针，不含有该关键字对应记录的存储地址。而在B树中，每个关键字对应一个记录的存储地址。 在 B+ 树所有叶子节点链接成一个不定长的线性表。 B+树的查找 在B+树中可以采用两种查找方式： 直接从最小关键字开始顺序查找。 从B+树的根节点开始随机查找。这种查找方式与B树的查找方式类似，只是在分支节点上的关键字与查找值相等时，查找并不会结束，要继续查到叶子节点为止，此时若查找成功，则按所给指针取出对应元素。 在B+树中，不管查找是否成功，每次查找都是经历一条树从根节点到叶子节点的路径。 B+树的插入 首先，查找要插入其中的节点的位置。接着把值插入这个节点中。 如果没有节点处于违规状态则处理结束。 如果某个节点有过多元素，则把它分裂为两个节点，每个都有最小数目的元素。在树上递归向上继续这个处理直到到达根节点，如果根节点被分裂，则创建一个新根节点。为了使它工作，元素的最小和最大数目典型的必须选择为使最小数不小于最大数的一半。 B+树的删除 首先，查找要删除的值。接着从包含它的节点中删除这个值。 如果没有节点处于违规状态则处理结束。 如果节点处于违规状态则有两种可能情况： 它的兄弟节点，可以把一个或多个它的子节点转移到当前节点，而把它返回为合法状态。如果是这样，在更改父节点和两个兄弟节点的分离值之后处理结束。 它的兄弟节点由于处在低边界上而没有额外的子节点。在这种情况下把两个兄弟节点合并到一个单一的节点中，而且我们递归到父节点上，因为它被删除了一个子节点。持续这个处理直到当前节点是合法状态或者到达根节点，在其上根节点的子节点被合并而且合并后的节点成为新的根节点。 B+树的优势所在 为什么说B+树比B树更适合实际应用中操作系统的文件索引和数据库索引？ B+树的中间节点能存储更多指针 B+树的查询效率更加稳定：关键字查询的路径长度相同 减少回溯：由于B+树中叶子节点存在指针，所以在范围查找时不需要回溯到父节点，直接类型链表遍历即可，减少IO Trie树 Trie树，又称前缀树，字典树， 是一种有序树，用于保存关联数组，其中的键通常是字符串。与二叉查找树不同，键不是直接保存在节点中，而是由节点在树中的位置决定。一个节点的所有子孙都有相同的前缀，也就是这个节点对应的字符串，而根节点对应空字符串。一般情况下，不是所有的节点都有对应的值，只有叶子节点和部分内部节点所对应的键才有相关的值。 Trie树查询和插入时间复杂度都是 O(n)，是一种以空间换时间的方法。当节点树较多的时候，Trie 树占用的内存会很大。 Trie树常用于搜索提示。如当输入一个网址，可以自动搜索出可能的选择。当没有完全匹配的搜索结果，可以返回前缀最相似的可能。 "},"zother2-interview/basic/algo/_index.html":{"url":"zother2-interview/basic/algo/_index.html","title":"算法","keywords":"","body":"算法 "},"zother2-interview/basic/cryptology/":{"url":"zother2-interview/basic/cryptology/","title":"密码学","keywords":"","body":"密码学 对称加密 对称加密算法的加密和解密使用的密匙是相同的，也就是说如果通讯两方如果使用对称加密算法来加密通讯数据，那么通讯双方就需要都知道这个密匙，收到通讯数据后用这个密匙来解密数据。 这类算法在加密和解密时使用相同的密钥，或是使用两个可以简单地相互推算的密钥。事实上，这组密钥成为在两个或多个成员间的共同秘密，以便维持专属的通信联系。与非对称加密相比，要求双方获取相同的密钥是对称密钥加密的主要缺点之一。常见的对称加密算法有 DES、3DES、AES、Blowfish、IDEA、RC5、RC6。 对称加密的速度比公钥加密快很多，在很多场合都需要对称加密。 非对称加密 它需要两个密钥，一个是公开密钥，另一个是私有密钥；一个用作加密的时候，另一个则用作解密。使用其中一个密钥把明文加密后所得的密文，只能用相对应的另一个密钥才能解密得到原本的明文；甚至连最初用来加密的密钥也不能用作解密。由于加密和解密需要两个不同的密钥，故被称为非对称加密； 虽然两个密钥在数学上相关，但如果知道了其中一个，并不能凭此计算出另外一个；因此其中一个可以公开，称为 公钥，任意向外发布；不公开的密钥为 私钥 ，必须由用户自行严格秘密保管，绝不透过任何途径向任何人提供，也不会透露给要通信的另一方，即使他被信任。 公钥 & 私钥 均可以作为加密密钥 数字签名 数字签名是一种类似写在纸上的签名，但是使用了 公钥加密领域的技术实现 ，用于鉴别数字信息的方法。在网络上，我们可以使用“数字签名”来进行身份确认。数字签名是一个独一无二的数值，若公钥能通过验证，那我们就能确定对应的公钥的正确性，数字签名兼具这两种双重属性：\"可确认性\"及\"不可否认性（不需要笔迹专家验证）\"。 数字签名就是将公钥密码反过来使用。签名者将讯息用私钥加密（这是一种反用，因为通常非对称加密中私钥用于解密），然后公布公钥;验证者使用公钥将加密讯息解密并比对消息（一般签名对象为消息的散列值）。 密码散列函数 密码散列函数（英语：Cryptographic hash function），又译为加密散列函数、密码散列函数、加密散列函数，是散列函数的一种。它被认为是一种 单向函数，也就是说极其难以由散列函数输出的结果，回推输入的数据是什么。这种散列函数的输入数据，通常被称为消息（ message ），而它的输出结果，经常被称为消息摘要（ message digest ）或摘要（ digest ）。 "},"zother2-interview/basic/database/mysql/architecture/":{"url":"zother2-interview/basic/database/mysql/architecture/","title":"MySQL 架构","keywords":"","body":"MySQL 架构 总体来说 MySQL 可以分为两层，第一层是 MySQL 的服务层，包含 MySQL 核心服务功能：解析、分析、优化、缓存以及内置函数，所有跨存储引擎的功能都在这一层实现：存储过程、触发器、视图等。 第二层是 MySQL 的 存储引擎层，MySQL 中可使用多种存储引擎：InnoDB、MyISAM、Memory。存储引擎负责 MySQL 中数据的存取。服务层通过统一的 API 与存储引擎进行通信，这些 API 屏蔽来同步存储引擎之间的差异，使得这些差异对上层的查询过程透明。 MySQL Server 连接器 连接器负责跟客户端建立连接、获取权限、维持和管理连接。 查询缓存 查询缓存将查询结果按 K-V 的形式进行缓存，K 是查询的语句，V 是查询的结果。当一个表发生更新后，该表对应的所有缓存均会失效。 分析器 分析器有两个功能：词法分析、语法分析。对于一个 SQL 语句，分析器首先进行词法分析，对 SQL 语句进行拆分，识别出各个字符串代表的含义。然后就是语法分析，分析器根据定义的语法规则判断 SQL 是否满足 MySQL 语法。 优化器 优化器在获取到分析器的结果后，通过表结构和 SQL 语句选择执行方案，比如：多表关联时，各个表如何进行连接；当表中有索引时，应该怎样选择索引 等等。 执行器 获取到执行方案后，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。在进行查询时，MySQL 执行器内部执行步骤如下： 调用引擎接口取这个表的第一行，判断该行是否满足 WHERE 子句，如果满足则将这行存在结果集中，否则跳过。 调用引擎接口取下一行，重复相同的判断逻辑，直到取到这个表的最后一行。 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。 对于走索引的查询，执行的逻辑也差不多。第一次调用的是 取满足条件的第一行 这个接口，之后循环取 满足条件的下一行 这个接口，这些接口都是引擎中已经定义好的。 Update 处理逻辑 这里简单的分析下 Update 的处理逻辑 MySQL Server 发送更新请求到 InnoDB 引擎 从 Buffer Pool 加载对应记录的 Data Page（P1） 若 Buffer Pool 中没有该记录，则从磁盘加载该记录 将 P1 存储到 Undo Page 中，并在 Redo Log Buffer 中记录 Undo 操作 更新 P1 为 P1' ，并将 P1' 写入 Dirty Page ，记录变更到 Redo Log Buffer（Prepare 状态） 返回 MySQL Server 执行完成 MySQL Server 记录 binlog MySQL Server 提交 Commit Redo Log Buffer 状态有 Prepare 更改为 Commit，并刷入磁盘 当 Dirty Page 过多时，启动 ChcekPoint 机制，将脏页刷入磁盘 "},"zother2-interview/basic/database/mysql/innodb/concurrent/":{"url":"zother2-interview/basic/database/mysql/innodb/concurrent/","title":"InnoDB 并发控制","keywords":"","body":"InnoDB 并发控制 InnoDB 锁机制 InnoDB默认使用行锁，实现了两种标准的行锁——共享锁与排他锁； 行锁类型 锁功能 锁兼容性 加锁 释放锁 共享锁（读锁、S锁） 允许获取共享锁的亊务读数据 与共享锁兼容，与排它锁不兼容 只有 SerializaWe 隔离级别会默认为：读加共享锁；其他隔离级别下，可显示使用 select...lock in share model 为读加共享锁 在事务提交或回滚后会自动同时释放锁；除了使用 start transaction 的方式显式开启事务，InnoDB 也会自动为增删改査语句开启事务，并自动提交或回滚；(autocommit=1) 排它锁（写锁、X锁） 允许获取排它锁的事务更新或删除数据 与共享锁不兼容，与排它锁不兼容 在默认的 Reapeatable Read 隔离级别下，InnoDB 会自动为增删改操作的行加排它锁；也可显式使用 select...for update 为读加排它锁 ... 除了显式加锁的情况，其他情况下的加锁与解锁都无需人工干预 InnoDB 所有的行锁算法都是基于索引实现的，锁定的也都是索引或索引区间 当前读 & 快照读 当前读：即加锁读，读取记录的最新版本，会加锁保证其他并发事务不能修改当前记录，直至获取锁的事务释放锁；使用当前读的操作主要包括：显式加锁的读操作与插入/更新/删除等写操作，如下所示： select * from table where ? lock in share mode; select * from table where ? for update; insert into table values (…); update table set ? where ?; delete from table where ?; 注：当 Update SQL 被发给 MySQL 后， MySQL Server 会根据where条件，读取第一条满足条件的记录，然后 InnoDB 引擎会将第一条记录返回，并加锁，待 MySQL Server 收到这条加锁的记录之后，会再发起一个 Update 请求，更新这条记录。一条记录操作完成，再读取下一条记录，直至没有满足条件的记录为止。因此， Update 操作内部，就包含了当前读。同理， Delete 操作也一样。 Insert 操作会稍微有些不同，简单来说，就是 Insert 操作可能会触发 Unique Key 的冲突检查，也会进行一个当前读。 快照读：即不加锁读，读取记录的快照版本而非最新版本，通过MVCC实现； InnoDB 默认的 RR 事务隔离级别下，不显式加lock in share mode与for update的 select 操作都属于快照读，保证事务执行过程中只有第一次读之前提交的修改和自己的修改可见，其他的均不可见； 共享锁与独占锁 意向锁 InnoDB 支持多粒度的锁，允许表级锁和行级锁共存。一个类似于 LOCK TABLES ... WRITE 的语句会获得这个表的 x 锁。为了实现多粒度锁，InnoDB 使用了意向锁（简称 I 锁）。I 锁是表明一个事务稍后要获得针对一行记录的某种锁（s or x）的对应表的表级锁，有两种： 意向排它锁（简称 IX 锁）表明一个事务意图在某个表中设置某些行的 x 锁 意向共享锁（简称 IS 锁）表明一个事务意图在某个表中设置某些行的 s 锁 SELECT ... LOCK IN SHARE MODE 设置一个 IS 锁, SELECT ... FOR UPDATE 设置一个 IX 锁。意向锁的原则如下： 一个事务必须先持有该表上的 IS 或者更强的锁才能持有该表中某行的 S 锁 一个事务必须先持有该表上的 IX 锁才能持有该表中某行的 X 锁 新请求的锁只有兼容已有锁才能被允许，否则必须等待不兼容的已有锁被释放。一个不兼容的锁请求不被允许是因为它会引起死锁，错误会发生。意向锁只会阻塞全表请求（比如 LOCK TABLES ... WRITE ）。意向锁的主要目的是展示某人正在锁定表中一行，或者将要锁定一行。 Record Lock 记录锁（Record Lock）是加到索引记录上的锁，假设我们存在下面的一张表 users： CREATE TABLE users( id INT NOT NULL AUTO_INCREMENT, last_name VARCHAR(255) NOT NULL, first_name VARCHAR(255), age INT, PRIMARY KEY(id), KEY(last_name), KEY(age) ); 如果我们使用 id 或者 last_name 作为 SQL 中 WHERE 语句的过滤条件，那么 InnoDB 就可以通过索引建立的 B+ 树找到行记录并添加索引，但是如果使用 first_name 作为过滤条件时，由于 InnoDB 不知道待修改的记录具体存放的位置，也无法对将要修改哪条记录提前做出判断就会锁定整个表。 Gap Lock 记录锁是在存储引擎中最为常见的锁，除了记录锁之外，InnoDB 中还存在间隙锁（Gap Lock），间隙锁是对索引记录中的一段连续区域的锁；当使用类似 SELECT * FROM users WHERE id BETWEEN 10 AND 20 FOR UPDATE; 的 SQL 语句时，就会阻止其他事务向表中插入 id = 15 的记录，因为整个范围都被间隙锁锁定了。 间隙锁是存储引擎对于性能和并发做出的权衡，并且只用于某些事务隔离级别。 虽然间隙锁中也分为共享锁和互斥锁，不过它们之间并不是互斥的，也就是不同的事务可以同时持有一段相同范围的共享锁和互斥锁，它唯一阻止的就是其他事务向这个范围中添加新的记录。 间隙锁的缺点 间隙锁有一个比较致命的弱点，就是当锁定一个范围键值之后，即使某些不存在的键值也会被无辜的锁定，而造成在锁定的时候无法插入锁定键值范围内的任何数据。在某些场景下这可能会对性能造成很大的危害 当Query无法利用索引的时候， InnoDB会放弃使用行级别锁定而改用表级别的锁定，造成并发性能的降低； 当Quuery使用的索引并不包含所有过滤条件的时候，数据检索使用到的索引键所指向的数据可能有部分并不属于该Query的结果集的行列，但是也会被锁定，因为间隙锁锁定的是一个范围，而不是具体的索引键； 当Query在使用索引定位数据的时候，如果使用的索引键一样但访问的数据行不同的时候（索引只是过滤条件的一部分），一样会被锁定 Next-Key Lock Next-Key 锁相比前两者就稍微有一些复杂，它是记录锁和记录前的间隙锁的结合，在 users 表中有以下记录： +------|-------------|--------------|-------+ | id | last_name | first_name | age | |------|-------------|--------------|-------| | 4 | stark | tony | 21 | | 1 | tom | hiddleston | 30 | | 3 | morgan | freeman | 40 | | 5 | jeff | dean | 50 | | 2 | donald | trump | 80 | +------|-------------|--------------|-------+ 如果使用 Next-Key 锁，那么 Next-Key 锁就可以在需要的时候锁定以下的范围： (-∞, 21] (21, 30] (30, 40] (40, 50] (50, 80] (80, ∞) 既然叫 Next-Key 锁，锁定的应该是当前值和后面的范围，但是实际上却不是，Next-Key 锁锁定的是当前值和前面的范围。 当我们更新一条记录，比如 SELECT * FROM users WHERE age = 30 FOR UPDATE;，InnoDB 不仅会在范围 (21, 30] 上加 Next-Key 锁，还会在这条该记录索引增长方向的范围 (30, 40] 加间隙锁，所以插入 (21, 40] 范围内的记录都会被锁定。 Next-Key 锁的作用其实是为了解决幻读的问题。 插入意向锁 插入意向锁是在插入一行记录操作之前设置的一种间隙锁，这个锁释放了一种插入方式的信号，亦即多个事务在相同的索引间隙插入时如果不是插入间隙中相同的位置就不需要互相等待。假设有索引值4、7，几个不同的事务准备插入5、6，每个锁都在获得插入行的独占锁之前用插入意向锁各自锁住了4、7之间的间隙，但是不阻塞对方因为插入行不冲突。 自增锁 自增锁是一个特殊的表级锁，事务插入自增列的时候需要获取，最简单情况下如果一个事务插入一个值到表中，任何其他事务都要等待，这样第一个事物才能获得连续的主键值。 锁选择 +——-+————-+ | id | name | +——-+————-+ | 1 | title1 | +——-+————-+ | 2 | title2 | +——-+————-+ | 3 | title3 | +——-+————-+ | 9 | title9 | +——-+————-+ | 10 | title10 | +——-+————-+ 按照原理来说，id>5 and id这个查询条件，在表中找不到满足条件的项，因此会对第一个不满足条件的项(id = 9)上加GAP锁，防止后续其他事务插入满足条件的记录。 而 GAP 锁与GAP 锁是不冲突的，那么为什么两个同时执行id>5 and id查询的事务会冲突呢？ 原因在于，MySQL Server并没有将id这个查询条件下降到InnoDB引擎层，因此InnoDB看到的查询，是id>5，正向扫描。读出的记录id=9，先加上next key锁(Lock X + GAP lock)，然后返回给 MySQL Server 进行判断。 MySQL Server 此时才会判断返回的记录是否满足id的查询条件。此处不满足，查询结束。 因此，id=9记录上，真正持有的锁是next key锁，而next key锁之间是相互冲突的，这也说明了为什么两个id>5 and id查询的事务会冲突的原因。 MVCC InnoDB 引擎支持 MVCC(Multiversion Concurrency Control)：InnoDB 保存了行的历史版本，以支持事务的并发控制和回滚。这些历史信息保存在表空间的 回滚段（Rollback Segment） 里，回滚段中存储着 Undo Log。当事务需要进行回滚时，InnoDB 就会使用这些信息来进行 Undo 操作，同时这些信息也可用来实现 一致性读。 InnoDB 在存储的每行数据中都增加了三列隐藏属性： DB_TRX_ID：最后一次插入或更新的事务ID DB_ROLL_PTR：指向已写入回滚段的 Undo Log 记录。如果这行记录是更新的，那么就可以根据这个 Undo Log 记录重建之间的数据 DB_ROW_ID：自增序列，如果表未指定主键，则由该列作为主键 在回滚段的 Undo Log 被分为 Insert Undo Log 和 Update Undo Log。Insert Undo Log 只是在事务回滚的时候需要，在事务提交后就可丢弃。Update Undo Log 不仅仅在回滚的时候需要，还要提供一致性读，所以只有在所有需要该 Update Undo Log 构建历史版本数据的事务都提交后才能丢弃。MySQL 建议尽量频繁的提交事务，这样可以保证 InnoDB 快速的丢弃 Update Undo Log，防止其过大。 在 InnoDB 中，行数据的物理删除不是立刻执行，InnoDB 会在行删除的 Undo Log 被丢弃时才会进行物理删除。这个过程被称之为 清理（Purge），其执行过程十分迅速。 MVCC 二级索引 InnoDB 在更新时对 二级索引 和 聚集索引的处理方式不一样。在聚集索引上的更新是原地更新（in-place），其中的隐藏属性 DB_ROLL_PTR 指向的 Undo Log 可以重建历史数据。但是二级索引没有隐藏属性，所以不能原地更新。 当二级索引的数据被更新时，旧的二级索引记录标记为 标记删除（delete-marked），然后插入一条新的索引记录，最终标记删除的索引记录会被清除。当二级索引记录被标记为 delete-marked 或者有更新的事务更新时，InnoDB 会查找聚集索引。在聚集索引中检查行的 DB_TRX_ID，如果事务修改了记录，则从 Undo Log 中构建行数据的正确版本。如果二级索引记录被标记为 delete-marked 或者 二级索引有更新的事务更新，覆盖索引技术不会被使用（获取行任意数据均需要回表）。 MVCC vs 乐观锁 MVCC 并不是一个与乐观和悲观并发控制对立的东西，它能够与两者很好的结合以增加事务的并发量，在目前最流行的 SQL 数据库 MySQL 和 PostgreSQL 中都对 MVCC 进行了实现；但是由于它们分别实现了悲观锁和乐观锁，所以 MVCC 实现的方式也不同。 MVCC 可以保证不阻塞地读到一致的数据。但是，MVCC 并没有对实现细节做约束，为此不同的数据库的语义有所不同，比如： postgres 对写操作也是乐观并发控制；在表中保存同一行数据记录的多个不同版本，每次写操作，都是创建，而回避更新；在事务提交时，按版本号检查当前事务提交的数据是否存在写冲突，则抛异常告知用户，回滚事务； innodb 则只对读无锁，写操作仍是上锁的悲观并发控制，这也意味着，innodb 中只能见到因死锁和不变性约束而回滚，而见不到因为写冲突而回滚，不像 postgres 那样对数据修改在表中创建新纪录，而是每行数据只在表中保留一份，在更新数据时上行锁，同时将旧版数据写入 undo log。表和 undo log 中行数据都记录着事务ID，在检索时，只读取来自当前已提交的事务的行数据。 可见 MVCC 中的写操作仍可以按悲观并发控制实现，而 CAS 的写操作只能是乐观并发控制。还有一个不同在于，MVCC 在语境中倾向于 “对多行数据打快照造平行宇宙”，然而 CAS 一般只是保护单行数据而已。比如 mongodb 有 CAS 的支持，但不能说这是 MVCC。 "},"zother2-interview/basic/database/mysql/innodb/index/":{"url":"zother2-interview/basic/database/mysql/innodb/index/","title":"InnoDB 索引","keywords":"","body":"InnoDB 索引 数据存储 当 InnoDB 存储数据时，它可以使用不同的行格式进行存储；MySQL 5.7 版本支持以下格式的行存储方式： Antelope 是 InnoDB 最开始支持的文件格式，它包含两种行格式 Compact 和 Redundant ，它最开始并没有名字； Antelope 的名字是在新的文件格式 Barracuda 出现后才起的， Barracuda 的出现引入了两种新的行格式 Compressed 和 Dynamic ；InnoDB 对于文件格式都会向前兼容，而官方文档中也对之后会出现的新文件格式预先定义好了名字：Cheetah、Dragon、Elk 等等。 两种行记录格式 Compact 和 Redundant 在磁盘上按照以下方式存储： Compact 和 Redundant 格式最大的不同就是记录格式的第一个部分；在 Compact 中，行记录的第一部分倒序存放了一行数据中列的长度（Length），而 Redundant 中存的是每一列的偏移量（Offset），从总体上上看， Compact 行记录格式相比 Redundant 格式能够减少 20% 的存储空间。 行溢出数据 当 InnoDB 使用 Compact 或者 Redundant 格式存储极长的 VARCHAR 或者 BLOB 这类大对象时，我们并不会直接将所有的内容都存放在数据页节点中，而是将数据中的前 768 个字节存储在数据页中，后面会通过偏移量指向溢出页（off-page），最大768字节的作用是便于创建 前缀索引。溢出页（off-page）不存储在 B+tree 中，使用的是uncompress BLOB page，并且每个字段的溢出都是存储独享。 但是当我们使用新的行记录格式 Compressed 或者 Dynamic 时都只会在行记录中保存 20 个字节的指针，实际的数据都会存放在溢出页面中。 当然在实际存储中，可能会对不同长度的 TEXT 和 BLOB 列进行优化。 想要了解更多与 InnoDB 存储引擎中记录的数据格式的相关信息，可以阅读 InnoDB Record Structure 数据页结构 与现有的大多数存储引擎一样，InnoDB 使用页作为磁盘管理的最小单位；数据在 InnoDB 存储引擎中都是按行存储的，每个 16KB 大小的页中可以存放 2-200 行的记录。 页是 InnoDB 存储引擎管理数据的最小磁盘单位，而 B-Tree 节点就是实际存放表中数据的页面，我们在这里将要介绍页是如何组织和存储记录的；首先，一个 InnoDB 页有以下七个部分： 每一个页中包含了两对 header/trailer：内部的 Page Header/Page Directory 关心的是页的状态信息，而 Fil Header/Fil Trailer 关心的是记录页的头信息。 在页的头部和尾部之间就是用户记录和空闲空间了，每一个数据页中都包含 Infimum 和 Supremum 这两个虚拟的记录（可以理解为占位符）， Infimum 记录是比该页中任何主键值都要小的值， Supremum 是该页中的最大值： User Records 就是整个页面中真正用于存放行记录的部分，而 Free Space 就是空余空间了，它是一个链表的数据结构，为了保证插入和删除的效率，整个页面并不会按照主键顺序对所有记录进行排序，它会自动从左侧向右寻找空白节点进行插入，行记录在物理存储上并不是按照顺序的，它们之间的顺序是由 next_record 这一指针控制的。 B+ 树在查找对应的记录时，并不会直接从树中找出对应的行记录，它只能获取记录所在的页，将整个页加载到内存中，再通过 Page Directory 中存储的稀疏索引和 n_owned、next_record 属性取出对应的记录，不过因为这一操作是在内存中进行的，所以通常会忽略这部分查找的耗时。这样就存在一个命中率的问题，如果一个page中能够相对的存放足够多的行，那么命中率就会相对高一些，性能就会有提升。 B+树底层的叶子节点为一双向链表，因此 每个页中至少应该有两行记录，这就决定了 InnoDB 在存储一行数据的时候不能够超过 8kb，但事实上应该更小，因为还有一些 InnoDB 内部数据结构要存储。 通常我们认为 blob 这类的大对象的存储会把数据存放在 off-page，其实不然，关键点还是要看一个 page 中到底能否存放两行数据，blob 可以完全存放在数据页中(单行长度没有超过 8kb)，而 varchar 类型的也有可能存放在溢出页中(单行长度超过 8kb，前 768byte 存放在数据页中)。 索引 索引是数据库中非常非常重要的概念，它是存储引擎能够快速定位记录的秘密武器，对于提升数据库的性能、减轻数据库服务器的负担有着非常重要的作用；索引优化是对查询性能优化的最有效手段，它能够轻松地将查询的性能提高几个数量级。 InnoDB 存储引擎在绝大多数情况下使用 B+ 树建立索引，这是关系型数据库中查找最为常用和有效的索引，但是 B+ 树索引并不能找到一个给定键对应的具体值，它只能找到数据行对应的页，然后正如上一节所提到的，数据库把整个页读入到内存中，并在内存中查找具体的数据行。 B+ 树是平衡树，它查找任意节点所耗费的时间都是完全相同的，比较的次数就是 B+ 树的高度； B+ 树的叶子节点存放所有指向关键字的指针，节点内部关键字记录和节点之间都根据关键字的大小排列。当顺序递增插入的时候，只有最后一个节点会在满掉的时候引起索引分裂，此时无需移动记录，只需创建一个新的节点即可。而当非递增插入的时候，会使得旧的节点分裂，还可能伴随移动记录，以便使得新数据能够插入其中。一般建议使用一列顺序递增的 ID 来作为主键，但不必是数据库的 autoincrement 字段，只要满足顺序增加即可，如 twitter 的 snowflake 即为顺序递增的 ID 生成器。 B+ 树的高度 这里我们先假设 B+ 树高为2，即存在一个根节点和若干个叶子节点，那么这棵 B+ 树的存放总记录数为：根节点指针数*单个叶子节点记录行数。这里假设一行记录的大小为1k，那么一个页上的能放 16 行数据。假设主键ID为 bigint 类型，长度为 8 字节，而指针大小在 InnoDB 源码中设置为 6 字节，这样一共14字节，那么可以算出一棵高度为 2 的 B+ 树，能存放 16 \\times 1024\\div 14\\times 16=18720 条这样的数据记录。 根据同样的原理我们可以算出一个高度为3的B+树可以存放：1170\\times 1170\\times 16=21,902,400 条这样的记录。所以在 InnoDB 中 B+ 树高度一般为 1~3 层，它就能满足千万级的数据存储。 聚集索引 InnoDB 存储引擎中的表都是使用索引组织的，也就是按照键的顺序存放；聚集索引就是按照表中主键的顺序构建一颗 B+ 树，并在叶节点中存放表中的行记录数据。 如果没有定义主键，则会使用非空的 UNIQUE键 做主键 ; 如果没有非空的 UNIQUE键 ，则系统生成一个6字节的 rowid 做主键; CREATE TABLE users( id INT NOT NULL, first_name VARCHAR(20) NOT NULL, last_name VARCHAR(20) NOT NULL, age INT NOT NULL, PRIMARY KEY(id), KEY(last_name, first_name, age) KEY(first_name) ); 如果使用上面的 SQL 在数据库中创建一张表，B+ 树就会使用 id 作为索引的键，并在叶子节点中存储一条记录中的所有信息。 图中对 B+ 树的描述与真实情况下 B+ 树中的数据结构有一些差别，不过这里想要表达的主要意思是：聚集索引叶节点中保存的是整条行记录，而不是其中的一部分。 聚集索引与表的物理存储方式有着非常密切的关系，所有正常的表应该 有且仅有一个 聚集索引（绝大多数情况下都是主键），表中的所有行记录数据都是按照 聚集索引 的顺序存放的。 当我们使用聚集索引对表中的数据进行检索时，可以直接获得聚集索引所对应的整条行记录数据所在的页，不需要进行第二次操作。 辅助索引 数据库将 所有的非聚集索引都划分为辅助索引，但是这个概念对我们理解辅助索引并没有什么帮助；辅助索引也是通过 B+ 树实现的，但是它的叶节点并不包含行记录的全部数据，仅包含索引中的所有键和一个用于查找对应行记录的『书签』，在 InnoDB 中这个书签就是当前记录的主键。 辅助索引的存在并不会影响聚集索引，因为聚集索引构成的 B+ 树是数据实际存储的形式，而辅助索引只用于加速数据的查找，所以一张表上往往有多个辅助索引以此来提升数据库的性能。 一张表一定包含一个聚集索引构成的 B+ 树以及若干辅助索引的构成的 B+ 树。 如果在表 users 中存在一个辅助索引 (first_name, age)，那么它构成的 B+ 树大致就是上图这样，按照 (first_name, age) 的字母顺序对表中的数据进行排序，当查找到主键时，再通过聚集索引获取到整条行记录。 上图展示了一个使用辅助索引查找一条表记录的过程：通过辅助索引查找到对应的主键，最后在聚集索引中使用主键获取对应的行记录，这也是通常情况下行记录的查找方式。 覆盖索引 聚簇索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录，这种行为被称之为 回表。回表会导致查询时多次读取磁盘，为减少IO MySQL 在辅助索引上进行优化，将辅助索引作为 覆盖索引（Covering index）。在查询的时候，如果 SELECT 子句中的字段为主键、辅助索引的键则不进行回表。 索引失效 索引并不是时时都会生效的，比如以下几种情况，将导致索引失效： 如果条件中有 or，即使其中有条件带索引也不会使用。要想使用or，又想让索引生效，只能将 or 条件中的每个列都加上索引 对于多列索引，不是使用的最左匹配，则不会使用索引。 如果 mysql 估计使用全表扫描要比使用索引快，则不使用索引。例如，使用<>、not in 、not exist，对于这三种情况大多数情况下认为结果集很大，MySQL 就有可能不使用索引。 索引使用 (7) - SELECT (8) - DISTINCT (1) - FROM (3) - JOIN (2) - ON (4) - WHERE (5) - GROUP BY (6) - HAVING (9) - ORDER BY (10) - LIMIT 关于 SQL 语句的执行顺序，有三个值得我们注意的地方： FROM 才是 SQL 语句执行的第一步，并非 SELECT。 数据库在执行 SQL 语句的第一步是将数据从硬盘加载到数据缓冲区中，以便对这些数据进行操作。 SELECT 是在大部分语句执行了之后才执行的，严格的说是在 FROM 和 GROUP BY 之后执行的。理解这一点是非常重要的，这就是你不能在 WHERE 中使用在 SELECT 中设定别名的字段作为判断条件的原因。 无论在语法上还是在执行顺序上， UNION 总是排在在 ORDER BY 之前。很多人认为每个 UNION 段都能使用 ORDER BY 排序，但是根据 SQL 语言标准和各个数据库 SQL 的执行差异来看，这并不是真的。尽管某些数据库允许 SQL 语句对子查询（subqueries）或者派生表（derived tables）进行排序，但是这并不说明这个排序在 UNION 操作过后仍保持排序后的顺序。 虽然SQL的逻辑查询是根据上述进行查询，但是数据库也许并不会完全按照逻辑查询处理的方式来进行查询。 MySQL 数据库有两个组件 Parser（分析SQL语句）和 Optimizer（优化）。 从官方手册上看，可以理解为， MySQL 采用了基于开销的优化器，以确定处理查询的最解方式，也就是说执行查询之前，都会先选择一条自以为最优的方案，然后执行这个方案来获取结果。在很多情况下， MySQL 能够计算最佳的可能查询计划，但在某些情况下， MySQL 没有关于数据的足够信息，或者是提供太多的相关数据信息，估测就不那么友好了。 存在索引的情况下，优化器优先使用条件用到索引且最优的方案。当 SQL 条件有多个索引可以选择， MySQL 优化器将直接使用效率最高的索引执行。 "},"zother2-interview/basic/database/mysql/innodb/transaction/":{"url":"zother2-interview/basic/database/mysql/innodb/transaction/","title":"InnoDB 事务","keywords":"","body":"InnoDB 事务隔离 几种隔离级别 事务的隔离性是数据库处理数据的几大基础之一，而隔离级别其实就是提供给用户用于在性能和可靠性做出选择和权衡的配置项。 ISO 和 ANIS SQL 标准制定了四种事务隔离级别，而 InnoDB 遵循了 SQL:1992 标准中的四种隔离级别：READ UNCOMMITED、READ COMMITED、REPEATABLE READ 和 SERIALIZABLE；每个事务的隔离级别其实都比上一级多解决了一个问题： RAED UNCOMMITED：使用查询语句不会加锁，可能会读到未提交的行（Dirty Read）； 可以读取未提交记录。此隔离级别，不会使用，忽略。 READ COMMITED：只对记录加记录锁，而不会在记录之间加间隙锁，所以允许新的记录插入到被锁定记录的附近，所以再多次使用查询语句时，可能得到不同的结果（Non-Repeatable Read）； 快照读忽略，本文不考虑。针对当前读，RC隔离级别保证对读取到的记录加锁 (记录锁)，存在幻读现象。 REPEATABLE READ：快照读忽略，本文不考虑。针对当前读，RR隔离级别保证对读取到的记录加锁 (记录锁)，同时保证对读取的范围加锁，新的满足查询条件的记录不能够插入 (间隙锁)，不存在幻读现象。 SERIALIZABLE：从MVCC并发控制退化为基于锁的并发控制。不区别快照读与当前读，所有的读操作均为当前读，读加读锁 (S锁)，写加写锁 (X锁)。 Serializable隔离级别下，读写冲突，因此并发度急剧下降，在MySQL/InnoDB下不建议使用。 MySQL 中默认的事务隔离级别就是 REPEATABLE READ，但是它通过 Next-Key 锁也能够在某种程度上解决幻读的问题。 接下来，我们将数据库中创建如下的表并通过个例子来展示在不同的事务隔离级别之下，会发生什么样的问题： CREATE TABLE test( id INT NOT NULL, UNIQUE(id) ); 脏读 在一个事务中，读取了其他事务未提交的数据。 当事务的隔离级别为 READ UNCOMMITED 时，我们在 SESSION 2 中插入的未提交数据在 SESSION 1 中是可以访问的。 不可重复读 在一个事务中，同一行记录被访问了两次却得到了不同的结果。 当事务的隔离级别为 READ COMMITED 时，虽然解决了脏读的问题，但是如果在 SESSION 1 先查询了一行数据，在这之后 SESSION 2 中修改了同一行数据并且提交了修改，在这时，如果 SESSION 1 中再次使用相同的查询语句，就会发现两次查询的结果不一样。 不可重复读的原因就是，在 READ COMMITED 的隔离级别下，存储引擎不会在查询记录时添加行锁，锁定 id = 3 这条记录。 幻读 在一个事务中，同一个范围内的记录被读取时，其他事务向这个范围添加了新的记录。 重新开启了两个会话 SESSION 1 和 SESSION 2，在 SESSION 1 中我们查询全表的信息，没有得到任何记录；在 SESSION 2 中向表中插入一条数据并提交；由于 REPEATABLE READ 的原因，再次查询全表的数据时，我们获得到的仍然是空集，但是在向表中插入同样的数据却出现了错误。 这种现象在数据库中就被称作幻读，虽然我们使用查询语句得到了一个空的集合，但是插入数据时却得到了错误，好像之前的查询是幻觉一样。 在标准的事务隔离级别中，幻读是由更高的隔离级别 SERIALIZABLE 解决的，但是它也可以通过 MySQL 提供的 Next-Key 锁解决： REPEATABLE READ 和 READ UNCOMMITED 其实是矛盾的，如果保证了前者就看不到已经提交的事务，如果保证了后者，就会导致两次查询的结果不同，MySQL 为我们提供了一种折中的方式，能够在 REPEATABLE READ 模式下加锁访问已经提交的数据，其本身并不能解决幻读的问题，而是通过文章前面提到的 Next-Key 锁来解决。 "},"zother2-interview/basic/database/mysql/innodb/_index.html":{"url":"zother2-interview/basic/database/mysql/innodb/_index.html","title":"Index","keywords":"","body":""},"zother2-interview/basic/database/mysql/sharding/":{"url":"zother2-interview/basic/database/mysql/sharding/","title":"MySQL 集群","keywords":"","body":"分库分表 垂直拆分 垂直分表 也就是 大表拆小表，基于列字段进行的。一般是表中的字段较多，将不常用的， 数据较大，长度较长（比如text类型字段）的拆分到 扩展表。 一般是针对那种几百列的大表，也避免查询时，数据量太大造成的 off-page 问题。 垂直分库 针对的是一个系统中的不同业务进行拆分。将多个业务系统的数据放在单个数据库中（服务化拆分），这会让数据库的单库处理能力成为瓶颈。将单个数据库，按业务进行拆分，同一业务领域的数据表放到同一数据库中。并且多个数据库分布在多个机器上，防止由于单机的磁盘、内存、IO等资源造成 MySQL 性能下降。 数据库的连接资源比较宝贵且单机处理能力也有限，在高并发场景下，垂直分库一定程度上能够突破 IO、连接数等单机硬件资源的瓶颈。 水平拆分 目前绝大多数应用采取的两种分库分表规则 离散映射：如 mod 或 dayofweek ， 这种类型的映射能够很好的解决热点问题，但带来了数据迁移和历史数据问题。 连续映射；如按 id 或 gmt_create_time 的连续范围做映射。这种类型的映射可以避免数据迁移，但又带来热点问题。 随着数据量的增大，每个表或库的数据量都是各自增长。当一个表或库的数据量增长到了一个极限，要加库或加表的时候，介于这种分库分表算法的离散性，必需要做 数据迁移 才能完成。 考虑到数据增长的特点，如果我们以代表时间增长的字段，按递增的范围分库，则可以避免数据迁移。这样的方式下，在数据量再增加达到前几个库/表的上限时，则继续水平增加库表，原先的数据就不需要迁移了。但是这样的方式会带来一个 热点问题：当前的数据量达到某个库表的范围时，所有的插入操作，都集中在这个库/表了。 结合离散分库/分表和连续分库/分表的优点，可使要热点和新数据均匀分配在每个库，同时又保证易于水平扩展。分库分表的主要经历以下三个阶段： 阶段一 一个数据库，两个表，rule0 = id % 2 分库规则dbRule: “DB0″ 分表规则tbRule: “t” + (id % 2) 阶段二 当单库的数据量接近 1千万，单表的数据量接近 500 万时，进行扩容（数据量只是举例，具体扩容量要根据数据库和实际压力状况决定）：增加一个数据库 DB1，将 DB0.t0 整表迁移到新库 DB1.t1。每个库各增加1个表，未来10M-20M的数据mod2分别写入这2个表：t0_1，t1_1： 分库规则dbRule: “DB” + (id % 2) 分表规则tbRule: if(id 这样 10M 以后的新生数据会均匀分布在 DB0 和 DB1; 插入更新和查询热点仍然能够在每个库中均匀分布。每个库中同时有老数据和不断增长的新数据。每表的数据仍然控制在 500万 以下。 阶段三 当两个库的容量接近上限继续水平扩展时，进行如下操作： 新增加两个库：DB2和DB3，以id % 4分库。余数0、1、2、3分别对应DB的下标. t0和t1不变， 将DB0.t0_1整表迁移到DB2; 将DB1.t1_1整表迁移到DB3 20M-40M的数据 mod4 分为 4 个表：t0_2，t1_2，t2_2，t3_2，分别放到4个库中： 新的分库分表规则如下： 分库规则dbRule: if(id 分表规则tbRule: if(id 随着时间的推移，当第一阶段的t0/t1，第二阶段的t0_1/t1_1逐渐成为历史数据，不再使用时，可以直接truncate掉整个表。省去了历史数据迁移的麻烦。 分库分表规则的设计和配置，长远说来必须满足以下要求 可以动态推送修改 规则可以分层级叠加，旧规则可以在新规则下继续使用，新规则是旧规则在更宽尺度上的拓展，以此支持新旧规则的兼容，避免数据迁移 用 mod 方式时，最好选 2 的指数级倍分库分表，这样方便以后切割。 数据迁移 在上述的水平扩容方案中，如何进行数据迁移，是在扩容中需要考虑的问题。一般情况下，数据迁移分为：停机迁移、双写迁移。 停机迁移 是最简单、最安全、最快速的迁移方案，但一般线上业务系统很少允许停机迁移。在停机迁移中，首先停掉数据库 A 的写入请求，复制 A 数据到 B，待复制完成后，切换线上数据源。 双写迁移 方案就是同时写两个库，一个是老库，一个是新库。也就是在线上系统里面，除了对所有老库的增删改地方，同时对新库同样执行增删改。主要经历以下三个阶段： 导入历史数据，数据库双写（事务成功以老数据源为准），查询走老数据源，通过定时任务补全新老差异数据 新老数据无差异，依旧双写（事务成功以新数据源为准），查询走新数据源 稳定运行无误后，下线老数据源 Join 在拆分之前，系统中很多列表和详情页所需的数据是可以通过 Join 来完成的。而拆分后，数据库可能是分布式在不同实例和不同的主机上，Join 将变得非常麻烦。首先要考虑下垂直分库的设计问题，如果可以调整，那就优先调整。如果无法调整的情况，可以考虑以下解决方案： 全局表：就是有可能系统中所有模块都可能会依赖到的一些表。为了避免跨库 join 查询，我们可以将这类表在其他每个数据库中均保存一份。同时，这类数据通常也很少发生修改（甚至几乎不会），所以也不用太担心 一致性 问题； 字段冗余：字段冗余能带来便利，是一种 空间换时间 的体现。但其适用场景也比较有限，比较适合依赖字段较少的情况。最复杂的还是数据一致性问题，这点很难保证； 系统层组装：在系统层面，通过调用不同模块的组件或者服务，获取到数据并进行字段拼装； 主从复制 MySQL 主从复制涉及到三个线程，一个运行在主节点（log dump thread），其余两个(I/O thread, SQL thread)运行在从节点。 Log Dump Thread：当从节点连接主节点时，主节点会创建一个 log dump 线程，用于发送 bin-log 的内容。在读取 bin-log 中的操作时，此线程会对主节点上的 bin-log 加锁，当读取完成，甚至在发动给从节点之前，锁会被释放。 I/O Thread：当从节点上执行 start slave 命令之后，从节点会创建一个 I/O 线程用来连接主节点，请求主库中更新的 bin-log。I/O线程接收到主节点 binlog dump 进程发来的更新之后，保存在本地 relay-log 中。 SQL Thread：负责读取 relay log 中的内容，解析成具体的操作并执行，最终保证主从数据的一致性。 一个 slave 节点可同时从多个 master 进行数据复制，在这种情况下，不同 master 的 bin-log 存储在不同的 relay log中。 同步模式 异步模式（mysql async-mode）：MySQL增删改操作会全部记录在 binary log 中，当 slave 节点连接 master 时，会主动从 master 处获取最新的 bin log 文件。 半同步模式(mysql semi-sync)：这种模式下主节点只需要接收到其中一台从节点的返回信息，就会 commit ；否则需要等待直到超时时间然后切换成异步模式再提交；这样做的目的可以使主从数据库的数据延迟缩小，可以提高数据安全性，确保了事务提交后，binlog 至少传输到了一个从节点上，不能保证从节点将此事务更新到 db 中。性能上会有一定的降低，响应时间会变长。 全同步模式 是指主节点和从节点全部执行了commit并确认才会向客户端返回成功。 主从复制的延迟问题 进行主从同步的过程中，如果使用异步或半异步模式，均会有主从节点数据不一致的窗口时间。同时，从节点上的 SQL Thread 只能串行执行 relay-log 中的记录，当某条 DDL/DML 耗时较长时，会加剧这个窗口时间；再者在某些场景下会使用 slave 节点进行数据读取，这也可能导致数据加锁等待。基于以上原因在处理主从复制延迟问题上有以下几种方向： 优化主从节点之间的网络延迟 降低 master 负载，以减少 TPS 降低 slave 负载，slave 只做备份使用，不提供服务 调整 slave 参数：关闭 slave bin-log 等 多线程的主从复制：不同 schema 下的表并发提交时的数据不会相互影响，即 slave 节点可以用对 relay log 中不同的 schema 各分配一个SQL Thread，来重放 relay log 中主库已经提交的事务 全局ID 数据库自增 id 设置数据库 sequence 或者表自增字段步长 UUID Snowflake 算法 Snowflake twitter 开源的分布式 id 生成算法，采用 Scala 语言实现，是把一个 64 位的 long 型的 id ，1 个 bit 是不用的，用其中的 41 bit 作为毫秒数，用 10 bit 作为工作机器 id ，12 bit 作为序列号。 |–1位符号位–|--41位时间戳–|--10位机器ID–|--12位序列号–| 1 bit：不用，为啥呢？因为二进制里第一个 bit 为如果是 1，那么都是负数，但是我们生成的 id 都是正数，所以第一个 bit 统一都是 0。 41 bit：表示的是时间戳，单位是毫秒。41 bit 可以表示的数字多达 2^41 - 1，也就是可以标识 2^41 - 1 个毫秒值，换算成年就是表示69年的时间。 10 bit：记录工作机器 id，代表的是这个服务最多可以部署在 2^10台机器上哪，也就是1024台机器。但是 10 bit 里 5 个 bit 代表机房 id，5 个 bit 代表机器 id。意思就是最多代表 2^5个机房（32个机房），每个机房里可以代表 2^5 个机器（32台机器）。 12 bit：这个是用来记录同一个毫秒内产生的不同 id，12 bit 可以代表的最大正整数是 2^12 - 1 = 4096，也就是说可以用这个 12 bit 代表的数字来区分同一个毫秒内的 4096 个不同的 id。 Snowflake 的问题 Snowflake 这样依赖时间的 ID 生成算法注定存在一个问题：时间的准确度问题。这一算法有一个默认前提：分布式环境下时间获取总是准确的，即时间总是递增的。而现实环境中，这样的条件很难满足。总会因为硬件、软件、人的原因造成时间变化。如果你的硬件时间本身就比正常时间快，而你接入了一个 NTP 服务，每当进行 NTP 时间校准时，你的机器时间总会向后 回拨 一段时间，这时悲剧就来了：有极大可能性生成重复ID。 针对上面提到的两个问题，可如下改进： 时间戳由毫秒变为秒 使用环形列表对时间戳对应的序列进行缓存 使用 CAS 操作避免大粒度悲观锁 为了 缓解 时钟回拨问题，对之前的序列进行缓存，而原生算法很显然是不利于缓存的，最坏的情况下每秒需要缓存 1000 个值，这显然对内存很不友好。于是我将时间戳改为秒为单位，同时可以把省出来的位交给序列。此时缓存一个小时的数据（即可以容忍一个小时的时钟回拨）也就只需要缓存 3600 个序列，完全可以接受。改进后的 Snowflake 生成的ID是这样组成的： |–1位符号位–|--32位时间戳–|--10位机器ID–|--21位序列号–| 环形列表：即整个列表的容量是一定的，当列表满了以后再加入的元素会按照入列的先后顺序覆盖之前的元素。 "},"zother2-interview/basic/database/mysql/_index.html":{"url":"zother2-interview/basic/database/mysql/_index.html","title":"Index","keywords":"","body":""},"zother2-interview/basic/database/redis/":{"url":"zother2-interview/basic/database/redis/","title":"Redis","keywords":"","body":"Redis 线程模型 Redis 在处理网络请求是使用单线程模型，并通过 IO 多路复用来提高并发。但是在其他模块，比如：持久化，会使用多个线程。 Redis 内部使用文件事件处理器 file event handler，这个文件事件处理器是单线程的，所以 Redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 socket ，将产生事件的 socket 压入内存队列中，事件分派器根据 socket 上的事件类型来选择对应的事件处理器进行处理。 文件事件处理器的结构包含 4 个部分： 多个 socket IO 多路复用程序 文件事件分派器 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器） 多个 socket 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 socket ，会将产生事件的 socket 放入队列中排队，事件分派器每次从队列中取出一个 socket ，根据 socket 的事件类型交给对应的事件处理器进行处理。 客户端与 Redis 的一次通信过程： 为啥 Redis 单线程模型也能效率这么高？ 纯内存操作 核心是基于非阻塞的 IO 多路复用机制 单线程反而避免了多线程的频繁上下文切换问题 持久化 RDB RDB 持久化机制，是对 Redis 中的数据执行周期性的持久化。 RDB 会生成多个数据文件，每个数据文件都代表了某一个时刻中 Redis 的数据，非常适合做冷备 RDB 对 Redis 对外提供的读写服务，影响非常小，可以让 Redis 保持高性能，因为 Redis 主进程只需要 fork 一个子进程，让子进程执行磁盘 IO 操作来进行 RDB 持久化即可。 相对于 AOF 持久化机制来说，直接基于 RDB 数据文件来重启和恢复 Redis 进程，更加快速。 一般来说，RDB 数据快照文件，都是每隔 5 分钟，或者更长时间生成一次，这个时候就得接受一旦 Redis 进程宕机，那么会丢失最近 5 分钟的数据。 RDB 每次在 fork 子进程来执行 RDB 快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒。 AOF AOF 机制对每条写入命令作为日志，以 append-only 的模式写入一个日志文件中，在 Redis 重启的时候，可以通过回放 AOF 日志中的写入指令来重新构建整个数据集 AOF 可以更好的保护数据不丢失，一般 AOF 会每隔 1 秒，通过一个后台线程执行一次 fsync 操作，最多丢失 1 秒钟的数据。 AOF 日志文件以 append-only 模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修复。 AOF 日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在 rewrite log 的时候，会对其中的指令进行压缩，创建出一份需要恢复数据的最小日志出来。在创建新日志文件的时候，老的日志文件还是照常写入。当新的 merge 后的日志文件 ready 的时候，再交换新老日志文件即可。 AOF 日志文件的命令通过非常可读的方式进行记录，这个特性非常 适合做灾难性的误删除的紧急恢复。比如某人不小心用 flushall 命令清空了所有数据，只要这个时候后台 rewrite 还没有发生，那么就可以立即拷贝 AOF 文件，将最后一条 flushall 命令给删了，然后再将该 AOF 文件放回去，就可以通过恢复机制，自动恢复所有数据。 对于同一份数据来说，AOF 日志文件通常比 RDB 数据快照文件更大。 AOF 开启后，支持的写 QPS 会比 RDB 支持的写 QPS 低，因为 AOF 一般会配置成每秒 fsync 一次日志文件，当然，每秒一次 fsync ，性能也还是很高的。（如果实时写入，那么 QPS 会大降，Redis 性能会大大降低） 以前 AOF 发生过 bug，就是通过 AOF 记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似 AOF 这种较为复杂的基于命令日志 / merge / 回放的方式，比基于 RDB 每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有 bug。不过 AOF 就是为了避免 rewrite 过程导致的 bug，因此每次 rewrite 并不是基于旧的指令日志进行 merge 的，而是基于当时内存中的数据进行指令的重新构建，这样健壮性会好很多。 RDB 和 AOF 到底该如何选择 不要仅仅使用 RDB，因为那样会导致你丢失很多数据； 也不要仅仅使用 AOF，因为那样有两个问题：第一，你通过 AOF 做冷备，没有 RDB 做冷备来的恢复速度更快；第二，RDB 每次简单粗暴生成数据快照，更加健壮，可以避免 AOF 这种复杂的备份和恢复机制的 bug； Redis 支持同时开启开启两种持久化方式，我们可以综合使用 AOF 和 RDB 两种持久化机制，用 AOF 来保证数据不丢失，作为数据恢复的第一选择; 用 RDB 来做不同程度的冷备，在 AOF 文件都丢失或损坏不可用的时候，还可以使用 RDB 来进行快速的数据恢复。 一致性哈希算法 一致哈希 是一种特殊的哈希算法。在使用一致哈希算法后，哈希表槽位数（大小）的改变平均只需要对 K/n 个关键字重新映射，其中 K 是关键字的数量，n 是槽位数量。然而在传统的哈希表中，添加或删除一个槽位的几乎需要对所有关键字进行重新映射。 一致哈希也可用于实现健壮缓存来减少大型 Web 应用中系统部分失效带来的负面影响 需求 在使用 n 台缓存服务器时，一种常用的负载均衡方式是，对资源 o 的请求使用 hash(o)= o mod n 来映射到某一台缓存服务器。当增加或减少一台缓存服务器时这种方式可能会改变所有资源对应的 hash 值，也就是所有的缓存都失效了，这会使得缓存服务器大量集中地向原始内容服务器更新缓存。 因此需要一致哈希算法来避免这样的问题。 一致哈希尽可能使同一个资源映射到同一台缓存服务器。这种方式要求增加一台缓存服务器时，新的服务器尽量分担存储其他所有服务器的缓存资源。减少一台缓存服务器时，其他所有服务器也可以尽量分担存储它的缓存资源。 一致哈希算法的主要思想是将每个缓存服务器与一个或多个哈希值域区间关联起来，其中区间边界通过计算缓存服务器对应的哈希值来决定。如果一个缓存服务器被移除，则它所对应的区间会被并入到邻近的区间，其他的缓存服务器不需要任何改变。 实现 一致哈希将每个对象映射到圆环边上的一个点，系统再将可用的节点机器映射到圆环的不同位置。查找某个对象对应的机器时，需要用一致哈希算法计算得到对象对应圆环边上位置，沿着圆环边上查找直到遇到某个节点机器，这台机器即为对象应该保存的位置。 当删除一台节点机器时，这台机器上保存的所有对象都要移动到下一台机器。添加一台机器到圆环边上某个点时，这个点的下一台机器需要将这个节点前对应的对象移动到新机器上。更改对象在节点机器上的分布可以通过调整节点机器的位置来实现。 实践 假设有1000w个数据项，100个存储节点，请设计一种算法合理地将他们存储在这些节点上。 看一看普通Hash算法的原理： for item in range(ITEMS): k = md5(str(item)).digest() h = unpack_from(\">I\", k)[0] # 通过取余的方式进行映射 n = h % NODES node_stat[n] += 1 普通的Hash算法均匀地将这些数据项打散到了这些节点上，并且分布最少和最多的存储节点数据项数目小于 1%。之所以分布均匀，主要是依赖 Hash 算法（实现使用的MD5算法）能够比较随机的分布。 然而，我们看看存在一个问题，由于 该算法使用节点数取余的方法，强依赖 node 的数目，因此，当是 node 数发生变化的时候，item 所对应的 node 发生剧烈变化，而发生变化的成本就是我们需要在 node 数发生变化的时候，数据需要迁移，这对存储产品来说显然是不能忍的。 一致性哈希 普通 Hash 算法的劣势，即当 node 数发生变化（增加、移除）后，数据项会被重新“打散”，导致大部分数据项不能落到原来的节点上，从而导致大量数据需要迁移。 那么，一个亟待解决的问题就变成了：当 node 数发生变化时，如何保证尽量少引起迁移呢？即当增加或者删除节点时，对于大多数 item ，保证原来分配到的某个 node ，现在仍然应该分配到那个 node ，将数据迁移量的降到最低。 for n in range(NODES): h = _hash(n) ring.append(h) ring.sort() hash2node[h] = n for item in range(ITEMS): h = _hash(item) n = bisect_left(ring, h) % NODES node_stat[hash2node[ring[n]]] += 1 虽然一致性Hash算法解决了节点变化导致的数据迁移问题，但是，数据项分布的均匀性很差。 主要是因为这 100 个节点 Hash 后，在环上分布不均匀，导致了每个节点实际占据环上的区间大小不一造成的。 改进 -- 虚节点 当我们将 node 进行哈希后，这些值并没有均匀地落在环上，因此，最终会导致，这些节点所管辖的范围并不均匀，最终导致了数据分布的不均匀。 for n in range(NODES): for v in range(VNODES): h = _hash(str(n) + str(v)) # 构造ring ring.append(h) # 记录hash所对应节点 hash2node[h] = n ring.sort() for item in range(ITEMS): h = _hash(str(item)) # 搜索ring上最近的hash n = bisect_left(ring, h) % (NODES*VNODES) node_stat[hash2node[ring[n]]] += 1 通过增加虚节点的方法，使得每个节点在环上所“管辖”更加均匀。这样就既保证了在节点变化时，尽可能小的影响数据分布的变化，而同时又保证了数据分布的均匀。也就是靠增加“节点数量”加强管辖区间的均匀。 集群 主从复制 单机的 Redis ，能够承载的 QPS 大概就在上万到几万不等。对于缓存来说，一般都是用来支撑读高并发的。因此架构做成 主从(Master-Slave)架构 ，一主多从，主负责写，并且将数据复制到其它的 Slave 节点，从节点负责读。所有的读请求全部走从节点。这样也可以很轻松实现水平扩容，支撑读高并发。 Redis 默认采用异步方式复制数据到 Slave Node，同时 Slave Node 会周期性地确认自己每次复制的数据量： 当 Master 和 Slave 网络连接顺畅时，Master 会持续向 Slave 推送命令，以保持在 Master 数据集合上执行的：客户端写、Key 过期、Key 淘汰等均在 Slave 数据集合上执行。 当 Master 和 Slave 网络连接由于网络问题、超时等中断时， Slave 会尝试重连并进行连接断开期间的命令 部分同步（partial resynchronization）。 当部分同步不可用时，Slave 会请求全量同步。在这个过程中，Master 会创建当前所有数据的镜像，发送给 Slave 并继续推送命令。 Redis 主从复制包含以下几个要点： 一个 Master 可以有多个 Slave Slave 支持级联结构，即 Slave 可以连接到其他 Slave 上 Redis 在复制过程中，不阻塞 Master ，不论是全量同步还是部分同步 在大部分时间里，复制也不会阻塞 Slave 。当 Slave 在进行初始化同步时，Slave 会先使用旧的数据集提供服务。但当初始化同步完成时，会删除旧数据集，这时 Slave 会拒绝服务。 Redis 主从复制可以用来做水平扩容，以提供读写分离，或作为数据备份和高可用 在主从复制的情况下，可以通过配置避免数据持久化，将 Slave 作为数据的备份或开启 Slave 的 AOF。但是这种情况下也会有风险：当 Master 重启后数据集将清空，这时如果 Slave 同步 Master 就会导致数据也被清空 当 Master 不进行持久化如何保证数据安全 在生产环境中，强烈建议开启 Redis 持久化，不论是在 Master 还是在 Slave。如果由于磁盘速度等问题，不能开启持久化，那么需要 避免 Redis 进程的自动重启。 哨兵 Sentinel 是 Redis 官方推荐的 高可用性( HA )解决方案，当用 Redis 做主从复制的高可用方案时，假如 Master 宕机了， Redis 本身都没有实现自动进行主备切换，而哨兵本身也是一个独立运行的进程，它能监控多个节点，发现 Master 宕机后能进行自动切换。 它的主要功能有以下几点 集群监控：负责监控 Redis Master 和 Slave 进程是否正常工作。 消息通知：如果某个 Redis 实例有故障，那么哨兵负责发送消息作为报警通知给管理员。 故障转移：如果 Master node 挂掉了，会自动转移到 Slave node 上。 配置中心：如果故障转移发生了，通知 client 客户端新的 Master 地址。 哨兵的核心知识 哨兵至少需要 3 个实例，来保证自己的健壮性。 哨兵 + Redis 主从的部署架构，是 不保证数据零丢失 的，只能保证 Redis 集群的高可用性。 对于哨兵 + Redis 主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练。 哨兵的个数与集群节点个数无关，每个哨兵都会 Check 所有节点 当启用哨兵后，客户端的连接是通过哨兵连接到 Node 的 哨兵集群必须部署 2 个以上节点，如果哨兵集群仅仅部署了 2 个哨兵实例，Quorum = 1。 +----+ +----+ | M1 |---------| R1 | | S1 | | S2 | +----+ +----+ 如果 Master 宕机， S1 和 S2 中只要有 1 个哨兵认为 Master 宕机了，就可以进行切换，同时 S1 和 S2 会选举出一个哨兵来执行故障转移。但是同时这个时候，需要 Majority ，也就是超过半数的哨兵都是运行的。 如果此时仅仅是 M1 进程宕机了，哨兵 s1 正常运行，那么故障转移是 OK 的。但是如果是整个 M1 和 S1 运行的机器宕机了，那么哨兵只有 1 个，此时就没有 Majority 来允许执行故障转移，虽然另外一台机器上还有一个 R1，但是故障转移不会执行。 经典的 3 节点哨兵集群是这样的： +----+ | M1 | | S1 | +----+ | +----+ | +----+ | R2 |----+----| R3 | | S2 | | S3 | +----+ +----+ 配置 Quorum=2，如果 M1 所在机器宕机了，那么三个哨兵还剩下 2 个， S2 和 S3 可以一致认为 Master 宕机了，然后选举出一个来执行故障转移，同时 3 个哨兵的 Majority 是 2，所以还剩下的 2 个哨兵运行着，就可以允许执行故障转移。 Slave 选主算法 如果一个 Master 被认为宕机，而且 Majority 数量的哨兵都允许主备切换，那么某个哨兵就会执行主备切换操作，此时首先要选举一个 Slave 来，会考虑 Slave 的一些信息： 跟 Master 断开连接的时长 Slave 优先级 复制 offset run id 接下来会对 Slave 进行排序： 按照 Slave 优先级进行排序，Slave Priority 越低，优先级就越高。 如果 Slave Priority 相同，那么看 Replica Offset，哪个 Slave 复制了越多的数据，Offset 越靠后，优先级就越高。 如果上面两个条件都相同，那么选择一个 run id 比较小的那个 Slave。 Redis Cluster Redis Cluster 是一种服务器 Sharding 技术，提供内置的高可用支持，部分 master 不可用时，还可以继续工作。Redis Cluster 功能强大，直接集成了 主从复制 和 哨兵 的功能。 高性能：在 Cluster 集群中没有代理，主从之间使用异步复制，并且不会对 Key 进行合并操作； 可接受的写入安全：当客户端连接到 majority master 时集群尽最大努力保留所有客户端的写操作。通常情况下，在一小段窗口时间内写请求会被丢失，当客户端连接到 minority master 时这个窗口时间会很大； 可用性：当 Redis Cluster 中大部分 master 是可达的，并且不可达 master 均有一个可用的 slave 时，Redis Cluster 能够在 NODE_TIMEOUT 时间后进行故障转移，使 Cluster 重新可用。此外，Cluster 还提供 副本迁移（replicas migration），当 master 没有 slave 时，可从其他 master 下重新分配一个 slave ； majority master：能与大多数 master 连通的 master minority master：未能与大多数 master 连通的 master 内部节点通信 在 Cluster 架构下，每个 Redis 都需要开启额外的端口来进行节点间通信，这种机制被称之为 Cluster Bus。 Redis 维护集群元数据采用 gossip 协议，所有节点都持有一份元数据，不同的节点如果出现了元数据的变更，就不断将元数据发送给其它的节点，让其它节点也进行元数据的变更。 gossip 好处在于，元数据的更新比较分散，不是集中在一个地方，更新请求会陆陆续续打到所有节点上去更新，降低了压力；不好在于，元数据的更新有延时，可能导致集群中的一些操作会有一些滞后。 寻址算法 Redis Cluster 有固定的 16384 个 Hash Slot，对每个 key 计算 CRC16 值，然后对 16384 取模，可以获取 key 对应的 Hash Slot。Redis Cluster 中每个 Master 都会持有部分 Slot，Slot 的分配在 Cluster 未进行重配置（reconfiguration）时是稳定的。当 Cluster 稳定时，一个 Hash Slot 只在一个 master 上提供服务。不过一个 master 会有一个或多个 slave ，以在发生网络分区或故障时，替换 master。这些 slave 还可以缓解 master 的读请求的压力。 重配置：Hash Slot 从一个节点转移到另一个节点 Keys hash tags 可以破坏上述的分配规则，Hash tags 是一种保证多个键被分配到同一个槽位的方法。 重定向 Redis Cluster 为了提高性能，不会提供代理，而是使用重定向的方式让 client 连接到正确的节点。 MOVED Redis 客户端可以向集群的任意一个节点发送查询请求，节点接收到请求后会对其进行解析，如果是操作单个 key 的命令或者是包含多个在相同槽位 key 的命令，那么该节点就会去查找这个 key 是属于哪个槽位的。如果 key 所属的槽位由该节点提供服务，那么就直接返回结果。否则就会返回一个 MOVED 错误： GET x -MOVED 3999 127.0.0.1:6381 这个错误包括了对应的 key 属于哪个槽位（3999）以及该槽位所在的节点的 IP 地址和端口号。client 收到这个错误信息后，就将这些信息存储起来以便可以更准确的找到正确的节点。 当客户端收到 MOVED 错误后，可以使用 CLUSTER NODES 或 CLUSTER SLOTS 命令来更新整个集群的信息，因为当重定向发生时，很少会是单个槽位的变更，一般都会是多个槽位一起更新。因此，在收到 MOVED 错误时，客户端应该尽早更新集群的分布信息。当集群达到稳定状态时，客户端保存的槽位和节点的对应信息都是正确的，cluster 的性能也会达到非常高效的状态。 ASK 对于 Redis Cluster 来讲， MOVED 重定向意味着请求的 slot 永久的由另一个节点提供服务，而 ASK 重定向仅代表将当前查询重定向到指定节点，不影响后续查询。在 Redis Cluster 迁移的时候会用到 ASK 重定向，下面看下 ASK 的处理流程： Client 向节点 A 查询数据 \bx，A 发现数据 x 所在的 slot 状态为 MIGRATING，如果 x 存在则返回，否则返回 ASK 重定向； Client 向 ASK 重定向节点 B 发送 ASKING ，再查询数据 x； B 查找 x 发现其所在 slot 状态为 IMPORTING，则 B 会进行查询。若第二步未发送 ASKING ，则 B 会返回 MOVED命令，重定向到 A； Redis Cluster 的迁移是以槽位单位的，一个槽位从节点 A 迁移到节点 B 需要经过以下步骤： 节点 A 将待迁移 slot 设置为 MIGRATING 状态，将 B 节点 slot 设置为 IMPORTING 状态 A 获取 slot 中的 key，逐个调用 MIGRATE 命令 MIGRATE 会将特定的 key 从 A 迁移到 B，这个过程是原子操作（A、B均会进行加锁） 容错能力 Redis Cluster和大多数集群一样，是通过心跳来判断一个节点是否存活的。心跳包的内容可以分为 header 和 gossip 消息两部分，其中header包含以下信息： NODE ID 节点在集群中的唯一标识 currentEpoch 和 configEpoch 字段 node flag，标识节点是 master 还是 slave ，另外还有一些其他的标识位 节点提供服务的 hash slot 的 bitmap 发送者的 TCP 端口 发送者认为的集群状态（down or ok） 如果是slave，则包含 master 的 NODE ID gossip包含了该节点认为的其他节点的状态，不过不是集群的全部节点。具体有以下信息： NODE ID 节点的IP和端口 NODE flags 故障检测 故障检测用于识别集群中的不可达节点是否已下线，如果一个 master 下线，则会将它的 slave提 升为master。如果无法提升，则集群会处于错误状态。在 gossip 消息中，NODE flags 的值包括两种 PFAIL 和 FAIL。 如果一个节点发现另外一个节点不可达的时间超过 NODE_TIMEOUT ，则会将这个节点标记为 PFAIL，也就是 Possible failure。 PFAIL 标志只是一个节点本地的信息，为了使 slave 提升为 master ，需要将 PFAIL 升级为 FAIL 。当集群中大部分节点都将某个节点标记为 PFAIL 时，则可升级为 FAIL。 FAIL 状态是单向的，只能从 PFAIL 升级为 FAIL ，当节点重新可达时，可清除 FAIL 标记。 数据结构 Redis 的数据结构包含两个层面，首先是 API 层面，即 Redis Client 操作的数据结构。另外就是 Redis 在实现 API 层面的数据结构使用的底层数据结构。 Redis API 层面的数据结构主要包括：String、List、Set、Sorted Set、Hash、BitMap，这些数据结构在 Redis 官方文档中有详细介绍。 下面我们主要介绍 Redis 底层数据结构，包括 SDS、dict、ziplist、quicklist、skiplist。 SDS Redis 没有直接使用 C 语言传统的字符串表示（以空字符结尾的字符数组，以下简称 C 字符串）， 而是自己构建了一种名为 简单动态字符串（simple dynamic string，SDS）的抽象类型， 并将 SDS 用作 Redis 的默认字符串表示。 在 Redis 里面， C 字符串只会作为字符串字面量（string literal）， 用在一些无须对字符串值进行修改的地方， 比如打印日志。 当 Redis 需要的不仅仅是一个字符串字面量， 而是一个可以被修改的字符串值时， Redis 就会使用 SDS 来表示字符串值： 比如在 Redis 的数据库里面， 包含字符串值的键值对在底层都是由 SDS 实现的。 C字符串 SDS 获取字符串长度的复杂度为 O(N) 。 获取字符串长度的复杂度为 O(1) 。 API 是不安全的，可能会造成缓冲区溢出。 API 是安全的，不会造成缓冲区溢出。 修改字符串长度 N 次必然需要执行 N 次内存重分配。 修改字符串长度 N 次最多需要执行 N 次内存重分配。 只能保存文本数据。 可以保存文本或者二进制数据。 可以使用所有 库中的函数。 可以使用一部分 库中的函数。 缓冲区溢出 因为 C 字符串不记录自身的长度， 所以 strcat 假定用户在执行这个函数时， 已经为 dest 分配了足够多的内存， 可以容纳 src 字符串中的所有内容， 而一旦这个假定不成立时， 就会产生缓冲区溢出。 举个例子， 假设程序里有两个在内存中紧邻着的 C 字符串 s1 和 s2 ， 其中 s1 保存了字符串 \"Redis\" ， 而 s2 则保存了字符串 \"MongoDB\" ， 如图所示。 如果一个程序员决定通过执行： strcat(s1, \" Cluster\"); 将 s1 的内容修改为 \"Redis Cluster\" ， 但粗心的他却忘了在执行 strcat 之前为 s1 分配足够的空间， 那么在 strcat 函数执行之后， s1 的数据将溢出到 s2 所在的空间中， 导致 s2 保存的内容被意外地修改， 如图所示。 与 C 字符串不同， SDS 的空间分配策略完全杜绝了发生缓冲区溢出的可能性： 当 SDS API 需要对 SDS 进行修改时， API 会先检查 SDS 的空间是否满足修改所需的要求， 如果不满足的话， API 会自动将 SDS 的空间扩展至执行修改所需的大小， 然后才执行实际的修改操作， 所以使用 SDS 既不需要手动修改 SDS 的空间大小， 也不会出现前面所说的缓冲区溢出问题。 减少修改字符串时带来的内存重分配次数 空间预分配：解决 append 问题 惰性空间释放：解决 strim 问题 二进制安全 C 字符串中的字符必须符合某种编码（比如 ASCII）， 并且 除了字符串的末尾之外， 字符串里面不能包含空字符， 否则最先被程序读入的空字符将被误认为是字符串结尾 —— 这些限制使得 C 字符串只能保存文本数据， 而不能保存像图片、音频、视频、压缩文件这样的二进制数据。 dict 在 Redis 中， dict 也是一个基于哈希表的算法。和传统的哈希算法类似，它采用哈希函数从 key 计算得到在哈希表中的位置，采用 拉链法 解决冲突，并在装载因子（load factor）超过预定值时自动扩展内存，引发重哈希（rehashing）。 Redis 的 dict 实现最显著的一个特点，就在于它的重哈希。它采用了一种称为 增量式重哈希（incremental rehashing） 的方法，在需要扩展内存时避免一次性对所有 key 进行重哈希，而是将重哈希操作分散到对于 dict 的各个增删改查的操作中去。这种方法能做到每次只对一小部分 key 进行重哈希，而每次重哈希之间不影响 dict 的操作。 dict 之所以这样设计，是为了避免重哈希期间单个请求的响应时间剧烈增加。 为了实现增量式重哈希（incremental rehashing），dict的数据结构里包含 两个哈希表。在重哈希期间，数据从一个哈希表向另一个哈希表迁移。 ziplist ziplist 是一个经过特殊编码的 双向链表，它的设计目标就是为了提高存储效率。 ziplist 可以用于存储字符串或整数，其中整数是按真正的二进制表示进行编码的，而不是编码成字符串序列。它能以 O(1) 的时间复杂度在表的两端提供 push 和 pop 操作。 一个普通的双向链表，链表中每一项都占用独立的一块内存，各项之间用地址指针（或引用）连接起来。这种方式会带来大量的内存碎片，而且地址指针也会占用额外的内存。而 ziplist 却是将表中每一项存放在前后 连续的地址空间 内，一个 ziplist 整体占用一大块内存。它是一个表（list），但其实不是一个链表（linked list）。 另外，ziplist 为了在细节上节省内存，对于值的存储采用了 变长编码方式，大概意思是说，对于大的整数，就多用一些字节来存储，而对于小的整数，就少用一些字节来存储。ziplist 的底层结构如下所示： zlbytes: 32bit，表示 ziplist 占用的字节总数。 zltail: 32bit，表示 ziplist 表中最后一项（entry）在 ziplist 中的偏移字节数。 zllen: 16bit， 表示 ziplist 中数据项（entry）的个数。 zllen 可以表达的最大值为 2^{16}-1。当 ziplist 长度超过 2^{16}-1时， zllen 不表示长度，长度需要进行遍历计算。 entry: 表示真正存放数据的数据项，长度不定。一个数据项（entry）也有它自己的内部结构。 zlend: ziplist 最后1个字节，是一个结束标记，值固定等于 255。 当ziplist变得很大的时候，它有如下几个缺点： 每次插入或修改引发的 realloc 操作会有更大的概率造成内存拷贝，从而降低性能。 一旦发生内存拷贝，内存拷贝的成本也相应增加，因为要拷贝更大的一块数据。 当 ziplist 数据项过多的时候，在它上面查找指定的数据项就会性能变得很低，因为 ziplist 上的查找需要进行遍历。 总之， ziplist 本来就设计为各个数据项挨在一起组成连续的内存空间，这种结构并不擅长做修改操作。一旦数据发生改动，就会引发内存realloc，可能导致内存拷贝。 quicklist quicklist 是由 ziplist 为节点组成的双向链表。 ziplist 本身也是一个能维持数据项先后顺序的列表（按插入位置），而且是一个内存紧缩的列表（各个数据项在内存上前后相邻）。比如，一个包含 3 个节点的 quicklist ，如果每个节点的 ziplist 又包含 4 个数据项，那么对外表现上，这个 list 就总共包含 12 个数据项。 quicklist 的结构为什么这样设计呢？总结起来，大概又是一个空间和时间的折中： 双向链表便于在表的两端进行 push 和 pop 操作，但是它的内存开销比较大。首先，它在每个节点上除了要保存数据之外，还要额外保存两个指针；其次，双向链表的各个节点是单独的内存块，地址不连续，节点多了容易产生内存碎片。 ziplist 由于是一整块连续内存，所以存储效率很高。但是 不利于修改操作，每次数据变动都会引发一次内存的 realloc （扩容）。特别是当 ziplist 长度很长的时候，一次 realloc 可能会导致大批量的数据拷贝，进一步降低性能。 quicklist 节点上的 ziplist 要保持一个合理的长度。那到底多长合理呢？这可能取决于具体应用场景。实际上，Redis提供了一个配置参数list-max-ziplist-size ，就是为了让使用者可以来根据自己的情况进行调整。 skiplist 跳跃表（skiplist） 是一种有序数据结构，它通过在每个节点中维持多个指向其他节点的指针，从而达到快速访问节点的目的。 Redis 只在两个地方用到了跳跃表， 一个是实现有序集合键， 另一个是在集群节点中用作内部数据结构， 除此之外， 跳跃表在 Redis 里面没有其他用途。 intset intset 是一个由整数组成的 有序集合，从而便于在上面进行二分查找，用于快速地判断一个元素是否属于这个集合。它在内存分配上与 ziplist 有些类似，是连续的一整块内存空间，而且对于大整数和小整数（按绝对值）采取了不同的编码，尽量对内存的使用进行了优化。 对于小集合使用 intset 来存储，主要的原因是节省内存。特别是当存储的元素个数较少的时候， dict 所带来的内存开销要大得多（包含两个哈希表、链表指针以及大量的其它元数据）。所以，当存储大量的小集合而且集合元素都是数字的时候，用 intset 能节省下一笔可观的内存空间。 实际上，从时间复杂度上比较， intset 的平均情况是没有 dict 性能高的。以查找为例，intset 是 O(\\lg^n) 的，而 dict 可以认为是 O(1) 的。但是，由于使用 intset 的时候集合元素个数比较少，所以这个影响不大。 API数据结构的实现 API数据结构 限制 底层数据结构 string 512 MB SDS list 最大长度 2^{32}-1 quicklist set 最大容量 2^{32}-1 - intset（小整数集） - dict sort set 最大容量 2^{32}-1 - ziplist（小集合） - dict + skiplist hash 最大KV容量 2^{32}-1 - ziplist（小集合） - dict bitmap 512 MB SDS 缓存穿透、缓存击穿、缓存雪崩 缓存穿透 访问一个不存在的key，缓存不起作用，请求会穿透到 DB，流量大时 DB 会挂掉。 解决方案 采用布隆过滤器，使用一个足够大的bitmap，用于存储可能访问的 key，不存在的key直接被过滤； 访问key未在DB查询到值，也将空值写进缓存，但可以设置较短过期时间。 缓存雪崩 大量的 key 设置了相同的过期时间，导致在缓存在同一时刻全部失效，造成瞬时DB请求量大、压力骤增，引起雪崩。 解决方案 可以给缓存设置过期时间时加上一个随机值时间，使得每个 key 的过期时间分布开来，不会集中在同一时刻失效。 缓存击穿 对于一些设置了过期时间的key，如果这些key可能会在某些时间点被超高并发地访问，是一种非常“热点”的数据。这个时候，需要考虑一个问题：缓存被“击穿”的问题，这个和缓存雪崩的区别在于这里针对某一 key 缓存，前者则是很多key。 缓存在某个时间点过期的时候，恰好在这个时间点对这个Key有大量的并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端 DB 压垮。 解决方案 在缓存失效的时候（判断拿出来的值为空），不是立即去 load db ，而是先使用缓存工具的某些带成功操作返回值的操作（比如Redis的 SETNX）去 set 一个 mutex key ，当操作返回成功时，再进行 load db 的操作并回设缓存；否则，就重试整个 get 缓存的方法。 数据淘汰机制 对象过期 Redis回收过期对象的策略：定期删除+惰性删除 惰性删除：当读/写一个已经过期的key时，会触发惰性删除策略，直接删除掉这个过期key 定期删除：由于惰性删除策略无法保证冷数据被及时删掉，所以Redis会定期主动淘汰一批已过期的key 内存淘汰 Redis提供了下面几种淘汰策略供用户选择，其中默认的策略为noeviction策略： noeviction：当内存使用达到阈值的时候，所有引起申请内存的命令会报错。 allkeys-lru：在主键空间中，优先移除最近未使用的key。 volatile-lru：在设置了过期时间的键空间中，优先移除最近未使用的key。 allkeys-random：在主键空间中，随机移除某个key。 volatile-random：在设置了过期时间的键空间中，随机移除某个key。 volatile-ttl：在设置了过期时间的键空间中，具有更早过期时间的key优先移除。 这里补充一下主键空间和设置了过期时间的键空间，举个例子，假设我们有一批键存储在Redis中，则有那么一个哈希表用于存储这批键及其值，如果这批键中有一部分设置了过期时间，那么这批键还会被存储到另外一个哈希表中，这个哈希表中的值对应的是键被设置的过期时间。设置了过期时间的键空间为主键空间的子集。 非精准的LRU 上面提到的LRU（Least Recently Used）策略，实际上 Redis 实现的 LRU 并不是可靠的 LRU，也就是名义上我们使用LRU算法淘汰键，但是实际上被淘汰的键并不一定是真正的最久没用的，这里涉及到一个权衡的问题，如果需要在全部键空间内搜索最优解，则必然会增加系统的开销，Redis是单线程的，也就是同一个实例在每一个时刻只能服务于一个客户端，所以耗时的操作一定要谨慎。 为了在一定成本内实现相对的LRU，早期的 Redis 版本是 基于采样的 LRU ，也就是放弃全部键空间内搜索解改为采样空间搜索最优解。自从 Redis3.0 版本之后，Redis 作者对于基于采样的 LRU 进行了一些优化，目的是在一定的成本内让结果更靠近真实的 LRU。 参考链接 Redis内部数据结构详解(1)——dict "},"zother2-interview/basic/database/sql/":{"url":"zother2-interview/basic/database/sql/","title":"SQL","keywords":"","body":"SQL 连接 在 MySQL 中 JOIN、 CROSS JOIN 、 INNER JOIN 是等价的，都是内连接。 内连接 JOIN、 CROSS JOIN 、 INNER JOIN 当使用这三个子句时，其结果都是笛卡尔积； 如果以上三个子句加上 ON，则为 等值连接：只会返回 ON 子句相等的结果 外连接 外连接分为 LEFT JOIN 、 RIGHT JOIN 和 NATURAL JOIN，所有外连接均可省略 OUTER 关键字，即 LEFT OUTER JOIN...ON... 与 LEFT JOIN...ON...等效。 T1 LEFT JOIN T2 ON T1.id=T2.id：左外连接，返回所有列、T1 所有行、T2 中 条件符合的行 T1 RIGHT JOIN T2 ON T1.id=T2.id：右外连接，返回所有列、T2 所有行、T1 中 条件符合的行 T1 NATURAL JOIN T2：自然连接，返回 T1 所有行、T2 中与 T1 匹配的行，相同的属性被合并 对于自然连接要多做说明，现在有表 join_test1 、join_test2： mysql> select * from join_test1; +----+------+ | id | name | +----+------+ | 1 | A | | 2 | B | | 3 | C | +----+------+ 3 rows in set (0.00 sec) mysql> select * from join_test2; +----+------+ | id | sex | +----+------+ | 1 | 男 | | 2 | 女 | | 4 | 男 | +----+------+ 3 rows in set (0.00 sec) 自然连接的结果： mysql> select * from join_test1 NATURAL join join_test2; +----+------+------+ | id | name | sex | +----+------+------+ | 1 | A | 男 | | 2 | B | 女 | +----+------+------+ 上面三种子句，又可组合出 NATURAL LEFT|RIGHT JOIN...：这种子句就结合了 NATURAL JOIN 和 LEFT|RIGHT JOIN..ON... 的特点：在执行左|右连接的同时，将相同属性合并。 mysql> select * from join_test1 NATURAL left join join_test2; +----+------+------+ | id | name | sex | +----+------+------+ | 1 | A | 男 | | 2 | B | 女 | | 3 | C | NULL | +----+------+------+ 3 rows in set (0.00 sec) 子查询 子查询作为标量 SELECT (SELECT s2 FROM t1); 子查询比较 non_subquery_operand comparison_operator (subquery) = > = != LIKE ANY IN SOME operand comparison_operator ANY (subquery) operand IN (subquery) operand comparison_operator SOME (subquery) EXISTS or NOT EXISTS SELECT column1 FROM t1 WHERE EXISTS (SELECT * FROM t2); ALL operand comparison_operator ALL (subquery) SELECT s1 FROM t1 WHERE s1 > ALL (SELECT s1 FROM t2); 关联查询 SELECT * FROM t1 WHERE column1 = ANY (SELECT column1 FROM t2 WHERE t2.column2 = t1.column2); select Score, (select count(distinct Score) from Scores b where b.Score>= s.Score) Rank from Scores s order by Score desc 派生表 SELECT ... FROM (subquery) [AS] tbl_name ... "},"zother2-interview/basic/net/http/":{"url":"zother2-interview/basic/net/http/","title":"HTTP","keywords":"","body":"HTTP HTTP构建于TCP/IP协议之上，默认端口号是80。 HTTP是 无连接无状态 的。 无连接的含义是 限制每次连接只处理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接。后来使用了Keep-Alive技术。 无状态是指 协议对于事务处理没有记忆能力，服务器不知道客户端是什么状态。即我们给服务器发送 HTTP 请求之后，服务器根据请求，会给我们发送数据过来，但是，发送完，不会记录任何信息。 HTTP 协议这种特性有优点也有缺点，优点在于解放了服务器，每一次请求“点到为止”不会造成不必要连接占用，缺点在于每次请求会传输大量重复的内容信息。 为了解决HTTP无状态的缺点，两种用于保持 HTTP 连接状态的技术就应运而生了，一个是 Cookie，而另一个则是 Session。Cookie在客户端记录状态，比如登录状态。Session在服务器记录状态。 Http的报文结构 HTTP 请求报文头部 User-Agent：产生请求的浏览器类型。 Accept：客户端可识别的响应内容类型列表; Accept-Language：客户端可接受的自然语言; Accept-Encoding：客户端可接受的编码压缩格式; Accept-Charset：可接受的应答的字符集; Host：请求的主机名，允许多个域名同处一个IP 地址，即虚拟主机;（必选） Connection：连接方式(close 或 keep-alive); Cookie：存储于客户端扩展字段，向同一域名的服务端发送属于该域的cookie; 请求包体：在POST方法中使用。 Referer：包含一个URL，用户从该URL代表的页面出发访问当前请求的页面。 If-Modified-Since：文档的最后改动时间 HTTP 响应头 Allow 服务器支持哪些请求方法（如GET、POST等）。 Content-Encoding 文档的编码（Encode）方法。 Content-Length 表示内容长度。只有当浏览器使用持久HTTP连接时才需要这个数据。 Content-Type 表示后面的文档属于什么MIME类型。 Date 当前的GMT时间。你可以用setDateHeader来设置这个头以避免转换时间格式的麻烦。 Expires 应该在什么时候认为文档已经过期，从而不再缓存它。 Last-Modified 文档的最后改动时间。 Refresh 表示浏览器应该在多少时间之后刷新文档，以秒计。 Server 服务器名字。 Set-Cookie 设置和页面关联的Cookie。 ETag：被请求变量的实体值。ETag是一个可以与Web资源关联的记号（MD5值）。 Cache-Control：这个字段用于指定所有缓存机制在整个请求/响应链中必须服从的指令。 max-age：表示当访问此网页后的 x 秒内再次访问不会去服务器；no-cache，实际上Cache-Control: no-cache是会被缓存的，只不过每次在向客户端（浏览器）提供响应数据时，缓存都要向服务器评估缓存响应的有效性；no-store，这个才是响应不被缓存的意思； Last-Modified与If-Modified-Since都是用来记录页面的最后修改时间。当客户端访问页面时，服务器会将页面最后修改时间通过 Last-Modified 标识由服务器发往客户端，客户端记录修改时间，再次请求本地存在的cache页面时，客户端会通过 If-Modified-Since 头将先前服务器端发过来的最后修改时间戳发送回去，服务器端通过这个时间戳判断客户端的页面是否是最新的，如果不是最新的，则返回新的内容，如果是最新的，则返回 304。 Http的状态码含义。 1** 信息，服务器收到请求，需要请求者继续执行操作 2** 成功，操作被成功接收并处理 3** 重定向，需要进一步的操作以完成请求 301 Moved Permanently。请求的资源已被永久的移动到新URI，返回信息会包括新的URI，浏览器会自动定向到新URI。今后任何新的请求都应使用新的URI代替 302 Moved Temporarily。与301类似。但资源只是临时被移动。客户端应继续使用原有URI 304 Not Modified。所请求的资源未修改，服务器返回此状态码时，不会返回任何资源。客户端通常会缓存访问过的资源，通过提供一个头信息指出客户端希望只返回在指定日期之后修改的资源。 4** 客户端错误，请求包含语法错误或无法完成请求 400 Bad Request 由于客户端请求有语法错误，不能被服务器所理解。 401 Unauthorized 请求未经授权。这个状态代码必须和WWW-Authenticate报头域一起使用 403 Forbidden 服务器收到请求，但是拒绝提供服务。服务器通常会在响应正文中给出不提供服务的原因 404 Not Found 请求的资源不存在，例如，输入了错误的URL 5** 服务器错误，服务器在处理请求的过程中发生了错误 500 Internal Server Error 服务器发生不可预期的错误，导致无法完成客户端的请求。 503 Service Unavailable 服务器当前不能够处理客户端的请求，在一段时间之后，服务器可能会恢复正常。 Http request的几种类型。 GET 请求指定的页面信息，并返回实体主体。 POST 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。 PUT 从客户端向服务器传送的数据取代指定的文档的内容。 DELETE 请求服务器删除指定的页面。 GET可提交的数据量受到URL长度的限制，HTTP协议规范没有对URL长度进行限制。这个限制是特定的浏览器及服务器对它的限制 理论上讲，POST是没有大小限制的，HTTP协议规范也没有进行大小限制，出于安全考虑，服务器软件在实现时会做一定限制 条件 GET HTTP条件GET 是 HTTP 协议为了减少不必要的带宽浪费，提出的一种方案。实际上就是利用If-Modified-Since做浏览器缓存。 持久连接 我们知道 HTTP 协议采用请求-应答模式，当使用普通模式，即非 Keep-Alive 模式时，每个请求/应答客户和服务器都要新建一个连接，完成之后立即断开连接（HTTP协议为无连接的协议）；当使用 Keep-Alive 模式（又称持久连接、连接重用）时，Keep-Alive 功能使客户端到服务器端的连接持续有效，当出现对服务器的后继请求时，Keep-Alive 功能避免了建立或者重新建立连接。 在 HTTP 1.0 中, 没有官方的 keep alive 的操作。通常是在现有协议上添加一个指数。如果浏览器支持 keep-alive，它会在请求的包头中添加： Connection: Keep-Alive 然后当服务器收到请求，作出回应的时候，它也添加一个头在响应中： Connection: Keep-Alive 这样做，连接就不会中断（超过 Keep-Alive 规定的时间--服务器设置，意外断电等情况除外），而是保持连接。当客户端发送另一个请求时，它会使用同一个连接。这一直继续到客户端或服务器端认为会话已经结束，其中一方中断连接。 在 HTTP 1.1 版本中，默认情况下所有连接都被保持，如果加入 \"Connection: close\" 才关闭。 HTTP Keep-Alive 简单说就是保持当前的TCP连接，避免了重新建立连接。 HTTP 长连接不可能一直保持，例如 Keep-Alive: timeout=5, max=100，表示这个TCP通道可以保持5秒，max=100，表示这个长连接最多接收100次请求就断开。 HTTP是一个无状态协议，这意味着每个请求都是独立的，Keep-Alive没能改变这个结果。另外，Keep-Alive也不能保证客户端和服务器之间的连接一定是活跃的，在HTTP1.1版本中也如此。唯一能保证的就是当连接被关闭时你能得到一个通知，所以不应该让程序依赖于Keep-Alive的保持连接特性，否则会有意想不到的后果。 使用长连接之后，客户端、服务端怎么知道本次传输结束呢？两部分：1. 判断传输数据是否达到了Content-Length 指示的大小；2. 动态生成的文件没有 Content-Length ，它是分块传输（chunked），这时候就要根据 chunked 编码来判断，chunked 编码的数据在最后有一个空 chunked 块，表明本次传输数据结束。 跨站攻击 CSRF（Cross-site request forgery，跨站请求伪造）伪造请求，冒充用户在站内的正常操作，比如爬虫。 防范的方法 关键操作只接受POST请求 验证码 检测 Referer Token Token 要足够随机——只有这样才算不可预测 Token 是一次性的，即每次请求成功后要更新Token——这样可以增加攻击难度，增加预测难度 Token 要注意保密性——敏感操作使用 post，防止 Token 出现在 URL 中 断点续传 要实现断点续传的功能，通常都需要客户端记录下当前的下载进度，并在需要续传的时候通知服务端本次需要下载的内容片段。 HTTP1.1协议中定义了断点续传相关的HTTP头 Range 和 Content-Range 字段，一个最简单的断点续传实现大概如下： 客户端下载一个1024K的文件，已经下载了其中512K 网络中断，客户端请求续传，因此需要在HTTP头中申明本次需要续传的片段：Range:bytes=512000-，这个头通知服务端从文件的512K位置开始传输文件。 服务端收到断点续传请求，从文件的512K位置开始传输，并且在HTTP头中增加：Content-Range:bytes 512000-/1024000，并且此时服务端返回的HTTP状态码应该是206，而不是200。 但是在实际场景中，会出现一种情况，即在终端发起续传请求时，URL对应的文件内容在服务端已经发生变化，此时续传的数据肯定是错误的。如何解决这个问题了？显然此时我们需要有一个标识文件唯一性的方法。在RFC2616中也有相应的定义，比如 实现Last-Modified来标识文件的最后修改时间，这样即可判断出续传文件时是否已经发生过改动。同时RFC2616中还定义有一个ETag的头，可以使用ETag头来放置文件的唯一标识，比如文件的MD5值。 客户端在发起续传请求时应该在HTTP头中申明If-Match 或者 If-Modified-Since 字段，帮助服务端判别文件变化。 一次HTTP请求 域名解析 浏览器缓存 系统缓存 hosts ISP DNS 缓存 DNS 服务器搜索 浏览器发送 HTTP 请求到目标服务器 服务器永久重定向 浏览器跟踪重定向地址 服务器“处理”请求 服务器发回一个HTML响应 浏览器开始显示HTML 浏览器请求获取嵌入在 HTML 中的对象（图片&脚本等） 浏览器发送异步（AJAX）请求 "},"zother2-interview/basic/net/https/":{"url":"zother2-interview/basic/net/https/","title":"HTTPS","keywords":"","body":"HTTPS HTTPS 是一种通过计算机网络进行安全通信的传输协议。HTTPS经由HTTP进行通信，但利用 SSL/TLS 来加密数据包。 HTTPS 开发的主要目的，是提供对网站服务器的身份认证，保护交换数据的隐私与完整性。 HTTPS 的主要思想是在不安全的网络上创建一安全信道，并可在使用适当的加密包和服务器证书可被验证且可被信任时，对窃听和中间人攻击提供合理的防护。HTTPS的信任继承基于预先安装在浏览器中的证书颁发机构（如Symantec、Comodo、GoDaddy和GlobalSign等）（意即“我信任证书颁发机构告诉我应该信任的”） HTTP 为什么不安全 http 协议属于 明文传输协议 ，交互过程以及数据传输都没有进行加密，通信双方也没有进行任何认证，通信过程非常容易遭遇劫持、监听、篡改，严重情况下，会造成恶意的流量劫持等问题，甚至造成个人隐私泄露（比如银行卡卡号和密码泄露）等严重的安全问题。 比如常见的，在 http 通信过程中，“中间人”将广告链接嵌入到服务器发给用户的 http 报文里，导致用户界面出现很多不良链接； 或者是修改用户的请求头 URL ，导致用户的请求被劫持到另外一个网站，用户的请求永远到不了真正的服务器。这些都会导致用户得不到正确的服务，甚至是损失惨重。 HTTPS 如何保证安全 数字证书 TLS 握手的作用之一是 身份认证（authentication） ，被验证的一方需要提供一个身份证明，在 HTTPS 的世界里，这个身份证明就是 TLS 证书 ，或者称为 HTTPS 证书。 世界上的 CA 机构会遵守 X.509 规范来签发公钥证书（Public Key Certificate），证书内容的语法格式遵守 ASN.1，证书大致包含如下内容： Certificate: Data: Version: 3 (0x2) //版本号 Serial Number: //证书序列号 0e:3c:c1:49:94:b3:e1:74:a6:34:54:d9:90:64:66:d7 Signature Algorithm: sha256WithRSAEncryption //签名算法 Issuer: C=US, O=DigiCert Inc, OU=www.digicert.com, CN=GeoTrust RSA CA 2018 //签发机构 Validity //有效期 Not Before: Dec 25 00:00:00 2017 GMT Not After : Dec 24 12:00:00 2020 GMT Subject: C=CN, L=北京市, O=智者四海（北京）技术有限公司, OU=IT, CN=*.zhihu.com //证书主体 Subject Public Key Info: Public Key Algorithm: rsaEncryption //公钥算法 Public-Key: (2048 bit) Modulus: 00:a0:a8:71:... //公钥 Exponent: 65537 (0x10001) X509v3 extensions: //扩展信息 X509v3 Authority Key Identifier: keyid:90:58:FF:B0:9C:75:A8:51:54:77:B1:ED:F2:A3:43:16:38:9E:6C:C5 //授权密钥标识 X509v3 Subject Key Identifier: 31:63:1F:A1:0B:43:D7:A5:8C:3D:F6:2E:85:69:D4:E1:E3:56:91:46 //主体密钥标识 X509v3 Subject Alternative Name: DNS:*.zhihu.com, DNS:zhihu.com X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 CRL Distribution Points: Full Name: URI:http://cdp.geotrust.com/GeoTrustRSACA2018.crl X509v3 Certificate Policies: Policy: 2.16.840.1.114412.1.1 CPS: https://www.digicert.com/CPS Policy: 2.23.140.1.2.2 Authority Information Access: OCSP - URI:http://status.geotrust.com CA Issuers - URI:http://cacerts.geotrust.com/GeoTrustRSACA2018.crt X509v3 Basic Constraints: CA:FALSE Signature Algorithm: sha256WithRSAEncryption //签名算法 54:73:e6:02:... //数字签名 同一个CA颁发的证书序列号都必须是唯一的。 证书链 证书链是从终端用户证书后跟着一系列的 CA 证书，例如：CA_ZHIHU -> CA_GEO -> CA_ROOT，而通常 最后一个是自签名证书（根证书），并且有如下关系： A -> B 表示 \"A是由B签发的\" （更确切地说，A是由B中所载公钥对应的私钥签署的） 在证书链上除根证书外，证书颁发者等于其后一个证书的主题。即：CA_ZHIHU.Authority Key Identifier=CA_GEO.Subject Key Identifier 除了最后一个证书，每个证书都是由其后的一个证书签名的。即：CA_ZHIHU 由 CA_GEO 签名，CA_GEO 由 CA_ROOT 签名 最后的证书是信任主题，由于是通过可信过程得到的，你可以信任它，一般为系统内置。 证书链用于检查目标证书（证书链里的第一个证书）里的公钥及其它数据是否属于其主题。检查是这么做的，用证书链中的下一个证书的公钥来验证它的签名，一直检查到证书链的尾端，如果所有验证都成功通过，那个这个证书就是可信的。 证书认证 数字签名其实就是把 散列值 经过非对称加密算法加密得到的一个 加密的散列值 。数字签名一般用于身份认证和防止抵赖。 根认证机构的构建 根认证机构 CA 生成公钥 ca_KeyPub 和私钥 ca_KeyPri ，以及基本信息表 ca_Info （CSR）。ca_Info 中一般包含了 CA 的名称、证书的有效期等信息。 根认证机构 CA 对 ca_KeyPub + ca_Info 进行散列运算，得到散列值 ca_Hash 。 根认证机构 CA 使用其私钥 ca_KeyPri 对 ca_Hash 进行非对称加密，得到加密的散列值 enc_ca_Hash 。 根认证机构 CA 将 ca_KeyPub + ca_Info + enc_ca_Hash 组合生成自签名的数字证书 ca_Cert 。这张证书称之为根证书。 ca_Cert 可用于签署下一级的证书。 二级（或以上）认证机构的构建 二级认证机构 CA2 生成公钥 ca2_KeyPub 和私钥 ca2_KeyPri ，以及基本信息表 ca2_Info 。 ca2_Info 中一般包含了 CA2 的名称、证书要求的有效期等信息。 二级认证机构 CA2 将 ca2_KeyPub 、ca2_Info 送给根认证机构 CA 。 根认证机构 CA 通过某种方式验证 CA2 的身份之后，再加上根认证机构自己的一些信息 ca_Info ，然后对它们 ca2_KeyPub + ca2_Info + ca_Info 进行散列运算，得到散列值 ca2_Hash 。 根认证机构 CA 使用其私钥 ca_KeyPri 对 ca2_Hash 进行非对称加密，得到加密的散列值 enc_ca2_Hash 。 根认证机构 CA 将 ca2_KeyPub + ca2_Info + ca_Info + enc_ca2_Hash 组合签署成数字证书 ca2_Cert 并回送给 CA2 。 ca2_Cert 可用于签署下一级的证书。 二级（或以上）认证机构的证书签署 服务器 S2 生成公钥 s2_KeyPub 和私钥 s2_KeyPri ，以及基本信息表 s2_Info 。 s2_Info 中一般包含了 S2 的名称、证书要求的有效期等信息。 服务器 S2 将 s2_KeyPub 、 s2_Info 送给二级认证机构 CA2。 二级认证机构 CA2 通过某种方式验证 S2 的身份之后，再加上根认证机构自己的一些信息 ca2_Info ，然后对它们 s2_KeyPub + s2_Info + ca2_Info 进行散列运算，得到散列值 s2_Hash 。 二级认证机构 CA2 使用其私钥 ca2_KeyPri 对 s2_Hash 进行非对称加密，得到加密的散列值 enc_s2_Hash 。 二级认证机构 CA2 将 s2_KeyPub + s2_Info + ca2_Info + enc_s2_Hash 组合签署成数字证书 s2_Cert 并回送给 S2 。 s2_Cert 不可用于签署下一级的证书。 openssl ca 的 -extensions 参数控制，生成 s2_Cert 时是使用参数 server_cert 生成，所以不具备签署的能力 从上面可以看出，证书签署的流程是： ca_Cert -> ca2_Cert -> s2_Cert 。它是一条完整的链条，我们把它称之为 证书链 。 二级（或以上）认证机构的验证 服务器 S2 下发证书 s2_Cert 、 ca2_Cert （证书链）给客户端 C 。 客户端 C 检查到 s2_Cert 中的 ca2_Info ，发现它是由 CA2 签署的。 客户端 C 取出 ca2_Cert 中的 ca2_KeyPub ，对 s2_Cert 中的 enc_s2_Hash 进行解密得到 s2_Hash 。 客户端 C 对 s2_Cert 中的 s2_KeyPub + s2_Info + ca2_Info 进行散列运算，得到散列值 s2_Hash_tmp。 客户端 C 判断 s2_Hash 和 s2_Hash_tmp 是否相等。如果两者相等，则证明 s2_Cert 是由 ca2_Cert 签署的。 客户端 C 检查到 ca2_Cert 中的 ca_Info ，发现它是由 CA 签署的。 客户端 C 取出 ca_Cert 中的 ca_KeyPub ，对 ca2_Cert 中的 enc_ca2_Hash 进行解密得到 ca2_Hash 。 客户端 C 对 ca2_Cert 中的 ca2_KeyPub + ca2_Info + ca_Info 进行散列运算，得到散列值 ca2_Hash_tmp 。 客户端 C 判断 ca2_Hash 和 ca2_Hash_tmp 是否相等。如果两者相等，证明 ca2_Cert 是由 ca_Cert 签署的。 客户端 C 检查 ca_Cert ，发现该证书是根证书，且已经被系统信任，身份验证通过。 无 SNI 支持问题 很多公司由于业务众多，域名也是相当多的，为了方便运维，会让很多域名指向同样的 ip，然后统一将流量/请求分发到后端，此时就会面临一个问题：由于 TLS/SSL 在 HTTP 层之下，客户端和服务器握手的时候还拿不到 origin 字段，所以服务器不知道这个请求是从哪个域名过来的，而服务器这边每个域名都对应着一个证书，服务器就不知道该返回哪个证书啦。这个问题有两个通用解决方案： 使用 VIP 服务器，每个域名对应一个 VIP，然后 VIP 与统一接入服务对接，通过 ip 来分发证书，不过运维成本很高，可能也需要大量的 VIP 服务器 采用 多泛域名，将多个泛域名证书打包进一个证书。它的缺点是每次添加域名都需要更新证书。 证书选择 证书有多张加密方式，不同的加密方式对 CPU 计算的损耗不同，安全级别也不同。TLS 在进行第一次握手的时候，客户端会向服务器端 say hello，这个时候会告诉服务器，它支持哪些算法，此时 服务器可以将最适合的证书发给客户端。 证书的吊销 CA 证书的吊销存在两种机制，一种是 在线检查（OCSP），客户端向 CA 机构发送请求检查公钥的靠谱性；第二种是客户端储存一份 CA 提供的 证书吊销列表（CRL），定期更新。前者要求查询服务器具备良好性能，后者要求每次更新提供下次更新的时间，一般时差在几天。安全性要求高的网站建议采用第一种方案。 大部分 CA 并不会提供吊销机制（CRL/OCSP），靠谱的方案是 为根证书提供中间证书，一旦中间证书的私钥泄漏或者证书过期，可以直接吊销中间证书并给用户颁发新的证书。中间证书还可以产生下一级中间证书，多级证书可以减少根证书的管理负担。 SSL/TLS协议 不使用SSL/TLS的HTTP通信，就是不加密的通信。所有信息明文传播，带来了三大风险。 窃听风险（eavesdropping）：第三方可以获知通信内容。 篡改风险（tampering）：第三方可以修改通信内容。 冒充风险（pretending）：第三方可以冒充他人身份参与通信。 SSL/TLS协议是为了解决这三大风险而设计的，希望达到： 所有信息都是加密传播，第三方无法窃听。 具有校验机制，一旦被篡改，通信双方会立刻发现。 配备身份证书，防止身份被冒充。 目前，应用最广泛的是 TLS 1.0，接下来是SSL 3.0。但是，主流浏览器都已经实现了 TLS 1.2 的支持。TLS 1.0通常被标示为SSL 3.1，TLS 1.1为SSL 3.2，TLS 1.2为SSL 3.3。 TLS 运行过程 SSL/TLS协议的基本思路是采用 公钥加密法，也就是说，客户端先向服务器端索要公钥，然后用公钥加密信息，服务器收到密文后，用自己的私钥解密。因此，SSL/TLS协议的基本过程是这样的： 客户端向服务器端索要并验证公钥。 双方协商生成\"对话密钥\"。 双方采用\"对话密钥\"进行加密通信。 \"握手阶段\"涉及四次通信，我们一个个来看。需要注意的是，\"握手阶段\"的所有通信都是明文的。 客户端发出请求（ClientHello） 首先，客户端（通常是浏览器）先向服务器发出加密通信的请求，这被叫做 ClientHello 请求。 在这一步，客户端主要向服务器提供以下信息。 支持的协议版本，比如TLS 1.0版。 一个客户端生成的随机数，稍后用于生成对话密钥。 支持的加密方法，比如RSA公钥加密。 支持的压缩方法。 这里需要注意的是，客户端发送的信息之中不包括服务器的域名。也就是说，理论上服务器只能包含一个网站，否则会分不清应该向客户端提供哪一个网站的数字证书。这就是为什么通常一台服务器只能有一张数字证书的原因。 对于虚拟主机的用户来说，这当然很不方便。2006年，TLS协议加入了一个 Server Name Indication 扩展，允许客户端向服务器提供它所请求的域名。 服务器回应（SeverHello） 服务器收到客户端请求后，向客户端发出回应，这叫做 SeverHello 。服务器的回应包含以下内容。 确认使用的加密通信协议版本，比如TLS 1.0版本。如果浏览器与服务器支持的版本不一致，服务器关闭加密通信。 一个服务器生成的随机数，稍后用于生成对话密钥。 确认使用的加密方法，比如 RSA 公钥加密。 服务器证书。 除了上面这些信息，如果服务器需要确认客户端的身份，就会再包含一项请求，要求客户端提供 \"客户端证书\"。比如，金融机构往往只允许认证客户连入自己的网络，就会向正式客户提供 USB 密钥，里面就包含了一张客户端证书。 客户端回应 客户端收到服务器回应以后，首先验证服务器证书。如果证书不是可信机构颁布、或者证书中的域名与实际域名不一致、或者证书已经过期，就会向访问者显示一个警告，由其选择是否还要继续通信。 如果证书没有问题，客户端就会从证书中取出服务器的公钥。然后，向服务器发送加密信息，包含下面三项信息。 一个随机数。该随机数用服务器公钥加密，防止被窃听。 编码改变通知，表示随后的信息都将用双方商定的加密方法和密钥发送。 客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时也是前面发送的所有内容的hash值，用来供服务器校验。 上面第一项的随机数，是整个握手阶段出现的第三个随机数，又称 pre-master key 。有了它以后，客户端和服务器就同时有了三个随机数，接着双方就用事先商定的加密方法，各自生成本次会话所用的同一把\"会话密钥\"。 至于 为什么一定要用三个随机数，来生成\"会话密钥\"： 不管是客户端还是服务器，都需要随机数，这样生成的密钥才不会每次都一样。由于SSL协议中证书是静态的，因此十分有必要引入一种随机因素来保证协商出来的密钥的随机性。 对于 RSA 密钥交换算法来说，pre-master-key本身就是一个随机数，再加上 hello 消息中的随机，三个随机数通过一个密钥导出器最终导出一个对称密钥。 pre master 的存在在于 SSL 协议不信任每个主机都能产生完全随机的随机数，如果随机数不随机，那么 pre master secret（对称密钥） 就有可能被猜出来，那么仅适用 pre master secret 作为密钥就不合适了，因此必须引入新的随机因素，那么客户端和服务器三个随机数一同生成的密钥就不容易被猜出了，一个伪随机可能完全不随机，可是是三个伪随机就十分接近随机了，每增加一个自由度，随机性增加的可不是一个量级。 此外，如果前一步，服务器要求客户端证书，客户端会在这一步发送证书及相关信息。 服务器的最后回应 服务器收到客户端的第三个随机数 pre-master key 之后，计算生成本次会话所用的\"会话密钥\"。然后，向客户端最后发送下面信息。 编码改变通知，表示随后的信息都将用双方商定的加密方法和密钥发送。 服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时也是前面发送的所有内容的 hash 值，用来供客户端校验。 至此，整个握手阶段全部结束。接下来，客户端与服务器进入加密通信，就完全是使用普通的HTTP协议，只不过用\"会话密钥\"加密内容。 HTTPS 的七个误解 HTTPS无法缓存？：许多人以为，出于安全考虑，浏览器不会在本地保存HTTPS缓存。实际上，只要在HTTP头中使用特定命令，HTTPS是可以缓存的。 SSL证书很贵？：如果你在网上搜一下，就会发现很多便宜的SSL证书，大概10美元一年，这和一个 .com 域名的年费差不多。而且事实上，还能找到免费的 SSL 证书。 HTTPS站点必须有独享的IP地址？使用子域名通配符SSL证书（wildcard SSL certificate，价格大约是每年125美元），就能在一个IP地址上部署多个HTTPS子域名。 转移服务器时要购买新证书？ HTTPS太慢？：使用HTTPS不会使你的网站变得更快（实际上有可能，请看下文），但是有一些技巧可以大大减少额外开销。 有了HTTPS，Cookie和查询字符串就安全了？：虽然无法直接从HTTPS数据中读取Cookie和查询字符串，但是你仍然需要使它们的值变得难以预测。 只有注册登录页，才需要HTTPS？：这种想法很普遍。人们觉得，HTTPS可以保护用户的密码，此外就不需要了。Firefox浏览器新插件Firesheep，证明了这种想法是错的。我们可以看到，在Twitter和Facebook上，劫持其他人的session是非常容易的。 中间人攻击（MITM） TLS对中间人攻击的抵御 当然正常情况下，我们的网络安全肯定不会这么脆弱。得利于TLS证书体系，虽然我们能发起中间人攻击，不过浏览器察觉到了证书的异常。这是因为我们冒充了目标网站，但是并没有目标网站的证书，这样浏览器在校验证书时很容易发现证书错误。 无法抵御中间人攻击的实例 部分开发者忽视证书校验，或对证书异常处理不当，导致本来十分有效LTS失去原本的防御能力。有许多APP存在类似的问题，包括个别金融类应用，还有部分APP部分模块的流量存在被劫持的风险。 参考链接 HTTPS 精读之 TLS 证书校验 细说 CA 和证书 HTTPS中间人攻击实践（原理·实践） "},"zother2-interview/basic/net/ip/":{"url":"zother2-interview/basic/net/ip/","title":"IP","keywords":"","body":"IP 地址分类 A类：8位网络号，0_ _ _ _ _ _ _，1.0.0.0 ~ 126.0.0.0 B类：16位网络号，10 _ _ ...，128.0.0.0 ~ 191.255.255.255 C类：24位网络号，110_ _ _...，192.0.0.0 ~ 223.255.255.255 D类：多播地址，1110_ _ _... E类：保留地址，1111_ _ _ ... 私有地址 A类:10.0.0.0 ~ 10.255.255.255(长度相当于1个A类IP地址) B类:172.16.0.0 ~ 172.31.255.255(长度相当于16个连续的B类IP地址) C类:192.168.0.0 ~ 192.168.255.255(长度相当于256个连续的C类IP地址) 特殊的IP地址 0.0.0.0：已经不是一个真正意义上的IP地址。它表示的是这样一个集合：所有不清楚的主机和目的网络。这里的“不清楚”是指在本机的路由表里没有特定条目指明如何到达。如果在网络设置中设置了缺省网关,那么系统会自动产生一个目的地址为0.0.0.0的缺省路由.对本机来说,它就是一个“收容所”,所有不认识的“三无”人员,一 律送进去。 255.255.255.255： 限制广播地址，对本机来说,这个地址指本网段内(同一广播域)的所有主机。这个地址不能被路由器转发。 127.0.0.1：本机地址主要用于测试。这样一个地址,是不能把它发到网络接口的。 "},"zother2-interview/basic/net/protocol/":{"url":"zother2-interview/basic/net/protocol/","title":"网络协议","keywords":"","body":"底层网络协议 ARP（地址解析协议） 基本功能为透过目标设备的IP地址，查询目标设备的MAC地址，以保证通信的顺利进行。在每台安装有TCP/IP协议的电脑或路由器里都有一个ARP缓存表，表里的IP地址与MAC地址是一对应的。 当发送数据时，主机A会在自己的ARP缓存表中寻找是否有目标IP地址。如果找到就知道目标MAC地址为（00-BB-00-62-C2-02），直接把目标MAC地址写入帧里面发送就可；如果在ARP缓存表中没有找到相对应的IP地址，主机A就会在网络上发送一个 广播（ARP request），目标MAC地址是“FF.FF.FF.FF.FF.FF”，这表示向同一网段内的所有主机发出这样的询问：“192.168.38.11的MAC地址是什么？”网络上其他主机并不响应ARP询问，只有主机B接收到这个帧时，才向主机A做出这样的回应（ARP response）：“192.168.38.11的MAC地址是（00-BB-00-62-C2-02）”。这样，主机A就知道主机B的MAC地址，它就可以向主机B发送信息。同时它还更新自己的ARP缓存表，下次再向主机B发送信息时，直接从ARP缓存表里查找就可。ARP缓存表采用老化机制，在一段时间内如果表中的某一行没有使用，就会被删除，这样可以大大减少ARP缓存表的长度，加快查询速度。 当发送主机和目的主机不在同一个局域网中时，即便知道目的主机的MAC地址，两者也不能直接通信，必须经过路由转发才可以。所以此时，发送主机通过ARP协议获得的将不是目的主机的真实MAC地址，而是一台可以通往局域网外的路由器的MAC地址。于是此后发送主机发往目的主机的所有帧，都将发往该路由器，通过它向外发送。这种情况称为ARP代理（ARP Proxy）。 ICMP（互联网控制消息协议） 它 用于TCP/IP网络中发送控制消息，提供可能发生在通信环境中的各种问题反馈，通过这些信息，令管理者可以对所发生的问题作出诊断，然后采取适当的措施解决。它与传输协议最大的不同：它一般不用于在两点间传输数据，而常常 用于返回的错误信息或是分析路由。 ICMP控制的内容包括但不仅限于：echo响应（ping）、目标网络不可达、目标端口不可达、禁止访问的网络、拥塞控制、重定向、TTL超时... 路由选择协议 路由选择协议分为：静态的和动态的。Internet中使用的是动态路由选择协议，在Internet的概念中，将整个互联网划分为许多个小的自治系统（AS）。AS的最主要的特征：一个AS对其他AS表现出的是一个单一 和一致的路由选择策略。 由于AS的存在，路由选择协议又分为两种： 内部网关协议（IGP）：即在一个AS内部使用的路由选择协议，而这与互联网中其他AS选用什么路由协议无关。比如：OSPF 外部网关协议（EGP）：若源主机和目的主机不再同一个AS中，就需要使用一种协议将路由选择信息传递到另一个AS中，这就是EGP。比如：BGP。 OSPF（开放式最短路径优先） OSPF属于内部网关协议（IGP）的一种，使用Dijkstra提出的最短路径算法。 OSPF提出了“区域（Area）”的概念，一个网络可以由单一区域或者多个区域组成。其中，一个特别的区域被称为骨干区域（Backbone Area），该区域是整个OSPF网络的核心区域，并且所有其他的区域都与之直接连接。所有的内部路由都通过骨干区域传递到其他非骨干区域。所有的区域都必须直接连接到骨干区域，如果不能创建直接连接，那么可以通过虚拟链路（Virtual-link）和骨干区域创建虚拟连接。 划分区域的优点： 将洪泛法的范围限制在一个区域中。 减少每个区域内部路由信息交换的通信量。 OSPF使用的是分布式链路状态协议，使用 洪泛法向该路由器所有的相邻路由器发送信息。最终整个区域的所有路由器都得到一个这个信息的副本。这个副本就是 链路状态数据库（LSDB）用来保存当前网络拓扑结构，路由器上属于同一区域的链路状态数据库是相同的（属于多个区域的路由器会为每个区域维护一份链路状态数据库）。 OSPF使用 “代价（Cost）”作为路由度量。 只有当链路发生变化时才会更新信息。 如果同一个目的网络有多条路径，OSPF协议可以进行 负载均衡。 BGP（边界网关协议） 由于BGP是工作在AS之间的协议，并且各个AS的情况复杂，所以 BGP只是力求找到一个可以到达目的网络且比较好的路由，而并不是寻找一条最佳路由。每一个AS都应该有一个“BGP发言人“，一般来说，两个BGP发言人是通过一个共享网络连接在一起的，BGP发言人往往是BGP边界路由，但也可以不是。 一个BGP发言人与其他AS的BGP发言人要交换路由信息，首先要建立TCP连接，然后在此连接上交换BGP报文以建立BGP会话。当BGP发言人交换了路由信息后，就构造自治系统连通图，最后通过该图来进行路由选择。 DHCP（动态主机设置协议） DHCP是一个局域网的网络协议，使用UDP协议工作，主要有两个用途： 用于内部网络或网络服务供应商自动分配IP地址给用户 用于内部网络管理员作为对所有电脑作中央管理的手段 动态主机设置协议（DHCP）是一种使网络管理员能够集中管理和自动分配IP网络地址的通信协议。在IP网络中，每个连接Internet的设备都需要分配唯一的IP地址。DHCP使网络管理员能从中心结点监控和分配IP地址。当某台计算机移到网络中的其它位置时，能自动收到新的IP地址。 DHCP使用了 租约 的概念，或称为计算机IP地址的有效期。租用时间是不定的，主要取决于用户在某地连接Internet需要多久，这对于教育行业和其它用户频繁改变的环境是很实用的。通过较短的租期，DHCP能够在一个计算机比可用IP地址多的环境中动态地重新配置网络。DHCP支持为计算机分配静态地址，如需要永久性IP地址的Web服务器。 NAT（地址转换协议） NAT是一种 在IP封包通过路由器或防火墙时重写来源IP地址或目的IP地址的技术。这种技术被普遍使用在有多台主机但只通过一个公有IP地址访问因特网的私有网络中。 "},"zother2-interview/basic/net/tcp/":{"url":"zother2-interview/basic/net/tcp/","title":"TCP","keywords":"","body":"TCP TCP概述 TCP的特点 TCP是面向连接的传输层协议。 TCP连接是点对点的（套接字--IP:Port到套接字）。 TCP提供可靠交付的服务。 TCP提供全双工通信。 面向字节流。 TCP与UDP的区别 TCP UDP 是否连接 面向连接 面向非连接 传输可靠性 可靠 不可靠 应用场合 传输大量数据 少量数据 速度 慢 快 TCP报文结构 源端口、目的端口：16位长。标识出远端和本地的端口号。 序列号（seq）：32位长 如果含有同步标识（SYN），则此为最初的序列号；第一个数据比特的序列码为本序列号加一 如果没有同步标识（SYN），则此为第一个数据比特的序列码 确认号（ack）：32位长。希望收到的下一个数据报的序列号，表明到序列号 N-1 为止的所有数据已经正确收到。 TCP协议数据报头长：4位长。表明TCP头中包含多少个 4字节 保留：置0 ACK：期望收到的数据的开始序列号。也即已经收到的数据的字节长度加1 PSH：表示是带有PUSH标志的数据。接收方因此请求数据报一到便可送往应用程序而不必等到缓冲区装满时才传送。 RST：用于复位由于主机崩溃或其它原因而出现的错误的连接。还可以用于拒绝非法的数据报或拒绝连接请求。 SYN：用于建立连接。 FIN：用于释放连接。 窗口大小（WIN）：16位长。表示从确认号开始，本报文的发送方（数据发送端 or 数据接收端）可以接收的字节数，即接收窗口大小。用于流量控制。 校验和（Checksum）：16位长。是为了确保高可靠性而设置的。它校验头部、数据和伪TCP头部之和。 紧急指针：URG=1时才有意义。 可选项：长度可变，最长40个字节。每个选项的开始是 1 字节的 kind 字段，说明选项的类型。 0：选项表结束（1字节） 1：无操作（1字节）用于选项字段之间的字边界对齐 2：MMS 最大报文段长度，通常在创建连接而设置 SYN 标志的数据包中指明这个选项，指明本端所能接收的最大长度的报文段。通常将 MSS 设置为（MTU-40）字节，携带 TCP 报文段的 IP 数据报的长度就不会超过 MTU（MTU最大长度为1518字节，最短为64字节），从而避免本机发生IP分片。只能出现在同步报文段中，否则将被忽略。 3：窗口扩大因子（4字节，wscale），取值 0-14 。用来把 TCP 的窗口的值左移的位数，使窗口值乘倍。只能出现在同步报文段中，否则将被忽略。 4：sackOK 发送端支持并同意使用SACK选项。 5：SACK 选择确认选项 8：时间戳 计算 RTT；用于处理TCP序号超过 2^32 的情况，又称为防止序号回绕（PAWS）。 发送端的时间戳（Timestamp） 时间戳回显应答（Timestamp Echo） TCP最小长度为 20 个字节。 三次握手 第一次握手：建立连接时，客户端发送 SYN 包（seq=j）到服务器，并进入SYN_SENT状态，等待服务器确认。 第二次握手：服务器收到 SYN 包，必须确认客户的 SYN（ack=j+1），同时自己也发送一个 SYN 包（seq=k），即 SYN + ACK 包，此时服务器进入 SYN_RECV 状态； 第三次握手：客户端收到服务器的 SYN+ACK 包，向服务器发送确认包 ACK（ack=k+1），此包发送完毕，客户端和服务器进入ESTABLISHED（TCP连接成功）状态，完成三次握手。 TCP 连接使用三次握手的首要原因：为了 阻止历史的重复连接初始化造成的混乱问题。如果通信双方的通信次数只有两次，那么发送方一旦发出建立连接的请求之后它就没有办法撤回这一次请求，如果在网络状况复杂或者较差的网络中，发送方连续发送多次建立连接的请求，如果 TCP 建立连接只能通信两次，那么接收方只能选择接受或者拒绝发送方发起的请求，它并不清楚这一次请求是不是由于网络拥堵而早早过期的连接。 所以，TCP 选择使用三次握手来建立连接并在连接引入了 RST 这一控制消息，接收方当收到请求时会将发送方发来的 SEQ+1 发送给对方，这时由发送方来判断当前连接是否是历史连接： 如果当前连接是历史连接，即 SEQ 过期或者超时，那么发送方就会直接发送 RST 控制消息中止这一次连接； 如果当前连接不是历史连接，那么发送方就会发送 ACK 控制消息，通信双方就会成功建立连接； 使用三次握手和 RST 控制消息将是否建立连接的最终控制权交给了发送方，因为只有发送方有足够的上下文来判断当前连接是否是错误的或者过期的，这也是 TCP 使用三次握手建立连接的最主要原因。 内核对 TCP 的处理 Socket 是一个由 （源IP、源Port、目标IP、目标Port、协议） 组成的五元组，唯一标示一个 socket 连接。 TCP 建立连接的整体流程： 服务器端在调用 listen 之后，内核会建立两个队列，SYN队列和ACCEPT队列，其中ACCPET队列的长度由backlog指定。 服务器端在调用 accpet 之后，将阻塞，等待 ACCPT 队列有元素。 客户端在调用 connect 之后，将开始发起 SYN 请求，请求与服务器建立连接，此时称为第一次握手。 服务器端在接受到 SYN 请求之后，把请求方放入 SYN 队列中，并给客户端回复一个确认帧 ACK ，此帧还会携带一个请求与客户端建立连接的请求标志，也就是 SYN ，这称为第二次握手 客户端收到 SYN+ACK 帧后， connect 返回，并发送确认建立连接帧 ACK 给服务器端。这称为第三次握手 服务器端收到 ACK 帧后，会把请求方从 SYN 队列中移出，放至 ACCEPT 队列中，而 accept 函数也等到了自己的资源，从阻塞中唤醒，从 ACCEPT 队列中取出请求方，重新建立一个新的 sockfd ，并返回。 在服务端如何分发多个连接的请求？ 由于 TCP/IP 协议栈是维护着一个接收和发送缓冲区的。在接收到来自客户端的数据包后，服务器端的 TCP/IP 协议栈应该会做如下处理： 如果收到的是请求连接的数据包，则传给监听着连接请求端口的 socetfd 套接字。 如果是已经建立过连接后的客户端数据包，则将数据放入接收缓冲区。这样，当服务器端需要读取指定客户端的数据时，则可以利用 socketfd_new 套接字通过 recv 或者 read 函数到缓冲区里面去取指定的数据（因为 socketfd_new 代表的 socket 对象记录了客户端IP和端口，因此可以鉴别）。 数据包如何找到相对应的 socket ，这个方法在 Linux Kernel 代码里也是有体现的： static inline struct sock *__inet_lookup(struct net *net, struct inet_hashinfo *hashinfo, const __be32 saddr, const __be16 sport, const __be32 daddr, const __be16 dport, const int dif) { u16 hnum = ntohs(dport); /* 先尝试查找处于连接成功的 socket */ struct sock *sk = __inet_lookup_established(net, hashinfo, saddr, sport, daddr, hnum, dif); /* 如果没有找到连接成功的socket，那么就去处于 listen 状态的 socket 查找 */ return sk ? : __inet_lookup_listener(net, hashinfo, daddr, hnum, dif); } 四次挥手 由于TCP连接是全双工的，因此每个方向都必须单独进行关闭。 客户端A发送一个FIN，用来关闭客户A到服务器B的数据传送（报文段4）。 服务器B收到这个FIN，它发回一个ACK，确认序号为收到的序号加1（报文段5）。和SYN一样，一个FIN将占用一个序号。 服务器B关闭与客户端A的连接，发送一个FIN给客户端A（报文段6）。 客户端A发回ACK报文确认，并将确认序号设置为收到序号加1（报文段7） 对于复杂的网络状态，TCP 的实现提出了多种应对措施， TIME_WAIT 状态的提出就是为了应对其中一种异常状况。在 TIME_WAIT 阶段，主动端等待 2MSL（*最大分段寿命：表示一个 TCP 分段可以存在于互联网系统中的最大时间，由 TCP 的实现，超出这个寿命的分片都会被丢弃） 时间， MSL 建议为 2 分钟。 如果没有 TIME_WAIT 状态，Client 不再保存这个连接的信息，收到一个不存在的连接的包，Client 会响应 RST 包，导致 Server 端异常响应。此时， TIME_WAIT 是为了 保证全双工的 TCP 连接正常终止。 如果双方挥手之后，一个 网络四元组（src/dst ip/port）被回收，而此时网络中还有一个迟到的数据包没有被 Server 接收，Client 应用程序又立刻使用了同样的四元组再创建了一个新的连接后，这个迟到的数据包才到达 Server，那么这个数据包就会让 Server 以为是 Client 刚发过来的。此时， TIME_WAIT 的存在是为了 保证网络中迷失的数据包正常过期。 TCP采用四次挥手关闭连接如图所示为什么建立连接协议是三次握手，而关闭连接却是四次握手呢？ 这是因为服务端的 LISTEN 状态下的 SOCKET 当收到 SYN 报文的建连请求后，它可以把 ACK 和 SYN （ACK起应答作用，而SYN起同步作用）放在一个报文里来发送。但关闭连接时，当收到对方的FIN报文通知时，它仅仅表示对方没有数据发送给你了；但未必你所有的数据都全部发送给对方了，所以你可以未必会马上会关闭SOCKET,也即你可能还需要发送一些数据给对方之后，再发送FIN报文给对方来表示你同意现在可以关闭连接了，所以它这里的ACK报文和FIN报文多数情况下都是分开发送的。 数据传输 可靠传输 通常在每个 TCP 报文段中都有一对序号和确认号。TCP报文发送者称自己的字节流的编号为 序号 （sequence number），称接收到对方的字节流编号为 确认号 。TCP 报文的接收者为了确保可靠性，在接收到一定数量的连续字节流后才发送确认。这是对 TCP 的一种扩展，称为选择确认（Selective Acknowledgement）。选择确认使得 TCP 接收者可以对乱序到达的数据块进行确认。每一个字节传输过后，SN号都会递增1。 通过使用序号和确认号，TCP 层可以把收到的报文段中的字节按正确的顺序交付给应用层。序号是 32 位的无符号数，在它增大到 2^{32}-1 时，便会回绕到 0。对于初始化序列号(ISN)的选择是 TCP 中关键的一个操作，它可以确保强壮性和安全性。 TCP 协议使用序号标识每端发出的字节的顺序，从而另一端接收数据时可以重建顺序，无惧传输时的包的乱序交付或丢包。在发送第一个包时（SYN包），选择一个 随机数 作为序号的初值，以克制 TCP 序号预测攻击。 发送确认包（Acks），携带了接收到的对方发来的字节流的编号，称为确认号，以告诉对方 已经成功接收的数据流的字节位置。Ack并不意味着数据已经交付了上层应用程序。可靠性通过发送方检测到丢失的传输数据并重传这些数据。包括 超时重传（Retransmission timeout，RTO）与 重复累计确认 （duplicate cumulative acknowledgements，DupAcks）。 重复累计确认重传 如果一个包（不妨设它的序号是 100 ，即该包始于第 100 字节）丢失，接收方就不能确认这个包及其以后的包，因为采用了 累计ACK 。接收方在收到 100 以后的包时，发出对包含第 99 字节的包的确认。这种重复确认是包丢失的信号。发送方如果收到 3 次对同一个包的确认，就重传最后一个未被确认的包。阈值设为 3 被证实可以减少乱序包导致的无作用的重传（spurious retransmission）现象。选择性确认（SACK）的使用能明确反馈哪个包收到了，极大改善了TCP重传必要的包的能力。 超时重传 发送方使用一个保守估计的时间作为收到数据包的确认的超时上限。如果超过这个上限仍未收到确认包，发送方将重传这个数据包。每当发送方收到确认包后，会重置这个重传定时器。典型地，定时器的值设定为 {\\text{smoothed RTT}}+\\max(G,4\\times {\\text{RTT variation}}) 其中 G 是时钟粒度。进一步，如果重传定时器被触发，仍然没有收到确认包，定时器的值将被设为前次值的二倍（直到特定阈值）。这可对抗 中间人攻击方式的拒绝服务攻击，这种攻击愚弄发送者重传很多次导致接受者被压垮。 数据传输举例 发送方首先发送第一个包含序列号为1（可变化）和 1460 字节数据的 TCP 报文段给接收方。接收方以一个没有数据的 TCP 报文段来回复（只含报头），用确认号 1461 来表示已完全收到并请求下一个报文段。 发送方然后发送第二个包含序列号为 1461 ，长度为 1460 字节的数据的 TCP 报文段给接收方。正常情况下，接收方以一个没有数据的 TCP 报文段来回复，用确认号 2921（1461+1460）来表示已完全收到并请求下一个报文段。发送接收这样继续下去。 然而当这些数据包都是相连的情况下，接收方没有必要每一次都回应。比如，他收到第 1 到 5 条TCP报文段，只需回应第五条就行了。在例子中第3条TCP报文段被丢失了，所以尽管他收到了第 4 和 5 条，然而他只能回应第 2 条。 发送方在发送了第三条以后，没能收到回应，因此当时钟（timer）过时（expire）时，他重发第三条。（每次发送者发送一条TCP报文段后，都会再次启动一次时钟：RTT）。 这次第三条被成功接收，接收方可以直接确认第5条，因为4，5两条已收到。 流量控制 流量控制用来避免主机分组发送得过快而使接收方来不及完全收下，一般由接收方通告给发送方进行调控，这里的窗口被称为 接收通知窗口（Receiver's Advertised Window）。 流量控制通过 滑动窗口机制 来实现： 报文发送方 在 WIN 域指出还可接收的字节数量（rwnd）。报文接收方在没有新的确认包的情况下至多发送 WIN 允许的字节数量。在数据传输过程中，报文发送方可修改 WIN 的值。 报文发送方：即可以是握手的发起方（客户端），也可以是握手的被动接收方（服务端） 报文段 2 中提供的窗口大小为 6144 字节； 由于这是一个较大的窗口，因此发送端立即连续发送了6个报文段（4~9），停止； 报文段 10 确认了所有的数据（从第 1 到 6144 字节），但提供的窗口大小却为 2048，这很可能是接收程序不能读取多于 2048 字节的数据； 报文段 11 和 12 完成了客户的数据传输，且最后一个报文段带有 FIN 标志； 报文段 13 包含与报文段 10 相同的确认序号，但通告了一个更大的窗口大小； 报文段 14 确认了最后的 2048 字节的数据和 FIN ； 报文段 15 和 16 仅用于通告一个更大的窗口大小； 报文段 17 和 18 完成通常的关闭过程； 当接收方宣布接收窗口的值为 0，发送方停止进一步发送数据，开始了 保持定时器（persist timer），以 避免因随后的修改接收窗口的数据包丢失使连接的双侧进入死锁 ，发送方无法发出数据直至收到接收方修改窗口的指示。当定时器到期时， TCP 发送方尝试恢复发送一个小的 ZWP 包（Zero Window Probe），期待接收方回复一个带着新的接收窗口大小的确认包。一般 ZWP 包会设置成 3 次，如果 3 次过后还是 0 的话，有的 TCP 实现就会发 RST 把链接断了。 拥塞控制 TCP 拥塞控制算法是互联网上主要的拥塞控制措施，它使用一套基于 线増积减（Additive increase/multiplicative decrease，AIMD）的网络拥塞控制方法来控制拥塞，防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。 除了 拥塞窗口大小（cwnd） 之外，TCP 连接的双方都有 接收窗口大小（rwnd）。客户端能够同时传输的最大数据段的数量是接收窗口大小和拥塞窗口大小的最小值，即 min(rwnd, cwnd) 。 TCP 协议使用慢启动阈值（Slow start threshold, ssthresh）来决定使用慢启动或者拥塞避免算法： 当拥塞窗口大小小于慢启动阈值时，使用慢启动； 当拥塞窗口大小大于慢启动阈值时，使用拥塞避免算法； 当拥塞窗口大小等于慢启动阈值时，使用慢启动或者拥塞避免算法； 慢开始和拥塞避免 客户端维持一个 拥塞窗口 cwnd 的状态变量，初始值一般为 2\\times MSS。 慢开始：由小到大的指数增大拥塞窗口。首先将 cwnd 设置为一个最大报文段 MMS ，在收到一个对新的报文段的确认后，把拥塞窗口增加一个 MMS 。 拥塞避免：当慢开始到阈值（ssthresh）后，使用拥塞避免算法（ cwnd 每次加1 ）。当发送方发送的数据包丢包时，将 ssthresh 置为 cwnd 的一半，将 cwnd 置为1，再次执行慢开始。 快重传和快恢复 快速重传和恢复（fast retransmit and recovery，FRR） 是一种拥塞控制算法，它能快速恢复丢失的数据包。没有 FRR，如果数据包丢失了，TCP 将会使用定时器来要求传输暂停。在暂停的这段时间内，没有新的或复制的数据包被发送。有了FRR，如果接收机接收到一个不按顺序的数据段，它会立即给客户端发送一个重复确认。如果客户端接收到三个重复确认，它会认定数据段丢失，并立即重传这些丢失的数据段。 有了 FRR，就不会因为重传时要求的暂停被耽误。当有单独的数据包丢失时，快速重传和恢复（FRR）能最有效地工作。当有多个数据信息包在某一段很短的时间内丢失时，它则不能很有效地工作。 BBR BBR（Bottleneck Bandwidth and Round-trip propagation time）是 Google 研发的新的拥塞控制算法。自从 20 世纪 80年代后， TCP 中的拥塞控制算法都使用的是 基于丢包的拥塞控制（拥塞避免），在之前的网络带宽、路由器 Buffer 的情况下，该算法效果良好。 但是在当前的网络条件下，基于丢包的拥塞控制算法则会导致 TCP 性能问题： 在小 Buffer 路由器环境下，丢包发生在拥塞之前。在高速，长途链路中，基于丢包的拥塞控制会导致吞吐量过低，因为它反应过度，即使丢包是由瞬时流量突发引起的，也会因丢包而将发送速率减半（即使链路大部分处于空闲状态，这种丢包也可能非常频繁） 在大 Buffer 路由器环境下，拥塞发生在丢包之前。在互联网的边缘，基于丢包的拥塞控制通过反复填充大量的缓存，从而导致了臭名昭著的 bufferbloat 问题。 bufferbloat 问题：由于路由器的大缓存，减少链路丢包。再加上网络中 TCP 大量使用基于丢包的拥塞控制算法（丢包才触发速度下调，但是要丢包，缓存就得先被填满，缓存都填满，延迟更高） BBR 算法使用最大带宽和往返时间来建立网络的显式模型。每次对包传递进行累积或选择性确认，都会生成一个速率样本，该速率采样记录在数据包传输与该包确认之间的时间间隔内传递的数据量，从而使拥塞控制算法能够提供更高的吞吐量和更低的延迟。 最大分段大小 最大分段大小 (MSS) 是在单个分段中 TCP 愿意接受的数据的字节数最大值。MSS应当足够小以避免IP分片，它会导致丢包或过多的重传。 在 TCP 连接创建时，双端在 SYN 报文中用 MSS 选项宣布各自的 MSS ，这是从双端各自直接相连的数据链路层的最大传输单元(MTU)的尺寸减去固定的 IP 首部和 TCP 首部长度。以太网MTU为 1500 字节， MSS值可达 1460 字节。使用 IEEE 802.3 的 MTU 为 1492 字节，MSS 可达 1452 字节。 如果目的IP地址为“非本地的”，MSS通常的默认值为 536（这个默认值允许 20 字节的 IP 首部和 20 字节的 TCP 首部以适合 576字节 IP 数据报）。此外，发送方可用传输路径 MTU 发现（RFC 1191）推导出从发送方到接收方的网络路径上的最小 MTU，以此动态调整 MSS 以避免网络 IP 分片。 MSS 发布也被称作“MSS协商”（MSS negotiation）。严格讲，这并非是协商出来一个统一的MSS值，TCP 允许连接两端使用各自不同的MSS值。例如，这会发生在参与 TCP 连接的一台设备使用非常少的内存处理到来的 TCP 分组。 选择确认 最初采取累计确认的 TCP 协议在丢包时效率很低。例如，假设通过10个分组发出了1万个字节的数据。如果第一个分组丢失，在纯粹的累计确认协议下，接收方不能说它成功收到了 1,000 到 9,999 字节，但未收到包含 0 到 999 字节的第一个分组。因而，发送方可能必须重传所有1万个字节。 为此，TCP采取了 选择确认（selective acknowledgment，SACK） 选项。RFC 2018 对此定义为 允许接收方确认它成功收到的分组的不连续的块，以及基础 TCP 确认的成功收到最后连续字节序号。这种确认可以指出 SACK block，包含了已经成功收到的连续范围的开始与结束字节序号。在上述例子中，接收方可以发出 SACK 指出序号 1000 到 9999 ，发送方因此知道只需重发第一个分组(字节 0 到 999)。 TCP 发送方会把乱序收包当作丢包，因此会重传乱序收到的包，导致连接的性能下降。重复SACK选项（duplicate-SACK option）是定义在RFC 2883中的SACK的一项扩展，可解决这一问题。接收方发出 D-SACK 指出没有丢包，接收方恢复到高传输率。 D-SACK 使用了 SACK 的第一个段来做标志： 如果 SACK 的第一个段的范围被 ACK 所覆盖，那么就是 D-SACK; 如果 SACK 的第一个段的范围被 SACK 的第二个段覆盖，那么就是 D-SACK D-SACK旨在告诉发送端：收到了重复的数据，数据包没有丢，丢的是ACK包； SACK 选项并不是强制的。仅当双端都支持时才会被使用。 TCP 连接创建时会在 TCP 头中协商 SACK 细节。在 Linux下，可以通过 tcp_sack 参数打开 SACK 功能（Linux 2.4后默认打开）。Linux下的 tcp_dsack 参数用于开启D-SACK功能（Linux 2.4后默认打开）。选择确认也用于流控制传输协议 (SCTP)。 "},"zother2-interview/basic/net/websocket/":{"url":"zother2-interview/basic/net/websocket/","title":"Websocket","keywords":"","body":"Websocket 简介 WebSocket 是一种与 HTTP 不同的协议。两者都位于 OSI 模型的应用层，并且都依赖于传输层的 TCP 协议。 虽然它们不同，但 RFC 6455 规定：WebSocket设计为通过 80 和 443 端口工作，以及支持HTTP代理和中介，从而使其与HTTP协议兼容。为了实现兼容性， WebSocket 握手使用 HTTP Upgrade 头从 HTTP 协议更改为 WebSocket 协议。 与HTTP不同，WebSocket 提供全双工通信。此外，WebSocket 还可以在 TCP 之上启用消息流。 TCP 单独处理字节流，没有固有的消息概念。 WebSocket协议规范将 ws（WebSocket）和 wss （WebSocket Secure）定义为两个新的统一资源标识符（URI）方案，分别对应明文和加密连接。 优点 较少的控制开销。在连接创建后，服务器和客户端之间交换数据时，用于协议控制的数据包头部相对较小。在不包含扩展的情况下，对于服务器到客户端的内容，此头部大小只有2至10字节（和数据包长度有关）；对于客户端到服务器的内容，此头部还需要加上额外的4字节的掩码。相对于HTTP请求每次都要携带完整的头部，此项开销显著减少了。 更强的实时性。由于协议是全双工的，所以服务器可以随时主动给客户端下发数据。相对于HTTP请求需要等待客户端发起请求服务端才能响应，延迟明显更少； 保持连接状态。与 HTTP 不同的是，Websocket需要先创建连接，这就使得其成为一种有状态的协议，之后通信时可以省略部分状态信息。而HTTP请求可能需要在每个请求都携带状态信息（如身份认证等）。 更好的二进制支持。 Websocket 定义了二进制帧，相对HTTP，可以更轻松地处理二进制内容。 可以支持扩展。Websocket 定义了扩展，用户可以扩展协议、实现部分自定义的子协议。如部分浏览器支持压缩等。 更好的压缩效果。相对于HTTP压缩，Websocket 在适当的扩展支持下，可以沿用之前内容的上下文，在传递类似的数据时，可以显著地提高压缩率 连接过程 WebSocket 是独立的、创建在 TCP 上的协议。Websocket 通过 HTTP/1.1 协议的101状态码进行握手。为了创建Websocket连接，需要通过浏览器发出请求，之后服务器进行回应，这个过程通常称为 握手（handshaking）。 客户端请求 GET / HTTP/1.1 Upgrade: websocket Connection: Upgrade Host: example.com Origin: http://example.com Sec-WebSocket-Key: sN9cRrP/n9NdMgdcy2VJFQ== Sec-WebSocket-Version: 13 服务器回应 HTTP/1.1 101 Switching Protocols Upgrade: websocket Connection: Upgrade Sec-WebSocket-Accept: fFBooB7FAkLlXgRSz0BT3v4hq5s= Sec-WebSocket-Location: ws://example.com/ 参考链接 WebSocket "},"zother2-interview/basic/net/_index.html":{"url":"zother2-interview/basic/net/_index.html","title":"网络分层","keywords":"","body":"网络分层 OSI 层 功能 应用层 网络进程到应用程序。针对特定应用规定各层协议、时序、表示等，进行封装 。在端系统中用软件来实现，如HTTP等 表示层 数据表示形式，加密和解密，把机器相关的数据转换成独立于机器的数据。规定数据的格式化表示 ，数据格式的转换等 会话层 主机间通讯，管理应用程序之间的会话。规定通信时序 ；数据交换的定界、同步，创建检查点等 传输层 在网络的各个节点之间可靠地分发数据包。所有传输遗留问题；复用；流量；可靠 网络层 在网络的各个节点之间进行地址分配、路由和（不一定可靠的）分发报文。路由（ IP寻址）；拥塞控制。 数据链路层 一个可靠的点对点数据直链。检错与纠错（CRC码）；多路访问；寻址 物理层 一个（不一定可靠的）点对点数据直链。定义机械特性；电气特性；功能特性；规程特性 "},"zother2-interview/basic/os/arch/":{"url":"zother2-interview/basic/os/arch/","title":"计算机体系结构","keywords":"","body":"计算机体系结构 冯·诺依曼体系结构 计算机处理的数据和指令一律用二进制数表示 顺序执行程序 计算机运行过程中，把要执行的程序和处理的数据首先存入主存储器（内存），计算机执行程序时，将自动地并按顺序从主存储器中取出指令一条一条地执行，这一概念称作顺序执行程序。 计算机硬件由运算器、控制器、存储器、输入设备和输出设备五大部分组成。 数据的机内表示 二进制表示 机器数 由于计算机中符号和数字一样，都必须用二进制数串来表示，因此，正负号也必须用0,1来表示。 原码 原码用第一位表示符号, 其余位表示值. 比如如果是8位二进制: [+1]原 = 0000 0001 [-1]原 = 1000 0001 第一位是符号位. 因为第一位是符号位, 所以8位二进制数的取值范围就是: [1111 1111 , 0111 1111] 即 [-127 , 127] 原码是人脑最容易理解和计算的表示方式 反码 正数的反码是其本身 负数的反码是在其原码的基础上, 符号位不变，其余各个位取反. [+1] = [00000001]原 = [00000001]反 [-1] = [10000001]原 = [11111110]反 可见如果一个反码表示的是负数，人脑无法直观的看出来它的数值， 通常要将其转换成原码再计算。 补码 正数的补码就是其本身 负数的补码是在其原码的基础上, 符号位不变, 其余各位取反, 最后+1。 (即在反码的基础上+1) [+1] = [00000001]原 = [00000001]反 = [00000001]补 [-1] = [10000001]原 = [11111110]反 = [11111111]补 1+（-1)= 00000001 + 11111111 = 00000000 = 0 对于负数, 补码表示方式也是人脑无法直观看出其数值的. 通常也需要转换成原码在计算其数值. 在计算机系统中，数值一律用补码来表示和存储。原因在于，使用补码，可以将符号位和数值域统一处理；同时，加法和减法也可以统一处理。此外，补码与原码相互转换，其运算过程是相同的，不需要额外的硬件电路。 定点数与浮点数 定点数是小数点固定的数。在计算机中没有专门表示小数点的位，小数点的位置是约定默认的。一般固定在机器数的最低位之后，或是固定在符号位之后。前者称为定点纯整数，后者称为定点纯小数。 定点数表示法简单直观，但是 数值表示的范围太小，运算时容易产生溢出。 浮点数是小数点的位置可以变动的数。为增大数值表示范围，防止溢出，采用浮点数表示法。浮点表示法类似于十进制中的科学计数法。 在计算机上，通常使用2为基数的幂数来表式。一个浮点数a由两个数m和e来表示：a = m × b^e。在任意一个这样的系统中，我们选择一个基数b（记数系统的基）和精度p（即使用多少位来存储）。m （即尾数）是形如±d.ddd...ddd的p位数（每一位是一个介于0到b-1之间的整数，包括0和b-1）。如果m的第一位是非0整数，m称作正规化的。e是指数。 | 数符± | 阶码e | 尾数m | 数符表示尾数的符号位，阶码表示幂次，尾数表示规格化后的小数值。 32位单精度：单精度二进制小数，使用32位存储。1 8 23 位长 64位双精度：双精度二进制小数，使用64位存储。1 11 52 位长 位、字节、字 位(Bit)：电子计算机中最小的数据单位。每一位的状态只能是0或1。 字节(Byte)：8个二进制位构成1个字节，它是存储空间的基本计量单位。1个字节可以储存1个英文字母或者半个汉字，换句话说，1个汉字占据2个字节的存储空间。 字(Word)：由若干个字节构成，字的位数叫做字长，不同档次的机器有不同的字长。例如一台8位机，它的1个字就等于1个字节，字长为8位。如果是一台16位机，那么，它的1个字就由2个字节构成，字长为16位。字是计算机进行数据处理和运算的单位。 字节序 字节顺序是指占内存多于一个字节类型的数据在内存中的存放顺序，通常有小端、大端两种字节顺序。 小端字节序：低字节数据存放在内存低地址处，高字节数据存放在内存高地址处。 大端字节序：高字节数据存放在低地址处，低字节数据存放在高地址处。 基于X86平台的PC机是小端字节序的，而有的嵌入式平台则是大端字节序的。所有网络协议也都是采用大端字节序的方式来传输数据的。所以有时我们也会把大端字节序方式称之为网络字节序。 比如数字 0x12345678 在两种不同字节序CPU中的存储顺序如下所示： Big Endian 低地址 高地址 ----------------------------------------------------> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | 12 | 34 | 56 | 78 | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Little Endian 低地址 高地址 ----------------------------------------------------> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | 78 | 56 | 34 | 12 | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ 从上面两图可以看出，采用Big Endian方式存储数据是符合我们人类的思维习惯的。 联合体union的存放顺序是所有成员都从低地址开始存放，利用该特性，就能判断CPU对内存采用Little-endian还是Big-endian模式读写。 字节对齐 现代计算机中内存空间都是按照字节划分的，从理论上讲似乎对任何类型的变量的访问可以从任何地址开始，但实际情况是在访问特定类型变量的时候经常在特定的内存地址访问，这就需要各种类型数据按照一定的规则在空间上排列，而不是顺序的一个接一个的排放，这就是对齐。 为什么要进行字节对齐？ 某些平台只能在特定的地址处访问特定类型的数据; 最根本的原因是效率问题，字节对齐能提高存取数据的速度。 比如有的平台每次都是从偶地址处读取数据,对于一个int型的变量,若从偶地址单元处存放,则只需一个读取周期即可读取该变量，但是若从奇地址单元处存放，则需要2个读取周期读取该变量。 字节对齐的原则 数据成员对齐规则：结构体或联合体的数据成员，第一个数据成员放在 offset 为0的地方，以后每个数据成员存储的起始位置要从该成员大小或者成员的子成员大小（只要该成员有子成员，比如说是数组，结构体等）的整数倍开始（比如int在32位机为4字节,则要从4的整数倍地址开始存储）。 结构体作为成员：如果一个结构里有某些结构体成员，则结构体成员要从其内部最大元素大小的整数倍地址开始存储。(struct a里存有struct b,b里有char,int ,double等元素,那b应该从8的整数倍开始存储。) 收尾工作：结构体的总大小，也就是sizeof的结果，必须是其内部最大成员的整数倍，不足的要补齐。 "},"zother2-interview/basic/os/concurrency/":{"url":"zother2-interview/basic/os/concurrency/","title":"进程管理","keywords":"","body":"进程 进程是一个具有独立功能的程序关于某个数据集合的一次运行活动。它可以申请和拥有系统资源，是一个动态的概念，是一个活动的实体。它不只是程序的代码，还包括当前的活动，通过程序计数器的值和处理寄存器的内容来表示。 进程的概念主要有两点： 进程是一个实体，每一个进程都有它自己的地址空间，一般情况下，包括文本区域（text region）、数据区域（data region）和堆栈（stack region）。文本区域存储处理器执行的代码；数据区域存储变量和进程执行期间使用的动态分配的内存；堆栈区域存储着活动过程调用的指令和本地变量。 进程是一个“执行中的程序”，程序是一个没有生命的实体，只有处理器赋予程序生命时，它才能成为一个活动的实体，我们称其为进程。 进程的基本状态 阻塞态：等待某个事件的完成； 就绪态：等待系统分配处理器以便运行； 执行态：占有处理器正在运行。 执行态 -> 阻塞态：往往是由于等待外设，等待主存等资源分配或等待人工干预而引起的。 阻塞态 -> 就绪态：则是等待的条件已满足，只需分配到处理器后就能运行。 执行态 -> 就绪态：不是由于自身原因，而是由外界原因使运行状态的进程让出处理器，这时候就变成就绪态。例如时间片用完，或有更高优先级的进程来抢占处理器等。 就绪态 -> 执行态：系统按某种策略选中就绪队列中的一个进程占用处理器，此时就变成了运行态 进程调度 上下文切换 线程/进程的上下文（以下统称为：上下文）主要包含两个部分：寄存器（尤其是 PC）和操作系统需要的特定数据（PCB）。上下文切换（context switch），是一个存储和重建 CPU 的过程，完整的上下文会涉及到这两部分的切换，旧的上下文被保存，新的上下文被加载。 当系统发生中断或者 OS 进行线程调度时会进行上下文切换。 调度种类 高级、中级和低级调度作业从提交开始直到完成，往往要经历下述三级调度： 高级调度：又称为作业调度，它决定把后备作业调入内存运行； 中级调度：又称为在虚拟存储器中引入，在内、外存对换区进行进程对换。 低级调度：又称为进程调度，它决定把就绪队列的某进程获得CPU； 非抢占式调度与抢占式调度 非抢占式：分派程序一旦把处理机分配给某进程后便让它一直运行下去，直到进程完成或发生进程调度进程调度某事件而阻塞时，才把处理机分配给另一个进程。 抢占式：操作系统将正在运行的进程强行暂停，由调度程序将CPU分配给其他就绪进程的调度方式。 调度策略的设计 响应时间：从用户输入到产生反应的时间 周转时间：从任务开始到任务结束的时间 平均周转时间：周转总时间除以作业个数 CPU任务可以分为交互式任务和批处理任务，调度最终的目标是合理的使用CPU，使得交互式任务的响应时间尽可能短，用户不至于感到延迟，同时使得批处理任务的周转时间尽可能短，减少用户等待的时间。 调度算法 FCFS：调度的顺序就是任务到达就绪队列的顺序。对短作业不公平。 SJF：最短的作业(CPU区间长度最小)最先调度。 HRN：最高响应比优先法，是 FCFS 和 SJF 的综合平衡，响应比R定义如下： R =(W+T)/T 。 优先权调度：每个任务关联一个优先权，调度优先权最高的任务。 Round-Robin(RR)：设置一个时间片，按时间片来轮转调度 多级队列调度 按照一定的规则建立多个进程队列 不同的队列有固定的优先级（高优先级有抢占权） 不同的队列可以给不同的时间片和采用不同的调度方法 多级反馈队列：在多级队列的基础上，任务可以在队列之间移动，更细致的区分任务。可以根据“享用”CPU时间多少来移动队列，阻止“饥饿”。 进程同步 临界资源与临界区 在操作系统中，进程是占有资源的最小单位（线程可以访问其所在进程内的所有资源，但线程本身并不占有资源或仅仅占有一点必须资源）。但对于某些资源来说，其在同一时间只能被一个进程所占用。这些一次只能被一个进程所占用的资源就是所谓的临界资源。典型的临界资源比如物理上的打印机，或是存在硬盘或内存中被多个进程所共享的一些变量和数据等(如果这类资源不被看成临界资源加以保护，那么很有可能造成丢数据的问题)。 对于临界资源的访问，必须是互斥进行。也就是当临界资源被占用时，另一个申请临界资源的进程会被阻塞，直到其所申请的临界资源被释放。而进程内访问临界资源的代码被成为临界区。 对于临界区的访问过程分为四个部分： 进入区:查看临界区是否可访问，如果可以访问，则转到步骤二，否则进程会被阻塞 临界区:在临界区做操作 退出区:清除临界区被占用的标志 剩余区：进程与临界区不相关部分的代码 解决临界区问题可能的方法： 一般软件方法 关中断方法 硬件原子指令方法 信号量方法 信号量 信号量是一个确定的二元组（s，q），其中s是一个具有非负初值的整形变量，q是一个初始状态为空的队列，整形变量s表示系统中某类资源的数目： 当其值 >= 0 时，表示系统中当前可用资源的数目 当其值 时，其绝对值表示系统中因请求该类资源而被阻塞的进程数目 除信号量的初值外，信号量的值仅能由 P 操作和 V 操作更改，操作系统利用它的状态对进程和资源进行管理 锁 互斥锁：同一时间只能有一个线程访问加锁的数据。 自旋锁：互斥锁的一种实现，如果自旋锁已经被别的执行单元保持，调用者就一直 循环等待 是否该自旋锁的保持者已经释放了锁。 读写锁：一种特殊的自旋锁，它把对共享资源的访问者划分成读者和写者，读者只对共享资源进行读访问，写者则需要对共享资源进行写操作。写者是排他性的，一个读写锁同时只能有一个写者或多个读者（与CPU数相关），但不能同时既有读者又有写者。 阻塞锁：与自旋锁不同，改变了线程的运行状态。让线程进入阻塞状态进行等待，当获得相应的信号（唤醒，时间） 时，才可以进入线程的准备就绪状态，准备就绪状态的所有线程，通过竞争，进入运行状态。 在Java中synchronized,ReentrantLock,Object.wait() / notify()都属于阻塞锁。 可重入锁：也叫做递归锁，指的是同一线程上该锁是可重入的，对于不同线程则相当于普通的互斥锁。 公平锁：加锁前检查是否有排队等待的线程，优先排队等待的线程，先来先得。 非公平锁：加锁时不考虑排队等待问题，直接尝试获取锁，获取不到自动到队尾等待。ReentrantLock中的lock()默认就是非公平锁。 悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。加锁的时间可能会很长，也就是说悲观锁的并发访问性不好。 乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。乐观锁不能解决脏读的问题，可以通过添加时间戳和版本来来解决。 死锁 死锁是指多个进程因循环等待资源而造成无法执行的现象。死锁会造成进程无法执行，同时会造成系统资源的极大浪费(资源无法释放)。 死锁产生的四个必要条件 互斥使用：指进程对所分配到的资源进行排它性使用，即在一段时间内某资源只由一个进程占用。如果此时还有其它进程请求资源，则请求者只能等待，直至占有资源的进程用毕释放。 不可抢占：指进程已获得的资源，在未使用完之前，不能被剥夺，只能在使用完时由自己释放。 请求和保持：指进程已经保持至少一个资源，但又提出了新的资源请求，而该资源已被其它进程占有，此时请求进程阻塞，但又对自己已获得的其它资源保持不放。 循环等待：指在发生死锁时，必然存在一个进程——资源的环形链，即进程集合{P0，P1，P2，···，Pn}中的P0正在等待一个P1占用的资源；P1正在等待P2占用的资源，……，Pn正在等待已被P0占用的资源。 死锁避免 银行家算法：判断此次请求是否造成死锁若会造成死锁，则拒绝该请求。 CAS 比较并交换(compare and swap, CAS)，是原子操作的一种，可用于在多线程编程中实现不被打断的数据交换操作。该操作通过将内存中的值与指定数据进行比较，当数值一样时将内存中的数据替换为新的值。 在使用上，通常会记录下某块内存中的旧值，通过对旧值进行一系列的操作后得到新值，然后通过CAS操作将新值与旧值进行交换。如果这块内存的值在这期间内没被修改过，则旧值会与内存中的数据相同，这时CAS操作将会成功执行使内存中的数据变为新值。如果内存中的值在这期间内被修改过，则一般来说旧值会与内存中的数据不同，这时CAS操作将会失败，新值将不会被写入内存。 进程间通信 本地进程间通信的方式有很多，可以总结为下面四类： 消息传递（管道、FIFO、消息队列） 同步（互斥量、条件变量、读写锁、文件和写记录锁、信号量） 共享内存（匿名的和具名的） 远程过程调用（Solaris门和Sun RPC） 子进程 在 Unix 和类 Unix 系统中，子进程通常为系统调用 fork 的产物。调用 fork 后的父子进程会运行在不同的内存空间中，当 fork 发生时两者的内存空间有着完全相同的内容，对内存的写入和修改、文件的映射都是独立的，两个进程不会相互影响。除此之外，子进程几乎是父进程的完整副本。 既然父进程和子进程拥有完全相同的内存空间并且两者对内存的写入都不会相互影响，那么是否意味着子进程在 fork 时需要对父进程的内存进行全量的拷贝呢？ 在一些早期的 *nix 系统上，系统调用 fork 确实会立刻对父进程的内存空间进行复制，但是在今天的多数系统中， fork 并不会立刻触发这一过程，而是在内存被修改时，才会进行数据复制（Copy-On-Write）。 当 fork 函数调用时，父进程和子进程会被 Kernel 分配到不同的虚拟内存空间中，所以在两个进程看来它们访问的是不同的内存： 在真正访问虚拟内存空间时，Kernel 会将虚拟内存映射到物理内存上，所以父子进程共享了物理上的内存空间； 当父进程或者子进程对共享的内存进行修改时，共享的内存才会以页为单位进行拷贝，父进程会保留原有的物理空间，而子进程会使用拷贝后的新物理空间； 线程 线程是 操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。在Unix System V及SunOS中也被称为轻量进程(lightweight processes)，但轻量进程更多指内核线程(kernel thread)，而把用户线程(user thread)称为线程。 线程是独立调度和分派的基本单位。线程可以操作系统内核调度的内核线程，如Win32线程；由用户进程自行调度的用户线程，如Linux平台的POSIX Thread；或者由内核与用户进程，如Windows 7的线程，进行混合调度。 同一进程中的多条线程将共享该进程中的全部系统资源，如虚拟地址空间，文件描述符和信号处理等等。但同一进程中的多个线程有各自的调用栈，自己的寄存器环境，自己的线程本地存储。 线程的属性 轻型实体：线程中的实体基本上不拥有系统资源，只是有一点必不可少的、能保证独立运行的资源。线程的实体包括程序、数据和TCB。线程是动态概念，它的动态特性由线程控制块TCB（Thread Control Block）描述。TCB包括以下信息： 线程状态。 当线程不运行时，被保存的现场资源。 一组执行堆栈。 存放每个线程的局部变量主存区。 访问同一个进程中的主存和其它资源。 用于指示被执行指令序列的程序计数器、保留局部变量、少数状态参数和返回地址等的一组寄存器和堆栈。 独立调度和分派的基本单位：在多线程OS中，线程是能独立运行的基本单位，因而也是独立调度和分派的基本单位。由于线程很“轻”，故线程的切换非常迅速且开销小（在同一进程中的）。 可并发执行：在一个进程中的多个线程之间，可以并发执行，甚至允许在一个进程中所有线程都能并发执行；同样，不同进程中的线程也能并发执行，充分利用和发挥了处理机与外围设备并行工作的能力。 共享进程资源：在同一进程中的各个线程，都可以共享该进程所拥有的资源，这首先表现在：所有线程都具有相同的地址空间（进程的地址空间），这意味着，线程可以访问该地址空间的每一个虚地址；此外，还可以访问进程所拥有的已打开文件、定时器、信号量机构等。由于同一个进程内的线程共享内存和文件，所以线程之间互相通信不必调用内核。 线程共享的环境包括：进程代码段、进程的公有数据(利用这些共享的数据，线程很容易的实现相互之间的通讯)、进程打开的文件描述符、信号的处理器、进程的当前目录和进程用户ID与进程组ID。 线程是程序执行的一条路径，在多线程的OS中，线程是调度和分配的基本单位，而进程是拥有资源的基本单位。 进程 vs 线程 线程本质就是堆栈，当一段程序在执行，能代表它的是他的过去和现在。过去 在堆栈中，现在 则是 CPU 的所有寄存器，如果我们要挂起一个线程，我们把寄存器也保存到堆栈中，我们就具有它的所有状态，可以随时恢复它。 进程的本质是地址空间，当我们切换线程的时候，同时切换它的地址空间（通过修改MMU即可），则认为发生了进程切换。 中断 中断（英语：Interrupt）是指 处理器接收到来自硬件或软件的信号，提示发生了某个事件，应该被注意，这种情况就称为中断。 通常，在接收到来自外围硬件（相对于中央处理器和内存）的异步信号，或来自软件的同步信号之后，处理器将会进行相应的 硬件／软件 处理。发出这样的信号称为进行中断请求（interrupt request，IRQ）。硬件中断导致处理器通过一个运行信息切换（context switch）来保存执行状态（以程序计数器和程序状态字等寄存器信息为主）；软件中断则通常作为CPU指令集中的一个指令，以可编程的方式直接指示这种运行信息切换，并将处理导向一段中断处理代码。中断在计算机多任务处理，尤其是即时系统中尤为有用。 硬件中断 由硬件发出或产生的中断称为硬中断，按硬中断事件的来源和实现手段可将中断划分为外中断和内中断： 外中断：又称为中断或异步中断，是指 来自处理器以外的中断信号，包括时钟中断、键盘中断、外部设备中断等。外中断又分为可屏蔽中断和不可屏蔽中断，各个中断具有不同的优先级，表示事件的紧急程度，在处理高一级中断时，往往会部分或全部屏蔽低等级中断。 内中断：又称为异常或同步中断（产生时必须考虑与处理器时钟同步），是指 来自处理器内部的中断信号，通常是由于程序执行过程中，发现与当前指令关联的、不正常的或错误的事件。 软件中断 软件中断：是一条CPU指令，用以自陷一个中断。由于 软中断指令通常要运行一个切换CPU至内核态（Kernel Mode/Ring 0）的子例程，它常被用作实现系统调用（System call）。 处理器通常含有一个内部中断屏蔽位，并允许通过软件来设定。一旦被设定，所有外部中断都将被系统忽略。这个屏蔽位的访问速度显然快于中断控制器上的中断屏蔽寄存器，因此可提供更快速地中断屏蔽控制。 中断尽管可以提高计算机处理性能，但 过于密集的中断请求／响应反而会影响系统性能。这类情形被称作中断风暴（interrupt storm）。 内核态&用户态 内核态是一种 CPU 的特权态，CPU 可以在特权态下执行特殊的指令，访问这个特权态才运行访问的资源。上面提到的三种中断形式：外中断、异常中断、软件中断，均可使 OS 从用户态切换到内核态。 内核态切换 内核态切换不一定进行上下文切换。先考虑简单的内核态切换——获取当前系统时间。CPU 从用户态切换到内核态，保存用户态的寄存器数据，执行内核代码以获取数据，将数据存储到调用者可访问的存储器或寄存器，恢复用户态的寄存器数据并返回。这里未进行完整的上下文切换，只是涉及用户态到内核态的模式切换。 现在考虑一个系统调用，该调用会阻塞调用者，直到发生某些事件或数据可用为止。在这种情况下，内核将被迫保存调用者的完整上下文，将其标记为 阻塞态，以便调度程序在该事件或数据到达之前无法运行它。这时调度程序会加载另一个就绪线程/进程的上下文。这里的内核态切换就会导致上下文切换。 参考链接 system call and context switch "},"zother2-interview/basic/os/device/":{"url":"zother2-interview/basic/os/device/","title":"设备管理","keywords":"","body":"设备管理 外部设备分为两大类： 存储型设备：以存储大量信息和快速检索为目标，在系统中存储持久性信息。 I/O型设备：如显示器、打印机等。 I/O硬件原理 I/O系统 通常把I/O设备及其接口线路、控制部件、通道和管理软件称为I/O系统，把计算机的内存和设备介质之间的信息传送操作称为I/O操作。可按照不同方式对设备进行分类：按I/O操作特性分为输入型设备、输出型设备和存储型设备；按I/O信息交换单位分为字符设备和块设备。 输入、输出型设备通常是字符设备，存储型设备通常是块设备。 存储型设备又分为顺序存储设备和直接存取设备。前者严格依赖信息的物理位置进行读写和定位，如磁带。后者的特点是存取任何一个物理块所需要的时间几乎不依赖于此信息所处的位置，如磁盘。 I/O控制方式 轮询方式 轮询方式又称程序直接控制方式，使用查询指令测试设备控制器的忙闲状态位，确定内存和设备是否能交换数据。轮询方式采用三条指令：查询指令，查询设备是否就绪；读写指令，当设备就绪时执行数据交换；转移指令，当设备未就绪时执行转移指令指向查询指令继续查询。可见，在这种方式下CPU和设备只能串行工作。 中断方式 在这种方式下CPU和设备之间传输数据的过程如下： 进程发出启动I/O指令，CPU加载控制信息到设备控制器的寄存器，然后进程继续执行不涉及本次I/O数据的任务，或放弃CPU等待设备I/O操作完成。 设备控制器检查寄存器的内容，按照I/O指令的要求执行相应I/O操作，一旦传输完成，设备控制器发出I/O中断请求信号。 CPU收到并响应I/O中断后，转向设备的I/O中断处理程序执行。 中断处理程序执行数据读取操作，将I/O缓冲寄存器的内容写入内存，操作结束后退出中断处理程序，返回发生中断前的状态。 进程调度程序在适当的时候让得到数据的进程恢复执行。 在I/O中断方式中，如果设备控制器的数据缓冲区较小，当缓冲器装满后便会发生中断，那么在数据传输过程中发生中断次数会很多，这样就消耗了大量CPU时间。 DMA方式 虽然中断方式提高了CPU利用率，但是在响应中断请求后必须停止现行程序，转入中断处理程序并参与数据传输操作。在DMA(Direct Memory Access)方式中，内存和设备之间有一条数据通路成块地传送数据，无须CPU干预，实际数据传输操作由DMA直接完成。为实现DMA，至少需要以下逻辑部件： 内存地址寄存器：存放内存中需要交换数据的地址，DMA传送之前由程序送入首地址；DMA传送过程中，每次交换数据都把地址寄存器的内容加1。 字计数器：记录传送数据的总字数，每次传送一个字就把字计数器减1。 数据缓冲寄存器或数据缓冲区：暂存每次传送的数据。 设备地址寄存器：存放I/O信息的地址，如磁盘的柱面号。 中断机制和控制逻辑：用于向CPU提出I/O中断请求及CPU发来的I/O命令，管理DMA的传送过程。 通道方式 通道又称I/O处理器，能完成内存和设备之间的信息传送，与CPU并行地执行操作。采用I/O通道设计后，I/O操作过程如下：CPU在执行主程序时遇到I/O请求，启动在指定通道上选址的设备，一旦启动成功，通道开始控制设备进行操作，这时CPU就可以执行其他任务并与通道并行工作，直到I/O操作完成；当通道发出I/O操作结束中断时，处理器才响应并停止当前工作，转向I/O操作结束事件。 "},"zother2-interview/basic/os/disk/":{"url":"zother2-interview/basic/os/disk/","title":"磁盘与文件","keywords":"","body":"磁盘与文件 磁盘 磁盘是可以持久存储的设备，根据存储介质的不同，常见磁盘可以分为两类：机械磁盘和固态磁盘 机械磁盘，也称为硬盘驱动器，通常缩写为HDD。机械磁盘主要有盘片和读写磁头组成，数据就存储在盘片的环状磁道中。在读写数据前，需要移动读写磁头，定位到数据所在的磁道，然后才能访问数据。最小读写单位是 扇区 ，一般大小为512字节。 固态磁盘，通常缩写为SSD，有固态电子元器件组成。固态磁盘不需要磁道寻址，所以，不管是连续I/O还是随机I/O的性能，都比机械磁盘要好得多。 连续 I/O 还可以通过预读的方式，来减少I/O请求的次数，这也是其性能优异的一个原因。很多性能优化的方案，也都会从这个角度出发，来优化I/O性能。最小读写单位是 页，通常大小是 4KB ，8KB 等。 磁盘调度 磁盘访问延迟 = 队列时间 + 控制器时间 + 寻道时间 + 旋转时间 + 传输时间 磁盘调度的目的是减小延迟，其中前两项可以忽略，寻道时间是主要矛盾。 磁盘调度算法 FCFS：先进先出的调度策略，这个策略具有公平的优点，因为每个请求都会得到处理，并且是按照接收到的顺序进行处理。 SSTF(Shortest-seek-time First 最短寻道时间优先)：选择使磁头从当前位置开始移动最少的磁盘I/O请求，所以 SSTF 总是选择导致最小寻道时间的请求。总是选择最小寻找时间并不能保证平均寻找时间最小，但是能提供比 FCFS 算法更好的性能，会存在饥饿现象（会导致较远的I/O请求不能满足）。 SCAN：SSTF+中途不回折，每个请求都有处理机会。SCAN 要求磁头仅仅沿一个方向移动，并在途中满足所有未完成的请求，直到它到达这个方向上的最后一个磁道，或者在这个方向上没有其他请求为止。由于磁头移动规律与电梯运行相似，SCAN 也被称为电梯算法。 SCAN 算法对最近扫描过的区域不公平，因此，它在访问局部性方面不如 FCFS 算法和 SSTF 算法好。 C-SCAN：SCAN+直接移到另一端，两端请求都能很快处理。把扫描限定在一个方向，当访问到某个方向的最后一个磁道时，磁道返回磁盘相反方向磁道的末端，并再次开始扫描。其中“C”是Circular（环）的意思。 LOOK(C-LOOK)：釆用SCAN算法和C-SCAN算法时磁头总是严格地遵循从盘面的一端到另一端，显然，在实际使用时还可以改进，即磁头移动只需要到达最远端的一个请求即可返回，不需要到达磁盘端点。这种形式的SCAN算法和C-SCAN算法称为LOOK和C-LOOK调度。这是因为它们在朝一个给定方向移动前会查看是否有请求。 文件系统 文件系统是对存储设备上的文件，进行组织管理的一种机制。而 Linux 在各种文件系统实现上，抽象了一层虚拟文件系统 VFS ，它定义了一组，所有文件系统都支持的，数据结构和标准接口。VFS 内部通过目录项，索引节点，逻辑块以及超级块等数据结构，来管理文件。 目录项：记录了文件的名字，以及文件与其他目录项之间的目录关系。 索引节点：记录了文件的元数据 逻辑块：是由连续磁盘扇区构成的最小读写单元，用来存储文件系统 超级块：用来记录文件系统整体的状态，如索引节点和逻辑块的使用情况等。 其中，目录项是一个内存缓存；而超级块，索引节点和逻辑块，都是存储在磁盘中的持久数据。 如果每次都读写 512 字节这么小的单位的话，效率很低。所以，文件系统会把连续的扇区或页，组成逻辑块，然后以逻辑块作为最小单元来管理数据。常见的逻辑块的大小是 4KB ，也就是连续8个扇区，或者单独的一个页，都可以组成一个逻辑块。 通用块层 为了减小不同块设备的差异带来的影响，Linux 通过一个统一的通用块层，来管理各种不同的块设备。 通用块层，其实是处在文件系统和磁盘驱动中间的一个块设备抽象层。它主要有两个功能： 同虚拟文件系统的功能类似：向上，为文件系统和应用程序，提供访问块设备的标准接口；向下，把各种异构的磁盘设备抽象为统一的块设备，并提供统一框架来管理这些设备的驱动程序。 I/O调度：对文件系统和应用程序发来的I/O请求排队，并通过重新排序，请求合并等方式，提高磁盘读写的效率。Linux 内核支持四种 I/O 调度算法，分别是 NONE , NOOP , CFQ 以及 DeadLine IO 栈 可以把 Linux 存储系统的I/O栈，由上到下分为三个层次，分别是文件系统层，通用块层和设备层。 文件系统层：包括虚拟文件系统和其他各种文件系统的具体实现。它为上层的应用程序，提供标准的文件访问接口；对下会通过调用块层，来存储和管理磁盘数据。 调用块层：包扣块设备I/O队列和I/O调度器。它会对文件系统的I/O请求进行排队，再通过重新排序和请求合并，然后才要发送给下一级的设备层。 设备层：包扣存储设备和相应的驱动程序，负责最终物理设备的I/O操作。 Linux文件权限 Linux文件采用10个标志位来表示文件权限，如下所示： -rw-r--r-- 1 skyline staff 20B 1 27 10:34 1.txt drwxr-xr-x 5 skyline staff 170B 12 23 19:01 ABTableViewCell 第一个字符一般用来区分文件和目录，其中： d：表示是一个目录，事实上在ext2fs中，目录是一个特殊的文件。 －：表示这是一个普通的文件。 l: 表示这是一个符号链接文件，实际上它指向另一个文件。 b、c：分别表示区块设备和其他的外围设备，是特殊类型的文件。 s、p：这些文件关系到系统的数据结构和管道，通常很少见到。 第2～10个字符当中的每3个为一组，左边三个字符表示所有者权限，中间3个字符表示与所有者同一组的用户的权限，右边3个字符是其他用户的权限。 这三个一组共9个字符，代表的意义如下： r(Read，读取)：对文件而言，具有读取文件内容的权限；对目录来说，具有浏览目录的权限 w(Write,写入)：对文件而言，具有新增、修改文件内容的权限；对目录来说，具有删除、移动目录内文件的权限。 x(eXecute，执行)：对文件而言，具有执行文件的权限；对目录来说该用户具有进入目录的权限。 权限的掩码可以使用十进制数字表示： 如果可读，权限是二进制的100，十进制是4； 如果可写，权限是二进制的010，十进制是2； 如果可运行，权限是二进制的001，十进制是1； chmod命令 chmod命令非常重要，用于改变文件或目录的访问权限。用户用它控制文件或目录的访问权限。 该命令有两种用法。一种是包含字母和操作符表达式的文字设定法；另一种是包含数字的数字设定法。 文字设定法 chmod ［who］ ［+ | - | =］ ［mode］ 文件名 命令中各选项的含义为： 操作对象who可是下述字母中的任一个或者它们的组合： u 表示“用户（user）”，即文件或目录的所有者。 g 表示“同组（group）用户”，即与文件属主有相同组ID的所有用户。 o 表示“其他（others）用户”。 a 表示“所有（all）用户”。它是系统默认值。 操作符号可以是： 添加某个权限。 取消某个权限。 = 赋予给定权限并取消其他所有权限（如果有的话）。 设置mode所表示的权限可用下述字母的任意组合： r 可读。 w 可写。 x 可执行。 X 只有目标文件对某些用户是可执行的或该目标文件是目录时才追加x 属性。 s 在文件执行时把进程的属主或组ID置为该文件的文件属主。方式“u＋s”设置文件的用户ID位，“g＋s”设置组ID位。 t 保存程序的文本到交换设备上。 u 与文件属主拥有一样的权限。 g 与和文件属主同组的用户拥有一样的权限。 o 与其他用户拥有一样的权限。 文件名：以空格分开的要改变权限的文件列表，支持通配符。 在一个命令行中可给出多个权限方式，其间用逗号隔开。例如：chmod g+r，o+r example 使同组和其他用户对文件example 有读权限。 数字设定法 直接使用数字表示的权限来更改： 例： $ chmod 644 mm.txt chgrp命令 功能：改变文件或目录所属的组。 语法：chgrp ［选项］ group filename 例：$ chgrp - R book /opt/local /book 改变/opt/local /book/及其子目录下的所有文件的属组为book。 chown命令 功能：更改某个文件或目录的属主和属组。这个命令也很常用。例如root用户把自己的一个文件拷贝给用户xu，为了让用户xu能够存取这个文件，root用户应该把这个文件的属主设为xu，否则，用户xu无法存取这个文件。 语法：chown ［选项］ 用户或组 文件 说明：chown将指定文件的拥有者改为指定的用户或组。用户可以是用户名或用户ID。组可以是组名或组ID。文件是以空格分开的要改变权限的文件列表，支持通配符。 例：把文件shiyan.c的所有者改为wang。 chown wang shiyan.c "},"zother2-interview/basic/os/io/":{"url":"zother2-interview/basic/os/io/","title":"I O","keywords":"","body":"I/O 基本概念 文件描述符fd 文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。 文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于 UNIX、Linux 这样的操作系统。 缓存 I/O 缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。 缓存 I/O 的缺点：数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。 IO模式 刚才说了，对于一次IO访问（以read举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以说，当一个read操作发生时，它会经历两个阶段： 等待数据准备 将数据从内核拷贝到进程中 正式因为这两个阶段，Linux系统产生了下面五种网络模式的方案。 阻塞 I/O（blocking IO） 非阻塞 I/O（nonblocking IO） I/O 多路复用（ IO multiplexing） 信号驱动 I/O（ signal driven IO） 异步 I/O（asynchronous IO） 由于signal driven IO在实际中并不常用，所以这里只提及剩下的四种 IO Model。 阻塞IO 在 Linux 中，默认情况下所有的 socket 都是 blocking ，一个典型的读操作流程大概是这样： 当用户进程调用了 recvfrom 这个系统调用， kernel 就开始了 IO 的第一个阶段：准备数据（对于网络IO来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的 UDP 包。这个时候 kernel 就要等待足够的数据到来）。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞（当然，是进程自己选择的阻塞）。当 kernel 一直等到数据准备好了，它就会将数据从 kernel 中拷贝到用户内存，然后 kernel 返回结果，用户进程才解除 block 的状态，重新运行起来。 blocking IO的特点就是在IO执行的两个阶段都被block了 非阻塞 I/O Linux 下，可以通过设置 socket 使其变为 non-blocking 。当对一个 non-blocking socket 执行读操作时，流程是这个样子： 当用户进程发出 read 操作时，如果 kernel 中的数据还没有准备好，那么它并不会 block 用户进程，而是立刻返回一个 error 。从用户进程角度讲 ，它发起一个 read 操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个 error 时，它就知道数据还没有准备好，于是它可以再次发送 read 操作。一旦 kernel 中的数据准备好了，并且又再次收到了用户进程的 system call ，那么它马上就将数据拷贝到了用户内存，然后返回。 nonblocking IO的特点是用户进程需要不断的主动询问kernel数据好了没有 IO多路复用 IO多路复用就是我们说的 select，poll，epoll ，有些地方也称这种IO方式为 event driven IO 。select/epoll 的好处就在于单个 process 就可以同时处理多个网络连接的 IO 。它的基本原理就是 select，poll，epoll 这个 function 会不断的轮询所负责的所有 socket ，当某个 socket 有数据到达了，就通知用户进程。 当用户进程调用了 select，那么整个进程会被 block，而同时， kernel 会监视所有 select 负责的 socket ，当任何一个 socket 中的数据准备好了， select 就会返回。这个时候用户进程再调用 read 操作，将数据从 kernel 拷贝到用户进程。 I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select() 函数就可以返回。 这个图和 blocking IO 的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个 system call (select 和 recvfrom)，而 blocking IO 只调用了一个 system call (recvfrom)。但是，用 select 的优势在于它可以同时处理多个 connection 。 所以，如果处理的连接数不是很高的话，使用 select/epoll 的 web server 不一定比使用 multi-threading + blocking IO 的 web server 性能更好，可能延迟还更大。select/epoll 的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。 在IO多路复用实际使用中，对于每一个socket，一般都设置成为 non-blocking ，但是，如上图所示，整个用户的 process 其实是一直被block的。只不过 process 是被 select 这个函数 block ，而不是被 socket IO 给 block 。 基本概念 在 I/O 编程过程中,当需要同时处理多个客户端接入请求时，可以利用多线程或者 I/O 多路复用 技术进行处理。I/O多路复用 技术通过把多个I/O的阻塞复用到同一个selct的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。与传统的 多线程/多进程 模型比，I/O多路复用的最大优势是系统开销小，系统不需要创建新的额外进程或者线程，也不需要维护这些进程和线程的运行，降低了系统的维护工作量，节省了系统资源，I/O多路复用的主要应用场景如下。 服务器需要同时处理多个处于监听状态或者多个连接状态的套接字 服务器需要同时处理多种网络协议的套接字 目前支持I/O多路复用的系统调用有 select、pselect、poll、epoll，在Linux网络编程; 过程中，很长一段时间都使用 select 做轮询和网络事件通知，然而 select 的一些固有缺陷导致了它的应用受到了很大的限制。最终 Linux 不得不在新的内核版本中寻找 select 的替代方案，最终选择了 epoll。 epoll 与 select 的原理比较类似，为了克服 select 的缺点， epoll 作了很多重大改进，现总结如下。 支持一个进程打开的 socket 描述符（FD）不受限制（仅受限于操作系统的最大文件句柄数）。 select 最大的缺陷就是单个进程所打开的 FD 是有一定限制的，它由 FD_SETSIZE 设置，默认值是 1024 。对于那些需要支持上万个 TCP 连接的大型服务器来说显然太少了。可以选择修改这个宏然后重新编译内核，不过这会带来网络效率的下降。我们也可以通过选择多进程的方案（传统的 Apache 方案）解决这个问题，不过虽然在 Linux上创建进程的代价比较小，但仍旧是不可忽视的，另外，进程间的数据交换非常麻烦，对于 Java 由于没有共享内存，需要通过 Socket 通信或者其他方式进行数据同步，这带来了额外的性能损耗，增加了程序复杂度，所以也不是一种完美的解决方案。值得庆幸的是， epoll 并没有这个限制，它所支持的 FD 上限是操作系统的 最大文件句柄数，这个数字远远大于 1024 。例如，在 1 GB 内存的机器上大约是 10万个句柄左右，具体的值可以通过cat /proc/sys/fs/file- max 察看，通常情况下这个值跟系统的内存关系比较大。 I/O效率不会随着FD数目的增加而线性下降。 传统的 select/poll 另-个致命弱点就是当你拥有一个很大的 socket 集合，由于网络延时或者链路空闲，任一时刻只有少部分的 socket 是“活跃”的，但是 select/poll 每次调用都会线性扫描全部的集合，导致效率呈现线性下降。 epoll 不存在这个问题，它只会对“活跃”的 socket 进行操作，这是因为在内核实现中 epoll 是根据每个 fd 上面的 callback 函数实现的，那么，只有“活跃”的 socket 才会主动的去调用 callback 函数，其他 idle 状态 socket 则不会。在这点上， epoll 实现了一个伪 AIO。针对 epoll 和 select 性能对比的 benchmark 测试表明：如果所有的 socket 都处于活跃态，例如一个高速 LAN 环境， epoll 并不比 select/poll 效率高太多；相反，如果过多使用 epoll_ ctl , 效率相比还有稍微的下降。但是一旦使用 idleconnections 模拟 WAN 环境，epoll 的效率就远在 select/poll 之上了。 使用 mmap 加速内核与用户空间的消息传递 无论是 select，poll 还是 epoll 都需要内核把 FD 消息通知给用户空间，如何避免不必要的内存复制（Zero Copy）就显得非常重要， epoll 是通过内核和用户空间 mmap 共享同一块内存来实现。 Epoll 的 API 更加简单 包括创建一个 epoll 描述符、添加监听事件、阻塞等待所监听的事件发生，关闭 epoll 描述符等。 值得说明的是，用来克服 select/poll 缺点的方法不只有 epoll , epoll 只是一种 Linux 的 实现方案。在 freeBSD 下有 kqueue Epoll 边缘触发&水平触发 epoll 对文件描述符的操作有两种模式：LT（level trigger）和ET（edge trigger）。LT模式是 默认模式 ，LT模式与ET模式的区别如下： LT模式：当 epoll_wait 检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用 epoll_wait 时，会再次响应应用程序并通知此事件。 ET模式：当 epoll_wait 检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用 epoll_wait 时，不会再次响应应用程序并通知此事件。 ET模式 在很大程度上减少了 epoll 事件被重复触发的次数，因此 效率要比LT模式高。epoll 工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。 异步 I/O 用户进程发起 read 操作之后，立刻就可以开始去做其它的事。而另一方面，从 kernel 的角度，当它受到一个 asynchronous read 之后，首先它会立刻返回，所以不会对用户进程产生任何 block 。然后，kernel 会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel 会给用户进程发送一个 signal ，告诉它 read 操作完成了。 blocking vs non-blocking 调用 blocking IO 会一直 block 住对应的进程直到操作完成，而 non-blocking IO 在 kernel 还准备数据的情况下会立刻返回。 synchronous IO vs asynchronous IO 在说明synchronous IO和asynchronous IO的区别之前，需要先给出两者的定义。 POSIX 的定义是这样子的： A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes; An asynchronous I/O operation does not cause the requesting process to be blocked; 两者的区别就在于 synchronous IO 做 IO operation 的时候会将 process 阻塞。按照这个定义，之前所述的 blocking IO，non-blocking IO，IO multiplexing 都属于 synchronous IO。 有人会说，non-blocking IO 并没有被 block 啊。这里有个非常 狡猾 的地方，定义中所指的 IO operation 是指真实的 IO 操作，就是例子中的 recvfrom 这个 system call 。non-blocking IO 在执行 recvfrom 这个 system call 的时候，如果 kernel 的数据没有准备好，这时候不会 block 进程。但是，当 kernel 中数据准备好的时候，recvfrom 会将数据从 kernel 拷贝到用户内存中，这个时候进程是被 block 了，在这段时间内，进程是被 block 的。 而 asynchronous IO 则不一样，当进程发起 IO 操作之后，就直接返回再也不理睬了，直到 kernel 发送一个信号，告诉进程说IO完成。在这整个过程中，进程完全没有被 block 。 参考 Linux IO模式及 select、poll、epoll详解 "},"zother2-interview/basic/os/memory/":{"url":"zother2-interview/basic/os/memory/","title":"内存管理","keywords":"","body":"内存管理 存储器工作原理 应用程序如何在计算机系统上运行的呢？首先，用编程语言编写和编辑应用程序，所编写的程序称为源程序，源程序不能再计算机上直接被运行，需要通过三个阶段的处理：编译程序处理源程序并生成目标代码，链接程序把他们链接为一个可重定位代码，此时该程序处于逻辑地址空间中；下一步装载程序将可执行代码装入物理地址空间，直到此时程序才能运行。 程序编译 源程序经过编译程序的处理生成目标模块（目标代码）。一个程序可由独立编写且具有不同功能的多个源程序模块组成，由于模块包含外部引用（即指向其他模块中的数据或指令地址，或包含对库函数的引用），编译程序负责记录引用发生的位置，其处理结果将产生相应的多个目标模块，每个模块都附有供引用使用的内部符号表和外部符号表。符号表中依次给出各个符号名及在本目标模块中的名字地址，在模块链接时进行转换。 程序链接 链接程序(Linker)的作用是根据目标模块之间的调用和依赖关系，将主调模块、被调模块以及所用到的库函数装配和链接成一个完整的可装载执行模块。根据程序链接发生的时间和链接方式，程序链接可分为以下三种方式： 静态链接：在程序装载到内存和运行前，就已将它所有的目标模块及所需要的库函数进行链接和装配成一个完整的可执行程序且此后不再拆分。 动态链接：在程序装入内存前并未事先进行程序各目标模块的链接，而是在程序装载时一边装载一边链接，生成一个可执行程序。在装载目标模块时，若发生外部模块调用，将引发响应外部目标模块的搜索、装载和链接。 运行时链接：在程序执行过程中，若发现被调用模块或库函数尚未链接，先在内存中进行搜索以查看是否装入内存；若已装入，则直接将其链接到调用程序中，否则进行该模块在外存上的搜索，以及装入内存和进行链接，生成一个可执行程序。 运行时链接将链接推迟到程序执行时，可以很好的提高系统资源的利用率和系统效率。 程序装载 程序装载就是将可执行程序装入内存，这里有三种方式： 绝对装载：装载模块中的指令地址始终与其内存中的地址相同，即模块中出现的所有地址均为绝对地址。 可重定位装载：根据内存当时的使用情况，决定将装载代码模块放入内存的物理位置。模块内使用的都是相对地址。 动态运行时装载：为提高内存利用率，装入内存的程序可换出到磁盘上，适当时候再换入内存中，对换前后程序在内存中的位置可能不同，即允许进程的内存映像在不同时候处于不同位置，此时模块内使用的地址必定为相对地址。 磁盘中的装载模块所使用的是逻辑地址，其逻辑地址集合称为进程的逻辑地址空间。进程运行时，其装载代码模块将被装入物理地址空间中，此时程序和数据的实际地址不可能同原来的逻辑一致。可执行程序逻辑地址转换为物理地址的过程被称为 “地址重定位”。 静态地址重定位：由装载程序实现装载代码模块的加载和物理地址转换，把它装入分配给进程的内存指定区域，其中所有逻辑地址修改为物理地址。地址转换在进程执行前一次完成，易于实现，但不允许程序在执行过程中移动位置。 动态地址重定位：由装载程序实现装载代码模块的加载，把它装入分配给进程的内存指定区域，但对链接程序处理过的程序的逻辑地址不做任何改变，程序内存起始地址被置入硬件专用寄存器 —— 重定位寄存器。程序执行过程中，每当CPU引用内存地址时，有硬件截取此逻辑地址，并在它被发送到内存之前加上重定位寄存器的值，以实现地址转换。 运行时链接地址重定位：对于静态和动态地址重定位装载方式而言，装载代码模块是由整个程序的所有目标模块及库函数经链接和整合构成的可执行程序，即在程序启动执行前已经完成了程序的链接过程。可见，装载代码的正文结构是静态的，在程序运行期间保持不变。运行时链接装载方式必然采用运行时链接地址重定位。 重定位寄存器：用于保存程序内存起始地址。 连续存储管理 固定分区存储管理 固定分区存储管理又称为静态分区模式，基本思想是：内存空间被划分成数目固定不变的分区，各分区大小不等，每个分区装入一个作业，若多个分区中都有作业，则他们可以并发执行。 为说明各分区分配和使用情况，需要设置一张内存分配表，记录内存中划分的分区及其使用情况。内存分配表中指出各分区起始地址和长度，占用标志用来指示此分区是否被使用。 可变分区存储管理 可变分区存储管理按照作业大小来划分分区，但划分的时间、大小、位置都是动态的。系统把作业装入内存时，根据其所需要的内存容量查看是否有足够空间，若有则按需分割一个分区分配给此作业；若无则令此作业等待内存资源。 虚拟内存 在连续存储管理的方式下，程序直接操作物理内存。这样的内存管理方式会造成诸多错误：地址空间不隔离、运行时内存地址不确定、内存使用率低。为解决这些问题人们提出了 内存分段 技术，同时内存分段需要引入 虚拟地址空间 的概念。 地址空间：可寻址的一片空间，如果这个空间是虚拟的，就是虚拟地址空间；如果是物理的，就是物理地址空间 虚拟内存是计算机系统内存管理的一种技术，虚拟地址空间构成虚拟内存。它使得应用程序认为它拥有连续可用的内存（一个连续完整的地址空间），而实际上，它通常是被分隔成多个物理内存碎片。还有部分暂时存储在外部磁盘存储器上（Swap），在需要时进行数据交换。与没有使用虚拟内存技术的系统相比，使用这种技术的系统使得大型程序的编写变得更容易，对真正的物理内存的使用也更有效率。 虚拟内存不只是用磁盘空间来扩展物理内存 的意思——这只是扩充内存级别以使其包含硬盘驱动器而已。把内存扩展到磁盘只是使用虚拟内存技术的一个结果，它的作用也可以通过覆盖或者把处于不活动状态的程序以及它们的数据全部交换到磁盘上等方式来实现。 内存分页 内存分段能很好的解决地址空间不隔离、运行时内存地址不确定的问题，但对于内存使用率低不能很好的解决。这个问题的关键是能不能在换出一个完整的程序之后，把另一个完整的程序换进来。而这种分段机制，映射的是一片连续的物理内存，所以得不到解决。 内存分页仍然是一种虚拟地址空间到物理地址空间映射的机制，但粒度更加的小了。内存分页的虚拟地址空间仍然是连续的，但是，每一页映射后的物理地址就不一定是连续的了。正是因为有了分页的概念，程序的换入换出就可以以页为单位了。 MMU 内存管理单元（memory management unit，MMU），它是一种负责处理中央处理器（CPU）的内存访问请求的计算机硬件。它的功能包括 虚拟地址到物理地址的转换（即虚拟内存管理）、内存保护、中央处理器高速缓存的控制。 内存管理单元通常借助一种 转译旁观缓冲器（Translation Lookaside Buffer，TLB） 的高速缓存来将虚拟页号转换为物理页号。当 TLB 中没有转换记录时，则使用一种较慢的机制，其中包括专用硬件（hardware-specific）的数据结构（Data structure）或软件辅助手段。这个数据结构称为 分页表，页表中的数据就叫做 分页表项（page table entry，PTE）。物理页号结合页偏移量便提供出了完整的物理地址。 有时，TLB 或 PTE 会禁止对虚拟页的访问，这可能是因为没有物理随机存取存储器（random access memory）与虚拟页相关联。如果是这种情况， MMU 将向 CPU 发出 缺页错误（page fault） 的信号。操作系统将进行处理，也许会尝试寻找 RAM 的空白帧，同时创建一个新的 PTE 将之映射到所请求的虚拟地址。如果没有空闲的 RAM，可能必须关闭一个已经存在的页面，使用一些替换算法，将之保存到磁盘中（这被称之为 页面调度）。 TLB 最简单的分页表系统通常维护一个帧表和一个分页表，帧表处理帧映射信息。分页表处理页的虚拟地址和物理帧的映射。还有一些辅助信息，如当前存在标识位(present bit)，脏数据标识位 或已修改的标识位，地址空间或 进程ID 信息。 辅助存储，比如硬盘可以用于增加物理内存。页可以调入和调出到物理内存和磁盘。当前存在标识位可以指出那些页在物理内存，哪些页在硬盘上。页可以指出如何处理这些不同页。是否从硬盘读取一个页，或把其他页从物理内存调出。页面的频繁更换，导致整个系统效率急剧下降，这个现象称为 内存抖动（或颠簸）。抖动一般是内存分配算法不好，内存太小引或者程序的算法不佳引起的。 脏数据标识位可以优化性能。一个页从硬盘调入到物理内存中，读取后调出，当页没有更改不需要写回硬盘。但是如果页在调入物理内存后被修改，会标记其为脏数据，意味着该页必需被写回备用存储。这种策略需要当页被调入到内存后，后备存储保留一份页的拷贝。有了脏数据标志位，一些页随时会同时存在于物理内存和后备存储中。 地址空间 或 进程ID 信息是必要的，内存管理系统需要知道页对应的进程。两个不同的进程可以使用两个相同的虚拟地址。分页表必需为两个进程提供不同的虚拟内存。用 虚拟内存页关联进程ID 页可以帮助选择要调出的页，当页关联到不活跃进程上，特别是那些主要代码页被调出的进程，相比活跃进程需要页的可能性会更小。 进程空间 有内存分页后，进程的概念很自然就被发明出来了。TLB 是 MMU 的输入，那么一个自然的想法，我们有多个 TLB ，一会儿用这个，一会儿用那个，这样我们可以根据需要用不同的内存，这个用来表达不同 MMU 的概念，体现在软件的观感上，就是进程。 当然， MMU 封装为进程，还和“线程”概念进行了一定程度上进行了绑定，所以，软件的进程，不是简单的 MMU 的封装，但我们在理解寻址的概念的时候，是可以简单这样认为的。有了页寻址的概念，进程的空间和物理地址看到的空间就很不一样了。 进程内核空间 从进程发出的内存访问，都通过 MMU 翻译，包括请求从一个进程切换到另一个进程。这样一来，进程的权力太大了。所以，还是很自然的，我们在 MMU 中可以设置一个标志，说明 某些地址是具有更高优先级 的，一般情况下不允许访问，要访问就要提权。一旦提权，代码执行流就强行切换到这片高级内存的特定位置。这样，一个进程就有了两个权限级别：用户态和内核态，用户态用于一般的程序，内核态用来做进程的管理。 用户态和内核态都在同一个页表中管理，所以，本质上它们属于 同一个进程。只是它们属于进程的不同权限而已。为了实现的方便，现在通常让所有进程的 MMU 页表的内核空间指向物理空间的同一个位置，这样，看起来我们就有了一个独立于所有进程的实体，这个实体被称为 操作系统内核，它是管理所有进程的一个中心软件。 页面调度算法 FIFO算法：先入先出，即淘汰最早调入的页面。 OPT(MIN)算法：选未来最远将使用的页淘汰，是一种最优的方案，可以证明缺页数最小。可惜，MIN需要知道将来发生的事，只能在理论中存在，实际不可应用。 LRU(Least-Recently-Used)算法：用过去的历史预测将来，选最近最长时间没有使用的页淘汰(也称最近最少使用)。性能最接近OPT。与页面使用时间有关。 LFU(Least Frequently Used)算法：即最不经常使用页置换算法，要求在页置换时置换引用计数最小的页，因为经常使用的页应该有一个较大的引用次数。与页面使用次数有关。 Clock：给每个页帧关联一个使用位，当该页第一次装入内存或者被重新访问到时，将使用位置为1。每次需要替换时，查找使用位被置为0的第一个帧进行替换。在扫描过程中，如果碰到使用位为1的帧，将使用位置为0，在继续扫描。如果所谓帧的使用位都为0，则替换第一个帧。 参考链接 地址空间的故事 "},"zother2-interview/basic/os/questions/":{"url":"zother2-interview/basic/os/questions/","title":"Index","keywords":"","body":"面试题 PE文件 PE文件的全称是Portable Executable，意为可移植的可执行的文件，常见的EXE、DLL、OCX、SYS、COM都是PE文件，PE文件是微软Windows操作系统上的程序文件（可能是间接被执行，如DLL）。 什么是活锁？与死锁有和区别？ 活锁指的是 任务或者执行者没有被阻塞，由于某些条件没有满足，导致一直重复尝试，失败，尝试，失败。 活锁和死锁的区别在于，处于活锁的实体是在不断的改变状态，所谓的“活”， 而处于死锁的实体表现为等待；活锁有可能自行解开，死锁则不能。 活锁应该是一系列进程在轮询地等待某个不可能为真的条件为真。活锁的时候进程是不会blocked，这会导致耗尽CPU资源。 为解决活锁可以引入一些随机性，例如如果检测到冲突，那么就暂停随机的一定时间进行重试。这回大大减少碰撞的可能性。典型的例子是以太网的CSMA/CD检测机制。 直接寻址与间接寻址？ 寻址方式就是处理器根据指令中给出的地址信息来寻找物理地址的方式，是确定本条指令的数据地址以及下一条要执行的指令地址的方法。在操作系统中分为指令寻址和操作数寻址。 指令寻址：在内存中查找指令的方式。 顺序寻址方式：即采用PC计数器来计数指令的顺序； 跳跃寻址方式：下条指令的地址码不是由程序计数器给出，而是由本条指令给出。 操作数寻址：形成操作数的有效地址的方法称为操作数的寻址方式。 立即寻址：操作数作为指令的一部分而直接写在指令中； 直接寻址：直接寻址是一种基本的寻址方法。在指令格式的地址的字段中直接指出操作数在内存的地址。由于操作数的地址直接给出而不需要经过某种变换，所以称这种寻址方式为直接寻址方式。 简介寻址：间接寻址是相对直接寻址而言的，在间接寻址的情况下，指令地址字段中的形式地址不是操作数的真正地址，而是操作数地址的指示器，或者说此形式地址单元的内容才是操作数的有效地址。 如何从用户态切换到内核态？ 程序请求系统服务，执行系统调用 程序运行期间产生中断事件，运行程序被中断，转向中断处理程序处理 程序运行时产生异常事件，运行程序被打断，转向异常处理程序。 这三种情况都是通过中断机制发生，可以说 中断和异常是用户态到内核态转换的仅有途径。 实时操作系统和分时操作系统的区别？ 分时操作系统：多个联机用户同时适用一个计算机系统在各自终端上进行交互式会话，程序、数据和命令均在会话过程中提供，以问答方式控制程序运行。系统把处理器的时间划分为时间片轮流分配给各个连接终端。 实时操作系统：当外部时间或数据产生时，能够对其予以接受并以足够快的速度进行处理，所得结果能够在规定时间内控制生产过程或对控制对象作出快速响应，并控制所有实时任务协调的操作系统。因而，提供及时响应和高可靠性是其主要特点。实时操作系统有硬实时和软实时之分，硬实时要求在规定的时间内必须完成操作，这是在操作系统设计时保证的；软实时则只要按照任务的优先级，尽可能快地完成操作即可。我们通常使用的操作系统在经过一定改变之后就可以变成实时操作系统。 下面还要补充一个批处理操作系统：批处理是指用户将一批作业提交给操作系统后就不再干预，由操作系统控制它们自动运行。这种采用批量处理作业技术的操作系统称为批处理操作系统。批处理操作系统分为单道批处理系统和多道批处理系统。批处理操作系统不具有交互性，它是为了提高CPU的利用率而提出的一种操作系统。 如果某个操作系统兼有批处理、分时和实时处理的全部或两种功能，我们称为通用操作系统。 "},"zother2-interview/basic/os/_index.html":{"url":"zother2-interview/basic/os/_index.html","title":"操作系统基础","keywords":"","body":"操作系统基础 操作系统提供的服务 操作系统的五大功能，分别为：作业管理、文件管理、存储管理、输入输出设备管理、进程及处理机管理 中断 所谓的中断就是在计算机执行程序的过程中，由于出现了某些特殊事情，使得CPU暂停对程序的执行，转而去执行处理这一事件的程序。等这些特殊事情处理完之后再回去执行之前的程序。中断一般分为三类： 内部异常中断：由计算机硬件异常或故障引起的中断； 软中断：由程序中执行了引起中断的指令而造成的中断（这也是和我们将要说明的系统调用相关的中断）； 外部中断：由外部设备请求引起的中断，比如I/O请求。 简单来说，对中断的理解就是对一些特殊事情的处理。 与中断紧密相连的一个概念就是中断处理程序了。当中断发生的时候，系统需要去对中断进行处理，对这些中断的处理是由操作系统内核中的特定函数进行的，这些处理中断的特定的函数就是我们所说的中断处理程序了。 另一个与中断紧密相连的概念就是中断的优先级。中断的优先级说明的是当一个中断正在被处理的时候，处理器能接受的中断的级别。中断的优先级也表明了中断需要被处理的紧急程度。每个中断都有一个对应的优先级，当处理器在处理某一中断的时候，只有比这个中断优先级高的中断可以被处理器接受并且被处理。优先级比这个当前正在被处理的中断优先级要低的中断将会被忽略。 典型的中断优先级如下所示： 机器错误 > 时钟 > 磁盘 > 网络设备 > 终端 > 软件中断 当发生软件中断时，其他所有的中断都可能发生并被处理；但当发生磁盘中断时，就只有时钟中断和机器错误中断能被处理了。 系统调用 在讲系统调用之前，先说下进程的执行在系统上的两个级别：用户级和核心级，也称为用户态和系统态(user mode and kernel mode)。 程序的执行一般是在用户态下执行的，但当程序需要使用操作系统提供的服务时，比如说打开某一设备、创建文件、读写文件等，就需要向操作系统发出调用服务的请求，这就是系统调用。 Linux系统有专门的函数库来提供这些请求操作系统服务的入口，这个函数库中包含了操作系统所提供的对外服务的接口。当进程发出系统调用之后，它所处的运行状态就会由用户态变成核心态。但这个时候，进程本身其实并没有做什么事情，这个时候是由内核在做相应的操作，去完成进程所提出的这些请求。 系统调用和中断的关系就在于，当进程发出系统调用申请的时候，会产生一个软件中断。产生这个软件中断以后，系统会去对这个软中断进行处理，这个时候进程就处于核心态了。 那么用户态和核心态之间的区别是什么呢？ 用户态的进程能存取它们自己的指令和数据，但不能存取内核指令和数据（或其他进程的指令和数据）。然而，核心态下的进程能够存取内核和用户地址 某些机器指令是特权指令，在用户态下执行特权指令会引起错误 对此要理解的一个是，在系统中内核并不是作为一个与用户进程平行的估计的进程的集合，内核是为用户进程运行的。 "},"zother2-interview/basic/_index.html":{"url":"zother2-interview/basic/_index.html","title":"Index","keywords":"","body":"计算机基础 "},"zother2-interview/fromwork/mybatis/cache/":{"url":"zother2-interview/fromwork/mybatis/cache/","title":"Mybatis 缓存机制","keywords":"","body":"Mybatis 缓存机制 Mybatis 的缓存均缓存查询操作结果。按照作用域范围，可以分为： - **一级缓存**： `SqlSession` 级别的缓存 - **二级缓存**： `namespace` 级别的缓存 一级缓存 Mybatis 默认开启了一级缓存， 一级缓存有两个级别可以设置：分别是 SESSION 或者 STATEMENT 默认是 SESSION 级别，即在一个 MyBatis会话中执行的所有语句，都会共享这一个缓存。一种是 STATEMENT 级别，可以理解为缓存只对当前执行的这一个 Statement 有效。 STATEMENT 级别相当于关闭一级缓存 基本原理 在一级缓存中，当 sqlSession 执行写操作（执行插入、更新、删除），清空 SqlSession 中的一级缓存。 总结 MyBatis 一级缓存的生命周期和SqlSession一致。 MyBatis 一级缓存内部设计简单，只是一个没有容量限定的HashMap，在缓存的功能性上有所欠缺。 MyBatis 的一级缓存最大范围是SqlSession内部，有多个SqlSession或者分布式的环境下，数据库写操作会引起脏数据，建议设定缓存级别为Statement。 二级缓存 如果多个 SqlSession 之间需要共享缓存，则需要使用到二级缓存。开启二级缓存后，会使用 CachingExecutor 装饰 Executor ，进入一级缓存的查询流程前，先在C achingExecutor 进行二级缓存的查询，具体的工作流程如下所示。 二级缓存开启后，同一个namespace下的所有操作语句，都影响着同一个Cache，即二级缓存被多个SqlSession共享，是一个全局的变量。当开启缓存后，数据的查询执行的流程就是 二级缓存 -> 一级缓存 -> 数据库。 总结 MyBatis 的二级缓存相对于一级缓存来说，实现了 SqlSession 之间缓存数据的共享，同时粒度更加的细，能够到 namespace 级别，通过 Cache 接口实现类不同的组合，对Cache的可控性也更强。 MyBatis 在多表查询时，极大可能会出现脏数据，有设计上的缺陷，安全使用二级缓存的条件比较苛刻。 在分布式环境下，由于默认的 MyBatis Cache 实现都是基于本地的，分布式环境下必然会出现读取到脏数据，需要使用集中式缓存将 MyBatis 的 Cache 接口实现，有一定的开发成本，直接使用 Redis、Memcached 等分布式缓存可能成本更低，安全性也更高。 "},"zother2-interview/fromwork/mybatis/proxy/":{"url":"zother2-interview/fromwork/mybatis/proxy/","title":"Mybatis 动态代理","keywords":"","body":"Mybatis 动态代理 获取代理类流程 获取Mapper代理类的时序图如下： 重点说下MapperProxy类，声明如下： public class MapperProxy implements InvocationHandler, Serializable 获取到 MapperProxy 之后，根据调用不同的方法，会将最终的参数传递给 SqlSession。 "},"zother2-interview/fromwork/mybatis/question/":{"url":"zother2-interview/fromwork/mybatis/question/","title":"面试题","keywords":"","body":"面试题 #{}和${}的区别是什么？ #{}是预编译处理，${}是字符串替换。 Mybatis 在处理 #{} 时，会将sql中的 #{} 替换为 ? 号，调用 PreparedStatement 的 set 方法来赋值； Mybatis在处理${}时，就是把${}替换成变量的值。 使用#{}可以有效的防止SQL注入，提高系统安全性。 通常一个Xml映射文件，都会写一个Dao接口与之对应，请问，这个Dao接口的工作原理是什么？Dao接口里的方法，参数不同时，方法能重载吗？ Dao 接口，就是人们常说的 Mapper 接口，接口的全限名，就是映射文件中的 namespace 的值，接口的方法名，就是映射文件中 MappedStatement 的 id 值，接口方法内的参数，就是传递给sql的参数。 Mapper 接口是没有实现类的，当调用接口方法时，接口全限名+方法名 拼接字符串作为 key 值，可唯一定位一个MappedStatement。 在Mybatis中，每一个、、、 标签，都会被解析为一个 MappedStatement 对象。 Dao接口里的方法，是 不能重载 的，因为是全限名+方法名的保存和寻找策略。 Dao接口的工作原理是 JDK 动态代理， Mybatis 运行时会使用 JDK 动态代理为 Dao 接口生成代理 proxy 对象，代理对象 proxy 会拦截接口方法，转而执行 MappedStatement 所代表的sql，然后将sql执行结果返回。 参考连接 Mybatis 的常见面试题 "},"zother2-interview/fromwork/mybatis/_index.html":{"url":"zother2-interview/fromwork/mybatis/_index.html","title":"Index","keywords":"","body":""},"zother2-interview/fromwork/netty/":{"url":"zother2-interview/fromwork/netty/","title":"Netty","keywords":"","body":"Netty Netty 是一个 异步 事件驱动 的网络应用框架，用于快速开发高性能、可扩展协议的服务器和客户端 Reactor 无论是 C++ 还是 Java 编写的网络框架，大多数都是基于 Reactor 模式进行设计和开发，Reactor 模式基于事件驱动，特别适合处理海量的 I/O 事件。 反应器设计模式-维基百科 -- 反应器设计模式(Reactor pattern)是一种为处理服务请求并发 提交到一个或者多个服务处理程序的事件设计模式。当请求抵达后，服务处理程序使用解多路分配策略，然后同步地派发这些请求至相关的请求处理程序。 单线程模型 Reactor 单线程模型，指的是所有的 IO 操作都在同一个 NIO 线程上面完成，NIO 线程的职责如下： 作为 NIO 服务端，接收客户端的 TCP 连接； 作为 NIO 客户端，向服务端发起 TCP 连接； 读取通信对端的请求或者应答消息； 向通信对端发送消息请求或者应答消息。 由于 Reactor 模式使用的是异步非阻塞 IO，所有的 IO 操作都不会导致阻塞，理论上一个线程可以独立处理所有 IO 相关的操作。从架构层面看，一个 NIO 线程确实可以完成其承担的职责。例如，通过 Acceptor 类接收客户端的 TCP 连接请求消息，链路建立成功之后，通过 Dispatch 将对应的 ByteBuffer 派发到指定的 Handler 上进行消息解码。用户线程可以通过消息编码通过 NIO 线程将消息发送给客户端。 对于一些小容量应用场景，可以使用单线程模型。但是 对于高负载、大并发的应用场景却不合适。 多线程模型 Rector 多线程模型与单线程模型最大的区别就是有一组 NIO 线程处理 IO 操作，它的原理图如下： Reactor 多线程模型的特点： 有专门一个 NIO 线程 Acceptor 线程用于监听服务端，接收客户端的 TCP 连接请求； 网络 IO 操作 - 读、写等由一个 NIO 线程池负责，线程池可以采用标准的 JDK 线程池实现，它包含一个任务队列和 N 个可用的线程，由这些 NIO 线程负责消息的读取、解码、编码和发送； 1 个 NIO 线程可以同时处理 N 条链路，但是 1 个链路只对应 1 个 NIO 线程，防止发生并发操作问题。 主从多线程模型 主从 Reactor 线程模型的特点是：服务端用于接收客户端连接的不再是个 1 个单独的 NIO 线程，而是一个独立的 NIO 线程池。 Acceptor 接收到客户端 TCP 连接请求处理完成后（可能包含接入认证等），将新创建的 SocketChannel 注册到 IO 线程池（sub reactor 线程池）的某个 IO 线程上，由它负责 SocketChannel 的读写和编解码工作。 Acceptor 线程池仅仅只用于客户端的登陆、握手和安全认证，一旦链路建立成功，就将链路注册到后端 subReactor 线程池的 IO 线程上，由 IO 线程负责后续的 IO 操作。 它的工作流程总结如下： 从主线程池中随机选择一个 Reactor 线程作为 Acceptor 线程，用于绑定监听端口，接收客户端连接； Acceptor 线程接收客户端连接请求之后创建新的 SocketChannel ，将其注册到主线程池的其它 Reactor 线程上，由其负责接入认证、IP 黑白名单过滤、握手等操作； 步骤 2 完成之后，业务层的链路正式建立，将 SocketChannel 从主线程池的 Reactor 线程的多路复用器上摘除，重新注册到 Sub 线程池的线程上，用于处理 I/O 的读写操作。 Netty 的优势 多路复用，并在 NIO 的基础上进行更高层次的抽象 事件机制 功能强大，预置了多种编解码功能，支持多种主流协议 定制能力强，可以通过ChannelHandler对通信框架进行灵活的扩展 Netty 为什么性能好？ 纯异步：Reactor 线程模型 IO 多路复用 GC 优化：更少的分配内存、池化（Pooling）、复用、选择性的使用 sun.misc.Unsafe 更多的硬件相关优化（mechanical sympathy） 内存泄漏检测 \"Zero Copy\" Zero Copy Netty 的 Zero-copy 体现在如下几个个方面: Netty 提供了 CompositeByteBuf 类, 它可以将多个 ByteBuf 合并为一个逻辑上的 ByteBuf , 避免了各个 ByteBuf 之间的拷贝. 通过 wrap 操作, 我们可以将 byte[] 数组、ByteBuf 、 ByteBuffer 等包装成一个 Netty ByteBuf 对象, 进而避免了拷贝操作. ByteBuf 支持 slice 操作, 因此可以将 ByteBuf 分解为多个共享同一个存储区域的 ByteBuf, 避免了内存的拷贝. 通过 FileRegion 包装的 FileChannel.tranferTo 实现文件传输, 可以直接将文件缓冲区的数据发送到目标 Channel , 避免了传统通过循环 write 方式导致的内存拷贝问题. 垃圾回收 Netty 里 HeapByteBuffer 底下的 byte[] 能够依赖JVM GC自然回收；而 DirectByteBuffer 底下是 Java 堆外内存，除了等JVM GC，最好也能主动进行回收；所以，Netty ByteBuf需要在 JVM 的 GC 机制之外，有自己的引用计数器和回收过程。 原生的 JVM GC 很难回收掉 DirectByteBuffer 所占用的 Native Memory Netty 中采用引用计数对 DirectByteBuffer 进行对象可达性检测，当 DirectByteBuffer 上的引用计数为 0 时将对象释放。 @Override public boolean release() { for (;;) { int refCnt = this.refCnt; if (refCnt == 0) { throw new IllegalReferenceCountException(0, -1); } if (refCntUpdater.compareAndSet(this, refCnt, refCnt - 1)) { if (refCnt == 1) { deallocate(); return true; } return false; } } } Netty 内存泄漏，主要是针对池化的 ByteBuf 。 ByteBuf 对象被 JVM GC 掉之前，没有调用 release() 把底下的 DirectByteBuffer 或byte[] 归还，会导致池越来越大。而非池化的 ByteBuf ，即使像 DirectByteBuf 那样可能会用到 System.gc() ，但终归会被 release 掉的，不会出大事。因此 Netty 默认会从分配的 ByteBuf 里抽样出大约 1% 的来进行跟踪。 源码 ByteBuf ByteBuf 扩容采用先倍增后步进的方式 DirectBuffer vs HeapBuffer 在执行网络IO或者文件IO时，如果是使用 DirectBuffer 就会少一次内存拷贝。如果是非 DirectBuffer ，JDK 会先创建一个 DirectBuffer ，再去执行真正的写操作。这是因为，当我们把一个地址通过 JNI 传递给底层的C库的时候，有一个基本的要求，就是这个地址上的内容不能失效。然而，在 GC 管理下的对象是会在 Java 堆中移动的。也就是说，有可能我把一个地址传给底层的 write ，但是这段内存却因为 GC 整理内存而失效了。所以我必须要把待发送的数据放到一个 GC 管不着的地方。这就是调用 native 方法之前，数据一定要在堆外内存的原因。 Netty 启动以及链接建立过程 Epoll 触发 有两种模式，一是水平触发（LT），二是边缘触发（ET）。 在LT模式下，只要某个fd还有数据没读完，那么下次轮询还会被选出。而在ET模式下，只有fd状态发生改变后，该fd才会被再次选出。ET模式的特殊性，使在ET模式下的一次轮询必须处理完本次轮询出的fd的所有数据，否则该fd将不会在下次轮询中被选出。 NioChannel：是水平触发 EpollChannel：是边缘触发，Netty 为保证数据完整会在特定条件下自己触发 Epoll Event，来读取数据 JDK NIO BUG 正常情况下，selector.select() 操作是阻塞的，只有被监听的 fd 有读写操作时，才被唤醒 但是，在这个 bug 中，没有任何 fd 有读写请求，但是 select() 操作依旧被唤醒 很显然，这种情况下，selectedKeys() 返回的是个空数组 然后按照逻辑执行到 while(true) 处，循环执行，导致死循环。 Netty 解决方案： long currentTimeNanos = System.nanoTime(); for (;;) { // 1.定时任务截止事时间快到了，中断本次轮询 //... // 2.轮询过程中发现有任务加入，中断本次轮询 //... // 3.阻塞式select操作 selector.select(timeoutMillis); // 4.解决jdk的nio bug long time = System.nanoTime(); if (time - TimeUnit.MILLISECONDS.toNanos(timeoutMillis) >= currentTimeNanos) { selectCnt = 1; } else if (SELECTOR_AUTO_REBUILD_THRESHOLD > 0 && selectCnt >= SELECTOR_AUTO_REBUILD_THRESHOLD) { rebuildSelector(); selector = this.selector; selector.selectNow(); selectCnt = 1; break; } currentTimeNanos = time; //... } netty 会在每次进行 selector.select(timeoutMillis) 之前记录一下开始时间 currentTimeNanos ，在 select 之后记录一下结束时间，判断 select 操作是否至少持续了 timeoutMillis 秒。如果持续的时间大于等于 timeoutMillis ，说明就是一次有效的轮询，重置 selectCnt 标志，否则，表明该阻塞方法并没有阻塞这么长时间，可能触发了 jdk 的空轮询 bug ，当空轮询的次数超过一个阀值的时候，默认是 512 ，就开始重建 selector "},"zother2-interview/fromwork/spring/aop/":{"url":"zother2-interview/fromwork/spring/aop/","title":"AOP","keywords":"","body":"AOP AOP 的存在价值 在传统 OOP 编程里以对象为核心，整个软件系统由系列相互依赖的对象所组成，而这些对象将被抽象成一个一个的类，并允许使用类继承来管理类与类之间一般到特殊的关系。随着软件规模的增大，应用的逐渐升级，慢慢出现了一些 OOP 很难解决的问题。 我们可以通过分析、抽象出一系列具有一定属性与行为的对象，并通过这些对象之间的协作来形成一个完整的软件功能。由于对象可以继承，因此我们可以把具有相同功能或相同特性的属性抽象到一个层次分明的类结构体系中。随着软件规范的不断扩大，专业化分工越来越系列，以及 OOP 应用实践的不断增多，随之也暴露出了一些 OOP 无法很好解决的问题。 现在假设系统中有 3 段完全相似的代码，这些代码通常会采用“复制”、“粘贴”方式来完成，通过这种“复制”、“粘贴”方式开发出来的软件如图 1 所示。 看到如上图所示的示意图，可能有的读者已经发现了这种做法的不足之处：如果有一天，上图中的深色代码段需要修改，那是不是要打开 3 个地方的代码进行修改？如果不是 3 个地方包含这段代码，而是 100 个地方，甚至是 1000 个地方包含这段代码段，那会是什么后果？ 为了解决这个问题，我们通常会采用将如上图所示的深色代码部分定义成一个方法，然后在 3 个代码段中分别调用该方法即可。在这种方式下，软件系统的结构如下图所示。 "},"zother2-interview/fromwork/spring/design-partten/":{"url":"zother2-interview/fromwork/spring/design-partten/","title":"设计模式","keywords":"","body":"设计模式 代理模式：AOP 单例模式：默认 Bean 为单例 工厂模式：BeanFactory IOC：依赖倒置 or 依赖注入 MVC：spring web 模版方法模式：JdbcTemplate "},"zother2-interview/fromwork/spring/ioc/":{"url":"zother2-interview/fromwork/spring/ioc/","title":"IOC","keywords":"","body":"IOC Ioc—Inversion of Control，即“控制反转”，不是什么技术，而是一种设计思想。在Java 开发中，Ioc意味着将你设计好的对象交给容器控制，而不是传统的在你的对象内部直接控制。如何理解好Ioc呢？理解好Ioc的关键是要明确“谁控制谁，控制什么，为何是反转（有反转就应该有正转了），哪些方面反转了”，那我们来深入分析一下： 谁控制谁，控制什么：传统Java SE程序设计，我们直接在对象内部通过new进行创建对象，是程序主动去创建依赖对象；而IoC是有专门一个容器来创建这些对象，即由Ioc容器来控制对象的创建；谁控制谁？当然是IoC 容器控制了对象；控制什么？那就是主要控制了外部资源获取（不只是对象包括比如文件等）。 为何是反转，哪些方面反转了：有反转就有正转，传统应用程序是由我们自己在对象中主动控制去直接获取依赖对象，也就是正转；而反转则是由容器来帮忙创建及注入依赖对象；为何是反转？因为由容器帮我们查找及注入依赖对象，对象只是被动的接受依赖对象，所以是反转；哪些方面反转了？依赖对象的获取被反转了。 IoC能做什么 IoC 不是一种技术，只是一种思想，一个重要的面向对象编程的法则，它能指导我们如何设计出松耦合、更优良的程序。传统应用程序都是由我们在类内部主动创建依赖对象，从而导致类与类之间高耦合，难于测试；有了IoC容器后，把创建和查找依赖对象的控制权交给了容器，由容器进行注入组合对象，所以对象与对象之间是 松散耦合，这样也方便测试，利于功能复用，更重要的是使得程序的整个体系结构变得非常灵活。 IoC和DI DI—Dependency Injection，即“依赖注入”：组件之间依赖关系由容器在运行期决定，形象的说，即由容器动态的将某个依赖关系注入到组件之中。依赖注入的目的并非为软件系统带来更多功能，而是为了提升组件重用的频率，并为系统搭建一个灵活、可扩展的平台。通过依赖注入机制，我们只需要通过简单的配置，而无需任何代码就可指定目标需要的资源，完成自身的业务逻辑，而不需要关心具体的资源来自何处，由谁实现。 理解DI的关键是：“谁依赖谁，为什么需要依赖，谁注入谁，注入了什么”，那我们来深入分析一下： 谁依赖于谁：当然是应用程序依赖于IoC容器； 为什么需要依赖：应用程序需要IoC容器来提供对象需要的外部资源； 谁注入谁：很明显是IoC容器注入应用程序某个对象，应用程序依赖的对象； 注入了什么：就是注入某个对象所需要的外部资源（包括对象、资源、常量数据）。 IoC和DI由什么关系呢？其实它们是同一个概念的不同角度描述，由于控制反转概念比较含糊（可能只是理解为容器控制对象这一个层面，很难让人想到谁来维护对象关系），所以2004年大师级人物Martin Fowler又给出了一个新的名字：“依赖注入”，相对IoC 而言，“依赖注入”明确描述了“被注入对象依赖IoC容器配置依赖对象”。 IOC vs Factory 简单来说，IOC 与 工厂模式 分别代表了 push 与 pull 的机制： Pull 机制：类间接依赖于 Factory Method ，而 Factory Method 又依赖于具体类。 Push 机制：容器可以在一个位置配置所有相关组件，从而促进高维护和松耦合。 使用 工厂模式 的责任仍然在于类（尽管间接地）来创建新对象，而 依赖注入 将责任外包。 循环依赖 Spring 为了解决单例的循环依赖问题，使用了 三级缓存 ，递归调用时发现 Bean 还在创建中即为循环依赖 /** 一级缓存：用于存放完全初始化好的 bean **/ private final Map singletonObjects = new ConcurrentHashMap(256); /** 二级缓存：存放原始的 bean 对象（尚未填充属性），用于解决循环依赖 */ private final Map earlySingletonObjects = new HashMap(16); /** 三级级缓存：存放 bean 工厂对象，用于解决循环依赖 */ private final Map> singletonFactories = new HashMap>(16); /** bean 的获取过程：先从一级获取，失败再从二级、三级里面获取 创建中状态：是指对象已经 new 出来了但是所有的属性均为 null 等待被 init */ A 创建过程中需要 B，于是 A 将自己放到三级缓里面 ，去实例化 B B 实例化的时候发现需要 A，于是 B 先查一级缓存，没有，再查二级缓存，还是没有，再查三级缓存，找到了！ 然后把三级缓存里面的这个 A 放到二级缓存里面，并删除三级缓存里面的 A B 顺利初始化完毕，将自己放到一级缓存里面（此时B里面的A依然是创建中状态） 然后回来接着创建 A，此时 B 已经创建结束，直接从一级缓存里面拿到 B ，然后完成创建，并将自己放到一级缓存里面 如此一来便解决了循环依赖的问题 "},"zother2-interview/fromwork/spring/_index.html":{"url":"zother2-interview/fromwork/spring/_index.html","title":"Spring 基本","keywords":"","body":"Spring Spring Framework 是一个开源的Java／Java EE全功能栈（full-stack）的应用程序框架，其提供了一个简易的开发方式，这种开发方式，将避免那些可能致使底层代码变得繁杂混乱的大量的属性文件和帮助类。 Spring中包含的关键特性 强大的基于JavaBeans的采用 控制反转 （Inversion of Control，IoC）原则的配置管理，使得应用程序的组建更加快捷简易。 一个可用于 Java EE 等运行环境的核心 Bean 工厂。 数据库事务的一般化抽象层，允许声明式（Declarative）事务管理器，简化事务的划分使之与底层无关。 内建的针对 JTA 和单个 JDBC 数据源的一般化策略，使 Spring 的事务支持不要求Java EE环境，这与一般的JTA或者EJB CMT相反。 JDBC 抽象层提供了有针对性的异常等级（不再从SQL异常中提取原始代码），简化了错误处理，大大减少了程序员的编码量。再次利用JDBC时，你无需再写出另一个'终止'（finally）模块。并且面向JDBC的异常与Spring通用数据访问对象（Data Access Object）异常等级相一致。 以资源容器，DAO实现和事务策略等形式与 Hibernate，JDO 和 MyBatis、SQL Maps 集成。利用众多的翻转控制方便特性来全面支持，解决了许多典型的 Hibernate 集成问题。所有这些全部遵从 Spring 通用事务处理和通用数据访问对象异常等级规范。 灵活的基于核心 Spring 功能的 MVC 网页应用程序框架。开发者通过策略接口将拥有对该框架的高度控制，因而该框架将适应于多种呈现（View）技术，例如 JSP、FreeMarker、Velocity、Thymeleaf 等。值得注意的是，Spring 中间层可以轻易地结合于任何基于 MVC 框架的网页层，例如 Struts、WebWork 或 Tapestry。 提供诸如事务管理等服务的AOP框架。 "},"zother2-interview/java/annotation/":{"url":"zother2-interview/java/annotation/","title":"注解","keywords":"","body":"注解 注解(Annotation)是 Java1.5 中引入的一个重大修改之一，为我们在代码中添加信息提供了一种形式化的方法，使我们可以在稍后某个时刻非常方便的使用这些数据。注解在一定程度上是把元数据与源代码结合在一起，而不是保存在外部文档中。注解的含义可以理解为 java 中的元数据。元数据是描述数据的数据。 注解是一个继承自java.lang.annotation.Annotation的接口 可见性 根据注解在程序不同时期的可见性，可以把注解区分为： source：注解会在编译期间被丢弃，不会编译到 class 文件 class：注解会被编译到 class 文件中，但是在运行时不能获取 runtime：注解会被编译到 class 文件中，并且能够在运行时通过反射获取 继承 有@Inherited 没有@Inherited 子类的类上能否继承到父类的类上的注解？ 否 能 子类实现了父类上的抽象方法 否 否 子类继承了父类上的方法 能 能 子类覆盖了父类上的方法 否 否 @Inherited 只是可控制对类名上注解是否可以被继承。不能控制方法上的注解是否可以被继承。 注解的实现机制 注解是继承自：java.lang.annotation.Annotation 的接口 ... Compiled from \"TestAnnotation.java\" public interface TestAnnotation extends java.lang.annotation.Annotation ... 注解内部的属性是在编译期间确定的 ... SourceFile: \"SimpleTest.java\" RuntimeVisibleAnnotations: 0: #43(#44=s#45) ... 注解在运行时会生成 Proxy 代理类，并使用 AnnotationInvocationHandler.memberValues 来进行数据读取 ... default: //从 Map 中获取数据 Object var6 = this.memberValues.get(var4); if (var6 == null) { throw new IncompleteAnnotationException(this.type, var4); } else if (var6 instanceof ExceptionProxy) { throw ((ExceptionProxy)var6).generateException(); } else { if (var6.getClass().isArray() && Array.getLength(var6) != 0) { var6 = this.cloneArray(var6); } return var6; } } ... 参考链接 java注解是怎么实现的？ "},"zother2-interview/java/collection/BlockQueue/":{"url":"zother2-interview/java/collection/BlockQueue/","title":"Blocking Queue","keywords":"","body":"BlockingQueue BlockingQueue 支持当获取队列元素但是队列为空时，会阻塞等待队列中有元素再返回；也支持添加元素时，如果队列已满，那么等到队列可以放入新元素时再放入。 其提供了4种类型的方法： Throws exception Special value Blocks Times out Insert add(e) offer(e) put(e) offer(e, time, unit) Remove remove() poll() take() poll(time, unit) Examine element() peek() not applicable not applicable BlockingQueue不接受 null 元素。所有实现应当抛出 NullPointerException 在所有的 add,put以及offer方法上。null被用来标记poll失败。 在任意时刻，当有界BlockingQueue 队列元素放满之后，所有的元素都将在放入的时候阻塞。无界BlockingQueue 没有任何容量限制，容量大小始终是Integer.MAX_VALUE。 BlockingQueue的实现是用于 生产者-消费者 的队列，同时也支持 Collection 接口。所以可通过remove(x)来移除队列里的一个元素。通常情况下，这样的操作效率不是很好，只在诸如队列消息被取消的情况下才会偶尔使用。 BlockingQueue 的实现都是线程安全的。所有 queue 的方法都需要通过内部锁机制或者其他形式来进行并发控制来实现其原子操作。然而，Collection 接口的方法，比如：addAll, containsAll, retainAll 以及 removeAll 都没有必要进行原子操作，除非实现类有特别说明。所以对于addAll(c)有可能在添加部分c元素后抛出异常。 BlockingQueue 本质上不支持任何的 close 或者 shutdown 操作，来表明不会有新的元素添加。如果需要这些特性，得实现类来支持。 ArrayBlockingQueue ArrayBlockingQueue 是底层由数组存储的有界队列。遵循FIFO，所以在队首的元素是在队列中等待时间最长的，而在队尾的则是最短时间的元素。新元素被插入到队尾，队列的取出 操作队首元素。 这是一个经典的有界缓存，由一个长度确定的数组持有所有由生产者插入、由消费者取出的元素。一旦创建，整个队列的容量将不会改变。尝试向一个已满的队列 put 将会导致调用被阻塞，同样的向一个空队列 take 也会阻塞。 该队列支持队等待的生产者和消费者实施可选的公平策略。默认情况下，是非公平策略。可以通过构造函数来指定是否进行公平策略。一般情况下公平策略会减小吞吐量，但是也会降低可变性以及防止饥饿效应。 实现 ArrayBlockingQueue 内部使用了 ReentrantLock 以及两个 Condition 来实现。 /** Main lock guarding all access */ final ReentrantLock lock; /** Condition for waiting takes */ private final Condition notEmpty; /** Condition for waiting puts */ private final Condition notFull; PUT 方法也很简单，就是 Condition 的应用。 public void put(E e) throws InterruptedException { checkNotNull(e); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { //队列已满，wait 在 condition 上 while (count == items.length) notFull.await(); enqueue(e); } finally { lock.unlock(); } } take 方法也同样的。 public E take() throws InterruptedException { final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { //队列为空，wait 在 condition 上 while (count == 0) notEmpty.await(); return dequeue(); } finally { lock.unlock(); } } "},"zother2-interview/java/collection/Concurrenthashmap/":{"url":"zother2-interview/java/collection/Concurrenthashmap/","title":"Concurrent Hashmap","keywords":"","body":"ConcurrentHashmap JDK1.7 ConcurrentHashMap 的锁分段技术：假如容器里有多把锁，每一把锁用于锁容器其中一部分数据，那么当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效的提高并发访问效率，这就是 ConcurrentHashMap 所使用的锁分段技术。首先将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。 ConcurrentHashMap 不允许 Key 或者 Value 的值为 NULL，主要原因是：如果 map.get(key) 返回 null ，则无法检测 key 是否显式映射为 null 或者 key 未映射。 在非并发映射中，您可以通过 map.contains(key) 进行检查，但在并发映射中，映射可能在调用之间发生了变化。 JDK1.8 在JDK1.8中对 ConcurrentHashmap 进行了改进。取消segments字段，直接采用transient volatile HashEntry[] table保存数据，采用table数组元素作为锁，从而实现了对每一行数据进行加锁，进一步减少并发冲突的概率。 将原先 table数组＋单向链表 的数据结构，变更为 table数组＋单向链表＋红黑树 的结构。对于 hash 表来说，最核心的能力在于将 key hash 之后能均匀的分布在数组中。如果 hash 之后散列的很均匀，那么 table 数组中的每个队列长度主要为 0 或者 1 。但实际情况并非总是如此理想，虽然 ConcurrentHashMap 类默认的加载因子为 0.75，但是在数据量过大或者运气不佳的情况下，还是会存在一些队列长度过长的情况，如果还是采用单向列表方式，那么查询某个节点的时间复杂度为 O(n)；因此，对于个数超过 8 (默认值)的链表，jdk1.8 中采用了红黑树的结构，那么查询的时间复杂度可以降低到 O(logN)，可以改进性能。 PUT final V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null) throw new NullPointerException(); // 得到 hash 值 int hash = spread(key.hashCode()); // 用于记录相应链表的长度 int binCount = 0; for (Node[] tab = table;;) { Node f; int n, i, fh; // 如果数组\"空\"，进行数组初始化 if (tab == null || (n = tab.length) == 0) // 初始化数组，后面会详细介绍 tab = initTable(); // 找该 hash 值对应的数组下标，得到第一个节点 f else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) { // 如果数组该位置为空， // 用一次 CAS 操作将这个新值放入其中即可，这个 put 操作差不多就结束了，可以拉到最后面了 // 如果 CAS 失败，那就是有并发操作，进到下一个循环就好了 if (casTabAt(tab, i, null, new Node(hash, key, value, null))) break; // no lock when adding to empty bin } // hash 居然可以等于 MOVED，这个需要到后面才能看明白，不过从名字上也能猜到，肯定是因为在扩容 else if ((fh = f.hash) == MOVED) // 帮助数据迁移，这个等到看完数据迁移部分的介绍后，再理解这个就很简单了 tab = helpTransfer(tab, f); else { // 到这里就是说，f 是该位置的头结点，而且不为空 V oldVal = null; // 获取数组该位置的头结点的监视器锁 synchronized (f) { if (tabAt(tab, i) == f) { if (fh >= 0) { // 头结点的 hash 值大于 0，说明是链表 // 用于累加，记录链表的长度 binCount = 1; // 遍历链表 for (Node e = f;; ++binCount) { K ek; // 如果发现了\"相等\"的 key，判断是否要进行值覆盖，然后也就可以 break 了 if (e.hash == hash && ((ek = e.key) == key || (ek != null && key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; } // 到了链表的最末端，将这个新值放到链表的最后面 Node pred = e; if ((e = e.next) == null) { pred.next = new Node(hash, key, value, null); break; } } } else if (f instanceof TreeBin) { // 红黑树 Node p; binCount = 2; // 调用红黑树的插值方法插入新节点 if ((p = ((TreeBin)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } } } if (binCount != 0) { // 判断是否要将链表转换为红黑树，临界值和 HashMap 一样，也是 8 if (binCount >= TREEIFY_THRESHOLD) // 这个方法和 HashMap 中稍微有一点点不同，那就是它不是一定会进行红黑树转换， // 如果当前数组的长度小于 64，那么会选择进行数组扩容，而不是转换为红黑树 // 具体源码我们就不看了，扩容部分后面说 treeifyBin(tab, i); if (oldVal != null) return oldVal; break; } } } addCount(1L, binCount); return null; } GET get 不进行加锁，其只需要保证可见性，所以 volatile 就可以。 计算 hash 值 根据 hash 值找到数组对应位置: (n - 1) & h 根据该位置处结点性质进行相应查找 如果该位置为 null ，那么直接返回 null 就可以了 如果该位置处的节点刚好就是我们需要的，返回该节点的值即可 如果该位置节点的 hash 值小于 0，说明正在扩容，或者是红黑树，后面我们再介绍 find 方法 如果以上 3 条都不满足，那就是链表，进行遍历比对即可 [] tab = table; int index = (tab.length - 1) & hash; //根据这种哈希定位方式来定位对应的HashEntry HashEntry e = entryAt(tab, index); HashEntry pred = null; while (e != null) { K k; HashEntry next = e.next; if ((k = e.key) == key || (e.hash == hash && key.equals(k))) { V v = e.value; if (value == null || value == v || value.equals(v)) { if (pred == null) setEntryAt(tab, index, next); else pred.setNext(next); ++modCount; --count; oldValue = v; } break; } pred = e; e = next; } } finally { unlock(); } return oldValue; } ``` --> "},"zother2-interview/java/collection/HashMap/":{"url":"zother2-interview/java/collection/HashMap/","title":"Hash Map","keywords":"","body":"HashMap 在get和put的过程中，计算下标时，先对hashCode进行hash操作，然后再通过hash值进一步计算下标，如下图所示： 在对hashCode()计算hash时具体实现是这样的： static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16); } 可以看到这个函数大概的作用就是：高16bit不变，低16bit和高16bit做了一个异或。 在设计hash函数时，因为目前的table长度n为2的幂，而计算下标的时候，是这样实现的(使用&位操作，而非%求余)。设计者认为这方法很容易发生碰撞。为什么这么说呢？不妨思考一下，在n - 1为15(0x1111)时，其实散列真正生效的只是低4bit的有效位，当然容易碰撞了。 因此，设计者想了一个顾全大局的方法(综合考虑了速度、作用、质量)，就是把高16bit和低16bit异或了一下。设计者还解释到因为现在大多数的 hashCode 的分布已经很不错了，就算是发生了碰撞也用O(logn)的tree去做了。仅仅异或一下，既减少了系统的开销，也不会造成的因为高位没有参与下标的计算(table长度比较小时)，从而引起的碰撞。 如果还是产生了频繁的碰撞，会发生什么问题呢？作者注释说，他们使用树来处理频繁的碰撞(we use trees to handle large sets of collisions in bins)，在JEP-180中，描述了这个问题： Improve the performance of java.util.HashMap under high hash-collision conditions by using balanced trees rather than linked lists to store map entries. Implement the same improvement in the LinkedHashMap class. 之前已经提过，在获取HashMap的元素时，基本分两步： 首先根据 hashCode() 做 hash ，然后确定 bucket 的 index ； 如果 bucket 的节点的 key 不是我们需要的，则通过 keys.equals() 在链中找。 在Java 8之前的实现中是用链表解决冲突的，在产生碰撞的情况下，进行 get 时，两步的时间复杂度是 O(1)+O(n)。因此，当碰撞很厉害的时候n很大， O(n) 的速度显然是影响速度的。因此在Java 8中，如果一个 bucket 中碰撞冲突的元素超过某个限制(默认是8)，则使用红黑树来替换链表，这样复杂度就变成了 O(1)+O(logn) 了，这样在 n 很大的时候，能够比较理想的解决这个问题。 Resize 当put时，如果发现目前的bucket占用程度已经超过了Load Factor所希望的比例，那么就会发生resize。在resize的过程，简单的说就是把bucket扩充为2倍，之后重新计算index，把节点再放到新的bucket中。resize的注释是这样描述的： Initializes or doubles table size. If null, allocates in accord with initial capacity target held in field threshold. Otherwise, because we are using power-of-two expansion, the elements from each bin must either stay at same index, or move with a power of two offset in the new table. 大致意思就是说，当超过限制的时候会resize，然而又因为我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。例如我们从16扩展为32时，具体的变化如下所示： 因此元素在重新计算 hash 之后，因为n变为2倍，那么 n-1 的 mask 范围在高位多1bit(红色)，因此新的index就会发生这样的变化： 因此，我们在扩充HashMap的时候，不需要重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”。可以看看下图为16扩充为32的resize示意图： 这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。 并发问题 疫苗：JAVA HASHMAP的死循环 在 HashMap 并发进行 Resize 的过程中会出现环形链表，导致 get() 操作死循环。 "},"zother2-interview/java/collection/_index.html":{"url":"zother2-interview/java/collection/_index.html","title":"集合框架","keywords":"","body":"集合框架 Java集合框架提供了数据持有对象的方式，提供了对数据集合的操作。Java集合框架位于java.util包下，主要有三个大类：Collection、Map接口以及对集合进行操作的工具类。 Collection List ArrayList：线程不同步。默认初始容量为10，当数组大小不足时增长率为当前长度的50%。 Vector：线程同步。默认初始容量为10，当数组大小不足时增长率为当前长度的100%。它的同步是通过Iterator方法加synchronized实现的。 Stack：线程同步。继承自Vector，添加了几个方法来完成栈的功能。 LinkedList：线程不同步。双端队列形式。 Set：Set是一种不包含重复元素的Collection，Set最多只有一个null元素。 HashSet：线程不同步，内部使用HashMap进行数据存储，提供的方法基本都是调用HashMap的方法，所以两者本质是一样的。集合元素可以为NULL。 NavigableSet：添加了搜索功能，可以对给定元素进行搜索：小于、小于等于、大于、大于等于，放回一个符合条件的最接近给定元素的 key。 TreeSet：线程不同步，内部使用NavigableMap操作。默认元素“自然顺序”排列，可以通过Comparator改变排序。 EnumSet：线程不同步。内部使用Enum数组实现，速度比HashSet快。只能存储在构造函数传入的枚举类的枚举值。 Map HashMap：线程不同步。根据key的hashcode进行存储，内部使用静态内部类Node的数组进行存储，默认初始大小为16，每次扩大一倍。当发生Hash冲突时，采用拉链法（链表）。可以接受为null的键值(key)和值(value)。JDK 1.8中：当单个桶中元素个数大于等于8时，链表实现改为红黑树实现；当元素个数小于6时，变回链表实现。由此来防止hashCode攻击。 LinkedHashMap：保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的. 也可以在构造时用带参数，按照应用次数排序。在遍历的时候会比HashMap慢，不过有种情况例外，当HashMap容量很大，实际数据较少时，遍历起来可能会比LinkedHashMap慢，因为LinkedHashMap的遍历速度只和实际数据有关，和容量无关，而HashMap的遍历速度和他的容量有关。 TreeMap：线程不同步，基于 红黑树*- （Red-Black tree）的NavigableMap 实现，能够把它保存的记录根据键排序,默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator 遍历TreeMap时，得到的记录是排过序的。** HashTable：线程安全，HashMap的迭代器(Iterator)是fail-fast迭代器。HashTable不能存储NULL的key和value。 ConcurrentHashmap：支持并发操作的 Hash 表，ConcurrentHashmap 具有和 HashTable 同样的功能，并且具有相应的方法。即使所有操作都是线程安全的，但是并不需要进行加锁。 工具类 Collections、Arrays：集合类的一个工具类\\/帮助类，其中提供了一系列静态方法，用于对集合中元素进行排序、搜索以及线程安全等各种操作。 Comparable、Comparator：一般是用于对象的比较来实现排序，两者略有区别。 类设计者没有考虑到比较问题而没有实现Comparable接口。这是我们就可以通过使用Comparator，这种情况下，我们是不需要改变对象的。 一个集合中，我们可能需要有多重的排序标准，这时候如果使用Comparable就有些捉襟见肘了，可以自己继承Comparator提供多种标准的比较器进行排序。 "},"zother2-interview/java/concurrent/AQS/":{"url":"zother2-interview/java/concurrent/AQS/","title":"AQS","keywords":"","body":"AQS AQS 提供一个框架，用于实现依赖于先进先出（FIFO）等待队列 的阻塞锁和相关同步器（信号量，事件等）。对于大多数依赖单个原子 int 值表示状态的同步器，该类可以作为十分有用的基类。子类必须定义所有的protected方法（包括tryAcquire、tryRelease），来改变这个状态，并且定义哪些状态代表来对象被使用和被释放。鉴于这些，该类中其他的方法用来实现队列和阻塞的机制。子类可以维护其他状态字段，但是只有使用 getState 、setState以及 compareAndSetState 来原子的操作状态值。 子类需要定义非 public 的内部工具类用于实现其内部类的同步属性。AbstractQueuedSynchronizer 类不实现任何同步接口，相反，它定义了诸如acquireInterruptibly之类的方法，可以被具体的锁和相关的同步器适当地调用，以实现它们的公共方法。 该类支持默认的独占模式和共享模式。当一个线程处在独占模式下，其他试图 acquire 的线程都无法成功。共享模式可以同时被多个线程 acquire成功。在具体的应用场景中该类无法理解这些区别，当共享模式 acquire 成功之后，下一个线程（如果有一个存在）必须判定是否能够acquire。线程等待在不同的模式里但是会共享同一个FIFO队列。通常来说，子类只需要支持其中一种模式，但是如果都支持，可以参照ReadWriteLock。子类不需要定义不支持模式的方法。 该类定义AbstractQueuedSynchronizer.ConditionObject内部类，可以被子类使用的 Condition 实现，来支持独占模式 isHeldExclusively 判定当前线程的同步是否是独占模式，可用通过release方法与 getState 方法来完全释放当前对象，在将保存的状态值调用acquire，最终将此对象恢复到其先前获取的状态。AbstractQueuedSynchronizer没有方法来创建 Condition，所以如果无法满足这个约束，则不要使用它。AbstractQueuedSynchronizer.ConditionObject 的行为与具体的同步器实现有关。 该类为内部队列提供检查，检测和监视方法，以及 在condition objects上的类似方法。 这些方法可以根据需要使用 AbstractQueuedSynchronizer 用于它们的同步机制。该类的序列化仅存储 atomic int 的状态值，因此反序列化对象的线程队列为空。 使用 为了使用该类去创建一个同步器，需要重新定义以下方法，并使用 getState, setState, compareAndSetState 方法来改变同步状态。 tryAcquire tryRelease tryAcquireShared tryReleaseShared isHeldExclusively 上述所有方法默认实现都会抛出 UnsupportedOperationException。这个方法的具体实现必须保证内部的线程安全，并且应该快速并且不会阻塞。所有其他方法均为 final，因为他们不能独立变化。 也许你发现一些继承自 AbstractOwnableSynchronizer 的方法非常有助于线程保持拥有其独占同步器。同时我们也鼓励使用他们，有助于监控和诊断工具判定哪些线程持有来锁。 ReentrantLock 公平锁相比与非公平锁在 tryAcquire中会多判定一个 hasQueuedPredecessors，如果为 false（队列头为当前线程--已获取锁 or 队列为空）并且成功修改状态值，则可以认为获取锁成功，这样才是重入，不然加到队尾就会有麻烦。 ReentrantLock 中通过两个子类 FairSync 和 NoFairSync 继承 AQS 来实现锁。在Lock方法中，直接调用 AQS 的 acquire，acquire会调用 NoFairSync 中的tryAcquire来尝试让当前线程直接获取锁。如果失败则会创建链表节点，将当前线程加入队列，并park。当release方法被调用后，会寻找队列下一个节点进行 unpark，这样他就有机会在acquireQueued中获取锁。 公平和非公平就体现在 tryAcquire 方法中，FairSync会判定当前线程是否已获取锁 or 队列为空，在这样的情况下才会尝试获取锁。而NoFairSync会直接来获取锁。 Condition Condition 因子将 Object monitor 方法（wait, notify and notifyAll）拆分为不同的对象，通过将它们与 Lock 相结合来实现每个对象具有多个等待集的效果。任何 Lock 可以替代 synchronized 关键字的地方，都可以用Condition 来替换Object monitor 方法。 Conditions（也称为 条件队列 或者 条件变量）提供了一种方法 -- 让线程暂停执行，直到其他线程基于某种条件唤醒。在多个线程中访问一些共享的状态信息，是需要进行保护的，所以 Lock 与 Condition 有某种形式的关联。Condition提供的关键属性是它以原子方式释放关联的锁并挂起当前线程，就像Object.wait一样。 Condition 本质上是绑定到 Lock。可以通过 Lock.newCondition() 来获取一个 Condition 实例。 Condition 的实现可以提供相比于 Object monitor方法不一样的行为和语义，比如：被通知调起的顺序、在通知时不需要持有锁（ReentrantLock 不允许）。如果实现类提供了不一样的语义，必须在文档中进行说明。 Condition 实例只是普通的对象，可以用在同步语句中，并且有他们自己的 Object monitor的wait和 notification 方法。获取 Condition 对象的 Object monitor 或者使用其 monitor 方法，与Lock 中使用 Condition 的 wait 或者 signal 方法没有任何关系。为了避免混淆，不建议使用 Condition 的 Object monitor 方法，除非在它自己的实现里。 实现类需要注意 虚假唤醒（spurious wakeup）：开发者最好将条件 wait 方法放在循环中 Condition 有3中 wait 形式（interruptible, non-interruptible, and timed），在不同平台的底层实现可能不同。因此，不需要对三种 wait 定义一致的语义，也不需要支持中断形式的线程暂停。 AbstractQueuedSynchronizer.ConditionObject /** First node of condition queue. */ private transient Node firstWaiter; /** Last node of condition queue. */ private transient Node lastWaiter; 在 ConditionObject 的内部维护了一个队列：条件队列，与 AbstractQueuedSynchronizer 里的 等待队列 不同。 基本上，把这张图看懂，你也就知道 condition 的处理流程了。 条件队列和等待队列的节点，都是 Node 的实例，因为条件队列的节点是需要转移到等待队列中去的； 我们知道一个 ReentrantLock 实例可以通过多次调用 newCondition() 来产生多个 Condition 实例，这里对应 condition1 和 condition2。注意，ConditionObject 只有两个属性 firstWaiter 和 lastWaiter； 每个 condition 有一个关联的条件队列，如线程 1 调用 condition1.await() 方法即可将当前线程 1 包装成 Node 后加入到条件队列中，然后阻塞在这里，不继续往下执行，条件队列是一个单向链表； 调用condition1.signal() 触发一次唤醒，此时唤醒的是队头，会将condition1 对应的条件队列的 firstWaiter（队头） 移到等待队列的队尾，等待获取锁，获取锁后 await 方法才能返回，继续往下执行。 上面的 2->3->4 描述了一个最简单的流程，没有考虑中断、signalAll、还有带有超时参数的 await 方法等，不过把这里弄懂是这节的主要目的。 "},"zother2-interview/java/concurrent/atomic/":{"url":"zother2-interview/java/concurrent/atomic/","title":"Atomic Integer","keywords":"","body":"AtomicInteger AtomicInteger 是 Java 中常见的原子类，每种基础类型都对应 Atomic***。AtomicInteger 中最重要的就属于原子更新操作，这里我们来分析下 getAndAdd 的实现。 private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static { try { //获取 value 的偏移量 valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(\"value\")); } catch (Exception ex) { throw new Error(ex); } } private volatile int value; public final int getAndAdd(int delta) { //调用 unsafe.getAndAddInt return unsafe.getAndAddInt(this, valueOffset, delta); } getAndAdd 中直接调用 unsafe.getAndAddInt，原子更新的逻辑都在 UnSafe 类中： public final int getAndAddInt(Object var1, long var2, int var4) { int var5; //循环 CAS do { //获取 volatile 字段值 var5 = this.getIntVolatile(var1, var2); } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5; } getAndAdd 就是通过循环CAS，来执行原子更新的逻辑。 ABA 问题 CAS 并不是万能的，CAS 更新有 ABA 问题。即 T1 读取内存变量为 A ,T2 修改内存变量为 B ,T2 修改内存变量为 A ,这时 T1 再 CAS 操作 A 时是可行的。但实际上在 T1 第二次操作 A 时，已经被其他线程修改过了。举一个现实情况下的例子： 小明账户上有100元。现在小明取钱，小强汇钱，诈骗分子盗刷三个动作同时进行。 小明取50元。 诈骗分子盗刷50元。 小强给小明汇款50元。 此时，银行交易系统出问题，每笔交易无法通过短信告知小明。ABA问题就是： 小明验证账户上有100元后，取出50元。—— 账上有50元。 小强不会验证小明账户的余额，直接汇款50元。—— 账上有100元。 诈骗分子验证账户有100元后，取出50元。—— 账上有50元。 小强没有告诉小明自己汇钱，小明也没收到短信，那么小明就一直以为只有自己取款操作，最后损失了50元。 AtomicStampedReference 对于 ABA 问题，比较有效的方案是引入版本号，内存中的值每发生一次变化，版本号都 +1；在进行CAS操作时，不仅比较内存中的值，也会比较版本号，只有当二者都没有变化时，CAS才能执行成功。 AtomicStampedReference 便是使用版本号来解决ABA问题的。类似的还有 AtomicMarkableReference ， AtomicStampedReference 是使用 pair 的 int stamp 作为计数器使用， AtomicMarkableReference 的 pair 使用的是 boolean mark。 "},"zother2-interview/java/concurrent/count-down-latch/":{"url":"zother2-interview/java/concurrent/count-down-latch/","title":"Count Down Latch","keywords":"","body":"CountDownLatch CountDownLatch 是可以使一个或者多个线程等待其他线程完成某些操作的同步器。CountDownLatch 通过一个给定的数字 count 进行初始化。调用 await 方法的线程会一直阻塞到其他线程调用 countDown 将 count 变为0，这时所有的线程都将释放，并且后续的 await 方法调用都会立即返回。count 值不能重置。如果你需要重置 count 请考虑使用 CyclicBarrier。 CountDownLatch 是一个能力很强的同步工具，可以用在多种途径。CountDownLatch 最重要的属性是其不要求 调用 countDown 的线程等待到 count 为0，只是要求所有 await 调用线程等待。 CountDownLatch 内部使用的是 AQS，AQS 里面的 state 是一个整数值，这边用一个 int count 参数其实初始化就是设置了这个值，所有调用了 await 方法的等待线程会挂起，然后有其他一些线程会做 state = state - 1 操作，当 state 减到 0 的同时，那个将 state 减为 0 的线程会负责唤醒 所有调用了 await 方法的线程。 countDown() 方法每次调用都会将 state 减 1，直到 state 的值为 0；而 await 是一个阻塞方法，当 state 减为 0 的时候，await 方法才会返回。await 可以被多个线程调用，读者这个时候脑子里要有个图：所有调用了 await 方法的线程阻塞在 AQS 的阻塞队列中，等待条件满足（state == 0），将线程从队列中一个个唤醒过来。 await() 方法，它代表线程阻塞，等待 state 的值减为 0。 "},"zother2-interview/java/concurrent/interrupt/":{"url":"zother2-interview/java/concurrent/interrupt/","title":"线程中断","keywords":"","body":"线程中断 中断不是类似 linux 里面的命令 kill -9 pid，不是说我们中断某个线程，这个线程就停止运行了。中断代表线程状态，每个线程都关联了一个中断状态，是一个 true 或 false 的 boolean 值，初始值为 false。 关于中断状态，我们需要重点关注 Thread 类中的以下几个方法： // Thread 类中的实例方法，持有线程实例引用即可检测线程中断状态 public boolean isInterrupted() {} // Thread 中的静态方法，检测调用这个方法的线程是否已经中断 // 注意：这个方法返回中断状态的同时，会将此线程的中断状态重置为 false // 所以，如果我们连续调用两次这个方法的话，第二次的返回值肯定就是 false 了 public static boolean interrupted() {} // Thread 类中的实例方法，用于设置一个线程的中断状态为 true public void interrupt() {} 我们说 中断一个线程，其实就是设置了线程的 interrupted status 为 true，至于说被中断的线程怎么处理这个状态，那是那个线程自己的事。如以下代码： while (!Thread.interrupted()) { doWork(); System.out.println(\"我做完一件事了，准备做下一件，如果没有其他线程中断我的话\"); } 这种代码就是会响应中断的，它会在干活的时候先判断下中断状态，不过，除了 JDK 源码外，其他用中断的场景还是比较少的，毕竟 JDK 源码非常讲究。 当然，中断除了是线程状态外，还有其他含义，否则也不需要专门搞一个这个概念出来了。如果线程处于以下三种情况，那么当线程被中断的时候，能自动感知到： 来自 Object 类的 wait()、wait(long)、wait(long, int)，来自 Thread 类的join()、join(long)、join(long, int)、sleep(long)、sleep(long, int) 这几个方法的相同之处是，方法上都有: throws InterruptedException 如果线程阻塞在这些方法上（我们知道，这些方法会让当前线程阻塞），这个时候如果其他线程对这个线程进行了中断，那么这个线程会从这些方法中立即返回，抛出 InterruptedException 异常，同时重置中断状态为 false。 实现了 InterruptibleChannel 接口的类中的一些 I/O 阻塞操作，如 DatagramChannel 中的 connect 方法和 receive 方法等 如果线程阻塞在这里，中断线程会导致这些方法抛出 ClosedByInterruptException 并重置中断状态。 Selector 中的 select 方法 一旦中断，方法立即返回 对于以上 3 种情况是最特殊的，因为他们能自动感知到中断（这里说自动，当然也是基于底层实现），并且在做出相应的操作后都会重置中断状态为 false。 那是不是只有以上 3 种方法能自动感知到中断呢？不是的，如果线程阻塞在 LockSupport.park(Object obj) 方法，也叫挂起，这个时候的中断也会导致线程唤醒，但是唤醒后不会重置中断状态，所以唤醒后去检测中断状态将是 true。 InterruptedException 概述 它是一个特殊的异常，不是说 JVM 对其有特殊的处理，而是它的使用场景比较特殊。通常，我们可以看到，像 Object 中的 wait() 方法，ReentrantLock 中的 lockInterruptibly() 方法，Thread 中的 sleep() 方法等等，这些方法都带有 throws InterruptedException，我们通常称这些方法为阻塞方法（blocking method）。 阻塞方法一个很明显的特征是，它们需要花费比较长的时间（不是绝对的，只是说明时间不可控），还有它们的方法结束返回往往依赖于外部条件，如 wait 方法依赖于其他线程的 notify，lock 方法依赖于其他线程的 unlock 等等。 当我们看到方法上带有 throws InterruptedException 时，我们就要知道，这个方法应该是阻塞方法，我们如果希望它能早点返回的话，我们往往可以通过中断来实现。 除了几个特殊类（如 Object，Thread等）外，感知中断并提前返回是通过轮询中断状态来实现的。我们自己需要写可中断的方法的时候，就是通过在合适的时机（通常在循环的开始处）去判断线程的中断状态，然后做相应的操作（通常是方法直接返回或者抛出异常）。当然，我们也要看到，如果我们一次循环花的时间比较长的话，那么就需要比较长的时间才能感知到线程中断了。 处理中断 一旦中断发生，我们接收到了这个信息，然后怎么去处理中断呢？本小节将简单分析这个问题。我们经常会这么写代码： try { Thread.sleep(10000); } catch (InterruptedException e) { // ignore } // go on 当 sleep 结束继续往下执行的时候，我们往往都不知道这块代码是真的 sleep 了 10 秒，还是只休眠了 1 秒就被中断了。这个代码的问题在于，我们将这个异常信息吞掉了。（对于 sleep 方法，我相信大部分情况下，我们都不在意是否是中断了，这里是举例） AQS 的做法很值得我们借鉴，我们知道 ReentrantLock 有两种 lock 方法： public void lock() { sync.lock(); } public void lockInterruptibly() throws InterruptedException { sync.acquireInterruptibly(1); } 前面我们提到过，lock() 方法不响应中断。如果 thread1 调用了 lock() 方法，过了很久还没抢到锁，这个时候 thread2 对其进行了中断，thread1 是不响应这个请求的，它会继续抢锁，当然它不会把“被中断”这个信息扔掉。我们可以看以下代码： public final void acquire(int arg) { if (!tryAcquire(arg) && acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) // 我们看到，这里也没做任何特殊处理，就是记录下来中断状态。 // 这样，如果外层方法需要去检测的时候，至少我们没有把这个信息丢了 selfInterrupt();// Thread.currentThread().interrupt(); } 而对于 lockInterruptibly() 方法，因为其方法上面有 throws InterruptedException ，这个信号告诉我们，如果我们要取消线程抢锁，直接中断这个线程即可，它会立即返回，抛出 InterruptedException 异常。 在并发包中，有非常多的这种处理中断的例子，提供两个方法，分别为响应中断和不响应中断，对于不响应中断的方法，记录中断而不是丢失这个信息。如 Condition 中的两个方法就是这样的： void await() throws InterruptedException; void awaitUninterruptibly(); 实例分析 有以下代码： synchronized (this) { while (client == null) { try { this.wait(); } catch (InterruptedException e) { LOGGER.error(\"InterruptedException:{}\", e); Thread.currentThread().interrupt(); } } } 上面的代码会造成什么问题？仔细分析可以发现，代码中如果抛出 InterruptedException，就会陷入死循环中，导致异常日志打爆。为什么会这样呢？首先我们来看下这两个方法： wait(): if any thread interrupted the current thread before or while the current thread was waiting for a notification. The interrupted status of the current thread is cleared when this exception is thrown. Thread.currentThread().interrupt(): If none of the previous conditions hold then this thread's interrupt status will be set. wait() 在当前线程有中断标志位时抛出中断异常；而 interrupt() 如果当前线程没有在wait()等阻塞操作，则标记中断。这样就陷入死循环，无限的打印 ERROR 日志。正确的处理 InterruptedException 是很重要的。 "},"zother2-interview/java/concurrent/synchronized/":{"url":"zother2-interview/java/concurrent/synchronized/","title":"Synchronized","keywords":"","body":"Synchronized原理 基础 在多线程并发编程中 Synchronized 一直是元老级角色，很多人都会称呼它为重量级锁，但是随着 Java SE1.6 对 Synchronized 进行了各种优化，引入了 偏向锁 和 轻量级锁。所以在 Java SE1.6 里锁一共有四种状态，无锁状态，偏向锁状态，轻量级锁状态和重量级锁状态，它会随着竞争情况逐渐升级。锁可以升级但不能降级，意味着偏向锁升级成轻量级锁后不能降级成偏向锁，但是偏向锁状态可以被重置为无锁状态（锁撤销）。这种锁升级却不能降级的策略，目的是为了提高获得锁和释放锁的效率。 锁状态 优点 缺点 适用场景 偏向锁 加锁、解锁无额外消耗，和非同步方式近似 如果竞争线程多，会有额外锁撤销的消耗 基本没有线程竞争的场景 轻量级锁 竞争线程不会阻塞，使用自旋等待 如果长时间不能获取锁，会消耗CPU 少量线程竞争，且线程持有锁时间不长 重量级锁 竞争线程被阻塞，减少CPU空转 线程阻塞，响应时间长 很多线程竞争，锁持有时间长 Java中的每一个对象都可以作为锁。 对于同步方法，锁是当前实例对象。 对于静态同步方法，锁是当前对象的Class对象。 对于同步方法块，锁是Synchonized括号里配置的对象。 当一个线程试图访问同步代码块时，它首先必须得到锁，退出或抛出异常时必须释放锁。 锁的升级 目前锁一共有4种状态，级别从低到高依次是：无锁、偏向锁、轻量级锁和重量级锁。锁状态只能升级不能降级。 偏向锁 大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得。偏向锁是为了在只有一个线程执行同步块时提高性能。 轻量级锁 这里解释下其中几个重要的步骤： 复制 Mark Word 到锁记录：拷贝对象头中的 Mark Word 到锁记录中。 更新 Mark Word 指针：拷贝成功后，虚拟机将使用 CAS 操作尝试将对象的 Mark Word 更新为指向 Lock Record 指针，并将 Lock Record 里的 owner 指针指向对象的 Mark Word。 重量级锁 在重量级锁的状态下， JVM 基于进入和退出 Monitor 对象来实现方法同步和代码块同步，Monitor 的引用存储在对象头中。 Monitor 本身是依赖与操作系统的互斥锁（mutex lock）实现的。由于 JVM 线程是映射到操作系统的原生线程之上的，如果要阻塞或唤醒一条线程，都需要操作系统来帮忙完成，这就需要从用户态转换到核心态中，因此这种转换需要耗费很多的 CPU 时间。 锁粗化 同步块的作用范围应该尽可能小，仅在共享数据的实际作用域中才进行同步，这样做的目的是为了使需要同步的操作数量尽可能缩小，缩短阻塞时间，如果存在锁竞争，那么等待锁的线程也能尽快拿到锁。 但是加锁解锁也需要消耗资源，如果存在一系列的连续加锁解锁操作，可能会导致不必要的性能损耗。 锁粗化就是 JVM 将多个连续的加锁、解锁操作连接在一起，扩展成一个范围更大的锁，避免频繁的加锁解锁操作。 锁消除 Java 虚拟机在 JIT 编译时(可以简单理解为当某段代码即将第一次被执行时进行编译，又称即时编译)，通过对运行上下文的扫描，经过逃逸分析，去除不可能存在共享资源竞争的锁，通过这种方式消除没有必要的锁，可以节省毫无意义的请求锁时间 Synchronized vs ReentrantLock synchronized 是 Java 关键字，ReentrantLock 是基于 AQS 的 API 层面的互斥锁 ReentrantLock 设置等待超时时间 ReentrantLock 可进行公平锁与非公平锁设置 ReentrantLock 可绑定多个 Condition synchronized 不需要手动释放锁 synchronized 可以修饰方法、代码块 参考链接 不可不说的Java“锁”事 Threads and Locks 从jvm源码看 synchronized "},"zother2-interview/java/concurrent/thread/":{"url":"zother2-interview/java/concurrent/thread/","title":"Java线程","keywords":"","body":"Java线程 线程定义 线程（Thread）是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。 线程是独立调度和分派的基本单位。线程可以操作系统内核调度的内核线程，如Win32线程；由用户进程自行调度的用户线程，如Linux平台的POSIX Thread；或者由内核与用户进程，如Windows 7的线程，进行混合调度。 同一进程中的多条线程将共享该进程中的全部系统资源，如虚拟地址空间，文件描述符和信号处理等等。但同一进程中的多个线程有各自的调用栈（call stack），自己的寄存器环境（register context），自己的线程本地存储（thread-local storage）。 线程实现 Java中的线程都是调用的原生系统的本地函数，Java线程模型是基于操作系统原生线程模型实现的，实现线程有三种方式：内核线程实现、用户线程实现、混合线程实现。 内核线程实现 直接由操作系统内核支持的线程，通过内核来完成进程切换。每个内核线程就是一个内核的分身，这样操作系统就可以同时处理多件事情，支持多线程的内核被称为多线程内核。 程序一般不直接使用内核线程，而是使用一种高级接口——轻量级进程，轻量级进程就是我们通常意义上的线程，可以获得内核线程的支持，与内核线程构成1:1的线程模型。 由于得到内核线程的支持，每个轻量级进程都成为一个独立的调度单元，即时有一个轻量级进程在系统调用中阻塞，也不会影响整个进程，但也有其局限性：由于是基于内核线程实现的，各种操作，如创建、销毁及同步，都需要进行系统调用。而系统调用代价较高，需要在内核态和用户态来回切换。 用户线程实现 从广义上说，一个线程不是内核线程，就是用户线程，所以轻量级进程也属于用户线程。狭义的用户线程是指完全建立在用户空间上的，系统内核不能感知到其存在。 用户线程的创建、同步、销毁和调度都是在用户空间实现的，因此相对较快，代价相对较低。这种用户线程和进程是N:1的线程模型。 由于用户线程没有内核的支持，线程的创建、切换和调度是需要自己实现的，而且由于操作系统只把CPU资源分配到进程，那诸如“阻塞如何处理”、“多处理器系统中如何将线程映射到其他处理器”这类问题解决起来异常复杂。 混合实现 这种实现模式将内核线程与用户线程一起使用，在这种方式下既存在用户线程，也存在轻量级进程。用户线程还是完全建立在用户空间，因此用户线程的创建、切换等操作依旧低廉。而操作系统提供的轻量级进程则作为用户线程和内核线程的桥梁，这样就可以使用内核提供的线程调度及处理器映射。这种实现下，用户线程和轻量级进程是M:N的模式。 Java线程调度 线程调度分为协同式和抢占式。 协同式调度：线程的执行时间由线程自己控制，这种的实现很简单，但是很可能造成很严重的后果。 抢占式调度：由操作系统分配线程执行的时间，线程切换的决定权在操作系统。 有时候我们需要为某些线程多分配时间，这时我们就需要用到线程优先级的方法，Java提供了10种优先级。Java优先级是在操作系统的原生线程优先级上实现的，所以对于同一个优先级，不同的操作系统可能有不同的表现，也就是说 Java线程优先级不是可靠的。 Java线程状态切换 Java线程模型定义了 6 种状态，在任意一个时间点，一个线程有且只有其中一个状态： 新建（New）：新建的Thread，尚未开始。 运行（Runable）：包含操作系统线程状态中的Running、Ready，也就是处于正在执行或正在等待CPU分配时间的状态。 无限期等待（Waiting）：处于这种状态的线程不会被分配CPU时间，等待其他线程唤醒。 限期等待（Timed Waiting）：处于这种状态的线程不会被分配CPU时间，在一定时间后会由系统自动唤醒。 阻塞（Blocked）：在等待获得排他锁。 结束（Terminated）：已终止的线程。 线程安全 多线程访问同一代码，不会产生不确定的结果。 Java 线程池 线程池提供了一种限制和管理资源（包括执行一个任务）。 每个线程池还维护一些基本统计信息，例如已完成任务的数量。使用线程池的好处： 降低资源消耗：通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度：当任务到达时，任务可以不需要的等到线程创建就能立即执行。 提高线程的可管理性：线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 在创建线程池时不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。Executors 返回线程池对象的弊端如下： FixedThreadPool 和 SingleThreadExecutor ： 允许请求的队列长度为 Integer.MAX_VALUE ，可能堆积大量的请求，从而导致OOM。 CachedThreadPool 和 ScheduledThreadPool ： 允许创建的线程数量为 Integer.MAX_VALUE ，可能会创建大量线程，从而导致OOM。 ThreadPoolExecutor 构造函数重要参数： corePoolSize : 核心线程数线程数定义了最小可以同时运行的线程数量。 maximumPoolSize : 当队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。 workQueue: 当新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中。 keepAliveTime：当线程池中的线程数量大于 corePoolSize 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 keepAliveTime才会被回收销毁； unit ：keepAliveTime 参数的时间单位。 threadFactory ：executor 创建新线程的时候会用到。 handler ：饱和策略。 线程池提交任务 为了搞懂线程池的原理，我们需要首先分析一下 execute 方法。我们使用 executor.execute(worker) 来提交一个任务到线程池中去，这个方法非常重要，下面我们来看看它的源码： // 存放线程池的运行状态 (runState) 和线程池内有效线程的数量 (workerCount) private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); private static int workerCountOf(int c) { return c & CAPACITY; } private final BlockingQueue workQueue; public void execute(Runnable command) { // 如果任务为null，则抛出异常。 if (command == null) throw new NullPointerException(); // ctl 中保存的线程池当前的一些状态信息 int c = ctl.get(); // 下面会涉及到 3 步 操作 // 1.首先判断当前线程池中之行的任务数量是否小于 corePoolSize // 如果小于的话，通过addWorker(command, true)新建一个线程，并将任务(command)添加到该线程中；然后，启动该线程从而执行任务。 if (workerCountOf(c) "},"zother2-interview/java/concurrent/threadlocal/":{"url":"zother2-interview/java/concurrent/threadlocal/","title":"Thread Local","keywords":"","body":"Threadlocal原理 ThreadLocal 为解决多线程程序的并发问题提供了一种新的思路。使用这个工具类可以很简洁地编写出优美的多线程程序。当使用 ThreadLocal 维护变量时，ThreadLocal 为每个使用该变量的线程提供独立的变量副本，所以每一个线程都可以独立地改变自己的副本，而不会影响其它线程所对应的副本。 每个线程中都保有一个ThreadLocalMap的成员变量，ThreadLocalMap内部采用WeakReference数组保存，数组的key即为ThreadLocal内部的Hash值。 内存泄漏 ThreadLocalMap 使用 ThreadLocal 的弱引用作为 key ，如果一个 ThreadLocal 没有外部强引用来引用它，那么系统 GC 的时候，这个 ThreadLocal 势必会被回收，这样一来，ThreadLocalMap 中就会出现 key 为 null 的 Entry ，就没有办法访问这些 key 为 null 的 Entry 的 value，如果当前线程再迟迟不结束的话，这些 key 为 null 的 Entry 的 value 就会一直存在一条强引用链：Thread Ref -> Thread -> ThreaLocalMap -> Entry -> value 永远无法回收，造成内存泄漏。 static class Entry extends WeakReference> { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal k, Object v) { super(k); value = v; } } 其实，ThreadLocalMap 的设计中已经考虑到这种情况，也加上了一些防护措施：在 ThreadLocal 的 get(),set(),remove()的时候都会清除线程 ThreadLocalMap 里所有 key 为 null 的 value "},"zother2-interview/java/concurrent/volatile/":{"url":"zother2-interview/java/concurrent/volatile/","title":"Volatile","keywords":"","body":"Volatile原理 计算机内存模型 计算机在执行程序时，每条指令都是在CPU中执行的，而执行指令过程中，势必涉及到数据的读取和写入。由于程序运行过程中的临时数据是存放在主存（物理内存）当中的，这时就存在一个问题，由于CPU执行速度很快，而从内存读取数据和向内存写入数据的过程跟CPU执行指令的速度比起来要慢的多，因此如果任何时候对数据的操作都要通过和内存的交互来进行，会大大降低指令执行的速度。因此在CPU里面就有了高速缓存。当程序在运行过程中，会将运算需要的数据从主存复制一份到CPU的高速缓存当中，那么CPU进行计算时就可以直接从它的高速缓存读取数据和向其中写入数据，当运算结束之后，再将高速缓存中的数据刷新到主存当中。举个简单的例子，比如下面的这段代码： i = i + 1; 当线程执行这个语句时，会先从主存当中读取i的值，然后复制一份到高速缓存当中，然后 CPU 执行指令对i进行加1操作，然后将数据写入高速缓存，最后将高速缓存中i最新的值刷新到主存当中。 这个代码在单线程中运行是没有任何问题的，但是在多线程中运行就会有问题了。在多核 CPU 中，每条线程可能运行于不同的 CPU 中，因此 每个线程运行时有自己的高速缓存（对单核CPU来说，其实也会出现这种问题，只不过是以线程调度的形式来分别执行的）。比如同时有两个线程执行这段代码，假如初始时i的值为0，那么我们希望两个线程执行完之后i的值变为2。但是事实会是这样吗？ 可能出现这种情况：初始时，两个线程分别读取i的值存入各自所在的 CPU 的高速缓存当中，然后 线程1 进行加1操作，然后把i的最新值1写入到内存。此时线程2的高速缓存当中i的值还是0，进行加1操作之后，i的值为1，然后线程2把i的值写入内存。最终结果i的值是1，而不是2。这就是著名的缓存一致性问题。通常称这种被多个线程访问的变量为共享变量。 为了解决缓存不一致性问题，通常来说有以下两种解决方法： 通过在总线加LOCK#锁的方式 通过 缓存一致性协议 这两种方式都是硬件层面上提供的方式。 在早期的 CPU 当中，是通过在总线上加LOCK#锁的形式来解决缓存不一致的问题。因为 CPU 和其他部件进行通信都是通过总线来进行的，如果对总线加LOCK#锁的话，也就是说阻塞了其他 CPU 对其他部件访问（如内存），从而使得只能有一个 CPU 能使用这个变量的内存。比如上面例子中 如果一个线程在执行 i = i +1，如果在执行这段代码的过程中，在总线上发出了LCOK#锁的信号，那么只有等待这段代码完全执行完毕之后，其他CPU才能从变量i所在的内存读取变量，然后进行相应的操作。这样就解决了缓存不一致的问题。但是上面的方式会有一个问题，由于在锁住总线期间，其他CPU无法访问内存，导致效率低下。 所以就出现了缓存一致性协议。最出名的就是 Intel 的MESI协议，MESI协议保证了每个缓存中使用的共享变量的副本是一致的。它核心的思想是：当CPU写数据时，如果发现操作的变量是共享变量，即在其他CPU中也存在该变量的副本，会发出信号通知其他CPU将该变量的缓存行置为无效状态，因此当其他CPU需要读取这个变量时，发现自己缓存中缓存该变量的缓存行是无效的，那么它就会从内存重新读取。 Java内存模型 在Java虚拟机规范中试图定义一种Java内存模型（Java Memory Model，JMM）来屏蔽各个硬件平台和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。那么Java内存模型规定了程序中变量的访问规则，往大一点说是定义了程序执行的次序。注意，为了获得较好的执行性能，Java内存模型并没有限制执行引擎使用处理器的寄存器或者高速缓存来提升指令执行速度，也没有限制编译器对指令进行重排序。也就是说，在java内存模型中，也会存在缓存一致性问题和指令重排序的问题。 Java内存模型规定所有的变量都是存在主存当中（类似于前面说的物理内存），每个线程都有自己的工作内存（类似于前面的高速缓存）。线程对变量的所有操作都必须在工作内存中进行，而不能直接对主存进行操作。并且每个线程不能访问其他线程的工作内存。 在Java中，执行下面这个语句： i = 10; 执行线程必须先在自己的工作线程中对变量i所在的缓存行进行赋值操作，然后再写入主存当中。而不是直接将数值10写入主存当中。那么Java语言本身对 原子性、可见性以及有序性提供了哪些保证呢？ 原子性 即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。 在Java中，对基本数据类型的变量的读取和赋值操作是原子性操作，即这些操作是不可被中断的，要么执行，要么不执行。上面一句话虽然看起来简单，但是理解起来并不是那么容易。看下面一个例子，请分析以下哪些操作是原子性操作： x = 10; //语句1 y = x; //语句2 x++; //语句3 x = x + 1; //语句4 咋一看，有些朋友可能会说上面的4个语句中的操作都是原子性操作。其实只有语句1是原子性操作，其他三个语句都不是原子性操作。 语句1是直接将数值10赋值给x，也就是说线程执行这个语句的会直接将数值10写入到工作内存中。 语句2实际上包含2个操作，它先要去读取x的值，再将x的值写入工作内存，虽然读取x的值以及 将x的值写入工作内存 这2个操作都是原子性操作，但是合起来就不是原子性操作了。 同样的，x++和 x = x+1包括3个操作：读取x的值，进行加1操作，写入新的值。 也就是说，只有简单的读取、赋值（而且必须是将数字赋值给某个变量，变量之间的相互赋值不是原子操作）才是原子操作。不过这里有一点需要注意：在32位平台下，对64位数据的读取和赋值是需要通过两个操作来完成的，不能保证其原子性。但是好像在最新的JDK中，JVM已经保证对64位数据的读取和赋值也是原子性操作了。 从上面可以看出，Java内存模型只保证了基本读取和赋值是原子性操作，如果要实现更大范围操作的原子性，可以通过synchronized和Lock来实现。由于synchronized和Lock能够保证任一时刻只有一个线程执行该代码块，那么自然就不存在原子性问题了，从而保证了原子性。 可见性 可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。 对于可见性，Java提供了volatile关键字来保证可见性。当一个共享变量被volatile修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。而普通的共享变量不能保证可见性，因为普通共享变量被修改之后，什么时候被写入主存是不确定的，当其他线程去读取时，此时内存中可能还是原来的旧值，因此无法保证可见性。 另外，通过synchronized和Lock也能够保证可见性，synchronized和Lock能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。 有序性 即程序执行的顺序按照代码的先后顺序执行。 指令重排序，一般来说，处理器为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行先后顺序同代码中的顺序一致，但是它会保证程序最终执行结果和代码顺序执行的结果是一致的。 处理器在进行重排序时是会考虑指令之间的数据依赖性，如果一个指令Instruction 2必须用到Instruction 1的结果，那么处理器会保证Instruction 1会在Instruction 2之前执行。 在Java内存模型中，允许编译器和处理器对指令进行重排序，但是重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。 在Java里面，可以通过volatile关键字来保证一定的“有序性”（具体原理在下一节讲述）。另外可以通过synchronized和Lock来保证有序性，很显然，synchronized和Lock保证每个时刻是有一个线程执行同步代码，相当于是让线程顺序执行同步代码，自然就保证了有序性。 另外，Java内存模型具备一些先天的“有序性”，即不需要通过任何手段就能够得到保证的有序性，这个通常也称为 happens-before 原则，若线程 A 和线程 B 满足 happens-before 关系，则线程 A 执行操作的结果对线程 B 是可见的。如果两个操作的执行次序无法从happens-before原则推导出来，那么它们就不能保证它们的有序性，虚拟机可以随意地对它们进行重排序。 下面就来具体介绍下happens-before原则（先行发生原则）： 程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作 锁定规则：一个unLock操作先行发生于后面对同一个锁额lock操作 volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作 传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C 线程启动规则：Thread对象的start()方法先行发生于此线程的每个一个动作 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生 线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行 对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始 对于程序次序规则来说，我的理解就是一段程序代码的执行在单个线程中看起来是有序的。注意，虽然这条规则中提到“书写在前面的操作先行发生于书写在后面的操作”，这个应该是程序看起来执行的顺序是按照代码顺序执行的，因为虚拟机可能会对程序代码进行指令重排序。虽然进行重排序，但是最终执行的结果是与程序顺序执行的结果一致的，它只会对不存在数据依赖性的指令进行重排序。因此，在单个线程中，程序执行看起来是有序执行的，这一点要注意理解。事实上，这个规则是用来保证程序在单线程中执行结果的正确性，但无法保证程序在多线程中执行的正确性。 第二条规则也比较容易理解，也就是说无论在单线程中还是多线程中，同一个锁如果出于被锁定的状态，那么必须先对锁进行了释放操作，后面才能继续进行lock操作。 第三条规则是一条比较重要的规则，也是后文将要重点讲述的内容。直观地解释就是，如果一个线程先去写一个变量，然后一个线程去进行读取，那么写入操作肯定会先行发生于读操作。 第四条规则实际上就是体现happens-before原则具备传递性。 深入剖析Volatile关键字 Volatile 的语义 一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义： 保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的 禁止进行指令重排序 先看一段代码，假如线程1先执行，线程2后执行： //线程1 boolean stop = false; while(!stop){ doSomething(); } //线程2 stop = true; 这段代码是很典型的一段代码，很多人在中断线程时可能都会采用这种标记办法。但是事实上，这段代码会完全运行正确么？即一定会将线程中断么？不一定，也许在大多数时候，这个代码能够把线程中断，但是也有可能会导致无法中断线程（虽然这个可能性很小，但是只要一旦发生这种情况就会造成死循环了）。 下面解释一下这段代码为何有可能导致无法中断线程。在前面已经解释过，每个线程在运行过程中都有自己的工作内存，那么线程1在运行的时候，会将stop变量的值拷贝一份放在自己的工作内存当中。 那么当线程2更改了stop变量的值之后，但是还没来得及写入主存当中，线程2转去做其他事情了，那么线程1由于不知道线程2对stop变量的更改，因此还会一直循环下去。但是用volatile修饰之后就变得不一样了： 使用volatile关键字会强制将修改的值立即写入主存； 使用volatile关键字的话，当线程2进行修改时，会导致线程1的工作内存中缓存变量stop的缓存行无效（反映到硬件层的话，就是CPU的L1或者L2缓存中对应的缓存行无效）； 由于线程1的工作内存中缓存变量stop的缓存行无效，所以线程1再次读取变量stop的值时会去主存读取。 那么在线程2修改stop值时（当然这里包括2个操作，修改线程2工作内存中的值，然后将修改后的值写入内存），会使得线程1的工作内存中缓存变量stop的缓存行无效，然后线程1读取时，发现自己的缓存行无效，它会等待缓存行对应的主存地址被更新之后，然后去对应的主存读取最新的值。 那么线程1读取到的就是最新的正确的值。 Volatile与原子性 从上面知道volatile关键字保证了操作的可见性，但是volatile能保证对变量的操作是原子性吗？ 下面看一个例子： public class Test { public volatile int inc = 0; public void increase() { inc++; } public static void main(String[] args) { final Test test = new Test(); for(int i=0;i1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); } } 大家想一下这段程序的输出结果是多少？也许有些朋友认为是10000。但是事实上运行它会发现每次运行结果都不一致，都是一个小于10000的数字。可能有的朋友就会有疑问，不对啊，上面是对变量inc进行自增操作，由于volatile保证了可见性，那么在每个线程中对inc自增完之后，在其他线程中都能看到修改后的值啊，所以有10个线程分别进行了1000次操作，那么最终inc的值应该是1000*10=10000。 这里面就有一个误区了，volatile关键字能保证可见性没有错，但是上面的程序错在没能保证原子性。可见性只能保证每次读取的是最新的值，但是volatile没办法保证对变量的操作的原子性。 在前面已经提到过，自增操作是不具备原子性的，它包括读取变量的原始值、进行加1操作、写入工作内存。那么就是说自增操作的三个子操作可能会分割开执行，就有可能导致下面这种情况出现： 假如某个时刻变量inc的值为10， 线程1对变量进行自增操作，线程1先读取了变量inc的原始值，然后线程1被阻塞了； 然后线程2对变量进行自增操作，线程2也去读取变量inc的原始值，由于线程1只是对变量inc进行读取操作，而没有对变量进行修改操作，所以不会导致线程2的工作内存中缓存变量inc的缓存行无效，所以线程2会直接去主存读取inc的值，发现inc的值时10，然后进行加1操作，并把11写入工作内存，最后写入主存。 然后线程1接着进行加1操作，由于已经读取了inc的值，注意此时在线程1的工作内存中inc的值仍然为10，所以线程1对inc进行加1操作后inc的值为11，然后将11写入工作内存，最后写入主存。 那么两个线程分别进行了一次自增操作后，inc只增加了1。 解释到这里，可能有朋友会有疑问，不对啊，前面不是保证一个变量在修改volatile变量时，会让缓存行无效吗？然后其他线程去读就会读到新的值，对，这个没错。这个就是上面的happens-before规则中的volatile变量规则，但是要注意，线程1对变量进行读取操作之后，被阻塞了的话，并没有对inc值进行修改。然后虽然volatile能保证线程2对变量inc的值读取是从内存中读取的，但是线程1没有进行修改，所以线程2根本就不会看到修改的值。 根源就在这里，自增操作不是原子性操作，而且volatile也无法保证对变量的任何操作都是原子性的。解决的方法也就是对提供原子性的自增操作即可。 在Java 1.5的java.util.concurrent.atomic包下提供了一些原子操作类，即对基本数据类型的 自增（加1操作），自减（减1操作）、以及加法操作（加一个数），减法操作（减一个数）进行了封装，保证这些操作是原子性操作。atomic是利用CAS来实现原子性操作的（Compare And Swap），CAS实际上是利用处理器提供的 CMPXCHG 指令实现的，而处理器执行 CMPXCHG 指令是一个原子性操作。 Volatile与有序性 在前面提到volatile关键字能禁止指令重排序，所以volatile能在一定程度上保证有序性。volatile关键字禁止指令重排序有两层意思： 当程序执行到volatile变量的读操作或者写操作时，在其前面的操作的更改肯定全部已经进行，且结果已经对后面的操作可见，在其后面的操作肯定还没有进行； 在进行指令优化时，不能将在对volatile变量访问的语句放在其后面执行，也不能把volatile变量后面的语句放到其前面执行。 可能上面说的比较绕，举个简单的例子： //x、y为非volatile变量 //flag为volatile变量 x = 2; //语句1 y = 0; //语句2 flag = true; //语句3 x = 4; //语句4 y = -1; //语句5 由于flag变量为volatile变量，那么在进行指令重排序的过程的时候，不会将语句3放到语句1、语句2前面，也不会讲语句3放到语句4、语句5后面。但是要注意语句1和语句2的顺序、语句4和语句5的顺序是不作任何保证的。 并且volatile关键字能保证，执行到语句3时，语句1和语句2必定是执行完毕了的，且语句1和语句2的执行结果对语句3、语句4、语句5是可见的。 Volatile的原理和实现机制 前面讲述了源于volatile关键字的一些使用，下面我们来探讨一下volatile到底如何保证可见性和禁止指令重排序的。下面这段话摘自《深入理解Java虚拟机》： 观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令 lock前缀指令实际上相当于一个 内存屏障（也成内存栅栏），内存屏障会提供3个功能： 它 确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成； 它会 强制将对缓存的修改操作立即写入主存； 如果是写操作，它会导致其他CPU中对应的缓存行无效。 "},"zother2-interview/java/concurrent/_index.html":{"url":"zother2-interview/java/concurrent/_index.html","title":"Index","keywords":"","body":""},"zother2-interview/java/exception/":{"url":"zother2-interview/java/exception/","title":"Java 异常","keywords":"","body":"Java异常 Java中有Error和Exception，它们都是继承自Throwable类。 二者的不同之处 Exception： 可以是可被控制(checked) 或不可控制的(unchecked)。 表示一个由程序员导致的错误。 应该在应用程序级被处理。 Error： 总是不可控制的(unchecked)。 经常用来用于表示系统错误或低层资源的错误。 如何可能的话，应该在系统级被捕捉。 异常的分类 Checked exception: 这类异常都是Exception的子类。异常的向上抛出机制进行处理，假如子类可能产生A异常，那么在父类中也必须throws A异常。可能导致的问题：代码效率低，耦合度过高。 Unchecked exception: 这类异常都是RuntimeException的子类，虽然RuntimeException同样也是Exception的子类，但是它们是非凡的，它们不能通过client code来试图解决，所以称为Unchecked exception 。 "},"zother2-interview/java/generics/":{"url":"zother2-interview/java/generics/","title":"Java 泛型","keywords":"","body":"为什么使用泛型 简而言之，泛型可以使类型（类和接口）在定义类、接口和方法时进行参数化。与在方法定义的形参类似，类型参数化能让不同的输入使用同一份代码。差别在于，形参传入的是值，而类型参数化传入的是类型。 在使用泛型相比于直接使用 Object 有以下几个好处： 强制的类型检查：Java 编译器会对泛型代码进行强制类型检查，如果违反类型安全则会抛出错误。在编译阶段解决类型错误，能更有效的减少 Bug 消除类型强制转换：如果不使用泛型，则在进行代码编写是需要手动进行类型转换 允许程序员实现通用算法：通过泛型，程序员能在不同类型的集合上实现通用算法 泛型类型 泛型类型是指被参数化的类或接口，首先我们来看看一个简单的类：Box。其方法接收和返回的都是 Object 类型，因此可以除基本类型外的其他任何类型。这样也导致在编译期间不能进行任何校验。如果 Box 期望的是一个 Integer 类型，然而外部调用时，传入 String 类型，这就会在程序运行时抛出异常。 public class Box { private Object object; public void set(Object object) { this.object = object; } public Object get() { return object; } } 下面我们来看看泛型类型版本的 Box： /** * Generic version of the Box class. * @param the type of the value being boxed */ public class Box { // T stands for \"Type\" private T t; public void set(T t) { this.t = t; } public T get() { return t; } } 如上面代码所示，所有 Object 都被替换为了 T，T 是一个可以代表除基本类型外的所有类型：任意类、任意接口、任意数组类型甚至还可以是其他的类型参数（eg: List）。 原始类型 原始类型是泛型类型没有类型参数的形式，例如上面的 Box ，其原始类型就是 Box，但 非泛型类类型不是原始类型。原始类型的出现，只是为兼容 JDK5 之前的历史代码，比如：Collections。因此，将一个泛型类型赋值给原始类型是可以的： Box stringBox = new Box<>(); Box rawBox = stringBox; // OK 如果将原始类型赋值给泛型类型，编译器会报告一个警告： Box rawBox = new Box(); // rawBox is a raw type of Box Box intBox = rawBox; // warning: unchecked conversion 同样的，如果将一个原始类型参数传递给泛型方法，也会报告一个警告： Box stringBox = new Box<>(); Box rawBox = stringBox; rawBox.set(8); // warning: unchecked invocation to set(T) 泛型方法 泛型方法与泛型类型类似，只不过泛型方法拥有自己的参数化类型，并且其作用域只限制在声明的方法中。泛型方法可以是静态的、非静态以及构造函数。泛型方法的声明必须在返回参数之前，即： public class Util { public static boolean compare(Pair p1, Pair p2) { return p1.getKey().equals(p2.getKey()) && p1.getValue().equals(p2.getValue()); } } public class Pair { private K key; private V value; //泛型方法 public Pair(K key, V value) { this.key = key; this.value = value; } //泛型方法 public void setKey(K key) { this.key = key; } public void setValue(V value) { this.value = value; } public K getKey() { return key; } public V getValue() { return value; } } 有界类型参数 当一个方法进行数字计算，并且想接收所有 Number 类型及其子类时，就需要用到有界类型参数。要声明一个有界类型参数，先列出该类型参数的名称，然后是 extends 关键字，然后是其上界，在这里是 Number。 在这里 extends 即可以表示 extends，也可以表示 implements。 public void inspect(U u){ System.out.println(\"T: \" + t.getClass().getName()); System.out.println(\"U: \" + u.getClass().getName()); } 在进行有界类型参数定义后，可使用在上界类型中定义的方法，在这里就可以调用 Number 内的所有方法，例如：intValue。 多上界 前面的示例说明了使用带单个界限的类型参数，但是类型参数可以具有多个界限： 有多个上界时，如果上界中包含类型（Class），则需要放在第一位。 泛型&继承&子类型 如 Java 语言规范所描述，只要类型兼容，就可以将一种类型的对象分配给另一种类型的对象。例如：我们可以将 Integer 对象赋值给 Object ，因为 Object 是 Integer 的父类之一。 Object someObject = new Object(); Integer someInteger = new Integer(10); someObject = someInteger; // OK 用面向对象的术语来说，这是一种 is a 的关系。因为，Integer 是一种 Object ，这样的赋值也是允许的。同时，Integer 也是一种 Number 所以以下代码均正确： public void someMethod(Number n) { /* ... */ } someMethod(new Integer(10)); // OK someMethod(new Double(10.1)); // OK 这种关系同样可以在泛型中使用： Box box = new Box(); box.add(new Integer(10)); // OK box.add(new Double(10.1)); // OK 但是在泛型方法上会有所不同，比如以下方法： public void boxTest(Box n) { /* ... */ } 这个方法能够接收什么类型的参数呢？是否能够将 Box 或者 Box 类型的对象传入呢？答案是 “否”，因为 Box 和 Box 均不是 Box 的子类型。 这是一个常见的误解 不论 Integer 和 Number 是什么关系，Box 和 Box 的共同父类均是 Object。 子类型 可以通过 extends 或者 implement 创建泛型类型的子类型，他们之间的关系只依赖与被 extends 或者被 implements。我们可以看看 Collections 的相关类型，ArrayList 实现 List 并且 List 继承自 Collection，所以，ArrayList 是 List 的子类型，List 是 Collection 的子类型。 现在我们需要自己定义一个 list 接口：PayloadList，它拥有一个可选的泛型类型 P： interface PayloadList extends List { void setPayload(int index, P val); ... } 以下所有的 PayloadList 都是 List 的子类型: PayloadList PayloadList PayloadList 通配符 在泛型中，使用 ? 做为通配符，代表未知类型。有界类型参数在上面已有介绍： List。类似的，通配符可以没有上界，即 List，这被称之为未知类型的List。这种未知类型的泛型在下面两个场景中很适用： 泛型类型中只使用 Object 中声明的方法 泛型类型中的代码不依赖与类型参数，例如：List.size、List.clear 另外，通配符还可声明类型参数的下界：List。 通配符&子类型 当有了通配符后，泛型的继承关系又有新的规则。尽管 Integer 是 Number 的子类型，但 List 不是 List 的子类型，实际上，这两种类型无关。 List 和 List 的公共父类是 List。 List intList = new ArrayList<>(); List numList = intList; // OK. List is a subtype of List 因为 Integer 是 Number 的子类型，并且 numList 是 Number 对象的列表，所以 intList （一个 Integer 对象的列表）和 numList 之间存在关系。下图显示了使用上下界通配符声明的几个 List 类之间的关系。 类型擦除 Java 语言引入了泛型，以在编译时提供更严格的类型检查并支持泛型编程。为实现泛型 Java 编译器会进行类型擦除： 替换所有类型参数为他们的上界或者 Object，因此，字节码仅包含普通的类，接口和方法。 必要时插入类型转换，以保持类型安全。 生成桥接方法以在扩展的泛型类型中保留多态。 类型擦除可确保不会为参数化类型创建新的类；因此，泛型不会产生运行时开销。 泛型类型的擦除 在类型擦除过程中，Java 编译器将擦除所有类型参数，如果类型参数是有界的，则将每个参数替换为其第一个边界；如果类型参数是无界的，则将其替换为 Object。 public class Node { private T data; private Node next; public Node(T data, Node next) { this.data = data; this.next = next; } public T getData() { return data; } // ... } 由于 T 是无界的，所以其类型擦除后的代码为： public class Node { private Object data; private Node next; public Node(Object data, Node next) { this.data = data; this.next = next; } public Object getData() { return data; } // ... } 而对于以下代码的擦除又不一样： public class Node> { private T data; private Node next; public Node(T data, Node next) { this.data = data; this.next = next; } public T getData() { return data; } // ... } 擦除后： public class Node { private Comparable data; private Node next; public Node(Comparable data, Node next) { this.data = data; this.next = next; } public Comparable getData() { return data; } // ... } 泛型方法擦除 泛型方法的擦除规则和泛型类型的擦除规则类似： // Counts the number of occurrences of elem in anArray. public static int count(T[] anArray, T elem) { int cnt = 0; for (T e : anArray) if (e.equals(elem)) ++cnt; return cnt; } public static void draw(T shape) { /* ... */ } 擦除后： public static int count(Object[] anArray, Object elem) { int cnt = 0; for (Object e : anArray) if (e.equals(elem)) ++cnt; return cnt; } public static void draw(Shape shape) { /* ... */ } 桥接方法 对于以下两个类： public class Node { public T data; public Node(T data) { this.data = data; } public void setData(T data) { System.out.println(\"Node.setData\"); this.data = data; } } public class MyNode extends Node { public MyNode(Integer data) { super(data); } public void setData(Integer data) { System.out.println(\"MyNode.setData\"); super.setData(data); } } 类型擦除： public class Node { public Object data; public Node(Object data) { this.data = data; } public void setData(Object data) { System.out.println(\"Node.setData\"); this.data = data; } } public class MyNode extends Node { public MyNode(Integer data) { super(data); } public void setData(Integer data) { System.out.println(\"MyNode.setData\"); super.setData(data); } } 在类型擦除后，方法的签名不匹配，导致重写的方法不生效，Node.setData(T) 变成了 Node.setData(Object)。为解决这个问题，Java 编译器在子类型中生成桥接方法，对于 MyNode 其生成的方法如下： class MyNode extends Node { // Bridge method generated by the compiler // public void setData(Object data) { setData((Integer) data); } public void setData(Integer data) { System.out.println(\"MyNode.setData\"); super.setData(data); } // ... } 这样，在类型擦除之后，MyNode 具有与 Node 的 setData(Object) 方法相同的方法签名的桥接方法，并将其委托给的 setData(Integer) 方法。 未擦除的泛型 类型擦除只局限于 泛型类型 和 泛型方法，对于 MyNode 这种非泛型类型，泛型信息并不会擦除。在 MyNode 内同样可以通过反射获取到它父类的泛型信息。 public static void main(String[] args) throws Exception { ParameterizedTypeImpl superclass = (ParameterizedTypeImpl) MyNode.class.getGenericSuperclass(); System.out.println(Arrays.toString(superclass.getActualTypeArguments()));//[class java.lang.Integer] } "},"zother2-interview/java/jvm/architecture/":{"url":"zother2-interview/java/jvm/architecture/","title":"JVM 架构","keywords":"","body":"JVM 架构 Java 源码通过 javac 编译为 Java 字节码 ，Java 字节码是 Java 虚拟机执行的一套代码格式，其抽象了计算机的基本操作。大多数指令只有一个字节，而有些操作符需要参数，导致多使用了一些字节。 JVM 的基本架构如上图所示，其主要包含三个大块： 类加载器：负责动态加载Java类到Java虚拟机的内存空间中。 运行时数据区：存储 JVM 运行时所有数据 执行引擎：提供 JVM 在不同平台的运行能力 线程 在 JVM 中运行着许多线程，这里面有一部分是应用程序创建来执行代码逻辑的 应用线程，剩下的就是 JVM 创建来执行一些后台任务的 系统线程。 主要的系统线程有： Compile Threads：运行时将字节码编译为本地代码所使用的线程 GC Threads：包含所有和 GC 有关操作 Periodic Task Thread：JVM 周期性任务调度的线程，主要包含 JVM 内部的采样分析 Singal Dispatcher Thread：处理 OS 发来的信号 VM Thread：某些操作需要等待 JVM 到达 安全点（Safe Point），即堆区没有变化。比如：GC 操作、线程 Dump、线程挂起 这些操作都在 VM Thread 中进行。 按照线程类型来分，在 JVM 内部有两种线程： 守护线程：通常是由虚拟机自己使用，比如 GC 线程。但是，Java程序也可以把它自己创建的任何线程标记为守护线程（public final void setDaemon(boolean on)来设置，但必须在start()方法之前调用）。 非守护线程：main方法执行的线程，我们通常也称为用户线程。 只要有任何的非守护线程在运行，Java程序也会继续运行。当该程序中所有的非守护线程都终止时，虚拟机实例将自动退出（守护线程随 JVM 一同结束工作）。 守护线程中不适合进行IO、计算等操作，因为守护线程是在所有的非守护线程退出后结束，这样并不能判断守护线程是否完成了相应的操作，如果非守护线程退出后，还有大量的数据没来得及读写，这将造成很严重的后果。 "},"zother2-interview/java/jvm/classloader/":{"url":"zother2-interview/java/jvm/classloader/","title":"类加载器","keywords":"","body":"类加载器 类加载器是 Java 运行时环境（Java Runtime Environment）的一部分，负责动态加载 Java 类到 Java 虚拟机的内存空间中。类通常是按需加载，即第一次使用该类时才加载。 由于有了类加载器，Java 运行时系统不需要知道文件与文件系统。每个 Java 类必须由某个类加载器装入到内存。 类装载器除了要定位和导入二进制 class 文件外，还必须负责验证被导入类的正确性，为变量分配初始化内存，以及帮助解析符号引用。这些动作必须严格按一下顺序完成： 装载：查找并装载类型的二进制数据。 链接：执行验证、准备以及解析(可选) 验证：确保被导入类型的正确性 准备：为类变量分配内存，并将其初始化为默认值。 解析：把类型中的符号引用转换为直接引用。 初始化：把类变量初始化为正确的初始值。 装载 类加载器分类 在Java虚拟机中存在多个类装载器，Java应用程序可以使用两种类装载器： Bootstrap ClassLoader：此装载器是 Java 虚拟机实现的一部分。由原生代码（如C语言）编写，不继承自 java.lang.ClassLoader 。负责加载核心 Java 库，启动类装载器通常使用某种默认的方式从本地磁盘中加载类，包括 Java API。 Extention Classloader：用来在/jre/lib/ext ,或 java.ext.dirs 中指明的目录中加载 Java 的扩展库。 Java 虚拟机的实现会提供一个扩展库目录。 Application Classloader：根据 Java应用程序的类路径（ java.class.path 或 CLASSPATH 环境变量）来加载 Java 类。一般来说，Java 应用的类都是由它来完成加载的。可以通过 ClassLoader.getSystemClassLoader() 来获取它。 自定义类加载器：可以通过继承 java.lang.ClassLoader 类的方式实现自己的类加载器，以满足一些特殊的需求而不需要完全了解 Java 虚拟机的类加载的细节。 全盘负责双亲委托机制 在一个 JVM 系统中，至少有 3 种类加载器，那么这些类加载器如何配合工作？在 JVM 种类加载器通过 全盘负责双亲委托机制 来协调类加载器。 全盘负责：指当一个 ClassLoader 装载一个类的时，除非显式地使用另一个 ClassLoader ，该类所依赖及引用的类也由这个 ClassLoader 载入。 双亲委托机制：指先委托父装载器寻找目标类，只有在找不到的情况下才从自己的类路径中查找并装载目标类。 全盘负责双亲委托机制只是 Java 推荐的机制，并不是强制的机制。实现自己的类加载器时，如果想保持双亲委派模型，就应该重写 findClass(name) 方法；如果想破坏双亲委派模型，可以重写 loadClass(name) 方法。 装载入口 所有Java虚拟机实现必须在每个类或接口首次主动使用时初始化。以下六种情况符合主动使用的要求： 当创建某个类的新实例时(new、反射、克隆、序列化) 调用某个类的静态方法 使用某个类或接口的静态字段，或对该字段赋值(用final修饰的静态字段除外，它被初始化为一个编译时常量表达式) 当调用Java API的某些反射方法时。 初始化某个类的子类时。 当虚拟机启动时被标明为启动类的类。 除以上六种情况，所有其他使用Java类型的方式都是被动的，它们不会导致Java类型的初始化。 对于接口来说，只有在某个接口声明的非常量字段被使用时，该接口才会初始化，而不会因为事先这个接口的子接口或类要初始化而被初始化。 父类需要在子类初始化之前被初始化。当实现了接口的类被初始化的时候，不需要初始化父接口。然而，当实现了父接口的子类(或者是扩展了父接口的子接口)被装载时，父接口也要被装载。(只是被装载，没有初始化) 验证 确认装载后的类型符合Java语言的语义，并且不会危及虚拟机的完整性。 装载时验证：检查二进制数据以确保数据全部是预期格式、确保除 Object 之外的每个类都有父类、确保该类的所有父类都已经被装载。 正式验证阶段：检查 final 类不能有子类、确保 final 方法不被覆盖、确保在类型和超类型之间没有不兼容的方法声明(比如拥有两个名字相同的方法，参数在数量、顺序、类型上都相同，但返回类型不同)。 符号引用的验证：当虚拟机搜寻一个被符号引用的元素(类型、字段或方法)时，必须首先确认该元素存在。如果虚拟机发现元素存在，则必须进一步检查引用类型有访问该元素的权限。 准备 在准备阶段，Java虚拟机为类变量分配内存，设置默认初始值。但在到到初始化阶段之前，类变量都没有被初始化为真正的初始值。 类型 默认值 int 0 long 0L short (short)0 char '\\u0000' byte (byte)0 blooean false float 0.0f double 0.0d reference null 解析 解析的过程就是在类型的常量池总寻找类、接口、字段和方法的符号引用，把这些符号引用替换为直接引用的过程。 类或接口的解析：判断所要转化成的直接引用是数组类型，还是普通的对象类型的引用，从而进行不同的解析。 字段解析：对字段进行解析时，会先在本类中查找是否包含有简单名称和字段描述符都与目标相匹配的字段，如果有，则查找结束；如果没有，则会按照继承关系从上往下递归搜索该类所实现的各个接口和它们的父接口，还没有，则按照继承关系从上往下递归搜索其父类，直至查找结束， 初始化 所有的类变量(即静态量)初始化语句和类型的静态初始化器都被Java编译器收集在一起，放到一个特殊的方法中。 对于类来说，这个方法被称作类初始化方法；对于接口来说，它被称为接口初始化方法。在类和接口的 class 文件中，这个方法被称为。 如果存在直接父类，且直接父类没有被初始化，先初始化直接父类。 如果类存在一个类初始化方法，执行此方法。 这个步骤是递归执行的，即第一个初始化的类一定是Object。 Java虚拟机必须确保初始化过程被正确地同步。 如果多个线程需要初始化一个类，仅仅允许一个线程来进行初始化，其他线程需等待。 这个特性可以用来写单例模式。 Clinit 方法 对于静态变量和静态初始化语句来说：执行的顺序和它们在类或接口中出现的顺序有关。 并非所有的类都需要在它们的class文件中拥有()方法， 如果类没有声明任何类变量，也没有静态初始化语句，那么它就不会有()方法。如果类声明了类变量，但没有明确的使用类变量初始化语句或者静态代码块来初始化它们，也不会有()方法。如果类仅包含静态final常量的类变量初始化语句，而且这些类变量初始化语句采用编译时常量表达式，类也不会有()方法。只有那些需要执行Java代码来赋值的类才会有() final常量：Java虚拟机在使用它们的任何类的常量池或字节码中直接存放的是它们表示的常量值。 "},"zother2-interview/java/jvm/dispatcher/":{"url":"zother2-interview/java/jvm/dispatcher/","title":"Java 分派机制","keywords":"","body":"Java分派机制 在Java中，符合“编译时可知，运行时不可变”这个要求的方法主要是静态方法和私有方法。这两种方法都不能通过继承或别的方法重写，因此它们适合在类加载时进行解析。 Java虚拟机中有四种方法调用指令： invokestatic：调用静态方法。 invokespecial：调用实例构造器方法，私有方法和super。 invokeinterface：调用接口方法。 invokevirtual：调用以上指令不能调用的方法（虚方法）。 只要能被invokestatic和invokespecial指令调用的方法，都可以在解析阶段确定唯一的调用版本，符合这个条件的有：静态方法、私有方法、实例构造器、父类方法，他们在类加载的时候就会把符号引用解析为改方法的直接引用。这些方法被称为非虚方法，反之其他方法称为虚方法（final方法除外）。 虽然final方法是使用invokevirtual指令来调用的，但是由于它无法被覆盖，多态的选择是唯一的，所以是一种非虚方法。 静态分派 对于类字段的访问也是采用静态分派 People man = new Man() 静态分派主要针对重载，方法调用时如何选择。在上面的代码中，People被称为变量的引用类型，Man被称为变量的实际类型。静态类型是在编译时可知的，而动态类型是在运行时可知的，编译器不能知道一个变量的实际类型是什么。 编译器在重载时候通过参数的静态类型而不是实际类型作为判断依据。并且静态类型在编译时是可知的，所以编译器根据重载的参数的静态类型进行方法选择。 在某些情况下有多个重载，那编译器如何选择呢？ 编译器会选择\"最合适\"的函数版本，那么怎么判断\"最合适“呢？越接近传入参数的类型，越容易被调用。 动态分派 动态分派主要针对重写，使用invokevirtual指令调用。invokevirtual指令多态查找过程： 找到操作数栈顶的第一个元素所指向的对象的实际类型，记为C。 如果在类型C中找到与常量中的描述符合简单名称都相符的方法，则进行访问权限校验，如果通过则返回这个方法的直接引用，查找过程结束；如果权限校验不通过，返回java.lang.IllegalAccessError异常。 否则，按照继承关系从下往上一次对C的各个父类进行第2步的搜索和验证过程。 如果始终没有找到合适的方法，则抛出 java.lang.AbstractMethodError异常。 虚拟机动态分派的实现 由于动态分派是非常繁琐的动作，而且动态分派的方法版本选择需要考虑运行时在类的方法元数据中搜索合适的目标方法，因此在虚拟机的实现中基于性能的考虑，在方法区中建立一个虚方法表（invokeinterface有接口方法表），来提高性能。 虚方法表中存放各个方法的实际入口地址。如果某个方法在子类没有重写，那么子类的虚方法表里的入口和父类入口一致，如果子类重写了这个方法，那么子类方法表中的地址会被替换为子类实现版本的入口地址。 "},"zother2-interview/java/jvm/gc/":{"url":"zother2-interview/java/jvm/gc/","title":"垃圾回收","keywords":"","body":"垃圾回收 对象存活检测 Java堆中存放着大量的Java对象实例，在垃圾收集器回收内存前，第一件事情就是确定哪些对象是活着的，哪些是可以回收的。 引用计数算法 引用计数算法是判断对象是否存活的基本算法：给每个对象添加一个引用计数器，没当一个地方引用它的时候，计数器值加1；当引用失效后，计数器值减1。但是这种方法有一个致命的缺陷，当两个对象相互引用时会导致这两个都无法被回收。 根搜索算法 引用计数是通过为堆中每个对象保存一个计数来区分活动对象和垃圾。根搜索算法实际上是追踪从根结点开始的 引用图。 在根搜索算法追踪的过程中，起点即 GC Root，GC Root 根据 JVM 实现不同而不同，但是总会包含以下几个方面（堆外引用）： 虚拟机栈（栈帧中的本地变量表）中引用的对象。 方法区中的类静态属性引用的变量。 方法区中的常量引用的变量。 本地方法 JNI 的引用对象。 根搜索算法是从 GC Root 开始的引用图，引用图是一个有向图，其中节点是各个对象，边为引用类型。JVM 中的引用类型分为四种：强引用（StrongReference）、软引用（SoftReference）、弱引用（WeakReference） 和 虚引用（PhantomReference）。 除强引用外，其他引用在Java 由 Reference 的子类封装了指向其他对象的连接：被指向的对象称为 引用目标。 若一个对象的引用类型有多个，那到底如何判断它的回收策略呢？其实规则如下： 单条引用链以链上最弱的一个引用类型来决定； 多条引用链以多个单条引用链中最强的一个引用类型来决定； 在引用图中，当一个节点没有任何路径可达时，我们认为它是可回收的对象。 StrongReference 强引用在Java中是普遍存在的，类似 Object o = new Object(); 。强引用和其他引用的区别在于：强引用禁止引用目标被垃圾收集器收集，而其他引用不禁止。 SoftReference 对象可以从根节点通过一个或多个(未被清除的)软引用对象触及，垃圾收集器在要发生内存溢出前将这些对象列入回收范围中进行回收，如果该软引用对象和引用队列相关联，它会把该软引用对象加入队列。 JVM 的实现需要在抛出 OutOfMemoryError 之前清除 SoftReference，但在其他的情况下可以选择清理的时间或者是否清除它们。 WeakReference 对象可以从 GC Root 开始通过一个或多个(未被清除的)弱引用对象触及， 垃圾收集器在 GC 的时候会回收所有的 WeakReference，如果该弱引用对象和引用队列相关联，它会把该弱引用对象加入队列。 PhantomReference 垃圾收集器在 GC 不会清除 PhantomReference，所有的虚引用都必须由程序明确的清除。同时也不能通过虚引用来取得一个对象的实例。 垃圾回收算法 复制回收算法 将可用内存分为大小相等的两份，在同一时刻只使用其中的一份。当这一份内存使用完了，就将还存活的对象复制到另一份上，然后将这一份上的内存清空。复制算法能有效避免内存碎片，但是算法需要将内存一分为二，导致内存使用率大大降低。 标记清除算法 先暂停整个程序的全部运行线程，让回收线程以单线程进行扫描标记，并进行直接清除回收，然后回收完成后，恢复运行线程。标记清除后会产生大量不连续的内存碎片，造成空间浪费。 标记整理算法 和 标记清除 相似，不同的是，回收期间同时会将保留的存储对象搬运汇集到连续的内存空间，从而集成空闲空间。 增量回收 需要程序将所拥有的内存空间分成若干分区（Region）。程序运行所需的存储对象会分布在这些分区中，每次只对其中一个分区进行回收操作，从而避免程序全部运行线程暂停来进行回收，允许部分线程在不影响回收行为而保持运行，并且降低回收时间，增加程序响应速度。 分代回收 在 JVM 中不同的对象拥有不同的生命周期，因此对于不同生命周期的对象也可以采用不同的垃圾回收算法，以提高效率，这就是分代回收算法的核心思想。 记忆集 上面有说到进行 GC 的时候，会从 GC Root 进行搜索，做一个引用图。现有一个对象 C 在 Young Gen，其只被一个在 Old Gen 的对象 D 引用，其引用结构如下所示： 这个时候要进行 Young GC，要确定 C 是否被堆外引用，就需要遍历 Old Gen，这样的代价太大。所以 JVM 在进行对象引用的时候，会有个 记忆集（Remembered Set） 记录从 Old Gen 到 Young Gen 的引用关系，并把记忆集里的 Old Gen 作为 GC Root 来构建引用图。这样在进行 Young GC 时就不需要遍历 Old Gen。 但是使用记忆集也会有缺点：C & D 其实都可以进行回收，但是由于记忆集的存在，不会将 C 回收。这里其实有一点 空间换时间 的意思。不过无论如何，它依然确保了垃圾回收所遵循的原则：垃圾回收确保回收的对象必然是不可达对象，但是不确保所有的不可达对象都会被回收。 垃圾回收触发条件 堆内内存 针对 HotSpot VM 的实现，它里面的 GC 其实准确分类只有两大种： Partial GC：并不收集整个 GC 堆的模式 Young GC（Minor GC）：只收集 Young Gen 的 GC Old GC：只收集 Old Gen 的 GC。只有 CMS的 Concurrent Collection 是这个模式 Mixed GC：收集整个 Young Gen 以及部分 Old Gen 的 GC。只有 G1 有这个模式 Full GC（Major GC）：收集整个堆，包括 Young Gen、Old Gen、Perm Gen（如果存在的话）等所有部分的 GC 模式。 最简单的分代式GC策略，按 HotSpot VM 的 serial GC 的实现来看，触发条件是 Young GC：当 Young Gen 中的 eden 区分配满的时候触发。把 Eden 区存活的对象将被复制到一个 Survivor 区，当这个 Survivor 区满时，此区的存活对象将被复制到另外一个 Survivor 区。 Full GC： 当准备要触发一次 Young GC 时，如果发现之前 Young GC 的平均晋升大小比目前 Old Gen剩余的空间大，则不会触发 Young GC 而是转为触发 Full GC 除了 CMS 的 Concurrent Collection 之外，其它能收集 Old Gen 的GC都会同时收集整个 GC 堆，包括 Young Gen，所以不需要事先触发一次单独的Young GC 如果有 Perm Gen 的话，要在 Perm Gen分配空间但已经没有足够空间时 System.gc() Heap dump 并发 GC 的触发条件就不太一样。以 CMS GC 为例，它主要是定时去检查 Old Gen 的使用量，当使用量超过了触发比例就会启动一次 GC，对 Old Gen做并发收集。 堆外内存 DirectByteBuffer 的引用是直接分配在堆得 Old 区的，因此其回收时机是在 FullGC 时。因此，需要避免频繁的分配 DirectByteBuffer ，这样很容易导致 Native Memory 溢出。 DirectByteBuffer 申请的直接内存，不再GC范围之内，无法自动回收。JDK 提供了一种机制，可以为堆内存对象注册一个钩子函数(其实就是实现 Runnable 接口的子类)，当堆内存对象被GC回收的时候，会回调run方法，我们可以在这个方法中执行释放 DirectByteBuffer 引用的直接内存，即在run方法中调用 Unsafe 的 freeMemory 方法。注册是通过sun.misc.Cleaner 类来实现的。 垃圾收集器 垃圾收集器是内存回收的具体实现，下图展示了 7 种用于不同分代的收集器，两个收集器之间有连线表示可以搭配使用，每种收集器都有最适合的使用场景。 Serial 收集器 Serial 收集器是最基本的收集器，这是一个单线程收集器，它只用一个线程去完成垃圾收集工作。 虽然 Serial 收集器的缺点很明显，但是它仍然是 JVM 在 Client 模式下的默认新生代收集器。它有着优于其他收集器的地方：简单而高效（与其他收集器的单线程比较），Serial 收集器由于没有线程交互的开销，专心只做垃圾收集自然也获得最高的效率。在用户桌面场景下，分配给 JVM 的内存不会太多，停顿时间完全可以在几十到一百多毫秒之间，只要收集不频繁，这是完全可以接受的。 ParNew 收集器 ParNew 是 Serial 的多线程版本，在回收算法、对象分配原则上都是一致的。ParNew 收集器是许多运行在Server 模式下的默认新生代垃圾收集器，其主要与 CMS 收集器配合工作。 Parallel Scavenge 收集器 Parallel Scavenge 收集器是一个新生代垃圾收集器，也是并行的多线程收集器。 Parallel Scavenge 收集器更关注可控制的吞吐量，吞吐量等于运行用户代码的时间/(运行用户代码的时间+垃圾收集时间)。 Serial Old收集器 Serial Old 收集器是 Serial 收集器的老年代版本，也是一个单线程收集器，采用“标记-整理算法”进行回收。 Parallel Old 收集器 Parallel Old 收集器是 Parallel Scavenge 收集器的老年代版本，使用多线程进行垃圾回收，其通常与 Parallel Scavenge 收集器配合使用。 CMS 收集器 CMS（Concurrent Mark Sweep）收集器是一种以获取最短停顿时间为目标的收集器， CMS 收集器采用 标记--清除 算法，运行在老年代。主要包含以下几个步骤： 初始标记（Stop the world） 并发标记 重新标记（Stop the world） 并发清除 其中初始标记和重新标记仍然需要 Stop the world。初始标记仅仅标记 GC Root 能直接关联的对象，并发标记就是进行 GC Root Tracing 过程，而重新标记则是为了修正并发标记期间，因用户程序继续运行而导致标记变动的那部分对象的标记记录。 由于整个过程中最耗时的并发标记和并发清除，收集线程和用户线程一起工作，所以总体上来说， CMS 收集器回收过程是与用户线程并发执行的。虽然 CMS 优点是并发收集、低停顿，很大程度上已经是一个不错的垃圾收集器，但是还是有三个显著的缺点： CMS收集器对CPU资源很敏感：在并发阶段，虽然它不会导致用户线程停顿，但是会因为占用一部分线程（CPU资源）而导致应用程序变慢。 CMS收集器不能处理浮动垃圾：所谓的“浮动垃圾”，就是在并发标记阶段，由于用户程序在运行，那么自然就会有新的垃圾产生，这部分垃圾被标记过后，CMS 无法在当次集中处理它们，只好在下一次 GC 的时候处理，这部分未处理的垃圾就称为“浮动垃圾”。 GC 后产生大量内存碎片：当内存碎片过多时，将会给分配大对象带来困难，这是就会进行 Full GC。 正是由于在垃圾收集阶段程序还需要运行，即还需要预留足够的内存空间供用户使用，因此 CMS 收集器不能像其他收集器那样等到老年代几乎填满才进行收集，需要预留一部分空间提供并发收集时程序运作使用。要是 CMS 预留的内存空间不能满足程序的要求，这是 JVM 就会启动预备方案：临时启动 Serial Old 收集器来收集老年代，这样停顿的时间就会很长。 G1收集器 G1收集器与CMS相比有很大的改进： 标记整理算法：G1 收集器采用标记整理算法实现 增量回收模式：将 Heap 分割为多个 Region，并在后台维护一个优先列表，每次根据允许的时间，优先回收垃圾最多的区域 因此 G1 收集器可以实现在基本不牺牲吞吐量的情况下完成低停顿的内存回收，这是正是由于它极力的避免全区域的回收。 垃圾收集器 特性 算法 优点 缺点 Serial 串行 复制 高效：无线程切换 无法利用多核CPU ParNew 并行 复制 可利用多核CPU、唯一能与CMS配合的并行收集器 Parallel Scavenge 并行 复制 高吞吐量 Serial Old 串行 标记整理 高效 无法利用多核CPU Parallel Old 并行 标记整理 高吞吐量 CMS 并行 标记清除 低停顿 CPU敏感、浮动垃圾、内存碎片 G1 并行 增量回收 低停顿、高吞吐量 内存使用效率低：分区导致内存不能充分使用 "},"zother2-interview/java/jvm/jvm-object-lifecycle/":{"url":"zother2-interview/java/jvm/jvm-object-lifecycle/","title":"对象的生命周期","keywords":"","body":"对象的生命周期 一旦一个类被装载、连接和初始化，它就随时可以被使用。程序可以访问它的静态字段，调用它的静态方法，或者创建它的实例。作为Java程序员有必要了解Java对象的生命周期。 类实例化 在Java程序中，类可以被明确或隐含地实例化。明确的实例化类有四种途径： 明确调用new。 调用Class或者java.lang.reflect.Constructor对象的newInstance方法。 调用任何现有对象的clone。 通过java.io.ObjectInputStream.getObject()反序列化。 隐含的实例化： 可能是保存命令行参数的String对象。 对于Java虚拟机装载的每个类，都会暗中实例化一个Class对象来代表这个类型 当Java虚拟机装载了在常量池中包含CONSTANT_String_info入口的类的时候，它会创建新的String对象来表示这些常量字符串。 执行包含字符串连接操作符的表达式会产生新的对象。 Java编译器为它编译的每个类至少生成一个实例初始化方法。在Java class文件中，这个方法被称为。针对源代码中每个类的构造方法，Java编译器都会产生一个()方法。如果类没有明确的声明任何构造方法，编译器会默认产生一个无参数的构造方法，它仅仅调用父类的无参构造方法。 一个()中可能包含三种代码：调用另一个()、实现对任何实例变量的初始化、构造方法体的代码。 如果构造方法明确的调用了同一个类中的另一个构造方法(this())，那么它对应的()由两部分组成： 一个同类的()的调用。 实现了对应构造方法的方法体的字节码。 在它对应的()方法中不会有父类的()，但不代表不会调用父类的()，因为this()中也会调用父类() 如果构造方法不是通过一个this()调用开始的，而且这个对象不是Object，()则有三部分组成： 一个父类的()调用。如果这个类是Object,则没有这个部分 任意实例变量初始化方法的字节码。 实现了对应构造方法的方法体的字节码。 如果构造方法明确的调用父类的构造方法super()开始，它的()会调用对应父类的()。比如，如果一个构造方法明确的调用super(int,String)开始，对应的()会从调用父类的(int,String)方法开始。如果构造方法没有明确地从this()或super()开始，对应的()默认会调用父类的无参()。 垃圾收集和对象的终结 程序可以明确或隐含的为对象分配内存，但不能明确的释放内存。一个对象不再为程序引用，虚拟机必须回收那部分内存。 卸载类 在很多方面，Java虚拟机中类的生命周期和对象的生命周期很相似。当程序不再使用某个类的时候，可以选择卸载它们。 类的垃圾收集和卸载值所以在Java虚拟机中很重要，是因为Java程序可以在运行时通过用户自定义的类装载器装载类型来动态的扩展程序。所有被装载的类型都在方法区占据内存空间。 Java虚拟机通过判断类是否在被引用来进行垃圾收集。判断动态装载的类的Class实例在正常的垃圾收集过程中是否可触及有两种方式： 如果程序保持非Class实例的明确引用。 如果在堆中还存在一个可触及的对象，在方法区中它的类型数据指向一个Class实例。 "},"zother2-interview/java/jvm/runtime_area/":{"url":"zother2-interview/java/jvm/runtime_area/","title":"运行时数据区","keywords":"","body":"运行时数据区 运行时数据区用于保存 JVM 在运行过程中产生的数据，结构如图所示： Heap Java 堆是可供各线程共享的运行时内存区域，是 Java 虚拟机所管理的内存区域中最大的一块。此区域非常重要，几乎所有的对象实例和数组实例都要在 Java 堆上分配，但随着 JIT 编译器及逃逸分析技术的发展，也可能会被优化为栈上分配。 Heap 中除了作为对象分配使用，还包含字符串字面量 常量池（Internd Strings） 。 除此之外 Heap 中还包含一个 新生代（Yong Generation）、一个 老年代（Old Generation）。 新生代分三个区，一个Eden区，两个Survivor区，大部分对象在Eden区中生成。Survivor 区总有一个是空的。 老年代中保存一些生命周期较长的对象，当一个对象经过多次的 GC 后还没有被回收，那么它将被移动到老年代。 Methoad Area 方法区的数据由所有线程共享，因此为安全的使用方法区的数据，需要注意线程安全问题。 方法区主要保存类级别的数据，包括： ClassLoader Reference Runtime Constant Pool 数字常量 类属性引用 方法引用 Field Data：每个类属性的名称、类型等 Methoad Data：每个方法的名称、返回值类型、参数列表等 Methoad Code：每个方法的字节码、本地变量表等 方法区的实现在不同的 JVM 版本有不同，在 JVM 1.8 之前，方法区的实现为 永久代（PermGen），但是由于永久代的大小限制， 经常会出现内存溢出。于是在 JVM 1.8 方法区的实现改为 元空间（Metaspace），元空间是在 Native 的一块内存空间。 Stack 对于每个 JVM 线程，当线程启动时，都会分配一个独立的运行时栈，用以保存方法调用。每个方法调用，都会在栈顶增加一个栈帧（Stack Frame）。 每个栈帧都保存三个引用：本地变量表（Local Variable Array）、 操作数栈（Operand Stack） 和 当前方法所属类的运行时常量池（Runtime Constant Pool）。由于本地变量表和操作数栈的大小都在编译时确定，所以栈帧的大小是固定的。 当被调用的方法返回或抛出异常，栈帧会被弹出。在抛出异常时 printStackTrace() 打印的每一行就是一个栈帧。同时得益于栈帧的特点，栈帧内的数据是线程安全的。 栈的大小可以动态扩展，但是如果一个线程需要的栈大小超过了允许的大小，就会抛出 StackOverflowError。 PC Register 对于每个 JVM 线程，当线程启动时，都会有一个独立的 PC（Program Counter） 计数器，用来保存当前执行的代码地址（方法区中的内存地址）。如果当前方法是 Native 方法，PC 的值为 NULL。一旦执行完成，PC 计数器会被更新为下一个需要执行代码的地址。 Native Method Stack 本地方法栈和 Java 虚拟机栈的作用相似，Java 虚拟机栈执行的是字节码，而本地方法栈执行的是 native 方法。本地方法栈使用传统的栈（C Stack）来支持 native 方法。 Direct Memory Native Memory Tracking 在 JDK 1.4 中新加入了 NIO 类，它可以使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆里的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为 避免了在 Java 堆和 Native 堆中来回复制数据。 "},"zother2-interview/java/jvm/string-constant-pool/":{"url":"zother2-interview/java/jvm/string-constant-pool/","title":"String 常量池","keywords":"","body":"String 常量池 在 JAVA 语言中有 8 中基本类型和一种比较特殊的类型 String 。这些类型为了使他们在运行过程中速度更快，更节省内存，都提供了一种常量池的概念。常量池就类似一个 JAVA 系统级别提供的缓存。 String 类型的常量池比较特殊。它的主要使用方法有两种： 直接使用双引号声明出来的 String 对象会直接存储在常量池中 如果不是用双引号声明的 String 对象，可以使用 String 提供的 intern 方法。 intern 方法会从字符串常量池中查询当前字符串是否存在，若不存在就会将当前字符串放入常量池中 intern /** * Returns a canonical representation for the string object. * * A pool of strings, initially empty, is maintained privately by the * class {@code String}. * * When the intern method is invoked, if the pool already contains a * string equal to this {@code String} object as determined by * the {@link #equals(Object)} method, then the string from the pool is * returned. Otherwise, this {@code String} object is added to the * pool and a reference to this {@code String} object is returned. * * It follows that for any two strings {@code s} and {@code t}, * {@code s.intern() == t.intern()} is {@code true} * if and only if {@code s.equals(t)} is {@code true}. * * All literal strings and string-valued constant expressions are * interned. String literals are defined in section 3.10.5 of the * The Java&trade; Language Specification. * * @return a string that has the same contents as this string, but is * guaranteed to be from a pool of unique strings. */ public native String intern(); JAVA 使用 jni 调用 c++ 实现的 StringTable 的 intern 方法, StringTable 跟 Java 中的 HashMap 的实现是差不多的, 只是 不能自动扩容。默认大小是 1009 。 要注意的是， String 的 String Pool 是一个固定大小的 Hashtable ，默认值大小长度是 1009 ，如果放进 String Pool 的 String 非常多，就会造成 Hash 冲突严重，从而导致链表会很长，而链表长了后直接会造成的影响就是当调用 String.intern 时性能会大幅下降。 在 JDK6 中 StringTable 是固定的，就是 1009 的长度，所以如果常量池中的字符串过多就会导致效率下降很快。在 jdk7 中， StringTable 的长度可以通过一个参数指定： -XX:StringTableSize=99991 在 JDK6 以及以前的版本中，字符串的常量池是放在堆的 Perm 区。在 JDK7 的版本中，字符串常量池已经从 Perm 区移到正常的 Java Heap 区域 public static void main(String[] args) { String s = new String(\"1\"); s.intern(); String s2 = \"1\"; System.out.println(s == s2); String s3 = new String(\"1\") + new String(\"1\"); s3.intern(); String s4 = \"11\"; System.out.println(s3 == s4); } 上述代码的执行结果： JDK6: false false JDK7: false true public static void main(String[] args) { String s = new String(\"1\"); String s2 = \"1\"; s.intern(); System.out.println(s == s2); String s3 = new String(\"1\") + new String(\"1\"); String s4 = \"11\"; s3.intern(); System.out.println(s3 == s4); } 上述代码的执行结果： JDK6: false false JDK7: false false 由于 JDK7 将字符串常量池移动到 Heap 中，导致上述版本差异，下面具体来分析下。 JDK6 图中绿色线条代表 string 对象的内容指向，黑色线条代表地址指向 在 jdk6 中上述的所有打印都是 false ，因为 jdk6 中的常量池是放在 Perm 区中的， Perm 区和正常的 JAVA Heap 区域是完全分开的。上面说过如果是使用引号声明的字符串都是会直接在字符串常量池中生成，而 new 出来的 String 对象是放在 JAVA Heap 区域。所以拿一个 JAVA Heap 区域的对象地址和字符串常量池的对象地址进行比较肯定是不相同的，即使调用 String.intern 方法也是没有任何关系的。 JDK7 因为字符串常量池移动到 JAVA Heap 区域后，再来解释为什么会有上述的打印结果。 在第一段代码中，先看 s3 和 s4 字符串。String s3 = new String(\"1\") + new String(\"1\");，这句代码中现在生成了 2个 最终对象，是字符串常量池中的 “1” 和 JAVA Heap 中的 s3 引用指向的对象。中间还有 2个 匿名的 new String(\"1\") 我们不去讨论它们。此时 s3 引用对象内容是 ”11” ，但此时常量池中是没有 “11” 对象的。 接下来 s3.intern(); 这一句代码，是将 s3 中的 “11” 字符串放入 String 常量池中，因为此时常量池中不存在 “11” 字符串，因此常规做法是跟 jdk6 图中表示的那样，在常量池中生成一个 “11” 的对象，关键点是 jdk7 中常量池不在 Perm 区域了，这块做了调整。常量池中不需要再存储一份对象，可以直接存储堆中的引用。这份引用指向 s3 引用的对象。 也就是说引用地址是相同的。 最后 String s4 = \"11\"; 这句代码中 ”11” 是显示声明的，因此会直接去常量池中创建，创建的时候发现已经有这个对象了，此时也就是指向 s3 引用对象的一个引用。所以 s4 引用就指向和 s3 一样了。因此最后的比较 s3 == s4 是 true 。 再看 s 和 s2 对象。 String s = new String(\"1\"); 第一句代码，生成了2个对象。常量池中的 “1” 和 JAVA Heap 中的字符串对象。s.intern(); 这一句是 s 对象去常量池中寻找后发现 “1” 已经在常量池里了。 接下来 String s2 = \"1\"; 这句代码是生成一个 s2 的引用指向常量池中的 “1” 对象。 结果就是 s 和 s2 的引用地址明显不同。 接下来是第二段代码： 第一段代码和第二段代码的改变就是 s3.intern(); 的顺序是放在 String s4 = \"11\"; 后了。这样，首先执行 String s4 = \"11\"; 声明 s4 的时候常量池中是不存在 “11” 对象的，执行完毕后， “11“ 对象是 s4 声明产生的新对象。然后再执行 s3.intern(); 时，常量池中 “11” 对象已经存在了，因此 s3 和 s4 的引用是不同的。 第二段代码中的 s 和 s2 代码中，s.intern();，这一句往后放也不会有什么影响了，因为对象池中在执行第一句代码String s = new String(\"1\"); 的时候已经生成 “1” 对象了。下边的 s2 声明都是直接从常量池中取地址引用的。 s 和 s2 的引用地址是不会相等的。 小结 从上述的例子代码可以看出 jdk7 版本对 intern 操作和常量池都做了一定的修改。主要包括2点： 将 String 常量池 从 Perm 区移动到了 Java Heap 区 String#intern 方法时，如果存在堆中的对象，会直接保存对象的引用，而不会重新创建对象。 使用范例 static final int MAX = 1000 * 10000; static final String[] arr = new String[MAX]; public static void main(String[] args) throws Exception { Integer[] DB_DATA = new Integer[10]; Random random = new Random(10 * 10000); for (int i = 0; i 运行的参数是：-Xmx2g -Xms2g -Xmn1500M 上述代码是一个演示代码，其中有两条语句不一样，一条是使用 intern，一条是未使用 intern。 通过上述结果，我们发现不使用 intern 的代码生成了 1000w 个字符串，占用了大约 640m 空间。 使用了 intern 的代码生成了 1345 个字符串，占用总空间 133k 左右。其实通过观察程序中只是用到了 10 个字符串，所以准确计算后应该是正好相差 100w 倍。虽然例子有些极端，但确实能准确反应出 intern 使用后产生的巨大空间节省。 细心的同学会发现使用了 intern 方法后时间上有了一些增长。这是因为程序中每次都是用了 new String 后，然后又进行 intern 操作的耗时时间，这一点如果在内存空间充足的情况下确实是无法避免的，但我们平时使用时，内存空间肯定不是无限大的，不使用 intern 占用空间导致 jvm 垃圾回收的时间是要远远大于这点时间的。 毕竟这里使用了 1000w 次 intern 才多出来1秒钟多的时间。 不当使用 fastjson 中对所有的 json 的 key 使用了 intern 方法，缓存到了字符串常量池中，这样每次读取的时候就会非常快，大大减少时间和空间。而且 json 的 key 通常都是不变的。这个地方没有考虑到大量的 json key 如果是变化的，那就会给字符串常量池带来很大的负担。 这个问题 fastjson 在1.1.24版本中已经将这个漏洞修复了。程序加入了一个最大的缓存大小，超过这个大小后就不会再往字符串常量池中放了。 参考文档 深入解析String#intern "},"zother2-interview/java/jvm/_index.html":{"url":"zother2-interview/java/jvm/_index.html","title":"Index","keywords":"","body":""},"zother2-interview/java/nio/":{"url":"zother2-interview/java/nio/","title":"NIO","keywords":"","body":"NIO vs BIO NIO为所有的原始类型提供(Buffer)缓存支持，字符集编码解码解决方案。 提供多路(non-bloking) 非阻塞式的高伸缩性网络I/O 。 IO NIO 面向流 面向缓冲 阻塞IO 非阻塞IO 无 选择器 Channel Java NIO的通道类似流，但又有些不同： 既可以从通道中读取数据，又可以写数据到通道。但流的读写通常是单向的。 通道可以异步地读写。 通道中的数据总是要先读到一个Buffer，或者总是要从一个Buffer中写入。 Selector Selector（选择器）是Java NIO中能够检测一到多个NIO通道，并能够知晓通道是否为诸如读写事件做好准备的组件。这样，一个单独的线程可以管理多个channel，从而管理多个网络连接。 Channel 可以向 Selector 注册监听四种不同类型的事件： Connect Accept Read Write 一旦向 Selector 注册了一或多个通道，就可以调用几个重载的 select() 方法。这些方法返回你所感兴趣的事件（如连接、接受、读或写）已经准备就绪的那些通道。换句话说，如果你对“读就绪”的通道感兴趣， select() 方法会返回读事件已经就绪的那些通道。 select 方法空转 若 Selector 的轮询结果为空，也没有 wakeup 或新消息处理，则发生空轮询，CPU使用率 100%，Netty的解决办法：对 Selector 的 select 操作周期进行统计，每完成一次空的 select 操作进行一次计数。若在某个周期内连续发生N次空轮询，则触发了 epoll 死循环 bug 。重建 Selector 判断是否是其他线程发起的重建请求，若不是则将原 SocketChannel 从旧的 Selector 上去除注册，重新注册到新的 Selector 上，并将原来的 Selector 关闭。 SelectionKey 当向 Selector 注册 Channel 时，Channel.register() 方法会返回一个 SelectionKey 对象，这个对象代表了注册到该 Selector 和 Channel 的关联关系，并提供了一组方法来操作。当 Channel 注册的事件来到时，这个对象会在 Selector.selectedKeys() 中返回，直到 Channel 或者 Selector 被关闭。 isAcceptable isReadable channel selector Set selectedKeys = selector.selectedKeys(); Iterator keyIterator = selectedKeys.iterator(); while(keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if(key.isAcceptable()) { // a connection was accepted by a ServerSocketChannel. } else if (key.isConnectable()) { // a connection was established with a remote server. } else if (key.isReadable()) { // a channel is ready for reading } else if (key.isWritable()) { // a channel is ready for writing } keyIterator.remove(); } 注意每次迭代末尾的 keyIterator.remove() 调用。Selector 不会自己从已选择键集中移除 SelectionKey 实例。必须在处理完通道时自己移除。下次该通道变成就绪时， Selector 会再次将其放入已选择键集中。 Buffer 缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成 NIO Buffer 对象，并提供了一组方法，用来方便的访问该块内存。为了理解Buffer的工作原理，需要熟悉它的三个属性： capacity 、 position 、 limit。 position 和 limit 的含义取决于 Buffer 处在读模式还是写模式。不管 Buffer 处在什么模式， capacity 的含义总是一样的。 capacity：作为一个内存块，Buffer有一个固定的大小值。你只能往里写 capacity 个byte、long，char等类型。一旦 Buffer 满了，需要将其清空（通过读数据或者清除数据）才能继续写数据往里写数据。 flip()：方法将 Buffer 从写模式切换到读模式。调用 flip() 方法会将 position 设回 0 ，并将 limit 设置成之前 position 的值。 clear()： position 将被设回0， limit 被设置成 capacity 的值 compact()：将所有未读的数据拷贝到 Buffer 起始处。然后将 position 设到最后一个未读元素正后面。 limit 属性依然像 clear() 方法一样，设置成 capacity 。 rewind()：将 position 设回0，所以你可以重读 Buffer 中的所有数据，limit保持不变。 堆内内存（HeapByteBuffer） HeapByteBuffer 是在 Java Heap 上分配的，但是Java NIO在读写到相应的 Channel 的时候，会先将 Java Heap 的 buffer 内容拷贝至直接内存 —— Direct Memory。这样的话，无疑 DirectByteBuffer 的 IO 性能肯定强于使用 HeapByteBuffer ，它省去了临时 buffer 的拷贝开销。 堆外内存（DirectByteBuffer） DirectByteBuffer 底层的数据其实是维护在 JVM 堆外的用户空间中， DirectByteBuffer 里维护了一个引用 address 指向了数据，从而操作数据。虽然 GC 仍然管理着 DirectBuffer 的回收，但它是使用 PhantomReference 来达到的，在平常的 Young GC 或者 mark and compact 的时候却不会在内存里搬动。如果IO的数量比较大，比如在网络发送很大的文件，那么 GC 的压力下降就会很明显。只有在 Full GC 以及调用 System.gc 的时候才会进行回收。 DirectByteBuffer Java 堆内只会占用一个对象的指针引用的大小，堆外的的空间只有当 java 对象被回收时，才会被回收，这里会发现一个明显的不对称现象，就是堆外可能占用了很多，而堆内没占用多少，导致还没触发 GC ，那就很容易出现 Direct Memory 造成物理内存耗光。 EchoNIOServer @Slf4j public class NIOServer { private static final ByteBuffer buffer = ByteBuffer.allocate(32); public static void main(String[] args) throws Exception { ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.bind(new InetSocketAddress(8080)); serverSocketChannel.configureBlocking(false); Selector selector = Selector.open(); serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); while (true) { if (selector.select() == 0) { log.warn(\"selector.select() == 0\"); TimeUnit.MILLISECONDS.sleep(100); continue; } Iterator iterator = selector.selectedKeys().iterator(); while (iterator.hasNext()) { SelectionKey selectedKey = iterator.next(); if (selectedKey.isAcceptable()) { handleAccept(selectedKey); } else if (selectedKey.isReadable()) { handleRead(selectedKey); } else { log.warn(\"{}\", selectedKey); } iterator.remove(); } } } private static void handleRead(SelectionKey selectedKey) throws Exception { SocketChannel channel = (SocketChannel) selectedKey.channel(); int readSize = channel.read(buffer); if (readSize == -1) { channel.close(); return; } buffer.flip(); byte[] readed = Arrays.copyOf(buffer.array(), readSize); log.info(\"from:{} read data:{}\", channel, new String(readed)); buffer.clear(); buffer.put(\"echo:\".getBytes()); buffer.put(readed); buffer.flip(); channel.write(buffer); buffer.compact(); } private static void handleAccept(SelectionKey selectionKey) throws Exception { ServerSocketChannel serverSocketChannel = (ServerSocketChannel) selectionKey.channel(); SocketChannel socketChannel = serverSocketChannel.accept(); socketChannel.configureBlocking(false); socketChannel.register(selectionKey.selector(), SelectionKey.OP_READ); log.info(\"handleAccept from:{}\", socketChannel); } "},"zother2-interview/java/object/":{"url":"zother2-interview/java/object/","title":"Object","keywords":"","body":"Object getClass 返回该对象运行时的 class 对象，返回的 Class 对象是由所表示的类的静态同步方法锁定的对象。 hashCode 返回该对象的 hashcode，该方法对hash表提供支持，例如 HashMap。 对于该方法有几点需要注意： 在运行中的Java应用，如果用在 equals 中进行比较的信息没有改变，那么不论何时调用都需要返回一致的int值。这个hash值在应用的两次执行中不需要保持一致。 如果两个对象根据 equals 方法认为是相等的，那么这两个对象也应该返回相等的 hashcode。 不要求两个不相等的对象，在调用 hashCode 方法返回的结果是必须是不同的。然而，程序员应该了解不同的对象产生不同的 hashcode 能够提升哈希表的效率。 Object的hashcode对不同的对象，尽可能返回不同的 hashcode 。这通常通过将对象的内部地址转换为整数来实现，但Java编程语言不需要此实现技术。 Arrays.hashCode Arrays.hashCode 是一个数组的浅哈希码实现，深哈希可以使用 deepHashCode。并且当数组长度为1时，Arrays.hashCode(object) = object.hashCode 不一定成立 31 不论是String、Arrays在计算多个元素的哈希值的时候，都会有31这个数字。主要有以下两个原因： 31是一个不大不小的质数，是作为 hashCode 乘子的优选质数之一。 另外一些相近的质数，比如37、41、43等等，也都是不错的选择。那么为啥偏偏选中了31呢？请看第二个原因。 31可以被 JVM 优化，31 * i = (i 。 上面两个原因中，第一个需要解释一下，第二个比较简单，就不说了。一般在设计哈希算法时，会选择一个特殊的质数。至于为啥选择质数，我想应该是可以降低哈希算法的冲突率。 equals 判定两个对象是否相等。equals和hashCode需要同时被overwrite clone 创建一个该对象的副本，并且对于对象 x 应当满足以下表达式： x.clone() != x x.clone().getClass() == x.getClass() x.clone().equals(x) toString wait 当前线程等待知道其他线程调用该对象的 notify 或者 notifyAll方法。当前线程必须拥有该对象的 monitor。线程释放该对象monitor的拥有权，并且等待到别的线程通知等待在该对象monitor上的线程苏醒。然后线程重新拥有monitor并继续执行。在某些jdk版本中，中断和虚假唤醒是存在的，所以wait方法需要放在循环中。 synchronized (obj) { while () obj.wait(); ... // Perform action appropriate to condition } 该方法只能被拥有该对象monitor的线程调用。 虚假唤醒（spurious wakeup） 虚假唤醒就是一些obj.wait()会在除了obj.notify()和obj.notifyAll()的其他情况被唤醒，而此时是不应该唤醒的。 注意 Lock 的 Conditon.await 也有虚假唤醒的问题 解决的办法是基于while来反复判断进入正常操作的临界条件是否满足 同时也可以使用同步数据结构：BlokingQueue 解释 虚假唤醒（spurious wakeup）是一个表象，即在多处理器的系统下发出 wait 的程序有可能在没有 notify 唤醒的情形下苏醒继续执行。 以运行在 Linux 的 hotspot 虚拟机上的 java 程序为例， wait 方法在 jvm 执行时实质是调用了底层 pthread_cond_wait/pthread_cond_timedwait 函数，挂起等待条件变量来达到线程间同步通信的效果，而底层 wait 函数在设计之初为了不减慢条件变量操作的效率并没有去保证每次唤醒都是由 notify 触发，而是把这个任务交由上层应用去实现，即使用者需要定义一个循环去判断是否条件真能满足程序继续运行的需求，当然这样的实现也可以避免因为设计缺陷导致程序异常唤醒的问题。 notify 唤醒一个等待在该对象monitor上的线程。如果有多个线程等待，则会随机选择一个线程唤醒。线程等待是通过调用wait方法。 唤醒的线程不会立即执行，直到当前线程放弃对象上的锁。唤醒的线程也会以通常的方式和竞争该对象锁的线程进行竞争。也就是说，唤醒的线程在对该对象的加锁中没有任何优先级。 该方法只能被拥有该对象monitor的线程调用。线程拥有monitor有下面三种方式： 执行该对象的 synchronized 方法 执行以该对象作为同步语句的synchronized方法体 对于class对象，可以执行该对象的static synchronized方法 在同一时间只能有一个线程能够拥有该对象monitor finalize 当 GC 认为该对象已经没有任何引用的时候，该方法被GC收集器调用。子类可以 overwrite 该方法来关闭系统资源或者其他清理任务。 finalize 的一般契约是，如果 Java 虚拟机确定不再有任何方法可以通过任何尚未死亡的线程访问此对象，除非由于某个操作，它将被调用通过最终确定准备完成的其他一些对象或类来完成。 finalize 方法可以采取任何操作，包括使该对象再次可用于其他线程；但是，finalize 的通常目的是在对象被不可撤销地丢弃之前执行清理操作。例如，表示输入/输出连接的对象的 finalize 方法可能会执行显式 I/O 事务，以在永久丢弃对象之前断开连接。 类 Object 的 finalize 方法不执行任何特殊操作;它只是正常返回。 Object 的子类可以覆盖此定义。 Java 编程语言不保证哪个线程将为任何给定对象调用 finalize 方法。但是，可以保证，调用 finalize 时，调用 finalize 的线程不会持有任何用户可见的同步锁。如果 finalize 方法抛出未捕获的异常，则忽略该异常并终止该对象的终止。在为对象调用 finalize 方法之后，在 Java 虚拟机再次确定不再有任何方法可以通过任何尚未死亡的线程访问此对象之前，不会采取进一步操作，包括可能的操作通过准备完成的其他对象或类，此时可以丢弃该对象。 对于任何给定对象，Java 虚拟机永远不会多次调用 finalize 方法。 finalize 方法抛出的任何异常都会导致暂停此对象的终结，但会被忽略。 缺陷 一些与 finalize 相关的方法，由于一些致命的缺陷，已经被废弃了，如 System.runFinalizersOnExit() 方法、Runtime.runFinalizersOnExit()方法。 System.gc() 与 System.runFinalization() 方法增加了finalize方法执行的机会，但不可盲目依赖它们。 Java 语言规范并不保证 finalize 方法会被及时地执行、而且根本不会保证它们会被执行。 finalize 方法可能会带来性能问题。因为JVM通常在单独的低优先级线程中完成finalize的执行。 对象再生问题： finalize 方法中，可将待回收对象赋值给GC Roots可达的对象引用，从而达到对象再生的目的。 finalize 方法至多由GC执行一次(用户当然可以手动调用对象的 finalize 方法，但并不影响GC对 finalize 的行为)。 "},"zother2-interview/java/oop/":{"url":"zother2-interview/java/oop/","title":"面向对象基础","keywords":"","body":"面向对象基础 面向对象三要素：封装、继承、多态 封装：封装的意义，在于明确标识出允许外部使用的所有成员函数和数据项，或者叫接口。 继承： 继承基类的方法，并做出自己的扩展； 声明某个子类兼容于某基类（或者说，接口上完全兼容于基类），外部调用者可无需关注其差别（内部机制会自动把请求派发dispatch到合适的逻辑）。 多态：基于对象所属类的不同，外部对同一个方法的调用，实际执行的逻辑不同。很显然，多态实际上是依附于继承的第二种含义的。 多态 方法签名：方法名 + 参数列表(参数类型、个数、顺序) 重写 子类重写父类方法，只有实例方法可以被重写，重写后的方法必须仍为实例方法。成员变量和静态方法都不能被重写，只能被隐藏。 重写实例方法：超类Parent中有实例方法A，子类child定义了与A 相同签名和子集返回类型 的实例方法B，子类对象ChildObj只能调用自己的实例方法B。 方法的重写（override）两同两小一大原则： 方法名相同，参数类型相同 子类返回类型小于等于父类方法返回类型 子类抛出异常小于等于父类方法抛出异常 子类访问权限大于等于父类方法访问权限 注意： 不能重写static静态方法。(形式上可以写，但本质上不是重写，属于下面要讲的隐藏) 重写方法可以改变其它的方法修饰符，如final,synchronized,native。不管被重写方法中有无final修饰的参数，重写方法都可以增加、保留、去掉这个参数的 final 修饰符(参数修饰符不属于方法签名)。 重载 在同一个类中，有多个方法名相同，参数列表不同（参数个数不同，参数类型不同），与方法的返回值无关，与权限修饰符无关。编译器通过对方法签名的识别即可静态编译出不同的方法。这也是java中重载与重写的区别之一。 重载只是一种语言特性，与多态无关，与面向对象也无关。多态是为了实现接口重用。 Java中方法是可以和类名同名的，和构造方法唯一的区别就是，构造方法没有返回值。 隐藏 隐藏与覆盖在形式上极其类似(语法规则)，但有着本质的区别：只有成员变量(不管是不是静态)和静态方法可以被隐藏。 成员变量 超类 Parent 中有成员变量 A ，子类 Child 定义了与 A 同名的成员变量 B ，子类对象 ChildObj 调用的是自己的成员变量 B。如果把子类对象 ChildObj 转换为超类对象 ParentObj ，ParentObj 调用的是超类的成员变量 A ！ 隐藏成员变量时，只要同名即可，可以更改变量类型(无论基本类型还是隐藏类型) 不能隐藏超类中的 private 成员变量，换句话说，只能隐藏可以访问的成员变量。 隐藏超类成员变量 A 时，可以降低或提高子类成员变量B的访问权限，只要A不是 private。 隐藏成员变量与是否静态无关！静态变量可以隐藏实例变量，实例变量也可以隐藏静态变量。 可以隐藏超类中的final成员变量。 静态方法 超类 Parent 有静态方法 A ，子类 Child 定义了与 A 相同签名和子集返回类型 的静态方法 B ，子类对象 ChildObj 调用的是自己的静态方法 B 。如果把子类对象 ChildObj 转换为超类对象 ParentObj ，ParentObj 调用的是超类的静态方法 A ！ 隐藏后的方法必须仍为静态方法 "},"zother2-interview/java/operator/":{"url":"zother2-interview/java/operator/","title":"运算符","keywords":"","body":"运算符优先级 优先级从上到下依次递减，最上面具有最高的优先级，逗号操作符具有最低的优先级。 相同优先级中，按结合顺序计算。大多数运算是从左至右计算，只有三个优先级是从右至左结合的，它们是单目运算符、条件运算符、赋值运算符。 基本的优先级需要记住： 指针最优，单目运算优于双目运算。如正负号。 先乘除（模），后加减。 先算术运算，后移位运算，最后位运算。请特别注意：1 等价于 (1 . 逻辑运算最后计算。 优先级表 运算符 结合性 [ ] . ( ) (方法调用) 从左向右 ! ~ ++ -- +(一元运算) -(一元运算) 从右向左 * / % 从左向右 + -　 从左向右 > >>> 从左向右 >= instanceof 从左向右 == != 从左向右 & 从左向右 ^ 从左向右 ` ` 从左向右 && 从左向右 ` ` 从左向右 ?: 从右向左 `= += -= *= /= %= &= = ^= >= >>=` 从右向左 , 从左到右 无符号右移运算符 >>>，无符号右移的规则只记住一点：忽略了符号位扩展，0 补最高位。无符号右移规则和右移运算是一样的，只是填充时不管左边的数字是正是负都用 0 来填充，无符号右移运算只针对负数计算，因为对于正数来说这种运算没有意义。无符号右移运算符 >>> 只是对 32 位和 64 位的值有意义 "},"zother2-interview/java/proxy/":{"url":"zother2-interview/java/proxy/","title":"Java 代理","keywords":"","body":"代理 Java 代理 我们常说的代理分为静态代理和动态代理。 静态代理：代码中显式指定代理 动态代理：类比静态代理，可以发现，代理类不需要实现原接口了，而是实现InvocationHandler。 静态代理 因为需要对一些函数进行二次处理，或是某些函数不让外界知道时，可以使用代理模式，通过访问第三方，间接访问原函数的方式，达到以上目的。 弊端 如果要想为多个类进行代理，则需要建立多个代理类，维护难度加大。 仔细想想，为什么静态代理会有这些问题，是因为代理在编译期就已经决定，如果代理发生在运行期，这些问题解决起来就比较简单，所以动态代理的存在就很有必要了。 动态代理 当动态生成的代理类调用方法时，会触发 invoke 方法，在 invoke 方法中可以对被代理类的方法进行增强。 // 1. 首先实现一个InvocationHandler，方法调用会被转发到该类的invoke()方法。 class LogInvocationHandler implements InvocationHandler{ ... private Hello hello; public LogInvocationHandler(Hello hello) { this.hello = hello; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { if(\"sayHello\".equals(method.getName())) { logger.info(\"You said: \" + Arrays.toString(args)); } return method.invoke(hello, args); } } // 2. 然后在需要使用Hello的时候，通过JDK动态代理获取Hello的代理对象。 Hello hello = (Hello)Proxy.newProxyInstance( getClass().getClassLoader(), // 1. 类加载器 new Class[] {Hello.class}, // 2. 代理需要实现的接口，可以有多个 new LogInvocationHandler(new HelloImp()));// 3. 方法调用的实际处理者 System.out.println(hello.sayHello(\"I love you!\")); 通过动态代理可以很明显的看到它的好处，在使用静态代理时，如果不同接口的某些类想使用代理模式来实现相同的功能，将要实现多个代理类，但在动态代理中，只需要一个代理类就好了。 除了省去了编写代理类的工作量，动态代理实现了可以在原始类和接口还未知的时候，就确定代理类的代理行为，当代理类与原始类脱离直接联系后，就可以很灵活地重用于不同的应用场景中。 继承了Proxy类，实现了代理的接口，由于java不能多继承，这里已经继承了Proxy类了，不能再继承其他的类，所以JDK的动态代理不支持对实现类的代理，只支持接口的代理。 提供了一个使用InvocationHandler作为参数的构造方法。 生成静态代码块来初始化接口中方法的Method对象，以及Object类的equals、hashCode、toString方法 弊端 代理类和委托类需要都实现同一个接口。也就是说只有实现了某个接口的类可以使用Java动态代理机制。但是，事实上使用中并不是遇到的所有类都会给你实现一个接口。因此，对于没有实现接口的类，就不能使用该机制。 动态代理与静态代理的区别 Proxy类的代码被固定下来，不会因为业务的逐渐庞大而庞大； 代理对象是在程序运行时产生的，而不是编译期； 可以实现AOP编程，这是静态代理无法实现的； 解耦，如果用在web业务下，可以实现数据层和业务层的分离。 动态代理的优势就是实现无侵入式的代码扩展。 静态代理这个模式本身有个大问题，如果类方法数量越来越多的时候，代理类的代码量是十分庞大的。所以引入动态代理来解决此类问题 CGLib cglib 是针对类来实现代理的，他的 原理是对指定的目标类生成一个子类，并覆盖其中方法实现增强，但因为采用的是继承，所以不能对 final 修饰的类进行代理。同样的，final 方法是不能重载的，所以也不能通过CGLIB代理，遇到这种情况不会抛异常，而是会跳过 final 方法只代理其他方法。 CGLIB 代理主要通过对字节码的操作，为对象引入间接级别，以控制对象的访问。 CGLIB 底层使用了ASM（一个短小精悍的字节码操作框架）来操作字节码生成新的类。 CGLIB和Java动态代理的区别 Java 动态代理只能够对接口进行代理，不能对普通的类进行代理（因为所有生成的代理类的父类为 Proxy，Java 类继承机制不允许多重继承）；CGLIB能够代理普通类； Java 动态代理使用 Java 原生的反射 API 进行操作，在生成类上比较高效；CGLIB 使用 ASM 框架直接对字节码进行操作，在类的执行过程中比较高效 "},"zother2-interview/java/question/":{"url":"zother2-interview/java/question/","title":"Java面试题","keywords":"","body":"面向对象的特征有哪些方面？ 抽象：抽象是将一类对象的共同特征总结出来构造类的过程，包括数据抽象和行为抽象两方面。抽象只关注对象有哪些属性和行为，并不关注这些行为的细节是什么。 继承：继承是从已有类得到继承信息创建新类的过程。继承让变化中的软件系统有了一定的延续性，同时继承也是封装程序中可变因素的重要手段。 封装：通常认为封装是把数据和操作数据的方法绑定起来，对数据的访问只能通过已定义的接口。面向对象的本质就是将现实世界描绘成一系列完全自治、封闭的对象。 多态：多态性是指允许不同子类型的对象对同一消息作出不同的响应。 访问修饰符 public 、 private 、 protected 以及不写时的区别？ 类的成员不写访问修饰时默认为 package。默认对于同一个包中的其他类相当于公开（public），对于不是同一个包中的其他类相当于私有（private）。受保护（protected）对子类相当于公开，对不是同一包中的没有父子关系的类相当于私有。Java中，外部类的修饰符只能是 public 或默认，类的成员（包括内部类）的修饰符可以是以上四种。 String是最基本的数据类型吗？ 不是，Java中的基本数据类型只有8个：byte、short、int、long、float、double、char、boolean；除了基本类型（primitive type），剩下的都是引用类型（reference type），Java 5以后引入的枚举类型也算是一种比较特殊的引用类型。 float f=3.4;是否正确？ 不正确。3.4是双精度数，将双精度型（double）赋值给浮点型（float）属于下转型会造成精度损失，因此需要强制类型转换 float f =(float)3.4; 或者写成 float f =3.4F;。 short s1 = 1; s1 = s1 + 1;有错吗? short s1 = 1; s1 += 1; 有错吗？ 对于 short s1 = 1; s1 = s1 + 1; 由于 1 是 int 类型，因此 s1+1 运算结果也是int 型，需要强制转换类型才能赋值给 short 型，编译不通过。 而 short s1 = 1; s1 += 1; 可以正确编译，因为 s1+= 1; 相当于 s1 = (short)(s1 + 1); 其中有隐含的强制类型转换。 Java有没有goto？ goto 是Java中的保留字，但在目前版本的Java中没有使用。 int 和 Integer 有什么区别？ Java为每一个基本数据类型都引入了对应的包装类型（wrapper class）， int 的包装类就是 Integer ，从 Java 5 开始引入了自动装箱/拆箱机制，使得二者可以相互转换。 public static void main(String[] args) { Integer f1 = 100, f2 = 100, f3 = 150, f4 = 150; System.out.println(f1 == f2);//true System.out.println(f3 == f4);//false } 当我们给一个 Integer 对象赋一个 int 值的时候，会调用 Integer 类的静态方法 valueOf ，如果看看 valueOf 的源代码就知道发生了什么。 public static Integer valueOf(int i) { if (i >= IntegerCache.low && i 如果整型（byte、short、int、long）字面量的值在 -128 到 127 之间，那么不会 new 新的 Integer 对象，而是直接引用常量池中的 Integer 对象。 & 和 && 的区别？ & 运算符有两种用法：按位与；逻辑与。&& 运算符是短路与运算。 逻辑与跟短路与的差别是非常巨大的，虽然二者都要求运算符左右两端的布尔值都是 true 整个表达式的值才是 true 。 && 之所以称为短路运算是因为，如果 && 左边的表达式的值是 false ，右边的表达式会被直接短路掉，不会进行运算。 逻辑或运算符 | 和短路或运算符 || 的差别也是如此。 Math.round(11.5) 等于多少？Math.round(-11.5) 等于多少？ Math.round(11.5) 的返回值是 12 ，Math.round(-11.5) 的返回值是 -11。四舍五入的原理是在参数上加 0.5 然后进行下取整。 switch是否能作用在 byte 上，是否能作用在 long 上，是否能作用在 String 上？ 在 Java 5 以前， switch(expr) 中， expr 只能是 byte 、short、char、int；从 Java 5 开始，Java 中引入了枚举类型，expr 也可以是 enum 类型；从 Java 7 开始， expr 还可以是字符串 String ，但是长整型 long 在目前所有的版本中都是不可以的。 用最有效率的方法计算 2 乘以 8 ？ 2 （左移3位相当于乘以2的3次方，右移3位相当于除以2的3次方）。 数组有没有 length() 方法？ String 有没有 length() 方法？ 数组没有 length() 方法，有length的属性。String有 length() 方法。 在Java中，如何跳出当前的多重嵌套循环？ 在最外层循环前加一个标记如A，然后用break A;可以跳出多重循环。 构造器（constructor）是否可被重写（override）？ 构造器不能被继承，因此不能被重写，但可以被重载。 两个对象 x.equals(y) == true，但却可有不同的 hash code ？ 不对，如果两个对象 x 和 y 满足 x.equals(y) == true，它们的哈希码（hash code）应当相同 重载（Overload）和重写（Override）的区别。重载的方法能否根据返回类型进行区分？ 方法的重载和重写都是实现多态的方式，区别在于 重载是编译时的多态性，而 重写是运行时的多态性。 重载发生在一个类中，同名的方法如果有不同的参数列表（参数类型不同、参数个数不同或者二者都不同）则视为重载。 重写发生在子类与父类之间，重写要求子类被重写方法与父类被重写方法有相同的返回类型（或子类型），不能比父类被重写方法声明更多的异常（里氏代换原则）。 char 型变量中能不能存贮一个中文汉字，为什么？ char 类型可以存储一个中文汉字，因为 Java 中使用的编码是 Unicode （不选择任何特定的编码，直接使用字符在字符集中的编号，这是统一的唯一方法），一个 char 类型占2个字节，所以放一个中文是没问题的。 抽象类（abstract class）和接口（interface）有什么异同？ 抽象类和接口都不能够实例化，但可以定义抽象类和接口类型的引用。 一个类如果继承了某个抽象类或者实现了某个接口都需要对其中的抽象方法全部进行实现，否则该类仍然需要被声明为抽象类。 接口比抽象类更加抽象，因为抽象类中可以定义构造器，可以有抽象方法和具体方法，而接口中不能定义构造器而且其中的方法全部都是抽象方法。 抽象类中的成员可以是 private、默认、 protected 、public的，而接口中的成员全都是public的 抽象类中可以定义成员变量，而接口中定义的成员变量实际上都是常量 有抽象方法的类必须被声明为抽象类，而抽象类未必要有抽象方法 静态内部类（Static Nested Class）和内部类（Inner Class）的不同 Static Nested Class是被声明为静态（static）的内部类，它可以不依赖于外部类实例被实例化。而通常的 内部类需要在外部类实例化后才能实例化：new Outer().new Inner(); 抽象的（abstract）方法是否可同时是静态的（static），是否可同时是本地方法（native），是否可同时被 synchronized 修饰？ 都不能。抽象方法需要子类重写，而静态的方法是无法被重写的，因此二者是矛盾的。 本地方法是由本地代码（如C代码）实现的方法，而抽象方法是没有实现的，也是矛盾的。 synchronized 和方法的实现细节有关，抽象方法不涉及实现细节，因此也是相互矛盾的。 阐述静态变量和实例变量的区别 静态变量是被static修饰符修饰的变量，也称为类变量，它属于类，不属于类的任何一个对象，一个类不管创建多少个对象，静态变量在内存中有且仅有一个拷贝；实例变量必须依存于某一实例，需要先创建对象然后通过对象才能访问到它。静态变量可以实现让多个对象共享内存。 如何实现对象克隆？ 有两种方式： 实现 Cloneable 接口并重写 Object 类中的 clone() 方法； 实现 Serializable 接口，通过对象的序列化和反序列化实现克隆，可以实现真正的深度克隆。 String s = new String(\"xyz\");创建了几个字符串对象？ 两个对象，一个是常量池的 \"xyz\" ，一个是用 new 创建在堆上的对象。 接口是否可继承（extends）接口？抽象类是否可实现（implements）接口？抽象类是否可继承具体类（concrete class）？ 接口可以继承接口，而且支持多重继承。抽象类可以实现(implements)接口，抽象类可继承具体类也可以继承抽象类。 Error和Exception有什么区别？ Error表示系统级的错误和程序不必处理的异常，是恢复不是不可能但很困难的情况下的一种严重问题；比如内存溢出，不可能指望程序能处理这样的情况；Exception表示需要捕捉或者需要程序进行处理的异常，是一种设计或实现问题；也就是说，它表示如果程序运行正常，从不会发生的情况。 运行时异常与受检异常有何异同？ 运行时异常表示虚拟机的通常操作中可能遇到的异常，是一种常见运行错误，只要程序设计得没有问题通常就不会发生。 受检异常跟程序运行的上下文环境有关，即使程序设计无误，仍然可能因使用的问题而引发。 try{} 里有一个 return 语句，那么紧跟在这个 try 后的 finally{} 里的代码会不会被执行？如果 finally{} 里也进行了 return 最终返回的哪个值？ finally 会执行，在方法返回调用者前执行。如果 finally{} 里也进行了 return 最终返回 finally 里的 return List、Set、Map是否继承自Collection接口？ List、Set 是，Map 不是。Map是键值对映射容器，与List和Set有明显的区别 Thread类的 sleep() 方法和对象的 wait() 方法都可以让线程暂停执行，它们有什么区别？ sleep()方法是线程类（Thread）的静态方法，调用此方法会让当前线程暂停执行指定的时间，将执行机会（CPU）让给其他线程，但是对象的锁依然保持，因此休眠时间结束后会自动恢复。 wait()是Object类的方法，调用对象的 wait() 方法导致当前线程放弃对象的锁（线程暂停执行），进入对象的等待池（wait pool），只有调用对象的 notify() 方法（或 notifyAll() 方法）时才能唤醒等待池中的线程进入等锁池（lock pool），如果线程重新获得对象的锁就可以进入就绪状态。 线程的 sleep() 方法和 yield() 方法有什么区别？ sleep()方法给其他线程运行机会时不考虑线程的优先级，因此会给低优先级的线程以运行的机会；yield()方法只会给相同优先级或更高优先级的线程以运行的机会； 线程执行sleep()方法后转入阻塞（blocked）状态，而执行yield()方法后转入就绪（ready）状态； sleep()方法声明抛出InterruptedException，而yield()方法没有声明任何异常； sleep()方法比 yield() 方法（跟操作系统CPU调度相关）具有更好的可移植性。 "},"zother2-interview/java/serilaser/":{"url":"zother2-interview/java/serilaser/","title":"序列化","keywords":"","body":"序列化 ProtoBuffer Protocol Buffers 是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。它很适合做数据存储或 RPC 数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。 Protobuf 的优点 Protobuf 更小、更快、也更简单。你可以定义自己的数据结构，然后使用代码生成器生成的代码来读写这个数据结构。你甚至可以在无需重新部署程序的情况下更新数据结构。只需使用 Protobuf 对数据结构进行一次描述，即可利用各种不同语言或从各种不同数据流中对你的结构化数据轻松读写。 “向后”兼容性好，人们不必破坏已部署的、依靠“老”数据格式的程序就可以对数据结构进行升级。这样您的程序就可以不必担心因为消息结构的改变而造成的大规模的代码重构或者迁移的问题。因为添加新的消息中的 field 并不会引起已经发布的程序的任何改变。 Protobuf 语义更清晰，无需类似 XML 解析器的东西（因为 Protobuf 编译器会将 .proto 文件编译生成对应的数据访问类以对 Protobuf 数据进行序列化、反序列化操作）。 Protobuf 的编程模式比较友好，简单易学，同时它拥有良好的文档和示例，对于喜欢简单事物的人们而言，Protobuf 比其他的技术更加有吸引力。 Protobuf 的不足 由于文本并不适合用来描述数据结构，所以 Protobuf 也不适合用来对基于文本的标记文档（如 HTML）建模。另外，由于 XML 具有某种程度上的自解释性，它可以被人直接读取编辑，在这一点上 Protobuf 不行，它以二进制的方式存储，除非你有 .proto 定义，否则你没法直接读出 Protobuf 的任何内容。 "},"zother2-interview/java/string-builder/":{"url":"zother2-interview/java/string-builder/","title":"String Builder","keywords":"","body":"StringBuilder StringBuilder类也封装了一个字符数组，定义如下： char[] value; 与String不同，它不是final的，可以修改。另外，与String不同，字符数组中不一定所有位置都已经被使用，它有一个实例变量，表示数组中已经使用的字符个数，定义如下： int count; StringBuilder继承自AbstractStringBuilder，它的默认构造方法是： public StringBuilder() { super(16); } 调用父类的构造方法，父类对应的构造方法是： AbstractStringBuilder(int capacity) { value = new char[capacity]; } 也就是说，new StringBuilder()这句代码，内部会创建一个长度为16的字符数组，count的默认值为0。 append的实现 public AbstractStringBuilder append(String str) { if (str == null) str = \"null\"; int len = str.length(); ensureCapacityInternal(count + len); str.getChars(0, len, value, count); count += len; return this; } append会直接拷贝字符到内部的字符数组中，如果字符数组长度不够，会进行扩展，实际使用的长度用count体现。具体来说，ensureCapacityInternal(count+len)会确保数组的长度足以容纳新添加的字符，str.getChars会拷贝新添加的字符到字符数组中，count+=len会增加实际使用的长度。 ensureCapacityInternal的代码如下： private void ensureCapacityInternal(int minimumCapacity) { if (minimumCapacity - value.length > 0) expandCapacity(minimumCapacity); } 如果字符数组的长度小于需要的长度，则调用expandCapacity进行扩展，expandCapacity的代码是： void expandCapacity(int minimumCapacity) { int newCapacity = value.length * 2 + 2; if (newCapacity - minimumCapacity 扩展的逻辑是，分配一个足够长度的新数组，然后将原内容拷贝到这个新数组中，最后让内部的字符数组指向这个新数组，这个逻辑主要靠下面这句代码实现： value = Arrays.copyOf(value, newCapacity); toString实现 字符串构建完后，我们来看toString代码： public String toString() { return new String(value, 0, count); } "},"zother2-interview/leetcode/addTwoNumbers/":{"url":"zother2-interview/leetcode/addTwoNumbers/","title":"两数相加","keywords":"","body":"两数相加 头条重点 题目 给出两个 非空 的链表用来表示两个非负的整数。其中，它们各自的位数是按照 逆序 的方式存储的，并且它们的每个节点只能存储 一位 数字。 如果，我们将这两个数相加起来，则会返回一个新的链表来表示它们的和。 您可以假设除了数字 0 之外，这两个数都不会以 0 开头。 示例： 输入：(2 -> 4 -> 3) + (5 -> 6 -> 4) 输出：7 -> 0 -> 8 原因：342 + 465 = 807 解题思路 public ListNode addTwoNumbers(ListNode l1, ListNode l2) { if (l1 == null || l2 == null) { return null; } StringBuilder builder1 = new StringBuilder(); while (l1 != null) { builder1.append(l1.val); l1 = l1.next; } StringBuilder builder2 = new StringBuilder(); while (l2 != null) { builder2.append(l2.val); l2 = l2.next; } BigDecimal bigDecimal1 = new BigDecimal(builder1.reverse().toString()); BigDecimal bigDecimal2 = new BigDecimal(builder2.reverse().toString()); String resStr = bigDecimal1.add(bigDecimal2).toPlainString(); ListNode head = new ListNode(Integer.parseInt(String.valueOf(resStr.charAt(resStr.length() - 1)))); ListNode cur = head; for (int i = resStr.length() - 2; i >= 0; i--) { cur.next = new ListNode(Integer.parseInt(String.valueOf(resStr.charAt(i)))); cur = cur.next; } return head; } "},"zother2-interview/leetcode/AllOne/":{"url":"zother2-interview/leetcode/AllOne/","title":"全 O(1) 的数据结构","keywords":"","body":"全 O(1) 的数据结构 题目 实现一个数据结构支持以下操作： Inc(key) - 插入一个新的值为 1 的 key。或者使一个存在的 key 增加一，保证 key 不为空字符串。 Dec(key) - 如果这个 key 的值是 1，那么把他从数据结构中移除掉。否者使一个存在的 key 值减一。如果这个 key 不存在，这个函数不做任何事情。key 保证不为空字符串。 GetMaxKey() - 返回 key 中值最大的任意一个。如果没有元素存在，返回一个空字符串\"\"。 GetMinKey() - 返回 key 中值最小的任意一个。如果没有元素存在，返回一个空字符串\"\"。 挑战：以 O(1) 的时间复杂度实现所有操作。 解题思路 设计一个 Bucket 保存所有值为 value 的 key 并且有临近 value 的 Bucket 指针 class AllOne { /** Initialize your data structure here. */ public AllOne() { } private static class Bucket { private int value; private Set keys = new HashSet<>(); private Bucket next; private Bucket pre; public Bucket(int value) { this.value = value; } @Override public String toString() { return \"Bucket{\" + \"value=\" + value + \", keys=\" + keys + '}'; } } private Map data = new HashMap<>(); private List bucketList = new ArrayList<>(); /** * Inserts a new key with value 1. Or increments an existing key by 1. */ public void inc(String key) { if (data.containsKey(key)) { Bucket bucket = data.get(key); bucket.keys.remove(key); if (bucket.next == null) { bucket.next = new Bucket(bucket.value + 1); bucket.next.pre = bucket; bucketList.add(bucket.next); } bucket.next.keys.add(key); data.put(key, bucket.next); } else { if (bucketList.size() == 0) { bucketList.add(new Bucket(1)); } Bucket bucket = bucketList.get(0); bucket.keys.add(key); data.put(key, bucket); } } /** * Decrements an existing key by 1. If Key's value is 1, remove it from the data structure. */ public void dec(String key) { if (!data.containsKey(key)) { return; } Bucket bucket = data.get(key); if (bucket.pre == null) { bucket.keys.remove(key); data.remove(key); } else { bucket.keys.remove(key); bucket.pre.keys.add(key); data.put(key, bucket.pre); } } /** * Returns one of the keys with maximal value. */ public String getMaxKey() { if (bucketList.size() == 0) { return \"\"; } for (int i = bucketList.size() - 1; i >= 0; i--) { Bucket bucket = bucketList.get(i); if (!bucket.keys.isEmpty()) { Iterator iterator = bucket.keys.iterator(); if (iterator.hasNext()) { return iterator.next(); } else { return \"\"; } } } return \"\"; } /** * Returns one of the keys with Minimal value. */ public String getMinKey() { if (bucketList.size() == 0) { return \"\"; } for (Bucket bucket : bucketList) { if (!bucket.keys.isEmpty()) { Iterator iterator = bucket.keys.iterator(); if (iterator.hasNext()) { return iterator.next(); } else { return \"\"; } } } return \"\"; } } "},"zother2-interview/leetcode/checkInclusion/":{"url":"zother2-interview/leetcode/checkInclusion/","title":"字符串的排列","keywords":"","body":"字符串的排列 题目 给定两个字符串 s1 和 s2，写一个函数来判断 s2 是否包含 s1 的排列。 换句话说，第一个字符串的排列之一是第二个字符串的子串。 示例1: 输入: s1 = \"ab\" s2 = \"eidbaooo\" 输出: True 解释: s2 包含 s1 的排列之一 (\"ba\"). 解题思路 这道题，我们用到的算法是 滑动窗口 首先字符串s1的排列的可能性应该是它的长度的阶乘，因为字符串长度可能为10000，所以找出所有排列情况是不太可能。 我们可以转换思路，不要关注排列的形式，而是关注排列中元素的数量关系 比如 aab，那么，转换为数量关系就是{a:2,b:1}，因为 S1 长度为 3，所以我们的窗口长度也为3 如果我们在 S2 的找到了这样一个窗口符合出现 a 的次数是两个， b 是一个，那么 S2 就是包含 S1 的排列的 public boolean checkInclusion(String s1, String s2) { int len1 = s1.length(); int len2 = s2.length(); int[] c1 = new int[26]; int[] c2 = new int[26]; for (char c : s1.toCharArray()) { c1[c - 'a']++; } for (int i = 0; i = len1) --c2[s2.charAt(i - len1) - 'a'];//先把坐标查过的减掉 c2[s2.charAt(i) - 'a']++; if (Arrays.equals(c1, c2)) return true; } return false; } "},"zother2-interview/leetcode/detectCycle/":{"url":"zother2-interview/leetcode/detectCycle/","title":"环形链表 II","keywords":"","body":"环形链表 II 题目 给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。 为了表示给定链表中的环，我们使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。 如果 pos 是 -1，则在该链表中没有环。 解题思路 首先通过快慢指针确定链表是否有环 再使用一个指针从头节点与快慢指针相遇节点同步长前进，最终找到环的入口 public ListNode detectCycle(ListNode head) { ListNode fast = head, slow = head; ListNode meetNode = null; while (fast != null && fast.next != null) { fast = fast.next.next; slow = slow.next; if (fast == slow) { meetNode = fast; break; } } if (meetNode == null) { return meetNode; } while (head != meetNode) { head = head.next; if (head == meetNode) { break; } meetNode = meetNode.next; } return meetNode; } "},"zother2-interview/leetcode/findCircleNum/":{"url":"zother2-interview/leetcode/findCircleNum/","title":"朋友圈","keywords":"","body":"朋友圈 头条重点 题目 班上有 N 名学生。其中有些人是朋友，有些则不是。他们的友谊具有是传递性。如果已知 A 是 B 的朋友，B 是 C 的朋友，那么我们可以认为 A 也是 C 的朋友。所谓的朋友圈，是指所有朋友的集合。 给定一个 N * N 的矩阵 M，表示班级中学生之间的朋友关系。如果M[i][j] = 1，表示已知第 i 个和 j 个学生互为朋友关系，否则为不知道。你必须输出所有学生中的已知的朋友圈总数。 示例 1: 输入: [[1,1,0], [1,1,0], [0,0,1]] 输出: 2 说明：已知学生0和学生1互为朋友，他们在一个朋友圈。 第2个学生自己在一个朋友圈。所以返回2。 示例 2: 输入: [[1,1,0], [1,1,1], [0,1,1]] 输出: 1 说明：已知学生0和学生1互为朋友，学生1和学生2互为朋友，所以学生0和学生2也是朋友，所以他们三个在一个朋友圈，返回1。 注意： N 在[1,200]的范围内。 对于所有学生，有M[i][i] = 1。 如果有M[i][j] = 1，则有M[j][i] = 1。 解题思路 逐个遍历所有学生，将他所有朋友标记 public int findCircleNum(int[][] M) { if (M.length == 0) { return 0; } int[] marks = new int[M.length]; int total = 0; for (int i = 0; i "},"zother2-interview/leetcode/findKthLargest/":{"url":"zother2-interview/leetcode/findKthLargest/","title":"数组中的第K个最大元素","keywords":"","body":"数组中的第K个最大元素 题目 在未排序的数组中找到第 k 个最大的元素。请注意，你需要找的是数组排序后的第 k 个最大的元素，而不是第 k 个不同的元素。 示例 1: 输入: [3,2,1,5,6,4] 和 k = 2 输出: 5 示例 2: 输入: [3,2,3,1,2,4,5,5,6] 和 k = 4 输出: 4 解题思路 利用快排的思想，当排序到 k 后，停止排序，输出结果 public static int findKthLargest(int[] nums, int k) { fastSort(nums, 0, nums.length - 1); return nums[nums.length - k]; } public static void fastSort(int[] nums, int start, int end) { if (nums.length end) { return; } if (end nums.length - 1 || start > nums.length - 1) { return; } int left = start, right = end; int keyIndex = (left + right) / 2; while (left keyIndex && nums[right] > nums[keyIndex]) { right--; } if (right > keyIndex) { swap(nums, keyIndex, right); keyIndex = right; } while (left "},"zother2-interview/leetcode/findLengthOfLCIS/":{"url":"zother2-interview/leetcode/findLengthOfLCIS/","title":"Index","keywords":"","body":" title: 最长连续递增序列 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 题目 给定一个未经排序的整数数组，找到最长且连续的的递增序列。 示例 1: 输入: [1,3,5,4,7] 输出: 3 解释: 最长连续递增序列是 [1,3,5], 长度为3。 尽管 [1,3,5,7] 也是升序的子序列, 但它不是连续的，因为5和7在原数组里被4隔开。 示例 2: 输入: [2,2,2,2,2] 输出: 1 解释: 最长连续递增序列是 [2], 长度为1。 解题思路 用两个变量记录序列开始和结束的下标 从左到右遍历，如果下一个节点小当前节点则移动 start，否则移动end，并更新 max public static int findLengthOfLCIS(int[] nums) { if (nums.length == 0) { return 0; } if (nums.length == 1) { return 1; } int start = 0, end = 0; int max = 1; for (int i = 1; i nums[i - 1]) { end = i; max = Math.max(max, end - start + 1); } else { start = i; } } return max; } "},"zother2-interview/leetcode/getIntersectionNode/":{"url":"zother2-interview/leetcode/getIntersectionNode/","title":"Index","keywords":"","body":" title: 相交链表 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 题目 编写一个程序，找到两个单链表相交的起始节点。 解题思路 首先将两个链表中长的一个向前遍历，直到两个链表长度一致 两个链表同时向前遍历，便可找到交点 public ListNode getIntersectionNode(ListNode headA, ListNode headB) { if (headA == null || headB == null) { return null; } if (headA == headB) { return headA; } int lenA = 1; int lenB = 1; ListNode temp = headA; while (temp.next != null) { temp = temp.next; lenA++; } ListNode tailA = temp; temp = headB; while (temp.next != null) { temp = temp.next; lenB++; } ListNode tailB = temp; if (tailB != tailA) { return null; } if (lenA > lenB) { for (int i = 0; i "},"zother2-interview/leetcode/lengthOfLongestSubstring/":{"url":"zother2-interview/leetcode/lengthOfLongestSubstring/","title":"Index","keywords":"","body":" title: 无重复字符的最长子串 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 头条重点 题目 给定一个字符串，请你找出其中不含有重复字符的 最长子串 的长度。 输入: \"abcabcbb\" 输出: 3 解释: 因为无重复字符的最长子串是 \"abc\"，所以其长度为 3。 解题思路 用 Map 记录字符所在位置，当遇到重复字符时，移动 start 指针 替换 Map 中下标，并计算子串长度 public int lengthOfLongestSubstring(String str) { if (str == null || str.length() == 0) return 0; HashMap temp = new HashMap<>(); char[] chars = str.toCharArray(); int res = 0, start = 0; for (int i = 0; i "},"zother2-interview/leetcode/longestCommonPrefix/":{"url":"zother2-interview/leetcode/longestCommonPrefix/","title":"Index","keywords":"","body":" title: 最长公共前缀 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 题目 编写一个函数来查找字符串数组中的最长公共前缀。 如果不存在公共前缀，返回空字符串 \"\"。 示例 1: 输入: [\"flower\",\"flow\",\"flight\"] 输出: \"fl\" 解题思路 找到最短字符串 多个字符串逐个字符比较 public String longestCommonPrefix(String[] strs) { if (strs.length == 0) { return \"\"; } int minLen = strs[0].length(); for (String str : strs) { minLen = Math.min(minLen, str.length()); } char[][] data = new char[strs.length][minLen]; for (int i = 0; i "},"zother2-interview/leetcode/longestConsecutive/":{"url":"zother2-interview/leetcode/longestConsecutive/","title":"Index","keywords":"","body":" title: 最长连续序列 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 题目 给定一个未排序的整数数组，找出最长连续序列的长度。 要求算法的时间复杂度为 O(n)。 示例: 输入: [100, 4, 200, 1, 3, 2] 输出: 4 解释: 最长连续序列是 [1, 2, 3, 4]。它的长度为 4。 解题思路 用 Set 保存所有数字 遍历数组，查找当前数字之前、之后的数，并计算个数 public static int longestConsecutive(int[] nums) { if (nums.length set = new HashSet<>(); for (int num : nums) { set.add(num); } int pre, after, max = 0; for (int num : nums) { int temp = 1; set.remove(num); pre = num; after = num; while (set.contains(--pre)) { temp++; set.remove(pre); } while (set.contains(++after)) { temp++; set.remove(after); } max = Math.max(max, temp); if (max > nums.length / 2) { return max; } } return max; } "},"zother2-interview/leetcode/lowestCommonAncestor/":{"url":"zother2-interview/leetcode/lowestCommonAncestor/","title":"Index","keywords":"","body":" title: 二叉树的最近公共祖先 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 题目 给定一个二叉树, 找到该树中两个指定节点的最近公共祖先。 百度百科中最近公共祖先的定义为：“对于有根树 T 的两个结点 p、q，最近公共祖先表示为一个结点 x，满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。” 示例 1: 输入: root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 1 输出: 3 解释: 节点 5 和节点 1 的最近公共祖先是节点 3。 解题思路 通过 DFS 找到节点的路径 从头开始遍历两个节点的路径，找到最后一个相等的节点 public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { LinkedList pathP = new LinkedList<>(); LinkedList pathQ = new LinkedList<>(); findNodePath(pathP, root, p); findNodePath(pathQ, root, q); TreeNode last = null; while (!pathP.isEmpty() && !pathQ.isEmpty()) { TreeNode pi = pathP.pollFirst(); TreeNode qi = pathQ.pollFirst(); if (qi==pi) { last = pi; }else break; } return last; } private void findNodePath(LinkedList path, TreeNode root, TreeNode target) { if (root == null) { return; } if (!path.isEmpty() && path.getLast().val == target.val) { return; } path.addLast(root); findNodePath(path, root.left, target); if (!path.isEmpty() && path.getLast().val == target.val) { return; } findNodePath(path, root.right, target); if (!path.isEmpty() && path.getLast().val != target.val) { path.removeLast(); } } "},"zother2-interview/leetcode/LRUCache/":{"url":"zother2-interview/leetcode/LRUCache/","title":"Index","keywords":"","body":" title: 缓存机制 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 头条重点 题目 运用你所掌握的数据结构，设计和实现一个 LRU (最近最少使用) 缓存机制。它应该支持以下操作： 获取数据 get 和 写入数据 put 。 获取数据 get(key) - 如果密钥 (key) 存在于缓存中，则获取密钥的值（总是正数），否则返回 -1。 写入数据 put(key, value) - 如果密钥不存在，则写入其数据值。当缓存容量达到上限时，它应该在写入新数据之前删除最近最少使用的数据值，从而为新的数据值留出空间。 进阶: 你是否可以在 O(1) 时间复杂度内完成这两种操作？ 示例: LRUCache cache = new LRUCache( 2 /* 缓存容量 */ ); cache.put(1, 1); cache.put(2, 2); cache.get(1); // 返回 1 cache.put(3, 3); // 该操作会使得密钥 2 作废 cache.get(2); // 返回 -1 (未找到) cache.put(4, 4); // 该操作会使得密钥 1 作废 cache.get(1); // 返回 -1 (未找到) cache.get(3); // 返回 3 cache.get(4); // 返回 4 解题思路 class LRUCache extends LinkedHashMap{ private final int capacity; public LRUCache(int capacity) { super(capacity*, 0.75f, true); this.capacity = capacity; } public int get(int key) { Integer integer = super.get(key); return integer == null ? -1 : integer; } public void put(int key, int value) { super.put(key, value); } @Override protected boolean removeEldestEntry(Map.Entry eldest) { return size() > this.capacity; } } "},"zother2-interview/leetcode/maxAreaOfIsland/":{"url":"zother2-interview/leetcode/maxAreaOfIsland/","title":"Index","keywords":"","body":" title: 岛屿的最大面积 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 头条重点 题目 给定一个包含了一些 0 和 1的非空二维数组 grid , 一个 岛屿 是由四个方向 (水平或垂直) 的 1 (代表土地) 构成的组合。你可以假设二维矩阵的四个边缘都被水包围着。 找到给定的二维数组中最大的岛屿面积。(如果没有岛屿，则返回面积为0。) 示例 1: [[0,0,1,0,0,0,0,1,0,0,0,0,0], [0,0,0,0,0,0,0,1,1,1,0,0,0], [0,1,1,0,1,0,0,0,0,0,0,0,0], [0,1,0,0,1,1,0,0,1,0,1,0,0], [0,1,0,0,1,1,0,0,1,1,1,0,0], [0,0,0,0,0,0,0,0,0,0,1,0,0], [0,0,0,0,0,0,0,1,1,1,0,0,0], [0,0,0,0,0,0,0,1,1,0,0,0,0]] 对于上面这个给定矩阵应返回 6。注意答案不应该是11，因为岛屿只能包含水平或垂直的四个方向的‘1’。 示例 2: [[0,0,0,0,0,0,0,0]] 对于上面这个给定的矩阵, 返回 0。 注意: 给定的矩阵 grid 的长度和宽度都不超过 50。 解题思路 通过循环遍历，找到 1 再通过递归遍历该 1 临近的所有 1，并计算总面积 private static int[][] steps = new int[][]{{1, 0}, {0, 1}, {-1, 0}, {0, -1}}; /** * 上学时做过，属于图的 DFS * @param grid * @return */ public static int maxAreaOfIsland(int[][] grid) { if (grid.length == 0) { return 0; } int[][] marks = new int[grid.length][grid[0].length]; for (int[] mark : marks) { Arrays.fill(mark, 0); } Wrapper maxArea = new Wrapper<>(0); for (int i = 0; i (0), maxArea); } } return maxArea.v; } private static void p(int[][] grid, int[][] mark, int i, int j, Wrapper curArea, Wrapper maxArea) { if (i = grid.length || j >= grid[0].length) { return; } if (grid[i][j] == 1 && mark[i][j] == 0) { curArea.v++; maxArea.v = Math.max(maxArea.v, curArea.v); } else { return; } for (int[] step : steps) { mark[i][j] = 1; p(grid, mark, i + step[0], j + step[1], curArea, maxArea); // mark[i][j] = 0; } } private static final class Wrapper { V v; public Wrapper(V v) { this.v = v; } } "},"zother2-interview/leetcode/maxProfit/":{"url":"zother2-interview/leetcode/maxProfit/","title":"Index","keywords":"","body":" title: 买卖股票的最佳时机 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 头条重点 题目 给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。如果你最多只允许完成一笔交易（即买入和卖出一支股票），设计一个算法来计算你所能获取的最大利润。 注意你不能在买入股票前卖出股票。 示例 1: 输入: [7,1,5,3,6,4] 输出: 5 解释: 在第 2 天（股票价格 = 1）的时候买入，在第 5 天（股票价格 = 6）的时候卖出，最大利润 = 6-1 = 5 。 注意利润不能是 7-1 = 6, 因为卖出价格需要大于买入价格。 示例 2: 输入: [7,6,4,3,1] 输出: 0 解释: 在这种情况下, 没有交易完成, 所以最大利润为 0。 解题思路 要先买入才能卖出，先找最低价格点 再找最低价格之后的最高价格，用 res 表示最大利润 public int maxProfit(int[] prices) { if (prices.length "},"zother2-interview/leetcode/maxProfit2/":{"url":"zother2-interview/leetcode/maxProfit2/","title":"Index","keywords":"","body":" title: 买卖股票的最佳时机 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 头条重点 题目 给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。设计一个算法来计算你所能获取的最大利润。你可以尽可能地完成更多的交易（多次买卖一支股票）。 注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 示例 1: 输入: [7,1,5,3,6,4] 输出: 7 解释: 在第 2 天（股票价格 = 1）的时候买入，在第 3 天（股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4 。 随后，在第 4 天（股票价格 = 3）的时候买入，在第 5 天（股票价格 = 6）的时候卖出, 这笔交易所能获得利润 = 6-3 = 3 。 示例 2: 输入: [1,2,3,4,5] 输出: 4 解释: 在第 1 天（股票价格 = 1）的时候买入，在第 5 天 （股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4 。 注意你不能在第 1 天和第 2 天接连购买股票，之后再将它们卖出。 因为这样属于同时参与了多笔交易，你必须在再次购买前出售掉之前的股票。 示例 3: 输入: [7,6,4,3,1] 输出: 0 解释: 在这种情况下, 没有交易完成, 所以最大利润为 0。 解题思路 贪心算法，尽可能的多进行交易 public int maxProfit(int[] prices) { if (prices.length 0) { res += profit; } } return res; } "},"zother2-interview/leetcode/maxSubArray/":{"url":"zother2-interview/leetcode/maxSubArray/","title":"Index","keywords":"","body":" title: 最大子序和 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 头条重点 题目 给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。 示例: 输入: [-2,1,-3,4,-1,2,1,-5,4], 输出: 6 解释: 连续子数组 [4,-1,2,1] 的和最大，为 6。 进阶: 如果你已经实现复杂度为 O(n) 的解法，尝试使用更为精妙的分治法求解。 解题思路 动态规划：f(i)=\\begin{cases}num[i]&f(i-1)+num[i]num[i]\\end{cases} 用result[i]保存以数字nums[i]结尾的最大子序和，然后不断更新result数组的最大值即可。总的时间复杂度O(n) public int maxSubArray(int[] nums) { if (nums.length == 0) { return 0; } if (nums.length == 1) { return nums[0]; } int[] res = new int[nums.length]; res[0] = nums[0]; int max = res[0]; for (int i = 1; i nums[i]) { res[i] = curMax; } else { res[i] = nums[i]; } max = Math.max(max, res[i]); } return max; } "},"zother2-interview/leetcode/mergeKLists/":{"url":"zother2-interview/leetcode/mergeKLists/","title":"Index","keywords":"","body":" title: 合并 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 头条重点 题目 合并 k 个排序链表，返回合并后的排序链表。请分析和描述算法的复杂度。 示例: 输入: [ 1->4->5, 1->3->4, 2->6 ] 输出: 1->1->2->3->4->4->5->6 解题思路 通过小根堆，将所有元素放入小根堆 从小根堆依次取出数据 public ListNode mergeKLists(ListNode[] lists) { if (lists == null) { return null; } Queue set = new PriorityQueue<>(Comparator.comparingInt(o -> o.val)); for (ListNode node : lists) { while (node != null) { set.add(node); node = node.next; } } ListNode head = new ListNode(-1); ListNode res = head; ListNode cur; while ((cur = set.poll()) != null) { head.next = cur; head = head.next; } head.next = null; return res.next; } "},"zother2-interview/leetcode/mergeRagen/":{"url":"zother2-interview/leetcode/mergeRagen/","title":"Index","keywords":"","body":" title: 合并区间 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 题目 给出一个区间的集合，请合并所有重叠的区间。 示例 1: 输入: [[1,3],[2,6],[8,10],[15,18]] 输出: [[1,6],[8,10],[15,18]] 解释: 区间 [1,3] 和 [2,6] 重叠, 将它们合并为 [1,6]. 示例 2: 输入: [[1,4],[4,5]] 输出: [[1,5]] 解释: 区间 [1,4] 和 [4,5] 可被视为重叠区间。 解题思路 将区间按起始地址排序 遍历所有区间，如果 Last 与当前区间没有重合，则将当前区间加入结果集合。 如果重合，并且 last.end ，修改 Last 的边界 public List merge(List intervals) { if (intervals.size() o.start)); List res = new ArrayList<>(); res.add(intervals.get(0)); for (int i = 1; i = t.start) { if (last.end "},"zother2-interview/leetcode/mergeTwoLists/":{"url":"zother2-interview/leetcode/mergeTwoLists/","title":"Index","keywords":"","body":" title: 合并两个有序链表 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 题目 将两个有序链表合并为一个新的有序链表并返回。新链表是通过拼接给定的两个链表的所有节点组成的。 示例： 输入：1->2->4, 1->3->4 输出：1->1->2->3->4->4 解题思路 public ListNode mergeTwoLists(ListNode l1, ListNode l2) { if (l1 == null && l2 == null) { return null; } if (l1 == null) { return l2; } if (l2 == null) { return l1; } ListNode head; if (l1.val > l2.val) { head = l2; l2 = l2.next; } else { head = l1; l1 = l1.next; } ListNode res = head; while (true) { ListNode cur; if (l1 == null && l2 == null) { break; } if (l1 == null) { cur = l2; l2 = l2.next; } else if (l2 == null) { cur = l1; l1 = l1.next; } else if (l1.val > l2.val) { cur = l2; l2 = l2.next; } else { cur = l1; l1 = l1.next; } head.next = cur; head = head.next; } return res; } "},"zother2-interview/leetcode/MinStack/":{"url":"zother2-interview/leetcode/MinStack/","title":"Index","keywords":"","body":" title: 最小栈 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 头条重点 题目 设计一个支持 push，pop，top 操作，并能在常数时间内检索到最小元素的栈。 push(x) -- 将元素 x 推入栈中。 pop() -- 删除栈顶的元素。 top() -- 获取栈顶元素。 getMin() -- 检索栈中的最小元素。 示例: MinStack minStack = new MinStack(); minStack.push(-2); minStack.push(0); minStack.push(-3); minStack.getMin(); --> 返回 -3. minStack.pop(); minStack.top(); --> 返回 0. minStack.getMin(); --> 返回 -2. 解题思路 class MinStack { /** initialize your data structure here. */ public MinStack() { } private LinkedList stack = new LinkedList<>(); private Queue minStack = new PriorityQueue<>(); public void push(int x) { stack.offerFirst(x); minStack.offer(x); } public void pop() { Integer poll = stack.pollFirst(); if (poll != null) { minStack.remove(poll); } } public int top() { Integer first = stack.peekFirst(); return first == null ? 0 : first; } public int getMin() { Integer first = minStack.peek(); return first == null ? 0 : first; } } "},"zother2-interview/leetcode/mySqrt/":{"url":"zother2-interview/leetcode/mySqrt/","title":"Index","keywords":"","body":" title: 的平方根 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 头条重点 题目 实现 int sqrt(int x) 函数。 计算并返回 x 的平方根，其中 x 是非负整数。 由于返回类型是整数，结果只保留整数的部分，小数部分将被舍去。 示例 1: 输入: 4 输出: 2 示例 2: 输入: 8 输出: 2 说明: 8 的平方根是 2.82842...,由于返回类型是整数，小数部分将被舍去。 解题思路 牛顿迭代法：a{i}=(x/a{i-1}+a_{i-1})/2 public int mySqrt(int x) { double a = 1, diff = 0; do { a = (x / a + a) / 2.0; diff = Math.abs(a * a - x); } while (diff > 0.1); return (int) a; } "},"zother2-interview/leetcode/restoreIpAddresses/":{"url":"zother2-interview/leetcode/restoreIpAddresses/","title":"Index","keywords":"","body":" title: 复原 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 头条重点 题目 给定一个只包含数字的字符串，复原它并返回所有可能的 IP 地址格式。 示例: 输入: \"25525511135\" 输出: [\"255.255.11.135\", \"255.255.111.35\"] 解题思路 利用回溯法，遍历所有可能的 IP public static List restoreIpAddresses(String s) { if (s.length() > 12 || s.length() res = new ArrayList<>(); ArrayList ip = new ArrayList<>(); for (int i = 0; i res, char[] chars, int startIndex, List ip, int segmentIndex) { StringBuilder builder = new StringBuilder(); for (int i = startIndex; i 255) { return; } if (ipStr.length() > 1 && ipStr.startsWith(\"0\")) { return; } ip.set(segmentIndex, ipStr); if (segmentIndex == 3 && i == chars.length - 1) { res.add(String.join(\".\", ip)); } if (segmentIndex "},"zother2-interview/leetcode/reverseList/":{"url":"zother2-interview/leetcode/reverseList/","title":"Index","keywords":"","body":" title: 反转链表 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 头条重点 题目 反转一个单链表。 示例: 输入: 1->2->3->4->5->NULL 输出: 5->4->3->2->1->NULL 解题思路 三个指针进行反转 public ListNode reverseList(ListNode head) { if (head == null) { return head; } if (head.next == null) { return head; } ListNode pre = head; ListNode cur = head.next; while (cur != null) { ListNode next = cur.next; cur.next = pre; pre = cur; cur = next; } head.next = null; return pre; } "},"zother2-interview/leetcode/reverseWords/":{"url":"zother2-interview/leetcode/reverseWords/","title":"Index","keywords":"","body":" title: 翻转字符串里的单词 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 题目 给定一个字符串，逐个翻转字符串中的每个单词。 示例 1： 输入: \"the sky is blue\" 输出: \"blue is sky the\" 说明： 无空格字符构成一个单词。 输入字符串可以在前面或者后面包含多余的空格，但是反转后的字符不能包括。 如果两个单词间有多余的空格，将反转后单词间的空格减少到只含一个。 解题思路 按空格拆分字符串为字符串数组 t 逆序遍历字符串数组 t，并组成新的字符串 public String reverseWords(String s) { String trimed = s.trim(); String[] split = trimed.split(\" \"); StringBuilder builder = new StringBuilder(); for (int i = split.length - 1; i >= 0; i--) { String t = split[i]; if (t.trim().isEmpty()) { continue; } builder.append(t).append(\" \"); } return builder.toString().trim(); } "},"zother2-interview/leetcode/salary/":{"url":"zother2-interview/leetcode/salary/","title":"Index","keywords":"","body":" title: 第二高的薪水 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 头条重点 题目 编写一个 SQL 查询，获取 Employee 表中第二高的薪水（Salary） 。 +----+--------+ | Id | Salary | +----+--------+ | 1 | 100 | | 2 | 200 | | 3 | 300 | +----+--------+ 例如上述 Employee 表，SQL查询应该返回 200 作为第二高的薪水。如果不存在第二高的薪水，那么查询应返回 null。 +---------------------+ | SecondHighestSalary | +---------------------+ | 200 | +---------------------+ 解题思路 子查询 select IFNULL((select Distinct Salary from Employee order by Salary DESC limit 1,1),null) as SecondHighestSalary "},"zother2-interview/leetcode/searchRote/":{"url":"zother2-interview/leetcode/searchRote/","title":"Index","keywords":"","body":" title: 搜索旋转排序数组 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 头条重点 题目 假设按照升序排序的数组在预先未知的某个点上进行了旋转。( 例如，数组 [0,1,2,4,5,6,7] 可能变为 [4,5,6,7,0,1,2] )。 搜索一个给定的目标值，如果数组中存在这个目标值，则返回它的索引，否则返回 -1 。 你可以假设数组中不存在重复的元素。 你的算法时间复杂度必须是 O(log n) 级别。 示例 1: 输入: nums = [4,5,6,7,0,1,2], target = 0 输出: 4 示例 2: 输入: nums = [4,5,6,7,0,1,2], target = 3 输出: -1 解题思路 旋转数组是分为两段有序，主要得注意 mid 落在哪个段上 public static int search(int[] nums, int target) { int start = 0, end = nums.length - 1; while (start = nums[start]) { if (target = nums[start]) { end = mid - 1; } else { start = mid + 1; } } if (nums[mid] nums[mid] && target "},"zother2-interview/leetcode/simplifyPath/":{"url":"zother2-interview/leetcode/simplifyPath/","title":"Index","keywords":"","body":" title: 简化路径 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 头条重点 题目 以 Unix 风格给出一个文件的绝对路径，你需要简化它。或者换句话说，将其转换为规范路径。 在 Unix 风格的文件系统中，一个点（.）表示当前目录本身；此外，两个点 （..） 表示将目录切换到上一级（指向父目录）；两者都可以是复杂相对路径的组成部分。 请注意，返回的规范路径必须始终以斜杠 / 开头，并且两个目录名之间必须只有一个斜杠 /。最后一个目录名（如果存在）不能以 / 结尾。此外，规范路径必须是表示绝对路径的最短字符串。 示例 1： 输入：\"/home/\" 输出：\"/home\" 解释：注意，最后一个目录名后面没有斜杠。 示例 2： 输入：\"/../\" 输出：\"/\" 解释：从根目录向上一级是不可行的，因为根是你可以到达的最高级。 示例 3： 输入：\"/home//foo/\" 输出：\"/home/foo\" 解释：在规范路径中，多个连续斜杠需要用一个斜杠替换。 示例 4： 输入：\"/a/./b/../../c/\" 输出：\"/c\" 示例 5： 输入：\"/a/../../b/../c//.//\" 输出：\"/c\" 示例 6： 输入：\"/a//b////c/d//././/..\" 输出：\"/a/b/c\" 解题思路 利用栈的特性，将有效路径名压入 当遇到 .. 时 pop 栈 最后按顺序 pop 组成最终路径 public static String simplifyPath(String path) { ArrayDeque stack = new ArrayDeque<>(); String[] split = path.split(\"/\"); for (String s : split) { if (s.isEmpty()) { continue; } switch (s) { case \"..\": stack.pollLast(); break; case \".\": break; default: stack.offerLast(s); } } StringBuilder builder = new StringBuilder(\"/\"); for (String s : stack) { builder.append(s); builder.append(\"/\"); } if (builder.length() > 1) { builder.deleteCharAt(builder.length() - 1); } return builder.toString(); } "},"zother2-interview/leetcode/sortList/":{"url":"zother2-interview/leetcode/sortList/","title":"Index","keywords":"","body":" title: 排序链表 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 头条重点 题目 在 O(n log n) 时间复杂度和常数级空间复杂度下，对链表进行排序。 示例 1: 输入: 4->2->1->3 输出: 1->2->3->4 示例 2: 输入: -1->5->3->4->0 输出: -1->0->3->4->5 解题思路 通过快慢指针将链表拆分 递归进行拆分，再通过合并两个排序链表的方式进行合并 类似于归并排序 public ListNode sortList(ListNode head) { if (head == null || head.next == null) { return head; } ListNode slow = head, fast = head; while (fast.next != null && fast.next.next != null) { fast = fast.next.next; slow = slow.next; } ListNode mid = slow.next; slow.next = null; ListNode l1 = sortList(head); ListNode l2 = sortList(mid); return merge(l1, l2); } private ListNode merge(ListNode l1, ListNode l2) { if (l1 == null) { return l2; } if (l2 == null) { return l1; } ListNode head,res; if (l1.val > l2.val) { head = l2; l2 = l2.next; } else { head = l1; l1 = l1.next; } res = head; // head.next = null; while (l1 != null || l2 != null) { if (l1 == null) { head.next = l2; l2 = l2.next; } else if (l2 == null) { head.next = l1; l1 = l1.next; } else { if (l1.val > l2.val) { head.next = l2; l2 = l2.next; } else { head.next = l1; l1 = l1.next; } } head = head.next; } return res; } "},"zother2-interview/leetcode/StringMultiply/":{"url":"zother2-interview/leetcode/StringMultiply/","title":"Index","keywords":"","body":" title: 字符串相乘 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 题目 给定两个以字符串形式表示的非负整数 num1 和 num2，返回 num1 和 num2 的乘积，它们的乘积也表示为字符串形式。 示例 1: 输入: num1 = \"2\", num2 = \"3\" 输出: \"6\" num1 和 num2 的长度小于110。 num1 和 num2 只包含数字 0-9。 num1 和 num2 均不以零开头，除非是数字 0 本身。 不能使用任何标准库的大数类型（比如 BigInteger）或直接将输入转换为整数来处理。 解题思路 对于字符串 num2 中的每一位数与字符串 num1 相乘所得的结果，不再分开计算最后相加，而是先全部累加，最后再考虑进位的影响。 对于最终结果的第i + j位数，可以由 num1 数组的第 i 位数和 num2 数组的第 j 位数组成。 public String multiply(String num1, String num2) { if (num1.length() == 0 || num2.length() == 0) { return ZEO; } if (num1.equals(ZEO) || num2.equals(ZEO)) { return ZEO; } if (num1.equals(ONE)) { return num2; } if (num2.equals(ONE)) { return num1; } int[] num = new int[num1.length() + num2.length() - 1]; Arrays.fill(num, 0); for (int i = 0; i = 0; i--) { int t = num[i] + addIn; addIn = t / 10; res.append(t % 10); } if (addIn > 0) { res.append(addIn); } return res.reverse().toString(); } "},"zother2-interview/leetcode/threeSum/":{"url":"zother2-interview/leetcode/threeSum/","title":"Index","keywords":"","body":" title: 三数之和 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 头条重点 题目 给定一个包含 n 个整数的数组 nums ，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？找出所有满足条件且不重复的三元组。 注意：答案中不可以包含重复的三元组。 例如, 给定数组 nums = [-1, 0, 1, 2, -1, -4]， 满足要求的三元组集合为： [ [-1, 0, 1], [-1, -1, 2] ] 解题思路 将数组排序 固定一位数，然后通过两个指针对撞，寻找总和为 0 的三个数 public static List> threeSum(int[] nums) { if (nums.length > res = new HashSet<>(); Arrays.sort(nums); int zCount = 0; for (int num : nums) { if (num == 0) { zCount++; } } for (int i = 0; i list = new ArrayList<>(); list.add(first); list.add(nums[j]); list.add(nums[k]); res.add(list); j++; k--; } else if (t > 0) { k--; } else { j++; } } } if (zCount >= 3) { List list = new ArrayList<>(); list.add(0); list.add(0); list.add(0); res.add(list); } return new ArrayList<>(res); } "},"zother2-interview/leetcode/trap/":{"url":"zother2-interview/leetcode/trap/","title":"Index","keywords":"","body":" title: 接雨水 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 头条重点 题目 给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。 上面是由数组 [0,1,0,2,1,0,1,3,2,1,2,1] 表示的高度图，在这种情况下，可以接 6 个单位的雨水（蓝色部分表示雨水）。 感谢 Marcos 贡献此图。 示例: 输入: [0,1,0,2,1,0,1,3,2,1,2,1] 输出: 6 解题思路 首先找到最高点，然后从首尾向中间遍历，找到局部高点，然后就可以计算总量 public int trap(int[] height) { if (height.length max) { max = height[i]; maxIndex = i; } } int total = 0; int topIndex = 0;//局部最高点 for (int i = 0; i maxIndex; i--) { if (height[topIndex] "},"zother2-interview/leetcode/validUtf8/":{"url":"zother2-interview/leetcode/validUtf8/","title":"Index","keywords":"","body":" title: 编码验证 date: 2019-08-21T11:00:41+08:00 draft: false categories: leetcode 头条重点 题目 UTF-8 中的一个字符可能的长度为 1 到 4 字节，遵循以下的规则： 对于 1 字节的字符，字节的第一位设为0，后面7位为这个符号的unicode码。 对于 n 字节的字符 (n > 1)，第一个字节的前 n 位都设为1，第 n+1 位设为0，后面字节的前两位一律设为10。剩下的没有提及的二进制位，全部为这个符号的unicode码。 这是 UTF-8 编码的工作方式： Char. number range | UTF-8 octet sequence (hexadecimal) | (binary) --------------------+--------------------------------------------- 0000 0000-0000 007F | 0xxxxxxx 0000 0080-0000 07FF | 110xxxxx 10xxxxxx 0000 0800-0000 FFFF | 1110xxxx 10xxxxxx 10xxxxxx 0001 0000-0010 FFFF | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx 给定一个表示数据的整数数组，返回它是否为有效的 utf-8 编码。 注意：输入是整数数组。只有每个整数的最低 8 个有效位用来存储数据。这意味着每个整数只表示 1 字节的数据。 示例 1: data = [197, 130, 1], 表示 8 位的序列: 11000101 10000010 00000001. 返回 true 。 这是有效的 utf-8 编码，为一个2字节字符，跟着一个1字节字符。 解题思路 class Solution { public boolean validUtf8(int[] data) { int totalByteCount = 0; for (int item : data) { if (totalByteCount == 0) { totalByteCount = totalByteCount(item); if (totalByteCount == -1) { return false; } totalByteCount--; continue; } //10xxxxxx检查 if ((item & 0xC0) != 0x80) { return false; } totalByteCount--; } return totalByteCount == 0; } private int totalByteCount(int i) { if ((i & 0x80) == 0) { return 1; } if ((i & 0xE0) == 0xC0) { return 2; } if ((i & 0xF0) == 0xE0) { return 3; } if ((i & 0xF8) == 0xF0) { return 4; } return -1; } } "},"zother2-interview/leetcode/zigzagLevelOrder/":{"url":"zother2-interview/leetcode/zigzagLevelOrder/","title":"Index","keywords":"","body":""},"zother2-interview/offer/Add/":{"url":"zother2-interview/offer/Add/","title":"Index","keywords":"","body":" title: 不用加减乘除做加法 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 写一个函数，求两个整数之和，要求在函数体内不得使用+、-、*、/四则运算符号。 解题思路 将加法分解成两步 两个数不计算进位相加得到 sum，计算进位 carry 再将进位加上：sum = sum + carry 直到没有进位为止 public int Add(int num1, int num2) { int sum, carry; do { sum = num1 ^ num2; carry = (num1 & num2) "},"zother2-interview/offer/BST-Link-Convert/":{"url":"zother2-interview/offer/BST-Link-Convert/","title":"Index","keywords":"","body":" title: 二叉搜索树与双向链表 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。 解题思路 由于 BST 的特性，采用中序遍历正好符合排序 要考虑 root 节点要与 左节点的最大值连接，与右节点的最小值连接 增加一个已排序链表的指针，指向最后一个已排序节点 public TreeNode Convert(TreeNode pRootOfTree) { if (pRootOfTree == null) { return null; } TreeNode[] nodeList = {new TreeNode(-1)}; ConvertToLink(pRootOfTree, nodeList); TreeNode cursor = pRootOfTree; while (cursor.left != null) { cursor = cursor.left; } cursor.right.left = null; return cursor.right; } private void ConvertToLink(TreeNode root, TreeNode[] nodeList) { if (root == null) { return; } ConvertToLink(root.left, nodeList); root.left = nodeList[0]; nodeList[0].right = root; nodeList[0] = root; ConvertToLink(root.right, nodeList); } "},"zother2-interview/offer/BSTKthNode/":{"url":"zother2-interview/offer/BSTKthNode/","title":"Index","keywords":"","body":" title: 二叉搜索树的第 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 给定一棵二叉搜索树，请找出其中的第 k 小的结点。例如，5，3，7，2，4，6，8 中，按结点数值大小顺序第三小结点的值为4。 牛客网 解题思路 BST 中序遍历的结果就是排序后的结果 public TreeNode KthNode(TreeNode pRoot, int k) { TreeNode[] nodes = new TreeNode[1]; int[] ints = {0}; KthNode(pRoot, k, nodes, ints); return nodes[0]; } private void KthNode(TreeNode root, int k, TreeNode[] res, int[] cursor) { if (root == null) return; if (res[0] != null) return; KthNode(root.left, k, res, cursor); cursor[0]++; if (cursor[0] == k) { res[0] = root; return; } KthNode(root.right, k, res, cursor); } "},"zother2-interview/offer/CloneLink/":{"url":"zother2-interview/offer/CloneLink/","title":"Index","keywords":"","body":" title: 复杂链表的复制 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的 head 。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空） 解题思路 复制每个节点，如：复制节点 A 得到 A1 ，将 A1 插入节点 A 后面 遍历链表，并将 A1->random = A->random->next; 将链表拆分成原链表和复制后的链表 public RandomListNode Clone(RandomListNode pHead) { if (pHead == null) { return null; } RandomListNode cursor = pHead; while (cursor != null) { RandomListNode copyNode = new RandomListNode(cursor.label); RandomListNode nextNode = cursor.next; cursor.next = copyNode; copyNode.next = nextNode; cursor = nextNode; } cursor = pHead; while (cursor != null) { RandomListNode copyNode = cursor.next; if (cursor.random == null) { cursor = copyNode.next; continue; } copyNode.random = cursor.random.next; cursor = copyNode.next; } RandomListNode copyHead = pHead.next; cursor = pHead; while (cursor.next != null) { RandomListNode copyNode = cursor.next; cursor.next = copyNode.next; cursor = copyNode; } return copyHead; } "},"zother2-interview/offer/CountOfSortedArray/":{"url":"zother2-interview/offer/CountOfSortedArray/","title":"Index","keywords":"","body":" title: 在排序数组中查找数字 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 统计一个数字在排序数组中出现的次数。 解题思路 通过二分查找分别找到 n 的第一个位置和最后一个位置 再进行计算就可以得出结果 public int countOfSortedArray2(int[] nums, int n) { if (nums == null || nums.length == 0) return 0; int firstN = getFirstN(nums, n); int lastN = getLastN(nums, n); return lastN - firstN + 1; } private int getFirstN(int[] nums, int n) { int s = 0, e = nums.length - 1; int mid = -1; while (s 0 && nums[mid - 1] == n) { e = mid - 1; continue; } if (nums[mid] > n) { e = mid - 1; continue; } if (nums[mid] n) { e = mid - 1; continue; } if (nums[mid] "},"zother2-interview/offer/CutRope/":{"url":"zother2-interview/offer/CutRope/","title":"Index","keywords":"","body":" title: 剪绳子 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 给定一根长度为n的绳子，请把绳子剪成m段（m、n都是整数，n>1并且m>1），每段绳子的长度记为k[0],k[1],…,k[m]。请问k[0]* k[1] * … *k[m]可能的最大乘积是多少？ 解题思路 尽可能剪长度为 3 的绳子 当长度剩下的为 4 时，不能再减去 3，而是 2*2 public int cutRope(int n) { if (n "},"zother2-interview/offer/Duplicate/":{"url":"zother2-interview/offer/Duplicate/","title":"Index","keywords":"","body":" title: 数组中重复的数字 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 在一个长度为n的数组里的所有数字都在0到 n-1 的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。 解题思路 解法一 由于数组内数字在 0 ~ n-1 的范围内，可以将数组按 数字做下标 进行重排序 将 n 放置到 num[n] 上，交换之前再判定在 num[n] 上是否为相同数字 public boolean duplicate(int numbers[], int length, int[] duplication) { if (numbers == null || numbers.length == 0) return false; for (int i = 0; i 解法二 把数字 1 ~ n 划分为 1 ~ m、m+1 ~ n，统计两个子数组中每个数字在 1~n 出现的次数 如果出现的次数大于 m，那么重复数字一定在 1 ~ m 中 继续这样进行划分，可以找到重复数组 "},"zother2-interview/offer/EntryNodeOfLoop/":{"url":"zother2-interview/offer/EntryNodeOfLoop/","title":"Index","keywords":"","body":" title: 链表中环的入口结点 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 给一个链表，若其中包含环，请找出该链表的环的入口结点，否则，输出null。 解题思路 首先通过 快慢指针（快：每次走两步；慢：每次走一步）确定是否有环 当有环时，再从头节点出发，与快指针按 相同速度 向前移动，当 cursor = fast 则找到环入口 public ListNode EntryNodeOfLoop(ListNode pHead) { if (pHead == null || pHead.next == null) return null; ListNode fast = pHead, slow = pHead; while (fast.next != null) { slow = slow.next; fast = fast.next.next; if (fast == slow) break; } if (fast != slow) return null; ListNode cursor = pHead; while (cursor != fast) { cursor = cursor.next; fast = fast.next; } return cursor; } "},"zother2-interview/offer/fibonacci/":{"url":"zother2-interview/offer/fibonacci/","title":"Index","keywords":"","body":" title: 斐波纳切数列 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项（从0开始，第0项为0）。n 解题思路 递归计算很慢，是最简单的算法 public int Fibonacci(int n) { if (n == 0) { return 0; } if (n == 1) { return 1; } int l = 1, ll = 0; for (int i = 2; i "},"zother2-interview/offer/find-minimum-in-rotated-sorted-array/":{"url":"zother2-interview/offer/find-minimum-in-rotated-sorted-array/","title":"Index","keywords":"","body":" title: 旋转数组的最小数字 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 解题思路 旋转之后的数组存在两个上升序列，最小元素在两个上升序列的中间 用两个指针在两个序列中找到最大和最小的值，这样 end 指向的数则为最小 public int minNumberInRotateArray(int[] array) { if (array.length == 0) { return 0; } int start = 0, end = array.length - 1; while (end - start != 1) { int mid = (start + end) / 2; if (array[mid] >= array[start]) { start = mid; } if (array[mid] "},"zother2-interview/offer/FindContinuousSequence/":{"url":"zother2-interview/offer/FindContinuousSequence/","title":"Index","keywords":"","body":" title: 和为 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 输出所有和为S的连续正数序列。序列内按照从小至大的顺序，序列间按照开始数字从小到大的顺序 解题思路 与上一个题目类似，需要确定的是序列的最大值，不超过 sum 使用窗口模式，两个指针定义一个窗口，和为 t public ArrayList> FindContinuousSequence(int sum) { ArrayList> res = new ArrayList<>(); if (sum == 1) { return res; } int start = 1, end = 2; int t = start + end; while (start ints = new ArrayList<>(); for (int i = start; i sum) { t -= start; start++; } else { if (end >= sum) break; end++; t += end; } } return res; } "},"zother2-interview/offer/FindFirstCommonNode/":{"url":"zother2-interview/offer/FindFirstCommonNode/","title":"Index","keywords":"","body":" title: 两个链表的第一个公共结点 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 输入两个链表，找出它们的第一个公共结点。 解决思路 空间复杂度 O(n) 的算法 使用辅助容器，保存第一个链表的所有元素 遍历第二个链表，并对比当前节点是否在辅助容器中 /** * 空间 O(n) * * @param pHead1 * @param pHead2 * @return */ public ListNode FindFirstCommonNode_1(ListNode pHead1, ListNode pHead2) { Set node1s = new HashSet<>(); while (pHead1 != null) { node1s.add(pHead1); pHead1 = pHead1.next; } while (pHead2 != null) { if (node1s.contains(pHead2)) { return pHead2; } pHead2 = pHead2.next; } return null; } 空间复杂度 O(1) 的算法 由于两个链表有可能不一样长，首先通过遍历找到他们的长度 移动较长的那个链表，使得两个链表长度一致 同步遍历两个链表 原理：如果两个链表相交，那么它们一定有相同的尾节点 /** * 空间 O(1) * * @param pHead1 * @param pHead2 * @return */ public ListNode FindFirstCommonNode_2(ListNode pHead1, ListNode pHead2) { int len1 = 0, len2 = 0; ListNode cursor1 = pHead1, cursor2 = pHead2; while (cursor1 != null) { cursor1 = cursor1.next; len1++; } while (cursor2 != null) { cursor2 = cursor2.next; len2++; } cursor1 = pHead1; cursor2 = pHead2; if (len1 > len2) { int i = len1; while (i != len2) { cursor1 = cursor1.next; i--; } } else if (len1 "},"zother2-interview/offer/FindGreatestSumOfSubArray/":{"url":"zother2-interview/offer/FindGreatestSumOfSubArray/","title":"Index","keywords":"","body":" title: 连续子数组的最大和 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)。给一个数组，返回它的最大连续子序列的和，你会不会被他忽悠住？(子向量的长度至少是1) 解题思路 通过动态规划计算最大和，f(i) 定义为以第 i 个数字结尾的子数组的最大和，那么 max(f(i)) 就有以下公式： $$ max(f(i))=\\begin{cases} num[i] & i=0 or f(i)0 \\end{cases} $$ public int FindGreatestSumOfSubArray(int[] array) { if (array == null || array.length == 0) { return 0; } int max = array[0]; int sum = 0; for (int a : array) { if (sum + a > a) { sum += a; } else { sum = a; } if (sum > max) { max = sum; } } return max; } "},"zother2-interview/offer/FindKthToTail/":{"url":"zother2-interview/offer/FindKthToTail/","title":"Index","keywords":"","body":" title: 链表中倒数第 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 输入一个链表，输出该链表中倒数第k个结点。 解题思路 两个指针，快指针先走 k 步，然后慢指针在向前移动，当快指针遍历结束，慢指针指向倒数第 k 个节点 需要考虑倒数 k 个节点不存在的情况 public ListNode FindKthToTail(ListNode head, int k) { if (head == null) { return null; } ListNode cursor = head; ListNode cursorK = head; int i = 0; while (cursorK != null) { cursorK = cursorK.next; if (i >= k) { cursor = cursor.next; } i++; } if (i "},"zother2-interview/offer/FindNumbersWithSum/":{"url":"zother2-interview/offer/FindNumbersWithSum/","title":"Index","keywords":"","body":" title: 和为 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 输入一个递增排序的数组和一个数字S，在数组中查找两个数，使得他们的和正好是S，如果有多对数字的和等于S，输出两个数的乘积最小的。 对应每个测试案例，输出两个数，小的先输出。 解题思路 利用二分查找的思想，由于是排序数组，通过两个指针来进行遍历 public ArrayList FindNumbersWithSum(int[] array, int sum) { ArrayList res = new ArrayList<>(); if (array == null || array.length == 1) { return res; } int start = 0, end = array.length - 1; int minMulti = Integer.MAX_VALUE; int a = -1, b = -1; while (start sum) end--; else start++; } if (a == -1 || b == -1) { return res; } res.add(a); res.add(b); return res; } "},"zother2-interview/offer/FindNumsAppearOnce/":{"url":"zother2-interview/offer/FindNumsAppearOnce/","title":"Index","keywords":"","body":" title: 数组中只出现一次的数字 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 一个整型数组里除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现一次的数字。 解题思路 两个相等的数字进行异或的结果为0 在这个特殊的数组中，重复出现的数字只能为2次，那么如果将所有数字异或 就等价与将两个不同的数字进行异或 异或的结果肯定有一位为1，那么这两个不同的数字，在这一位上不同。 找到第一个为1的位，并将第一位为1的位是否为1作为分组条件，相同的数字一定在同一个分组里，整个数组分组异或 得到两个结果，即为两个不同的数 /** * num1,num2分别为长度为1的数组。传出参数。将num1[0],num2[0]设置为返回结果 * @param array * @param num1 * @param num2 */ public void FindNumsAppearOnce(int[] array, int num1[], int num2[]) { if (array == null || array.length >>= 1; } int mask = 1; for (int i = 1; i "},"zother2-interview/offer/FindPath/":{"url":"zother2-interview/offer/FindPath/","title":"Index","keywords":"","body":" title: 二叉树中和为某一值的路径 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 二叉树中和为某一值的路径 输入一颗二叉树的跟节点和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。(注意: 在返回值的 list 中，数组长度大的数组靠前) 解题思路 将走过的路径记录下来，当走过路径总和 = target 并且当前节点是叶子节点时，该路径符合要求 通过递归遍历所有可能的路径 public ArrayList> FindPath(TreeNode root, int target) { ArrayList> res = new ArrayList<>(); FindPath(res, new LinkedList<>(), root, 0, target); res.sort(Comparator.comparingInt(list -> -list.size())); return res; } private void FindPath(ArrayList> res, LinkedList path, TreeNode node, int pathSum, int target) { if (node == null) { return; } if (pathSum > target) { return; } if (pathSum + node.val == target && node.right == null && node.left == null) { ArrayList resPath = new ArrayList<>(path); resPath.add(node.val); res.add(resPath); return; } path.addLast(node.val); if (node.left != null) { FindPath(res, path, node.left, pathSum + node.val, target); } if (node.right != null) { FindPath(res, path, node.right, pathSum + node.val, target); } path.removeLast(); } "},"zother2-interview/offer/FirstNotRepeatingChar/":{"url":"zother2-interview/offer/FirstNotRepeatingChar/","title":"Index","keywords":"","body":" title: 第一个只出现一次的字符 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 在一个字符串(0，全部由字母组成)中找到第一个只出现一次的字符,并返回它的位置, 如果没有则返回 -1（需要区分大小写）. 解题思路 通过 LinkedHashMap 记录数组顺序，然后计算字符出现的次数 遍历找到第一个只出现 1次 的字符 public int FirstNotRepeatingChar(String str) { LinkedHashMap data = new LinkedHashMap<>(); char[] chars = str.toCharArray(); for (char c : chars) { Integer count = data.getOrDefault(c, 0); data.put(c, count + 1); } Character res = null; for (Character c : data.keySet()) { if (data.get(c) == 1) { res = c; break; } } if (res == null) { return -1; } for (int i = 0; i "},"zother2-interview/offer/GetLeastNumbers/":{"url":"zother2-interview/offer/GetLeastNumbers/","title":"Index","keywords":"","body":" title: 最小的 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。 解题思路 Partition 该算法基于 Partition public ArrayList GetLeastNumbers_Solution_Partition(int[] input, int k) { ArrayList res = new ArrayList<>(); if (k > input.length || k k - 1) { end = index - 1; index = partition(input, start, end); } else { start = index + 1; index = partition(input, start, end); } } for (int i = 0; i key) { right--; } if (left 小根堆算法 该算法基于小根堆，适合海量数据，时间复杂度为：n*logk public ArrayList GetLeastNumbers_Solution(int[] input, int k) { ArrayList res = new ArrayList<>(); if (k > input.length||k==0) { return res; } for (int i = input.length - 1; i >= 0; i--) { minHeap(input, 0, i); swap(input, 0, i); res.add(input[i]); if (res.size() == k) break; } return res; } private void minHeap(int[] heap, int start, int end) { if (start == end) { return; } int childLeft = start * 2 + 1; int childRight = childLeft + 1; if (childLeft "},"zother2-interview/offer/GetLeastNumbersSolution/":{"url":"zother2-interview/offer/GetLeastNumbersSolution/","title":"Index","keywords":"","body":" title: 最小的 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。 解题思路 利用堆排序原理，计算出最小的 k 个数 public ArrayList GetLeastNumbers_Solution(int[] input, int k) { ArrayList res = new ArrayList<>(); if (k > input.length || k == 0) { return res; } for (int i = input.length - 1; i >= 0; i--) { minHeap(input, 0, i); swap(input, 0, i); res.add(input[i]); if (res.size() == k) break; } return res; } private void minHeap(int[] heap, int start, int end) { if (start == end) { return; } int childLeft = start * 2 + 1; int childRight = childLeft + 1; if (childLeft "},"zother2-interview/offer/GetNext/":{"url":"zother2-interview/offer/GetNext/","title":"二叉树的下一个结点","keywords":"","body":"题目 给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。 解题思路 public TreeLinkNode GetNext(TreeLinkNode pNode) { if (pNode == null) return null; TreeLinkNode parent = pNode.next; if (pNode.right == null) { if (parent == null) { return null; } //右节点 if (parent.right == pNode) { TreeLinkNode cursor = parent; while (true) { TreeLinkNode p = cursor.next; if (p == null) return null; if (cursor == p.left) return p; cursor = p; } } else { return parent; } } else { TreeLinkNode cursor = pNode.right; while (cursor.left != null) { cursor = cursor.left; } return cursor; } } "},"zother2-interview/offer/GetNumberOfK/":{"url":"zother2-interview/offer/GetNumberOfK/","title":"Index","keywords":"","body":" title: 数字在排序数组中出现的次数 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 统计一个数字在排序数组中出现的次数。 解题思路 利用二分查找，找到任意一个 k 由于 k 有多个，并且当前找到的 k 可能在任意位置。所以，在当前 k 的前后进行遍历查找 public int GetNumberOfK(int[] array, int k) { if (array == null || array.length == 0) { return 0; } //二分查找 int start = 0, end = array.length - 1; int t = -1; while (start k) { end = mid - 1; } else { start = mid + 1; } } if (array[start] == k) { t = start; } if (t == -1) { return 0; } //左侧 int sum = 0; int a = t; while (a >= 0 && array[a] == k) { sum++; a--; } //右侧 a = t + 1; while (a "},"zother2-interview/offer/GetUglyNumber/":{"url":"zother2-interview/offer/GetUglyNumber/","title":"Index","keywords":"","body":" title: 丑数 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 牛客网 把只包含质因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含质因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。 解题思路 通过保存已有丑数的方式，用空间换时间 对于已有丑数 M ，那么下一个丑数 M=\\min(M{2}\\times2,M{3}\\times3,M_{5}\\times5) M{max} 是目前最大的丑数，那么 M{2} 是已有丑数中 M{2}\\times2 第一个大于 M{max} 的丑数 public int GetUglyNumber_Solution(int index) { if (index == 0) { return 0; } if (index == 1) { return 1; } ArrayList list = new ArrayList<>(index); list.add(1); int preIndex2 = 0; int preIndex3 = 0; int preIndex5 = 0; for (int i = 0; i "},"zother2-interview/offer/hasPath/":{"url":"zother2-interview/offer/hasPath/","title":"Index","keywords":"","body":" title: 矩阵中的路径 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则之后不能再次进入这个格子。 例如 a b c e s f c s a d e e 这样的 3 X 4 矩阵中包含一条字符串\"bcced\"的路径，但是矩阵中不包含\"abcb\"路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。 解题思路 简单的回溯查找 static int[][] steps = {{0, 1}, {1, 0}, {0, -1}, {-1, 0}}; public boolean hasPath(char[] matrix, int rows, int cols, char[] str) { char[][] _matrix = new char[rows][cols]; int k = 0; for (int i = 0; i = matrix.length || y >= matrix[0].length) return false; if (flag[x][y] == 1) return false; boolean subRes = false; if (matrix[x][y] == str[index]) { if (index == str.length - 1) return true; flag[x][y] = 1; for (int[] step : steps) { subRes |= hasPath(matrix, flag, x + step[0], y + step[1], str, index + 1); } flag[x][y] = 0; } return subRes; } "},"zother2-interview/offer/HasSubtree/":{"url":"zother2-interview/offer/HasSubtree/","title":"Index","keywords":"","body":" title: 树的子结构 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构） 解题思路 遍历查找相等根节点 通过递归查找当前根节点下是否包含子树 root2 public boolean HasSubtree(TreeNode root1, TreeNode root2) { if (root2 == null) { return false; } LinkedList pipeline = new LinkedList<>(); pipeline.addLast(root1); while (!pipeline.isEmpty()) { TreeNode node = pipeline.pop(); if (node == null) { continue; } pipeline.addLast(node.left); pipeline.addLast(node.right); if (node.val == root2.val && isSub(node, root2)) { return true; } } return false; } private boolean isSub(TreeNode root1, TreeNode root2) { if (root1 == null && root2 == null) { return true; } if (root1 == null) { return false; } if (root2 == null) { return true; } if (root1.val == root2.val) { return isSub(root1.left, root2.left) && isSub(root1.right, root2.right); } else { return false; } } "},"zother2-interview/offer/InversePairs/":{"url":"zother2-interview/offer/InversePairs/","title":"Index","keywords":"","body":" title: 数组中的逆序对 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007 输入描述: 题目保证输入的数组中没有的相同的数字 数据范围： 对于%50的数据,size解题思路 1. 使用归并排序的方式，划分子数组 2. 两个子数组进行对比，有两个分别指向两个数组末尾的指针 `f,s`，数组分割下标为 `mid`，如果 `array[f] > array[s]`那么，就有`s - mid`个 `array[f]` 的逆序 3. 依此类推，最终将数组排序，并且获得结果 public int InversePairs(int[] array) { long[] sum = {0}; if (array == null || array.length == 0) { return (int) sum[0]; } int[] temp = new int[array.length]; mergeSort(array, 0, array.length - 1, temp, sum); return (int) (sum[0] % 1000000007); } private void mergeSort(int[] array, int start, int end, int[] temp, long[] sum) { if (start == end) { return; } int mid = (start + end) / 2; mergeSort(array, start, mid, temp, sum); mergeSort(array, mid + 1, end, temp, sum); int f = mid, s = end; int t = end; while (f >= start && s >= mid + 1) { if (array[f] > array[s]) { temp[t--] = array[f--]; sum[0] += s - mid; } else { temp[t--] = array[s--]; } } while (f >= start) { temp[t--] = array[f--]; } while (s >= mid + 1) { temp[t--] = array[s--]; } for (int i = end, j = end; i >= start; ) { array[j--] = temp[i--]; } } "},"zother2-interview/offer/isContinuous/":{"url":"zother2-interview/offer/isContinuous/","title":"Index","keywords":"","body":" title: 扑克牌顺子 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 LL今天心情特别好,因为他去买了一副扑克牌,发现里面居然有 2 个大王, 2 个小王(一副牌原本是 54 张)...他随机从中抽出了 5 张牌,想测测自己的手气,看看能不能抽到顺子,如果抽到的话,他决定去买体育彩票,嘿嘿！！“红心A,黑桃3,小王,大王,方片5”,“Oh My God!”不是顺子.....LL不高兴了,他想了想,决定大\\小 王可以看成任何数字,并且A看作1,J为11,Q为12,K为13。上面的5张牌就可以变成“1,2,3,4,5”(大小王分别看作2和4),“So Lucky!”。LL决定去买体育彩票啦。 现在,要求你使用这幅牌模拟上面的过程,然后告诉我们 LL 的运气如何， 如果牌能组成顺子就输出 true，否则就输出 false。为了方便起见,你可以认为大小王是0。 解题思路 对数组进行排序 计算非0元素之间的间隔总和 如果有相同元素则直接认为失败 如果间隔大于0，那么间隔的总个数等于0的总个数，即为成功 public boolean isContinuous(int[] numbers) { if (numbers == null || numbers.length 0) { count += t; } else if (t "},"zother2-interview/offer/IsNumeric/":{"url":"zother2-interview/offer/IsNumeric/","title":"Index","keywords":"","body":" title: 表示数值的字符串 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串\"+100\",\"5e2\",\"-123\",\"3.1416\"和\"-1E-16\"都表示数值。 但是\"12e\",\"1a3.14\",\"1.2.3\",\"+-5\"和\"12e+4.3\"都不是。 解题思路 数字符合 A[.[B]][e|EC] 和 .B[e|EC] 的表达式，其中 A 表示整数部分，B 表示小数部分，C 表示指数部分 A 可以有正负，但是 B 没有 e|E 之前、之后都必须有数字 public boolean isNumeric(char[] str) { if (str == null || str.length == 0) return false; int index = scanInteger(str, 0); boolean numeric = index != 0; //小数 if (index = '0' && str[s] "},"zother2-interview/offer/IsPopOrder/":{"url":"zother2-interview/offer/IsPopOrder/","title":"Index","keywords":"","body":" title: 栈的压入 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的） 解题思路 通过 Stack 进行模拟 push，当 pop 的节点等于 Stack 的 top 节点时，pop Stack 最后如果 Stack 剩余数据，则判定为 false public boolean IsPopOrder(int[] pushA, int[] popA) { if (pushA.length != popA.length) { return false; } if (pushA.length == 0) { return false; } LinkedList stack = new LinkedList<>(); int j = 0; for (int value : pushA) { stack.addLast(value); while (stack.peekLast() != null && popA[j] == stack.getLast()) { j++; stack.removeLast(); } } return stack.isEmpty(); } "},"zother2-interview/offer/IsSymmetrical/":{"url":"zother2-interview/offer/IsSymmetrical/","title":"Index","keywords":"","body":" title: 对称的二叉树 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。 解题思路 定义一个对称的前序遍历，即root -> right -> left 与普通的前序遍历进行对比 相同则认为树是对称的 boolean isSymmetrical(TreeNode pRoot) { LinkedList scanner = new LinkedList<>(); LinkedList symmetricalScanner = new LinkedList<>(); preScanner(scanner, pRoot); symmetricalPreScanner(symmetricalScanner, pRoot); return scanner.equals(symmetricalScanner); } /** * 普通的前序遍历 * @param res * @param root */ private void preScanner(LinkedList res, TreeNode root) { if (root == null) { res.addLast(null); return; } res.addLast(root.val); preScanner(res, root.left); preScanner(res, root.right); } /** * 先右再左的前序遍历 * @param res * @param root */ private void symmetricalPreScanner(LinkedList res, TreeNode root) { if (root == null) { res.addLast(null); return; } res.addLast(root.val); symmetricalPreScanner(res, root.right); symmetricalPreScanner(res, root.left); } "},"zother2-interview/offer/LastRemaining/":{"url":"zother2-interview/offer/LastRemaining/","title":"Index","keywords":"","body":" title: 圆圈中最后剩下的数 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF 作为牛客的资深元老,自然也准备了一些小游戏。其中,有个游戏是这样的:首先,让小朋友们围成一个大圈。然后,他随机指定一个数m,让编号为0的小朋友开始报数。每次喊到m-1的那个小朋友要出列唱首歌,然后可以在礼品箱中任意的挑选礼物,并且不再回到圈中,从他的下一个小朋友开始,继续0...m-1报数....这样下去....直到剩下最后一个小朋友,可以不用表演,并且拿到牛客名贵的“名侦探柯南”典藏版。请你试着想下,哪个小朋友会得到这份礼品呢？(注：小朋友的编号是从 0 到 n-1 ) 解题思路 模拟 最简单直接的解法，但是时间效率不够 public int LastRemaining_Solution(int n, int m) { if (n == 1) return 1; LinkedList data = new LinkedList<>(); for (int i = 0; i 通过数学推导的解法 时间效率和空间效率都很高，但是。。。没看懂 $$ f(n,m)= \\begin{cases} 0&n=1 \\ [f(n-1,m)+m]\\%n & n>1 \\end{cases} $$ public int LastRemaining_Solution(int n, int m) { if (n == 0) return -1; if (n == 1) return 0; int last = 0; for (int i = 2; i "},"zother2-interview/offer/LeftRotateString/":{"url":"zother2-interview/offer/LeftRotateString/","title":"Index","keywords":"","body":" title: 左旋转字符串 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 汇编语言中有一种移位指令叫做循环左移（ROL），现在有个简单的任务，就是用字符串模拟这个指令的运算结果。对于一个给定的字符序列S，请你把其循环左移K位后的序列输出。例如，字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。是不是很简单？OK，搞定它！ 解题思路 对于 abcXYZdef 左移 3位，可以将字符串分为两个部分：abc & XYZdef 分别将两个部分进行反转得到：cba & fedZYX 将两部分和在一起再进行反转：XYZdefabc public String LeftRotateString(String str, int n) { if (str == null || str.trim().equals(\"\")) return str; String res = revert(str, 0, n - 1); res = revert(res, n, str.length() - 1); res = revert(res, 0, str.length() - 1); return res; } private String revert(String str, int start, int end) { char[] chars = str.toCharArray(); while (start "},"zother2-interview/offer/LongestNoRepeatSubString/":{"url":"zother2-interview/offer/LongestNoRepeatSubString/","title":"Index","keywords":"","body":" title: 最长不含重复字符的子字符串 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 LeetCode 给定一个字符串，请你找出其中不含有重复字符的 最长子串 的长度。 输入: \"abcabcbb\" 输出: 3 解释: 因为无重复字符的最长子串是 \"abc\"，所以其长度为 3。 解题思路 用 Map 记录字符所在位置，当遇到重复字符时，移动 start 指针 替换 Map 中下标，并计算子串长度 public int longestNoRepeatSubString(String str) { if (str == null || str.length() == 0) return 0; HashMap temp = new HashMap<>(); char[] chars = str.toCharArray(); int res = 0, start = 0; for (int i = 0; i "},"zother2-interview/offer/MaxGift/":{"url":"zother2-interview/offer/MaxGift/","title":"礼物的最大值","keywords":"","body":"题目 在一个 m*n 的棋盘中的每一个格都放一个礼物，每个礼物都有一定的价值（价值大于0）.你可以从棋盘的左上角开始拿各种里的礼物，并每次向左或者向下移动一格，直到到达棋盘的右下角。给定一个棋盘及上面个的礼物，请计算你最多能拿走多少价值的礼物？ 比如说现在有一个如下的棋盘: [1,3,1] [1,5,1] [4,2,1] 在这个棋盘中，按照 1 -> 3 -> 5 -> 2 -> 1 可以拿到最多价值的礼物。 解题思路 动态规划，定义 f(x,y) 表示x,y点上能获取的最大数 状态转移方程：f(x,y)=\\max(f(x-1,y),f(x,y-1))+g(x,y) 可以考虑使用一维数组进行记录 public int maxGift(int[][] matrix) { for (int i = 0; i 0 ? matrix[i - 1][j] : 0; int b = j > 0 ? matrix[i][j - 1] : 0; matrix[i][j] += Math.max(a, b); } } System.out.println(Arrays.deepToString(matrix)); return matrix[matrix.length - 1][matrix[0].length - 1]; } "},"zother2-interview/offer/MaxInWindows/":{"url":"zother2-interview/offer/MaxInWindows/","title":"Index","keywords":"","body":" title: 滑动窗口的最大值 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}， {2,3,4,2,6,[2,5,1]}。 解题思路 使用一个队列来保存最大值和次大的值 public ArrayList maxInWindows(int[] num, int size) { ArrayList res = new ArrayList<>(); if (size == 0) return res; LinkedList queue = new LinkedList<>(); for (int i = 0; i = size) { queue.removeFirst(); } while (queue.peekLast() != null && i - queue.peekLast() >= size) { queue.removeLast(); } if (queue.isEmpty()) { queue.addFirst(i); } else { if (num[i] > num[queue.peekFirst()]) { queue.clear(); queue.addFirst(i); } else { while (num[i] > num[queue.peekLast()]) { queue.removeLast(); } queue.addLast(i); } } if (i >= size - 1) res.add(num[queue.peekFirst()]); } return res; } "},"zother2-interview/offer/MaxProfit/":{"url":"zother2-interview/offer/MaxProfit/","title":"Index","keywords":"","body":" title: 股票的最大利润 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 一只股票在某些时间节点的价格为{9,11,8,5,7,12,16,14}。如果我们能在价格为 5 的时候买入并在价格为 16 时卖出，则能获得最大的利润为 11. 解题思路 要先买入才能卖出，先找最低价格点 再找最低价格之后的最高价格，用 maxProfit 表示最大利润 public int maxProfit(int[] nums) { if (nums == null || nums.length == 0) return 0; int min = Integer.MAX_VALUE; int maxProfit = 0; for (int i = 0; i "},"zother2-interview/offer/merge-sort-link/":{"url":"zother2-interview/offer/merge-sort-link/","title":"Index","keywords":"","body":" title: 合并两个排序的链表 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。 解题思路 双指针指向两个链表 循环选取最小值，加入结果集 public ListNode Merge(ListNode list1, ListNode list2) { ListNode head = new ListNode(-1); ListNode cursor = head; while (list1 != null || list2 != null) { if (list1 == null) { while (list2 != null) { cursor.next = list2; cursor = cursor.next; list2 = list2.next; } continue; } if (list2 == null) { while (list1 != null) { cursor.next = list1; cursor = cursor.next; list1 = list1.next; } continue; } if (list1.val "},"zother2-interview/offer/MinStack/":{"url":"zother2-interview/offer/MinStack/","title":"Index","keywords":"","body":" title: 包含 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的 min 函数（时间复杂度应为O（1））。 解题思路 通过增加最小栈来记录当前最小节点 private LinkedList stack = new LinkedList<>(); private LinkedList min = new LinkedList<>(); public void push(int node) { stack.addLast(node); if (min.isEmpty()) { min.addLast(node); return; } if (node "},"zother2-interview/offer/mirror-tree/":{"url":"zother2-interview/offer/mirror-tree/","title":"Index","keywords":"","body":" title: 镜像二叉树 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 镜像二叉树 操作给定的二叉树，将其变换为源二叉树的镜像。 输入描述: 二叉树的镜像定义：源二叉树 8 / \\ 6 10 / \\ / \\ 5 7 9 11 镜像二叉树 8 / \\ 10 6 / \\ / \\ 11 9 7 5 解题思路 从上到下进行左右节点交换 public void Mirror(TreeNode root) { if (root == null) return; TreeNode temp = root.left; root.left = root.right; root.right = temp; Mirror(root.left); Mirror(root.right); } "},"zother2-interview/offer/MoreThanHalfNum/":{"url":"zother2-interview/offer/MoreThanHalfNum/","title":"Index","keywords":"","body":" title: 数组中出现次数超过一半的数字 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出 2 。如果不存在则输出 0 。 解题思路 由于数组的特性，在排序数组中，超过半数的数字一定包含中位数 通过 partition 方法，借用快排的思想，随机选取一个 key，将数组中小于 key 的移动到 key 的左侧，数组中大于 key 的移动到 key 的右侧 最终找到中位数的下标，还需要检查中位数是否超过半数 public int MoreThanHalfNum_Solution(int[] array) { int start = 0, end = array.length - 1; int mid = array.length / 2; int index = partition(array, start, end); if (index == mid) { return array[index]; } while (index != mid && start mid) { end = index - 1; index = partition(array, start, end); } else { start = index + 1; index = partition(array, start, end); } } if (checkIsHalf(array, index)) return array[index]; return 0; } private boolean checkIsHalf(int[] array, int index) { if (index array.length / 2; } private int partition(int[] array, int start, int end) { if (start >= array.length || start = array.length || end = key) { right--; } if (left "},"zother2-interview/offer/MovingCount/":{"url":"zother2-interview/offer/MovingCount/","title":"Index","keywords":"","body":" title: 机器人的运动范围 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 地上有一个m行和n列的方格。一个机器人从坐标 0,0 的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于k的格子。 例如，当k为 18 时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。但是，它不能进入方格（35,38），因为3+5+3+8 = 19。请问该机器人能够达到多少个格子？ 解题思路 "},"zother2-interview/offer/NOfNumberSerialize/":{"url":"zother2-interview/offer/NOfNumberSerialize/","title":"Index","keywords":"","body":" title: 数字序列中的某一位的数字 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 数字以0123456789101112131415…的格式序列化到一个字符序列中。在这个序列中，第5位（从0开始计数，即从第0位开始）是5，第13位是1，第19位是4，等等。请写一个函数，求任意第n位对应的数字。 解题思路 可以将 n 进行拆分，1位数一共10个数字、10位，2位数一共90个数字、180位，依此类推 当确定 n 所在位数范围时，对位数取商，计算出 n 位对应的数字 a，再取余，计算出结果位于 a 的第几位 public int nOfNumberSerialize(int n) { int i = 1; int count = 0; int nLeft = n; while (true) { nLeft -= count; count = countOfIntegers(i) * i; if (nLeft "},"zother2-interview/offer/number-of-one/":{"url":"zother2-interview/offer/number-of-one/","title":"Index","keywords":"","body":" title: 二进制中 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 解题思路 负数是补码表示 >>> 为无符号右移，>>为有符号右移，当 n 为负数是会增加多余的1 public int NumberOf1(int n) { int mask = 0x01; int res = 0; int t = n; while (t != 0) { if ((t & mask) == 1) { res++; } t = t >>> 1; } return res; } "},"zother2-interview/offer/NumberOfOneBetweenOneAndN/":{"url":"zother2-interview/offer/NumberOfOneBetweenOneAndN/","title":"Index","keywords":"","body":" title: 整数中 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 求出1~13的整数中 1 出现的次数,并算出 100~1300 的整数中1出现的次数？为此他特别数了一下 1~13 中包含1的数字有 1、10、11、12、13 因此共出现 6 次,但是对于后面问题他就没辙了。ACMer 希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数（从1 到 n 中1出现的次数）。 解题思路 假定 n=21345 将数字分为首位和非首位两个部分 对于首位为 1 的情况，如果首位 >1 那么sum=sum+10^{len(n)-1}，如果首位 =1 那么 sum=sum+1 对于非首位 1，指定其中一位为 1，根据排列组合有 10^{len(n)-2}\\times(len(n)-1) 个。那么非首位 1 总共有 2\\times10^{len(n)-2}\\times(len(n)-1) public int NumberOf1Between1AndN_Solution(int n) { int[] res = {0}; NumberOf1Between1AndN(res, n); return res[0]; } private void NumberOf1Between1AndN(int[] res, int n) { //假设 num=21345 String num = String.valueOf(n); int firstNum = num.charAt(0) - '0'; if (num.length() == 1) { if (firstNum > 0) res[0]++; return; } String nextNum = num.substring(1); int nextN = Integer.valueOf(nextNum); //数字 10000 ～ 19999 的第一位中的个数 if (firstNum > 1) { res[0] += Math.pow(10, num.length() - 1); } else if (firstNum == 1) { res[0] += nextN + 1; } //1346 ～ 21345 除第一位之外的数的个数 res[0] += firstNum * (num.length() - 1) * Math.pow(10, num.length() - 2); NumberOf1Between1AndN(res, nextN); } "},"zother2-interview/offer/O1DeleteNode/":{"url":"zother2-interview/offer/O1DeleteNode/","title":"Index","keywords":"","body":" title: 在 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 给定单向链表的头指针以及待删除的指针，定义一个函数在 O(1) 的时间复杂度下删除 解题思路 待删除节点非尾节点，将后一个节点的值复制到当前节点，然后删除后一个节点 待删除节点为尾节点，从头节点开始，找到待删除节点的前一个节点进行删除 public void O1DeleteNode(ListNode head, ListNode needDelete) { if (needDelete.next != null) { ListNode next = needDelete.next.next; needDelete.val = needDelete.next.val; needDelete.next = next; } else { ListNode cursor = head; while (cursor != null) { if (cursor.next == needDelete) break; cursor = cursor.next; } if (cursor == null) return; cursor.next = needDelete.next; } } "},"zother2-interview/offer/PatternMatch/":{"url":"zother2-interview/offer/PatternMatch/","title":"Index","keywords":"","body":" title: 正则表达式匹配 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 请实现一个函数用来匹配包括'.'和'*'的正则表达式。模式中的字符'.'表示任意一个字符，而'*'表示它前面的字符可以出现任意次（包含0次）。 在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串\"aaa\"与模式\"a.a\"和\"ab*ac*a\"匹配，但是与\"aa.a\"和\"ab*a\"均不匹配 解题思路 对于 * 有三种匹配模式：匹配0次，1次以及多次 对于 . 只有一种匹配模式 public boolean match(char[] str, char[] pattern) { if (str.length == 0 && new String(pattern).replaceAll(\".\\\\*\", \"\").length() == 0) { return true; } return match(str, 0, pattern, 0); } private boolean match(char[] str, int i, char[] pattern, int j) { if (i == str.length && j == pattern.length) { return true; } if (j >= pattern.length) return false; if (j + 1 "},"zother2-interview/offer/Permutation/":{"url":"zother2-interview/offer/Permutation/","title":"Index","keywords":"","body":" title: 字符串的排列 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。 输入描述:输入一个字符串,长度不超过9(可能有字符重复),字符只包括大小写字母。 解题思路 将字符串划分为两个部分，第一个字符以及后面的其他字符 将第一个字符和后面所有字符进行交换 对于 abc 这个字符串，计算出的排列顺序为： abc acb bac bca cba cab 代码： public ArrayList Permutation(String str) { Set res = new HashSet<>(); if (str == null || str.length() == 0) { return new ArrayList<>(); } Permutation(res, str.toCharArray(), 0); ArrayList list = new ArrayList<>(res); list.sort(String::compareTo); return list; } private void Permutation(Set res, char[] chars, int start) { if (start == chars.length) { res.add(new String(chars)); return; } for (int i = start; i "},"zother2-interview/offer/power/":{"url":"zother2-interview/offer/power/","title":"Index","keywords":"","body":" title: 题目 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 牛客网 给定一个 double 类型的浮点数 base 和 int 类型的整数 exponent 。求 base 的 exponent 次方。 解题思路 当 n 为偶数时，a^n = a^{n/2} * a^{n/2} 当 n 为奇数时，a^n = a^{n/2} a^{n/2} a 可以利用类似斐波纳切的方式，利用递归来进行求解 public double Power(double base, int exponent) { if (base == 0) { return 0; } if (base == 1) { return 1; } int t_exponent = Math.abs(exponent); double t = PositivePower(base, t_exponent); return exponent > 0 ? t : 1 / t; } private double PositivePower(double base, int exponent) { if (exponent == 0) { return 1; } if (exponent == 1) { return base; } double t = PositivePower(base, exponent >> 1); t *= t; if ((exponent & 0x01) == 1) { t *= base; } return t; } "},"zother2-interview/offer/print-link-from-tail/":{"url":"zother2-interview/offer/print-link-from-tail/","title":"Index","keywords":"","body":" title: 从尾到头打印链表 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 输入一个链表，按链表值从尾到头的顺序返回一个ArrayList。 解题思路 栈 public ArrayList printListFromTailToHead(ListNode listNode) { LinkedList stack = new LinkedList<>(); while (listNode != null) { stack.addLast(listNode.val); listNode = listNode.next; } ArrayList res = new ArrayList<>(); while (!stack.isEmpty()) { res.add(stack.pollLast()); } return res; } 递归：当链表过长时，会导致栈溢出 public ArrayList printListFromTailToHead(ListNode listNode) { ArrayList res = new ArrayList<>(); print(res,listNode); return res; } private void print(ArrayList res, ListNode listNode) { if (listNode == null) return; print(res, listNode.next); res.add(listNode.val); } "},"zother2-interview/offer/PrintFromTopToBottom/":{"url":"zother2-interview/offer/PrintFromTopToBottom/","title":"Index","keywords":"","body":" title: 从上往下打印二叉树 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 从上往下打印出二叉树的每个节点，同层节点从左至右打印。 解题思路 层次遍历，通过队列进行辅助遍历 public ArrayList PrintFromTopToBottom(TreeNode root) { ArrayList res = new ArrayList<>(); LinkedList nodeQueue = new LinkedList<>(); if (root == null) { return res; } nodeQueue.addLast(root); while (!nodeQueue.isEmpty()) { TreeNode node = nodeQueue.pollFirst(); if (node == null) { continue; } nodeQueue.addLast(node.left); nodeQueue.addLast(node.right); res.add(node.val); } return res; } "},"zother2-interview/offer/PrintMatrix/":{"url":"zother2-interview/offer/PrintMatrix/","title":"Index","keywords":"","body":" title: 顺时针打印矩阵 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字，例如，如果输入如下4 X 4矩阵： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 则依次打印出数字1,2,3,4,8,12,16,15,14,13,9,5,6,7,11,10. 解题思路 通过4个指针，表示可打印区域，并对区域进行收缩 非 n*n 的矩阵，对于剩余非 4 边遍历的元素，要考虑边界 public ArrayList printMatrix(int[][] matrix) { ArrayList res = new ArrayList<>(); if (matrix.length == 0) { return res; } if (matrix.length == 1) { for (int i : matrix[0]) { res.add(i); } return res; } int top = 0, bottom = matrix.length - 1, left = 0, right = matrix[0].length - 1; for (; left = left; p--) { res.add(matrix[bottom][p]); } bottom--; for (int p = bottom; p >= top; p--) { res.add(matrix[p][left]); } left++; } return res; } "},"zother2-interview/offer/PrintMinNumber/":{"url":"zother2-interview/offer/PrintMinNumber/","title":"Index","keywords":"","body":" title: 把数组排成最小的数 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 把数组排成最小的数 输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。 解题思路 最直接的办法就是，找到数组中数字的所有排列组合，找到最小的 对于 m, n，可以组成 mn , nm 这两个数，如果 mn 那么，m 应该在 n 之前 对于一组数，可以通过上述规则进行排序，依次打印出来就是最小的数 由于组合之后的数可能超出 int 的表示范围，注意使用字符串来处理大数问题 public String PrintMinNumber(int[] numbers) { List nums = new ArrayList<>(); for (int number : numbers) { nums.add(String.valueOf(number)); } nums.sort(Comparator.comparing(s -> s, (o1, o2) -> (o1 + o2).compareTo(o2 + o1))); StringJoiner joiner = new StringJoiner(\"\"); nums.forEach(joiner::add); return joiner.toString(); } "},"zother2-interview/offer/printn/":{"url":"zother2-interview/offer/printn/","title":"Index","keywords":"","body":" title: 打印最大的 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 输入n，打印出 1 到最大的 n 位十进制数。比如输入3，则打印出1、2、3 直到最大的 3 位数 999。 解题思路 n 可能很大，导致输出的数字超过 int 或者 long public void PrintN(int n) { if (n = 0; j--) { int a = chars[j]; if (flag) { a++; flag = false; } if (a > '9') { flag = true; a = a - '9' + '0' - 1; } res.append((char) a); } if (flag) { res.append('1'); } return res.reverse().toString(); } "},"zother2-interview/offer/reConstructBinaryTree/":{"url":"zother2-interview/offer/reConstructBinaryTree/","title":"Index","keywords":"","body":" title: 重建二叉树 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。 解题思路 通过前序遍历找到 root 节点 那么在 中序遍历中 root 节点的左侧则是左子树，右侧是右子树 依次类推，递归生成节点的左子树和右子树 构建过程由下往上 public TreeNode reConstructBinaryTree(int[] pre, int[] in) { Map preIndex = new HashMap<>(); for (int i = 0; i preIndex, int[] in, int start, int end) { if (start == end) { return new TreeNode(in[start]); } int indexOfRoot = start; for (int i = start; i "},"zother2-interview/offer/reOrderArray/":{"url":"zother2-interview/offer/reOrderArray/","title":"Index","keywords":"","body":" title: 调整数组顺序使奇数位于偶数前面 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。 解题思路 需要保证排序的稳定性 采用冒泡算法进行排序 public void reOrderArray(int[] array) { if (array.length = 0; i--) { for (int j = i; j "},"zother2-interview/offer/replay-space/":{"url":"zother2-interview/offer/replay-space/","title":"Index","keywords":"","body":" title: 替换空格 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为 We%20Are%20Happy。 解题思路 通过字符串中空格的个数，计算新字符串长度 两个指针进行字符串拷贝，当遇到‘ ’时替换为 %20 public String replaceSpace(StringBuffer str) { char[] chars = str.toString().toCharArray(); StringBuilder res = new StringBuilder(); for (char c : chars) { if (c == ' ') res.append(\"%20\"); else res.append(c); } return res.toString(); } "},"zother2-interview/offer/ReverseSentence/":{"url":"zother2-interview/offer/ReverseSentence/","title":"Index","keywords":"","body":" title: 翻转单词顺序列 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 牛客最近来了一个新员工 Fish ，每天早晨总是会拿着一本英文杂志，写些句子在本子上。同事Cat对Fish写的内容颇感兴趣，有一天他向Fish借来翻看，但却读不懂它的意思。例如，“student. a am I”。后来才意识到，这家伙原来把句子单词的顺序翻转了，正确的句子应该是“I am a student.”。Cat对一一的翻转这些单词顺序可不在行，你能帮助他么？ 解题思路 public String ReverseSentence(String str) { if(str == null || str.trim().equals(\"\")) return str; String[] split = str.split(\" \"); StringBuilder builder = new StringBuilder(); for (int i = split.length - 1; i >= 0; i--) { builder.append(split[i]); if (i != 0) builder.append(\" \"); } return builder.toString(); } "},"zother2-interview/offer/revert-link/":{"url":"zother2-interview/offer/revert-link/","title":"Index","keywords":"","body":" title: 反转链表 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 输入一个链表，反转链表后，输出新链表的表头。 解题思路 三个指针 public ListNode ReverseList(ListNode head) { if (head == null || head.next == null) { return head; } ListNode pre = head, cur = head.next, next; pre.next = null; while (cur != null) { next = cur.next; cur.next = pre; pre = cur; cur = next; } return pre; } "},"zother2-interview/offer/search-a-2d-matrix/":{"url":"zother2-interview/offer/search-a-2d-matrix/","title":"Index","keywords":"","body":" title: 搜索二维矩阵 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 Leetcode 编写一个高效的算法来搜索 m x n 矩阵 matrix 中的一个目标值 target。该矩阵具有以下特性： 每行的元素从左到右升序排列。 每列的元素从上到下升序排列。 示例: 现有矩阵 matrix 如下： [ [1, 4, 7, 11, 15], [2, 5, 8, 12, 19], [3, 6, 9, 16, 22], [10, 13, 14, 17, 24], [18, 21, 23, 26, 30] ] 给定 target = 5，返回 true。 给定 target = 20，返回 false。 解题思路 二维数组是有规律的：右上角的数字是一列中最小的、一行中最大的，通过这个数字和 target 进行对比，可以将一行或者一列作为候选区域排出，那么 target 可能存在的范围缩小，最终得出结果。 public boolean searchMatrix(int[][] matrix, int target) { if (matrix.length == 0) { return false; } for (int i = 0, j = matrix[0].length - 1; i = 0; ) { if (matrix[i][j] > target) { j--; } else if (matrix[i][j] "},"zother2-interview/offer/SerializeTree/":{"url":"zother2-interview/offer/SerializeTree/","title":"Index","keywords":"","body":" title: 序列化二叉树 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 请实现两个函数，分别用来序列化和反序列化二叉树 解题思路 通过前序遍历，进行序列化和反序列化 对于空节点用 $ 来代替 String Serialize(TreeNode root) { if (root==null) return \"\"; LinkedList res = new LinkedList<>(); serialize(res, root); StringBuilder builder = new StringBuilder(); res.forEach(v-> builder.append(v).append(\",\")); return builder.toString(); } private void serialize(LinkedList res, TreeNode root) { if (root == null) { res.addLast(\"$\"); return; } res.addLast(String.valueOf(root.val)); serialize(res, root.left); serialize(res, root.right); } TreeNode Deserialize(String str) { if (str == null || str.length() == 0) return null; return deserialize(str.split(\",\"), new int[]{0}); } private TreeNode deserialize(String[] str, int[] index) { if (index[0] >= str.length) return null; String c = str[index[0]++]; if (c.equals(\"$\")) return null; TreeNode node = new TreeNode(Integer.valueOf(c)); node.left = deserialize(str, index); node.right = deserialize(str, index); return node; } "},"zother2-interview/offer/Singleton/":{"url":"zother2-interview/offer/Singleton/","title":"Index","keywords":"","body":" title: 单例 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 设计一个类，我们只能生成该类的一个实例 解题思路 线程安全 延迟加载 序列化与反序列化安全 /** * 需要额外的工作(Serializable、transient、readResolve())来实现序列化，否则每次反序列化一个序列化的对象实例时都会创建一个新的实例。 * * 可能会有人使用反射强行调用我们的私有构造器（如果要避免这种情况，可以修改构造器，让它在创建第二个实例的时候抛异常）。 * * @author haoyang.shi */ public class Singleton { private Singleton() { } public static Singleton getInstance() { return Holder.instance; } private static final class Holder { private static Singleton instance = new Singleton(); } } /** * 使用枚举除了线程安全和防止反射强行调用构造器之外，还提供了自动序列化机制，防止反序列化的时候创建新的对象。 * * 因此，Effective Java推荐尽可能地使用枚举来实现单例。 */ enum SingletonEnum { INSTANCE; private String name; public String getName() { return name; } public void setName(String name) { this.name = name; } } "},"zother2-interview/offer/StreamMid/":{"url":"zother2-interview/offer/StreamMid/","title":"Index","keywords":"","body":" title: 数据流中的中位数 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。我们使用 Insert() 方法读取数据流，使用 GetMedian() 方法获取当前读取数据的中位数。 解题思路 同两个堆来表示中位数的左右两部分，左边是大根堆，右边是小根堆 在插入元素时，两边元素个数最多只能相差1，并且要保证左边的元素均小于右边的元素 当插入大堆的元素大于部分小堆元素时，需要将大堆的 top 元素移动到小堆，反之亦然 private PriorityQueue maxHeap = new PriorityQueue<>((o1, o2) -> -o1.compareTo(o2)); private PriorityQueue minHeap = new PriorityQueue<>(); private int size = 0; public void Insert(Integer num) { if (size % 2 == 0) { maxHeap.add(num); if (minHeap.isEmpty() || num > minHeap.peek()) { minHeap.add(maxHeap.poll()); } } else { minHeap.add(num); if (maxHeap.isEmpty() || num minHeap.size() ? maxHeap.peek() * 1.0 : minHeap.peek() * 1.0; } "},"zother2-interview/offer/sum/":{"url":"zother2-interview/offer/sum/","title":"Index","keywords":"","body":" title: 求 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 求1+2+3+...+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。 解题思路 利用递归代替循环 public int Sum_Solution(int n) { int ans = n; boolean t = ((ans != 0) && ((ans += Sum_Solution(n - 1)) != 0)); return ans; } "},"zother2-interview/offer/SumOfNDice/":{"url":"zother2-interview/offer/SumOfNDice/","title":"Index","keywords":"","body":" title: 个骰子的点数 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 把 n 个骰子扔在地上，所有骰子朝上一面的和为 s，输入 n，打印 s 所有可能值的概率 解题思路 首先考虑一个骰子的情况，那么有 1～6 出现的次数均为 1 再增加一个骰子时，由于各个点数出现的概率一致。用 f(n,s)=f(n-1,s-1)+f(n-1,s-2)+f(n-1,s-3)+f(n-1,s-4)+f(n-1,s-5)+f(n-1,s-6) 使用两个数组循环求解 public void SumOfNDice(int n) { if (n = 0); k++) { sum += nums[flag][j - k]; } nums[newFlag][j] = sum; } flag = newFlag; } //debug out System.out.println(Arrays.toString(nums[flag])); int sum = 0; for (int i : nums[flag]) { sum += i; } for (int i = 0; i "},"zother2-interview/offer/TranslateNumToStr/":{"url":"zother2-interview/offer/TranslateNumToStr/","title":"Index","keywords":"","body":" title: 把数字翻译成字符串 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 给定一个数字，按照如下规则翻译成字符串：0 翻译成“a”，1 翻译成“b”… 25翻译成“z”。一个数字有多种翻译可能，例如12258一共有5种，分别是bccfi，bwfi，bczi，mcfi，mzi。实现一个函数，用来计算一个数字有多少种不同的翻译方法。 解题思路 定义 f(i) 表示第 i 位有多少种翻译的方法，动态规划方程：f(i)=f(i+1)+g(i,i+1) \\times f(i+2) 其中 g(i,i+1) 表示 i,i+1 是否能组成 10 ~ 25 public int translateNumToStr(int num) { char[] str = String.valueOf(num).toCharArray(); int[] res = new int[str.length]; for (int i = str.length - 1; i >= 0; i--) { if (i + 1 >= str.length) { res[i] = 1; continue; } res[i] = res[i + 1]; if (i + 2 = '1' && str[i + 1] "},"zother2-interview/offer/TreeDepth/":{"url":"zother2-interview/offer/TreeDepth/","title":"Index","keywords":"","body":" title: 二叉树的深度 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。 解题思路 深度优先遍历 public int TreeDepth(TreeNode root) { int[] max = {0}; depth(root, max, 1); return max[0]; } private void depth(TreeNode root, int[] max, int curDepth) { if (root == null) return; if (curDepth > max[0]) max[0] = curDepth; depth(root.left, max, curDepth + 1); depth(root.right, max, curDepth + 1); } "},"zother2-interview/offer/two-stack-fifo/":{"url":"zother2-interview/offer/two-stack-fifo/","title":"Index","keywords":"","body":" title: 用两个栈实现一个队列 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 用两个栈来实现一个队列，完成队列的 Push 和 Pop 操作。 队列中的元素为int类型。 解题思路 用 stack1 作为 push 队列，将元素 push 到 stack1 用 stack2 作为 pop 队列，当 stack2 为空时则将 stack1 的数据 push 到 stack2，否则直接 pop stack2 相当于将两个 stack 拼接：-> stack1 stack2 -> Stack pushStack = new Stack<>(); Stack popStack = new Stack<>(); public void push(int node) { pushStack.push(node); } public int pop() { if (popStack.isEmpty()) { while (!pushStack.isEmpty()) { popStack.push(pushStack.pop()); } } if (popStack.isEmpty()) return -1; else return popStack.pop(); } "},"zother2-interview/offer/VerifySquenceOfBST/":{"url":"zother2-interview/offer/VerifySquenceOfBST/","title":"Index","keywords":"","body":" title: 二叉搜索树的后序遍历序列 date: 2019-08-21T11:00:41+08:00 draft: false categories: offer 题目 牛客网 输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出 Yes ,否则输出 No 。假设输入的数组的任意两个数字都互不相同。 解题思路 后序遍历中，最后一个节点为 root 节点 由于 BST 的左子树都小于 root，右子树都大于 root，那么可以判定该节点是否为 BST 依次类推，通过递归方式，再判定左右子树 public boolean VerifySquenceOfBST(int[] sequence) { if (sequence.length == 0) { return false; } if (sequence.length == 1) { return true; } return isBST(sequence, 0, sequence.length - 1); } private boolean isBST(int[] sequence, int start, int end) { if (start = end) { return true; } int rootV = sequence[end]; int rightIndex = -1, rightV = Integer.MIN_VALUE; for (int i = start; i rootV) { rightV = sequence[i]; rightIndex = i; continue; } if (rightV != Integer.MIN_VALUE && sequence[i] "},"zother2-interview/offer/_index.html":{"url":"zother2-interview/offer/_index.html","title":"Index","keywords":"","body":"剑指Offer 包含 剑指Offer 一直 60 道算法题目 常用技巧 异或运算 删除链表节点时，可通过复制下一个节点的方式减少遍历 保存计算结果来减少重复计算，优化时间效率 分治的思想 滑动窗口 数学建模 "},"zother3-java_interview/":{"url":"zother3-java_interview/","title":"Zother 3 Java Interview","keywords":"","body":"Java面试手册 《Java面试手册》整理了从业到现在看到的、经历过的一些Java面试题。 这些面试题的主要来源是一些网站还有github上的内容，由于平常在收藏到“印象笔记”中的时候没有保留来源出处，如果有介意版权的可以联系我。 github地址为：https://github.com/guanzhenxing/java_interview_manual 主要分为以下部分： Java基础 面向对象 基础 集合 多线程 JVM IO 设计模式 数据结构与算法 算法 数据结构 JavaWeb JavaWeb基础 Spring系列 MyBatis Hibernate 数据库与缓存 数据库基本理论 缓存基本理论 数据库索引 分库分表 MySQL MongoDB Redis 消息队列 MQ基础 分布式 微服务 安全和性能 安全 性能 网络与服务器 计算机网络 Nginx Tomcat Netty 软件工程 UML 业务 操作系统 主要参考： Java面试通关要点汇总集【终极版】 《后端架构师技术图谱》 https://github.com/hadyang/interview https://github.com/crossoverJie/Java-Interview 后台开发常问面试题集锦 "},"zother3-java_interview/business/":{"url":"zother3-java_interview/business/","title":"Index","keywords":"","body":"设计能力 说说你在项目中使用过的 UML 图 你如何考虑组件化 你如何考虑服务化 你如何进行领域建模 你如何划分领域边界 说说你项目中的领域建模 说说概要设计 你系统中的前后端分离是如何做的 说说你的开发流程 你和团队是如何沟通的 你如何进行代码评审 说说你对技术与业务的理解 说说你在项目中经常遇到的 Exception 说说你在项目中遇到感觉最难Bug，怎么解决的 说说你在项目中遇到印象最深困难，怎么解决的 你觉得你们项目还有哪些不足的地方 你是否遇到过 CPU 100% ，如何排查与解决 你是否遇到过 内存 OOM ，如何排查与解决 说说你对敏捷开发的实践 说说你对开发运维的实践 介绍下工作中的一个对自己最有价值的项目，以及在这个过程中的角色 "},"zother3-java_interview/data-structures-and-algorithms/algorithms.html":{"url":"zother3-java_interview/data-structures-and-algorithms/algorithms.html","title":"Algorithms","keywords":"","body":"算法 一致性Hash算法 参考： 一致性Hash算法 一致 Hash 算法 九种内部排序算法的Java实现及其性能测试 参考：九种内部排序算法的Java实现及其性能测试 排序算法汇总 参考：排序算法汇总 查找算法 参考：查找算法 限流算法 参考：限流算法 深度有限算法 广度优先算法 克鲁斯卡尔算法 普林母算法 迪克拉斯算法 "},"zother3-java_interview/data-structures-and-algorithms/data-structures.html":{"url":"zother3-java_interview/data-structures-and-algorithms/data-structures.html","title":"Data Structures","keywords":"","body":"数据结构 树 参考：树 Hash 参考：Hash "},"zother3-java_interview/db-cache/cache_basic.html":{"url":"zother3-java_interview/db-cache/cache_basic.html","title":"Cache Basic","keywords":"","body":"缓存基础 缓存雪崩 缓存雪崩是由于原有缓存失效(过期)，新缓存未到期间。所有请求都去查询数据库，而对数据库CPU和内存造成巨大压力，严重的会造成数据库宕机。从而形成一系列连锁反应，造成整个系统崩溃。 解决方法： 一般并发量不是特别多的时候，使用最多的解决方案是加锁排队。 给每一个缓存数据增加相应的缓存标记，记录缓存的是否失效，如果缓存标记失效，则更新数据缓存。 缓存标记：记录缓存数据是否过期，如果过期会触发通知另外的线程在后台去更新实际key的缓存。 缓存数据：它的过期时间比缓存标记的时间延长1倍，例：标记缓存时间30分钟，数据缓存设置为60分钟。 这样，当缓存标记key过期后，实际缓存还能把旧数据返回给调用端，直到另外的线程在后台更新完成后，才会返回新缓存。 加锁排队方案伪代码： //伪代码 public object GetProductListNew() { int cacheTime = 30; String cacheKey = \"product_list\"; String lockKey = cacheKey; String cacheValue = CacheHelper.get(cacheKey); if (cacheValue != null) { return cacheValue; } else { synchronized(lockKey) { cacheValue = CacheHelper.get(cacheKey); if (cacheValue != null) { return cacheValue; } else { //这里一般是sql查询数据 cacheValue = GetProductListFromDB(); CacheHelper.Add(cacheKey, cacheValue, cacheTime); } } return cacheValue; } } 缓存标记方案伪代码： //伪代码 public object GetProductListNew() { int cacheTime = 30; String cacheKey = \"product_list\"; //缓存标记 String cacheSign = cacheKey + \"_sign\"; String sign = CacheHelper.Get(cacheSign); //获取缓存值 String cacheValue = CacheHelper.Get(cacheKey); if (sign != null) { return cacheValue; //未过期，直接返回 } else { CacheHelper.Add(cacheSign, \"1\", cacheTime); ThreadPool.QueueUserWorkItem((arg) -> { //这里一般是 sql查询数据 cacheValue = GetProductListFromDB(); //日期设缓存时间的2倍，用于脏读 CacheHelper.Add(cacheKey, cacheValue, cacheTime * 2); }); return cacheValue; } } 缓存穿透 缓存穿透是指用户查询数据，在数据库没有，自然在缓存中也不会有。这样就导致用户查询的时候，在缓存中找不到，每次都要去数据库再查询一遍，然后返回空（相当于进行了两次无用的查询）。这样请求就绕过缓存直接查数据库，这也是经常提的缓存命中率问题。 解决方案： 布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。 如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。通过这个直接设置的默认值存放到缓存，这样第二次到缓冲中获取就有值了，而不会继续访问数据库，这种办法最简单粗暴！ 方案二伪代码： //伪代码 public object GetProductListNew() { int cacheTime = 30; String cacheKey = \"product_list\"; String cacheValue = CacheHelper.Get(cacheKey); if (cacheValue != null) { return cacheValue; } cacheValue = CacheHelper.Get(cacheKey); if (cacheValue != null) { return cacheValue; } else { //数据库查询不到，为空 cacheValue = GetProductListFromDB(); if (cacheValue == null) { //如果发现为空，设置个默认值，也缓存起来 cacheValue = string.Empty; } CacheHelper.Add(cacheKey, cacheValue, cacheTime); return cacheValue; } } 缓存预热 缓存预热这个应该是一个比较常见的概念，相信很多小伙伴都应该可以很容易的理解，缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！ 解决思路： 直接写个缓存刷新页面，上线时手工操作下； 数据量不大，可以在项目启动的时候自动进行加载； 定时刷新缓存； 缓存更新 除了缓存服务器自带的缓存失效策略之外，我们还可以根据具体的业务需求进行自定义的缓存淘汰，常见的策略有两种： 定时去清理过期的缓存。 当有用户请求过来时，再判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数据并更新缓存。 缓存降级 当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务。系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级。 降级的最终目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的。 在进行降级之前要对系统进行梳理，看看系统是不是可以丢卒保帅；从而梳理出哪些必须誓死保护，哪些可降级；比如可以参考日志级别设置预案： （1）一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级； （2）警告：有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级，并发送告警； （3）错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级； （4）严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。 参考文章： 缓存雪崩、缓存穿透、缓存预热、缓存更新、缓存降级等问题 "},"zother3-java_interview/db-cache/db_basic.html":{"url":"zother3-java_interview/db-cache/db_basic.html","title":"Db Basic","keywords":"","body":"数据库基础 数据库范式 第一范式：列不可分，eg:【联系人】（姓名，性别，电话），一个联系人有家庭电话和公司电话，那么这种表结构设计就没有达到 1NF； 第二范式：有主键，保证完全依赖。eg:订单明细表【OrderDetail】（OrderID，ProductID，UnitPrice，Discount，Quantity，ProductName），Discount（折扣），Quantity（数量）完全依赖（取决）于主键（OderID，ProductID），而 UnitPrice，ProductName 只依赖于 ProductID，不符合2NF； 第三范式：无传递依赖(非主键列 A 依赖于非主键列 B，非主键列 B 依赖于主键的情况)，eg:订单表【Order】（OrderID，OrderDate，CustomerID，CustomerName，CustomerAddr，CustomerCity）主键是（OrderID），CustomerName，CustomerAddr，CustomerCity 直接依赖的是 CustomerID（非主键列），而不是直接依赖于主键，它是通过传递才依赖于主键，所以不符合 3NF。 什么是反模式 范式可以避免数据冗余，减少数据库的空间，减轻维护数据完整性的麻烦。 然而，通过数据库范式化设计，将导致数据库业务涉及的表变多，并且可能需要将涉及的业务表进行多表连接查询，这样将导致性能变差，且不利于分库分表。因此，出于性能优先的考量，可能在数据库的结构中需要使用反模式的设计，即空间换取时间，采取数据冗余的方式避免表之间的关联查询。至于数据一致性问题，因为难以满足数据强一致性，一般情况下，使存储数据尽可能达到用户一致，保证系统经过一段较短的时间的自我恢复和修正，数据最终达到一致。 需要谨慎使用反模式设计数据库。一般情况下，尽可能使用范式化的数据库设计，因为范式化的数据库设计能让产品更加灵活，并且能在数据库层保持数据完整性。 有的时候，提升性能最好的方法是在同一表中保存冗余数据，如果能容许少量的脏数据，创建一张完全独立的汇总表或缓存表是非常好的方法。举个例子，设计一张“下载次数表”来缓存下载次数信息，可使在海量数据的情况下，提高查询总数信息的速度。 另外一个比较典型的场景，出于扩展性考虑，可能会使用 BLOB 和 TEXT 类型的列存储 JSON 结构的数据，这样的好处在于可以在任何时候，将新的属性添加到这个字段中，而不需要更改表结构。但是，这个设计的缺点也比较明显，就是需要获取整个字段内容进行解码来获取指定的属性，并且无法进行索引、排序、聚合等操作。因此，如果需要考虑更加复杂的使用场景，更加建议使用 MongoDB 这样的文档型数据库。 数据库事务 事务是一个不可分割的数据库操作序列，也是数据库并发控制的基本单位，其执行的结果必须使数据库从一种一致性状态变到另一种一致性状态。 (1). 事务的特征 原子性(Atomicity)：事务所包含的一系列数据库操作要么全部成功执行，要么全部回滚； 一致性(Consistency)：事务的执行结果必须使数据库从一个一致性状态到另一个一致性状态； 隔离性(Isolation)：并发执行的事务之间不能相互影响； 持久性(Durability)：事务一旦提交，对数据库中数据的改变是永久性的。 (2). 事务并发带来的问题 脏读：一个事务读取了另一个事务未提交的数据； 不可重复读：不可重复读的重点是修改，同样条件下两次读取结果不同，也就是说，被读取的数据可以被其它事务修改； 幻读：幻读的重点在于新增或者删除，同样条件下两次读出来的记录数不一样。 (3). 隔离级别 隔离级别决定了一个session中的事务可能对另一个session中的事务的影响。 ANSI标准定义了4个隔离级别，MySQL的InnoDB都支持，分别是： READ UNCOMMITTED（未提交读）：最低级别的隔离，通常又称为dirty read，它允许一个事务读取另一个事务还没commit的数据，这样可能会提高性能，但是会导致脏读问题； READ COMMITTED（提交读）：在一个事务中只允许对其它事务已经commit的记录可见，该隔离级别不能避免不可重复读问题； REPEATABLE READ（可重复读）：在一个事务开始后，其他事务对数据库的修改在本事务中不可见，直到本事务commit或rollback。但是，其他事务的insert/delete操作对该事务是可见的，也就是说，该隔离级别并不能避免幻读问题。在一个事务中重复select的结果一样，除非本事务中update数据库。 SERIALIZABLE（可串行化）：最高级别的隔离，只允许事务串行执行。 MySQL默认的隔离级别是REPEATABLE READ。 脏读 不可重复读 幻读可能性 加锁读 未提交读 YES YES YES NO 提交读 NO YES YES NO 可重复读 NO NO YES NO 可串行化 NO NO NO YES 什么是存储过程？有哪些优缺点？ 存储过程是事先经过编译并存储在数据库中的一段SQL语句的集合。进一步地说，存储过程是由一些T-SQL语句组成的代码块，这些T-SQL语句代码像一个方法一样实现一些功能（对单表或多表的增删改查），然后再给这个代码块取一个名字，在用到这个功能的时候调用他就行了。存储过程具有以下特点： 存储过程只在创建时进行编译，以后每次执行存储过程都不需再重新编译，而一般 SQL 语句每执行一次就编译一次，所以使用存储过程可提高数据库执行效率； 当SQL语句有变动时，可以只修改数据库中的存储过程而不必修改代码； 减少网络传输，在客户端调用一个存储过程当然比执行一串SQL传输的数据量要小； 通过存储过程能够使没有权限的用户在控制之下间接地存取数据库，从而确保数据的安全。 简单说一说drop、delete与truncate的区别 SQL中的drop、delete、truncate都表示删除，但是三者有一些差别： Delete用来删除表的全部或者一部分数据行，执行delete之后，用户需要提交(commmit)或者回滚(rollback)来执行删除或者撤销删除， delete命令会触发这个表上所有的delete触发器； Truncate删除表中的所有数据，这个操作不能回滚，也不会触发这个表上的触发器，TRUNCATE比delete更快，占用的空间更小； Drop命令从数据库中删除表，所有的数据行，索引和权限也会被删除，所有的DML触发器也不会被触发，这个命令也不能回滚。 因此，在不再需要一张表的时候，用drop；在想删除部分数据行时候，用delete；在保留表而删除所有数据的时候用truncate。 什么叫视图？游标是什么？ 视图是一种虚拟的表，通常是有一个表或者多个表的行或列的子集，具有和物理表相同的功能，可以对视图进行增，删，改，查等操作。特别地，对视图的修改不影响基本表。相比多表查询，它使得我们获取数据更容易。 游标是对查询出来的结果集作为一个单元来有效的处理。游标可以定在该单元中的特定行，从结果集的当前行检索一行或多行。可以对结果集当前行做修改。一般不使用游标，但是需要逐条处理数据的时候，游标显得十分重要。 在操作mysql的时候，我们知道MySQL检索操作返回一组称为结果集的行。这组返回的行都是与 SQL语句相匹配的行（零行或多行）。使用简单的 SELECT语句，例如，没有办法得到第一行、下一行或前 10行，也不存在每次一行地处理所有行的简单方法（相对于成批地处理它们）。有时，需要在检索出来的行中前进或后退一行或多行。这就是使用游标的原因。游标（cursor）是一个存储在MySQL服务器上的数据库查询，它不是一条 SELECT语句，而是被该语句检索出来的结果集。在存储了游标之后，应用程序可以根据需要滚动或浏览其中的数据。游标主要用于交互式应用，其中用户需要滚动屏幕上的数据，并对数据进行浏览或做出更改。 什么是触发器？ 触发器是与表相关的数据库对象，在满足定义条件时触发，并执行触发器中定义的语句集合。触发器的这种特性可以协助应用在数据库端确保数据库的完整性。 超键、候选键、主键、外键 超键：在关系中能唯一标识元组的属性集称为关系模式的超键。一个属性可以为作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。 候选键：是最小超键，即没有冗余元素的超键。 主键：数据库表中对储存数据对象予以唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（Null）。 外键：在一个表中存在的另一个表的主键称此表的外键。 什么是事务？什么是锁？ 事务：就是被绑定在一起作为一个逻辑工作单元的 SQL 语句分组，如果任何一个语句操作失败那么整个操作就被失败，以后操作就会回滚到操作前状态，或者是上有个节点。为了确保要么执行，要么不执行，就可以使用事务。要将有组语句作为事务考虑，就需要通过 ACID 测试，即原子性，一致性，隔离性和持久性。 锁：在所以的 DBMS 中，锁是实现事务的关键，锁可以保证事务的完整性和并发性。与现实生活中锁一样，它可以使某些数据的拥有者，在某段时间内不能使用某些数据或数据结构。当然锁还分级别的。 数据库锁机制 数据库锁定机制简单来说就是数据库为了保证数据的一致性而使各种共享资源在被并发访问，访问变得有序所设计的一种规则。MySQL各存储引擎使用了三种类型（级别）的锁定机制：行级锁定，页级锁定和表级锁定。 表级锁定（table-level）：表级别的锁定是MySQL各存储引擎中最大颗粒度的锁定机制。该锁定机制最大的特点是实现逻辑非常简单，带来的系统负面影响最小。所以获取锁和释放锁的速度很快。由于表级锁一次会将整个表锁定，所以可以很好的避免困扰我们的死锁问题。当然，锁定颗粒度大所带来最大的负面影响就是出现锁定资源争用的概率也会最高，致使并大度大打折扣。表级锁分为读锁和写锁。 页级锁定（page-level）：页级锁定的特点是锁定颗粒度介于行级锁定与表级锁之间，所以获取锁定所需要的资源开销，以及所能提供的并发处理能力也同样是介于上面二者之间。另外，页级锁定和行级锁定一样，会发生死锁。 行级锁定（row-level）：行级锁定最大的特点就是锁定对象的颗粒度很小，也是目前各大数据库管理软件所实现的锁定颗粒度最小的。由于锁定颗粒度很小，所以发生锁定资源争用的概率也最小，能够给予应用程序尽可能大的并发处理能力而提高一些需要高并发应用系统的整体性能。虽然能够在并发处理能力上面有较大的优势，但是行级锁定也因此带来了不少弊端。由于锁定资源的颗粒度很小，所以每次获取锁和释放锁需要做的事情也更多，带来的消耗自然也就更大了。此外，行级锁定也最容易发生死锁。InnoDB的行级锁同样分为两种，共享锁和排他锁，同样InnoDB也引入了意向锁（表级锁）的概念，所以也就有了意向共享锁和意向排他锁，所以InnoDB实际上有四种锁，即共享锁（S）、排他锁（X）、意向共享锁（IS）、意向排他锁（IX）； 在MySQL数据库中，使用表级锁定的主要是MyISAM，Memory，CSV等一些非事务性存储引擎，而使用行级锁定的主要是Innodb存储引擎和NDBCluster存储引擎，页级锁定主要是BerkeleyDB存储引擎的锁定方式。 而意向锁的作用就是当一个事务在需要获取资源锁定的时候，如果遇到自己需要的资源已经被排他锁占用的时候，该事务可以需要锁定行的表上面添加一个合适的意向锁。如果自己需要一个共享锁，那么就在表上面添加一个意向共享锁。而如果自己需要的是某行（或者某些行）上面添加一个排他锁的话，则先在表上面添加一个意向排他锁。意向共享锁可以同时并存多个，但是意向排他锁同时只能有一个存在。 共享锁（S） 排他锁（X） 意向共享锁（IS） 意向排他锁（IX） 共享锁（S） 兼容 冲突 兼容 冲突 排他锁（X） 冲突 冲突 冲突 冲突 意向共享锁（IS） 兼容 冲突 兼容 兼容 意向排他锁（IX） 冲突 冲突 兼容 兼容 参考地址：http://www.cnblogs.com/ggjucheng/archive/2012/11/14/2770445.html DDL、DML、DCL分别指什么 左连接、右连接、内连接、外连接、交叉连接、笛卡儿积 "},"zother3-java_interview/db-cache/db-index.html":{"url":"zother3-java_interview/db-cache/db-index.html","title":"Db Index","keywords":"","body":"索引 索引的优点 大大加快数据的检索速度，这也是创建索引的最主要的原因； 加速表和表之间的连接； 在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间； 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性； 什么情况下设置了索引但无法使用？ 以“%(表示任意0个或多个字符)”开头的LIKE语句，模糊匹配； OR语句前后没有同时使用索引； 数据类型出现隐式转化（如varchar不加单引号的话可能会自动转换为int型）； 对于多列索引，必须满足 最左匹配原则 (eg：多列索引col1、col2和col3，则 索引生效的情形包括 col1或col1，col2或col1，col2，col3)。 什么样的字段适合创建索引？ 经常作查询选择的字段 经常作表连接的字段 经常出现在order by, group by, distinct 后面的字段 创建索引时需要注意什么？ 非空字段：应该指定列为NOT NULL，除非你想存储NULL。在mysql中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。你应该用0、一个特殊的值或者一个空串代替空值； 取值离散大的字段：（变量各个取值之间的差异程度）的列放到联合索引的前面，可以通过count()函数查看字段的差异值，返回值越大说明字段的唯一值越多字段的离散程度高； 索引字段越小越好：数据库的数据存储以页为单位一页存储的数据越多一次IO操作获取的数据越大效率越高。 索引的缺点 时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度； 空间方面：索引需要占物理空间。 索引的分类 普通索引和唯一性索引：索引列的值的唯一性 单个索引和复合索引：索引列所包含的列数 聚簇索引与非聚簇索引：聚簇索引按照数据的物理存储进行划分的。对于一堆记录来说，使用聚集索引就是对这堆记录进行堆划分，即主要描述的是物理上的存储。正是因为这种划分方法，导致聚簇索引必须是唯一的。聚集索引可以帮助把很大的范围，迅速减小范围。但是查找该记录，就要从这个小范围中Scan了；而非聚集索引是把一个很大的范围，转换成一个小的地图，然后你需要在这个小地图中找你要寻找的信息的位置，最后通过这个位置，再去找你所需要的记录。 主键、自增主键、主键索引与唯一索引概念区别 主键：指字段 唯一、不为空值 的列； 主键索引：指的就是主键，主键是索引的一种，是唯一索引的特殊类型。创建主键的时候，数据库默认会为主键创建一个唯一索引； 自增主键：字段类型为数字、自增、并且是主键； 唯一索引：索引列的值必须唯一，但允许有空值。主键是唯一索引，这样说没错；但反过来说，唯一索引也是主键就错误了，因为唯一索引允许空值，主键不允许有空值，所以不能说唯一索引也是主键。 主键就是聚集索引吗？主键和索引有什么区别？ 主键是一种特殊的唯一性索引，其可以是聚集索引，也可以是非聚集索引。 在SQLServer中，主键的创建必须依赖于索引，默认创建的是聚集索引，但也可以显式指定为非聚集索引。 InnoDB作为MySQL存储引擎时，默认按照主键进行聚集，如果没有定义主键，InnoDB会试着使用唯一的非空索引来代替。如果没有这种索引，InnoDB就会定义隐藏的主键然后在上面进行聚集。所以，对于聚集索引来说，你创建主键的时候，自动就创建了主键的聚集索引。 索引的底层实现原理和优化 索引是对数据库表中一个或多个列的值进行排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常使用B_TREE及其变种。索引加速了数据访问，因为存储引擎不会再去扫描整张表得到需要的数据；相反，它从根节点开始，根节点保存了子节点的指针，存储引擎会根据指针快速寻找数据。 　　上图显示了一种索引方式。左边是数据库中的数据表，有col1和col2两个字段，一共有15条记录；右边是以col2列为索引列的B_TREE索引，每个节点包含索引的键值和对应数据表地址的指针，这样就可以都过B_TREE在 O(logn) 的时间复杂度内获取相应的数据，这样明显地加快了检索的速度。 在数据结构中，我们最为常见的搜索结构就是二叉搜索树和AVL树(高度平衡的二叉搜索树，为了提高二叉搜索树的效率，减少树的平均搜索长度)了。然而，无论二叉搜索树还是AVL树，当数据量比较大时，都会由于树的深度过大而造成I/O读写过于频繁，进而导致查询效率低下，因此对于索引而言，多叉树结构成为不二选择。特别地，B-Tree的各种操作能使B树保持较低的高度，从而保证高效的查找效率。 B-Tree(平衡多路查找树) B_TREE是一种平衡多路查找树，是一种动态查找效率很高的树形结构。B_TREE中所有结点的孩子结点的最大值称为B_TREE的阶，B_TREE的阶通常用m表示，简称为m叉树。一般来说，应该是m>=3。一颗m阶的B_TREE或是一颗空树，或者是满足下列条件的m叉树： 1)树中每个结点最多有m个孩子结点； 2)若根结点不是叶子节点，则根结点至少有2个孩子结点； 3)除根结点外，其它结点至少有(m/2的上界)个孩子结点； 结点的结构如下图所示，其中，n为结点中关键字个数，(m/2的上界)-1 所有的叶结点都在同一层上，并且不带信息（可以看作是外部结点或查找失败的结点，实际上这些结点不存在，指向这些结点的指针为空）。 下图是一棵4阶B_TREE，4叉树结点的孩子结点的个数范围[2,4]。其中，有2个结点有4个孩子结点，有1个结点有3个孩子结点，有5个结点有2个孩子结点。 B_TREE的查找类似二叉排序树的查找，所不同的是B-树每个结点上是多关键码的有序表，在到达某个结点时，先在有序表中查找，若找到，则查找成功；否则，到按照对应的指针信息指向的子树中去查找，当到达叶子结点时，则说明树中没有对应的关键码。由于B_TREE的高检索效率，B-树主要应用在文件系统和数据库中，对于存储在硬盘上的大型数据库文件，可以极大程度减少访问硬盘次数，大幅度提高数据检索效率。 B+Tree ： InnoDB存储引擎的索引实现 B+Tree是应文件系统所需而产生的一种B_TREE树的变形树。一棵m阶的B+树和m阶的B_TREE的差异在于以下三点： n棵子树的结点中含有n个关键码； 所有的叶子结点中包含了全部关键码的信息，及指向含有这些关键码记录的指针，且叶子结点本身依关键码的大小自小而大的顺序链接； 非终端结点可以看成是索引部分，结点中仅含有其子树根结点中最大（或最小）关键码。 下图为一棵3阶的B+树。通常在B+树上有两个头指针，一个指向根节点，另一个指向关键字最小的叶子节点。因此可以对B+树进行两种查找运算：一种是从最小关键字起顺序查找，另一种是从根节点开始，进行随机查找。 在B+树上进行随机查找、插入和删除的过程基本上与B-树类似。只是在查找时，若非终端结点上的关键码等于给定值，并不终止，而是继续向下直到叶子结点。因此，对于B+树，不管查找成功与否，每次查找都是走了一条从根到叶子结点的路径。 为什么说B+树比B树更适合实际应用中操作系统的文件索引和数据库索引？ B+tree的磁盘读写代价更低：B+tree的内部结点并没有指向关键字具体信息的指针(红色部分)，因此其内部结点相对B 树更小。如果把所有同一内部结点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多，相对来说IO读写次数也就降低了； B+tree的查询效率更加稳定：由于内部结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引，所以，任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当； 数据库索引采用B+树而不是B树的主要原因：B+树只要遍历叶子节点就可以实现整棵树的遍历，而且在数据库中基于范围的查询是非常频繁的，而B树只能中序遍历所有节点，效率太低。 文件索引和数据库索引为什么使用B+树？ 文件与数据库都是需要较大的存储，也就是说，它们都不可能全部存储在内存中，故需要存储到磁盘上。而所谓索引，则为了数据的快速定位与查找，那么索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数，因此B+树相比B树更为合适。数据库系统巧妙利用了局部性原理与磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入，而红黑树这种结构，高度明显要深的多，并且由于逻辑上很近的节点(父子)物理上可能很远，无法利用局部性。最重要的是，B+树还有一个最大的好处：方便扫库。B树必须用中序遍历的方法按序扫库，而B+树直接从叶子结点挨个扫一遍就完了，B+树支持range-query非常方便，而B树不支持，这是数据库选用B+树的最主要原因。 "},"zother3-java_interview/db-cache/":{"url":"zother3-java_interview/db-cache/","title":"Index","keywords":"","body":"数据库 数据库基本理论 缓存基本理论 MySQL MongoDB Redis "},"zother3-java_interview/db-cache/mongodb.html":{"url":"zother3-java_interview/db-cache/mongodb.html","title":"Mongodb","keywords":"","body":"MongoDB 为什么我们要使用MongoDB？ 特点:高性能、易部署、易使用，存储数据非常方便。 主要功能特性有： 面向集合存储，易存储对象类型的数据。 模式自由。 支持动态查询。 支持完全索引，包含内部对象。 支持查询。 支持复制和故障恢复。 使用高效的二进制数据存储，包括大型对象（如视频等）。 自动处理碎片，以支持云计算层次的扩展性 支持Python，PHP，Ruby，Java，C，C#，Javascript，Perl及C++语言的驱动程序，社区中也提供了对Erlang及.NET等平台的驱动程序。 文件存储格式为BSON（一种JSON的扩展）。 可通过网络访问。 功能: 面向集合的存储：适合存储对象及JSON形式的数据。 动态查询：Mongo支持丰富的查询表达式。查询指令使用JSON形式的标记，可轻易查询文档中内嵌的对象及数组。 完整的索引支持：包括文档内嵌对象及数组。Mongo的查询优化器会分析查询表达式，并生成一个高效的查询计划。 查询监视：Mongo包含一个监视工具用于分析数据库操作的性能。 复制及自动故障转移：Mongo数据库支持服务器之间的数据复制，支持主-从模式及服务器之间的相互复制。复制的主要目标是提供冗余及自动故障转移。 高效的传统存储方式：支持二进制数据及大型对象（如照片或图片） 自动分片以支持云级别的伸缩性：自动分片功能支持水平的数据库集群，可动态添加额外的机器。 适用场合: 网站数据：Mongo非常适合实时的插入，更新与查询，并具备网站实时数据存储所需的复制及高度伸缩性。 缓存：由于性能很高，Mongo也适合作为信息基础设施的缓存层。在系统重启之后，由Mongo搭建的持久化缓存层可以避免下层的数据源 过载。 大尺寸，低价值的数据：使用传统的关系型数据库存储一些数据时可能会比较昂贵，在此之前，很多时候程序员往往会选择传统的文件进行存储。 高伸缩性的场景：Mongo非常适合由数十或数百台服务器组成的数据库。Mongo的路线图中已经包含对MapReduce引擎的内置支持。 用于对象及JSON数据的存储：Mongo的BSON数据格式非常适合文档化格式的存储及查询。 MongoDB要注意的问题 因为MongoDB是全索引的，所以它直接把索引放在内存中，因此最多支持2.5G的数据。如果是64位的会更多。 因为没有恢复机制，因此要做好数据备份 因为默认监听地址是127.0.0.1，因此要进行身份验证，否则不够安全；如果是自己使用，建议配置成localhost主机名 通过GetLastError确保变更。（这个不懂，实际中没用过） ObjectId 规则 "},"zother3-java_interview/db-cache/mysql.html":{"url":"zother3-java_interview/db-cache/mysql.html","title":"Mysql","keywords":"","body":"MySQL 实践中如何优化MySQL 实践中，MySQL的优化主要涉及SQL语句及索引的优化、数据表结构的优化、系统配置的优化和硬件的优化四个方面，如下图所示： SQL语句及索引的优化 SQL语句的优化 SQL语句的优化主要包括三个问题，即如何发现有问题的SQL、如何分析SQL的执行计划以及如何优化SQL，下面将逐一解释。 怎么发现有问题的SQL?（通过MySQL慢查询日志对有效率问题的SQL进行监控） MySQL的慢查询日志是MySQL提供的一种日志记录，它用来记录在MySQL中响应时间超过阀值的语句，具体指运行时间超过long_query_time值的SQL，则会被记录到慢查询日志中。 long_query_time的默认值为10，意思是运行10s以上的语句。慢查询日志的相关参数如下所示： 通过MySQL的慢查询日志，我们可以查询出执行的次数多占用的时间长的SQL、可以通过pt_query_disgest(一种mysql慢日志分析工具)分析Rows examine(MySQL执行器需要检查的行数)项去找出IO大的SQL以及发现未命中索引的SQL，对于这些SQL，都是我们优化的对象。 通过explain查询和分析SQL的执行计划 使用 EXPLAIN 关键字可以知道MySQL是如何处理你的SQL语句的，以便分析查询语句或是表结构的性能瓶颈。通过explain命令可以得到表的读取顺序、数据读取操作的操作类型、哪些索引可以使用、哪些索引被实际使用、表之间的引用以及每张表有多少行被优化器查询等问题。当扩展列extra出现Using filesort和Using temporay，则往往表示SQL需要优化了。 优化SQL语句 优化insert语句：一次插入多值； 应尽量避免在 where 子句中使用!=或<>操作符，否则将引擎放弃使用索引而进行全表扫描； 应尽量避免在 where 子句中对字段进行null值判断，否则将导致引擎放弃使用索引而进行全表扫描； 优化嵌套查询：子查询可以被更有效率的连接(Join)替代； 很多时候用 exists 代替 in 是一个好的选择。 索引优化 建议在经常作查询选择的字段、经常作表连接的字段以及经常出现在order by、group by、distinct 后面的字段中建立索引。但必须注意以下几种可能会引起索引失效的情形： 以“%(表示任意0个或多个字符)”开头的LIKE语句，模糊匹配； OR语句前后没有同时使用索引； 数据类型出现隐式转化（如varchar不加单引号的话可能会自动转换为int型）； 对于多列索引，必须满足最左匹配原则(eg,多列索引col1、col2和col3，则 索引生效的情形包括col1或col1，col2或col1，col2，col3)。 数据库表结构的优化 数据库表结构的优化包括选择合适数据类型、表的范式的优化、表的垂直拆分和表的水平拆分等手段。 选择合适数据类型 使用较小的数据类型解决问题； 使用简单的数据类型(mysql处理int要比varchar容易)； 尽可能的使用not null 定义字段； 尽量避免使用text类型，非用不可时最好考虑分表； 表的范式的优化 一般情况下，表的设计应该遵循三大范式。 表的垂直拆分 把含有多个列的表拆分成多个表，解决表宽度问题，具体包括以下几种拆分手段： 把不常用的字段单独放在同一个表中； 把大字段独立放入一个表中； 把经常使用的字段放在一起； 这样做的好处是非常明显的，具体包括：拆分后业务清晰，拆分规则明确、系统之间整合或扩展容易、数据维护简单。 表的水平拆分 表的水平拆分用于解决数据表中数据过大的问题，水平拆分每一个表的结构都是完全一致的。一般地，将数据平分到N张表中的常用方法包括以下两种： 对ID进行hash运算，如果要拆分成5个表，mod(id,5)取出0~4个值； 针对不同的hashID将数据存入不同的表中； 表的水平拆分会带来一些问题和挑战，包括跨分区表的数据查询、统计及后台报表的操作等问题，但也带来了一些切实的好处： 表分割后可以降低在查询时需要读的数据和索引的页数，同时也降低了索引的层数，提高查询速度； 表中的数据本来就有独立性，例如表中分别记录各个地区的数据或不同时期的数据，特别是有些数据常用，而另外一些数据不常用。 需要把数据存放到多个数据库中，提高系统的总体可用性(分库，鸡蛋不能放在同一个篮子里)。 系统配置的优化 操作系统配置的优化：增加TCP支持的队列数 mysql配置文件优化：Innodb缓存池设置(innodb_buffer_pool_size，推荐总内存的75%)和缓存池的个数（innodb_buffer_pool_instances） 硬件的优化 CPU：核心数多并且主频高的 内存：增大内存 磁盘配置和选择：磁盘性能 MySQL中的悲观锁与乐观锁的实现 悲观锁与乐观锁是两种常见的资源并发锁设计思路，也是并发编程中一个非常基础的概念。 悲观锁 悲观锁的特点是先获取锁，再进行业务操作，即“悲观”的认为所有的操作均会导致并发安全问题，因此要先确保获取锁成功再进行业务操作。通常来讲，在数据库上的悲观锁需要数据库本身提供支持，即通过常用的select … for update操作来实现悲观锁。当数据库执行select … for update时会获取被select中的数据行的行锁，因此其他并发执行的select … for update如果试图选中同一行则会发生排斥（需要等待行锁被释放），因此达到锁的效果。select for update获取的行锁会在当前事务结束时自动释放，因此必须在事务中使用。 　　 这里需要特别注意的是，不同的数据库对select… for update的实现和支持都是有所区别的，例如oracle支持select for update no wait，表示如果拿不到锁立刻报错，而不是等待，mysql就没有no wait这个选项。另外，mysql还有个问题是: select… for update语句执行中所有扫描过的行都会被锁上，这一点很容易造成问题。因此，如果在mysql中用悲观锁务必要确定使用了索引，而不是全表扫描。 乐观锁 乐观锁的特点先进行业务操作，只在最后实际更新数据时进行检查数据是否被更新过，若未被更新过，则更新成功；否则，失败重试。乐观锁在数据库上的实现完全是逻辑的，不需要数据库提供特殊的支持。一般的做法是在需要锁的数据上增加一个版本号或者时间戳，然后按照如下方式实现： SELECT data AS old_data, version AS old_version FROM …; //根据获取的数据进行业务操作，得到new_data和new_version UPDATE SET data = new_data, version = new_version WHERE version = old_version if (updated row > 0) { // 乐观锁获取成功，操作完成 } else { // 乐观锁获取失败，回滚并重试 } 乐观锁是否在事务中其实都是无所谓的，其底层机制是这样：在数据库内部update同一行的时候是不允许并发的，即数据库每次执行一条update语句时会获取被update行的写锁，直到这一行被成功更新后才释放。因此在业务操作进行前获取需要锁的数据的当前版本号，然后实际更新数据时再次对比版本号确认与之前获取的相同，并更新版本号，即可确认这其间没有发生并发的修改。如果更新失败，即可认为老版本的数据已经被并发修改掉而不存在了，此时认为获取锁失败，需要回滚整个业务操作并可根据需要重试整个过程。 悲观锁与乐观锁的应用场景 一般情况下，读多写少更适合用乐观锁，读少写多更适合用悲观锁。乐观锁在不发生取锁失败的情况下开销比悲观锁小，但是一旦发生失败回滚开销则比较大，因此适合用在取锁失败概率比较小的场景，可以提升系统并发性能。 MySQL存储引擎中的MyISAM和InnoDB区别详解 在MySQL 5.5之前，MyISAM是mysql的默认数据库引擎，其由早期的ISAM（Indexed Sequential Access Method：有索引的顺序访问方法）所改良。虽然MyISAM性能极佳，但却有一个显著的缺点： 不支持事务处理。不过，MySQL也导入了另一种数据库引擎InnoDB，以强化参考完整性与并发违规处理机制，后来就逐渐取代MyISAM。 InnoDB是MySQL的数据库引擎之一，其由Innobase oy公司所开发，2006年五月由甲骨文公司并购。与传统的ISAM、MyISAM相比，InnoDB的最大特色就是支持ACID兼容的事务功能，类似于PostgreSQL。目前InnoDB采用双轨制授权，一是GPL授权，另一是专有软件授权。具体地，MyISAM与InnoDB作为MySQL的两大存储引擎的差异主要包括： 存储结构：每个MyISAM在磁盘上存储成三个文件：第一个文件的名字以表的名字开始，扩展名指出文件类型。.frm文件存储表定义，数据文件的扩展名为.MYD (MYData)，索引文件的扩展名是.MYI (MYIndex)。InnoDB所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB。 存储空间：MyISAM可被压缩，占据的存储空间较小，支持静态表、动态表、压缩表三种不同的存储格式。InnoDB需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。 可移植性、备份及恢复：MyISAM的数据是以文件的形式存储，所以在跨平台的数据转移中会很方便，同时在备份和恢复时也可单独针对某个表进行操作。InnoDB免费的方案可以是拷贝数据文件、备份 binlog，或者用 mysqldump，在数据量达到几十G的时候就相对痛苦了。 事务支持：MyISAM强调的是性能，每次查询具有原子性，其执行数度比InnoDB类型更快，但是不提供事务支持。InnoDB提供事务、外键等高级数据库功能，具有事务提交、回滚和崩溃修复能力。 AUTO_INCREMENT：在MyISAM中，可以和其他字段一起建立联合索引。引擎的自动增长列必须是索引，如果是组合索引，自动增长可以不是第一列，它可以根据前面几列进行排序后递增。InnoDB中必须包含只有该字段的索引，并且引擎的自动增长列必须是索引，如果是组合索引也必须是组合索引的第一列。 表锁差异：MyISAM只支持表级锁，用户在操作MyISAM表时，select、update、delete和insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。InnoDB支持事务和行级锁。行锁大幅度提高了多用户并发操作的新能，但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的。 全文索引：MyISAM支持 FULLTEXT类型的全文索引；InnoDB不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。 表主键：MyISAM允许没有任何索引和主键的表存在，索引都是保存行的地址。对于InnoDB，如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)，数据是主索引的一部分，附加索引保存的是主索引的值。 表的具体行数：MyISAM保存表的总行数，select count() from table;会直接取出出该值；而InnoDB没有保存表的总行数，如果使用select count() from table；就会遍历整个表，消耗相当大，但是在加了wehre条件后，myisam和innodb处理的方式都一样。 CURD操作：在MyISAM中，如果执行大量的SELECT，MyISAM是更好的选择。对于InnoDB，如果你的数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表。DELETE从性能上InnoDB更优，但DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的删除，在innodb上如果要清空保存有大量数据的表，最好使用truncate table这个命令。 外键：MyISAM不支持外键，而InnoDB支持外键。 通过上述的分析，基本上可以考虑使用InnoDB来替代MyISAM引擎了，原因是InnoDB自身很多良好的特点，比如事务支持、存储过程、视图、行级锁、外键等等。尤其在并发很多的情况下，相信InnoDB的表现肯定要比MyISAM强很多。另外，必须需要注意的是，任何一种表都不是万能的，合适的才是最好的，才能最大的发挥MySQL的性能优势。如果是不复杂的、非关键的Web应用，还是可以继续考虑MyISAM的，这个具体情况具体考虑。 MyISAM：不支持事务，不支持外键，表锁；插入数据时锁定整个表，查行数时无需整表扫描。主索引数据文件和索引文件分离；与主索引无区别； InnoDB：支持事务，外键，行锁，查表总行数时，全表扫描；主索引的数据文件本身就是索引文件；辅助索引记录主键的值； MySQL锁类型　　 根据锁的类型分，可以分为共享锁，排他锁，意向共享锁和意向排他锁。 根据锁的粒度分，又可以分为行锁，表锁。 对于mysql而言，事务机制更多是靠底层的存储引擎来实现，因此，mysql层面只有表锁，而支持事务的innodb存 储引擎则实现了行锁(记录锁（在行相应的索引记录上的锁）)，gap锁（是在索引记录间歇上的锁），next-key锁（是记录锁和在此索引记录之前的gap上的锁的结合）。Mysql的记录锁实质是索引记录的锁，因为innodb是索引组织表；gap锁是索引记录间隙的锁，这种锁只在RR隔离级别下有效；next-key锁是记录锁加上记录之前gap锁的组合。mysql通过gap锁和next-key锁实现RR隔离级别。 说明：对于更新操作(读不上锁)，只有走索引才可能上行锁；否则会对聚簇索引的每一行上写锁，实际等同于对表上写锁。 　　 若多个物理记录对应同一个索引，若同时访问，也会出现锁冲突； 当表有多个索引时，不同事务可以用不同的索引锁住不同的行，另外innodb会同时用行锁对数据记录(聚簇索引)加锁。 MVCC(多版本并发控制)并发控制机制下，任何操作都不会阻塞读操作，读操作也不会阻塞任何操作，只因为读不上锁。 　　 共享锁：由读表操作加上的锁，加锁后其他用户只能获取该表或行的共享锁，不能获取排它锁，也就是说只能读不能写 排它锁：由写表操作加上的锁，加锁后其他用户不能获取该表或行的任何锁，典型是mysql事务中的更新操作。 意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的IS锁。 意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的IX锁。 数据库死锁概念 多数情况下，可以认为如果一个资源被锁定，它总会在以后某个时间被释放。而死锁发生在当多个进程访问同一数据库时，其中每个进程拥有的锁都是其他进程所需的，由此造成每个进程都无法继续下去。简单的说，进程A等待进程B释放他的资源，B又等待A释放他的资源，这样就互相等待就形成死锁。 虽然进程在运行过程中，可能发生死锁，但死锁的发生也必须具备一定的条件，死锁的发生必须具备以下四个必要条件： 1）互斥条件：指进程对所分配到的资源进行排它性使用，即在一段时间内某资源只由一个进程占用。如果此时还有其它进程请求资源，则请求者只能等待，直至占有资源的进程用毕释放。 　　 2）请求和保持条件：指进程已经保持至少一个资源，但又提出了新的资源请求，而该资源已被其它进程占有，此时请求进程阻塞，但又对自己已获得的其它资源保持不放。 　　 3）不剥夺条件：指进程已获得的资源，在未使用完之前，不能被剥夺，只能在使用完时由自己释放。 　　 4）环路等待条件：指在发生死锁时，必然存在一个进程——资源的环形链，即进程集合{P0，P1，P2，•••，Pn}中的P0正在等待一个P1占用的资源；P1正在等待P2占用的资源，……，Pn正在等待已被P0占用的资源。 　　 下列方法有助于最大限度地降低死锁： 按同一顺序访问对象。 避免事务中的用户交互。 保持事务简短并在一个批处理中。 使用低隔离级别。 使用绑定连接。 千万级MySQL数据库建立索引的事项及提高性能的手段 对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。 应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如：select id from t where num is null可以在num上设置默认值0，确保表中num列没有null值，然后这样查询：select id from t where num=0 应尽量避免在 where 子句中使用!=或<>操作符，否则引擎将放弃使用索引而进行全表扫描。 应尽量避免在 where 子句中使用or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如：select id from t where num=10 or num=20可以这样查询：select id from t where num=10 union all select id from t where num=20 in 和 not in 也要慎用，否则会导致全表扫描，如：select id from t where num in(1,2,3) 对于连续的数值，能用 between 就不要用 in 了：select id from t where num between 1 and 3 避免使用通配符。下面的查询也将导致全表扫描：select id from t where name like ‘李%’若要提高效率，可以考虑全文检索。 如果在 where 子句中使用参数，也会导致全表扫描。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描：select id from t where num=@num可以改为强制查询使用索引：select id from t with(index(索引名)) where num=@num 应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如：select id from t where num/2=100应改为:select id from t where num=100*2 应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如：select id from t where substring(name,1,3)=’abc’ ，name以abc开头的id应改为:select id from t where name like ‘abc%’ 不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。 在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。 不要写一些没有意义的查询，如需要生成一个空表结构：select col1,col2 into #t from t where 1=0 这类代码不会返回任何结果集，但是会消耗系统资源的，应改成这样：create table #t(…) 很多时候用 exists 代替 in 是一个好的选择：select num from a where num in(select num from b)用下面的语句替换：select num from a where exists(select 1 from b where num=a.num) 并不是所有索引对查询都有效，SQL是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL查询可能不会去利用索引，如一表中有字段sex，male、female几乎各一半，那么即使在sex上建了索引也对查询效率起不了作用。 索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了insert 及 update 的 效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有 必要。 应尽可能的避免更新 clustered 索引数据列，因为 clustered 索引数据列的顺序就是表记录的物理存储 顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要频繁更新 clustered 索引数据列，那么需要考虑是否应将该索引建为 clustered 索引。 尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。 尽可能的使用 varchar/nvarchar 代替 char/nchar ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。 任何地方都不要使用 select * from t ，用具体的字段列表代替“*”，不要返回用不到的任何字段。 尽量使用表变量来代替临时表。如果表变量包含大量数据，请注意索引非常有限（只有主键索引）。 避免频繁创建和删除临时表，以减少系统表资源的消耗。 临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件，最好使用导出表。 在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度；如果数据量不大，为了缓和系统表的资源，应先create table，然后insert。 如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。 尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。 使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。 与临时表一样，游标并不是不可使用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。 在所有的存储过程和触发器的开始处设置 SET NOCOUNT ON ，在结束时设置 SET NOCOUNT OFF。无需在执行存储过程和触发器的每个语句后向客户端发送DONE_IN_PROC 消息。 尽量避免大事务操作，提高系统并发能力。 尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。 limit 20000 加载很慢怎么解决 树状结构的MYSQL存储 "},"zother3-java_interview/db-cache/redis.html":{"url":"zother3-java_interview/db-cache/redis.html","title":"Redis","keywords":"","body":"Redis 使用redis有哪些好处？ 速度快，因为数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1) 支持丰富数据类型，支持string，list，set，sorted set，hash 支持事务，操作都是原子性，所谓的原子性就是对数据的更改要么全部执行，要么全部不执行 丰富的特性：可用于缓存，消息，按key设置过期时间，过期后将会自动删除 redis相比memcached有哪些优势？ memcached所有的值均是简单的字符串，redis作为其替代者，支持更为丰富的数据类型 redis的速度比memcached快很多 redis可以持久化其数据 redis常见性能问题和解决方案： Master最好不要做任何持久化工作，如RDB内存快照和AOF日志文件 如果数据比较重要，某个Slave开启AOF备份数据，策略设置为每秒同步一次 为了主从复制的速度和连接的稳定性，Master和Slave最好在同一个局域网内 尽量避免在压力很大的主库上增加从库 主从复制不要用图状结构，用单向链表结构更为稳定，即：Master 这样的结构方便解决单点故障问题，实现Slave对Master的替换。如果Master挂了，可以立刻启用Slave1做Master，其他不变。 redis 最适合的场景 Redis最适合所有数据in-momory的场景，虽然Redis也提供持久化功能，但实际更多的是一个disk-backed的功能，跟传统意义上的持久化有比较大的差别，那么可能大家就会有疑问，似乎Redis更像一个加强版的Memcached，那么何时使用Memcached,何时使用Redis呢? 如果简单地比较Redis与Memcached的区别，大多数都会得到以下观点： Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 Redis支持数据的备份，即master-slave模式的数据备份。 Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。 （1）会话缓存（Session Cache） 最常用的一种使用Redis的情景是会话缓存（session cache）。用Redis缓存会话比其他存储（如Memcached）的优势在于：Redis提供持久化。当维护一个不是严格要求一致性的缓存时，如果用户的购物车信息全部丢失，大部分人都会不高兴的，现在，他们还会这样吗？ 幸运的是，随着 Redis 这些年的改进，很容易找到怎么恰当的使用Redis来缓存会话的文档。甚至广为人知的商业平台Magento也提供Redis的插件。 （2）全页缓存（FPC） 除基本的会话token之外，Redis还提供很简便的FPC平台。回到一致性问题，即使重启了Redis实例，因为有磁盘的持久化，用户也不会看到页面加载速度的下降，这是一个极大改进，类似PHP本地FPC。 再次以Magento为例，Magento提供一个插件来使用Redis作为全页缓存后端。 此外，对WordPress的用户来说，Pantheon有一个非常好的插件 wp-redis，这个插件能帮助你以最快速度加载你曾浏览过的页面。 （3）队列 Reids在内存存储引擎领域的一大优点是提供 list 和 set 操作，这使得Redis能作为一个很好的消息队列平台来使用。Redis作为队列使用的操作，就类似于本地程序语言（如Python）对 list 的 push/pop 操作。 如果你快速的在Google中搜索“Redis queues”，你马上就能找到大量的开源项目，这些项目的目的就是利用Redis创建非常好的后端工具，以满足各种队列需求。例如，Celery有一个后台就是使用Redis作为broker，你可以从这里去查看。 （4）排行榜/计数器 Redis在内存中对数字进行递增或递减的操作实现的非常好。集合（Set）和有序集合（Sorted Set）也使得我们在执行这些操作的时候变的非常简单，Redis只是正好提供了这两种数据结构。所以，我们要从排序集合中获取到排名最靠前的10个用户–我们称之为“user_scores”，我们只需要像下面一样执行即可： 当然，这是假定你是根据你用户的分数做递增的排序。如果你想返回用户及用户的分数，你需要这样执行： ZRANGE user_scores 0 10 WITHSCORES Agora Games就是一个很好的例子，用Ruby实现的，它的排行榜就是使用Redis来存储数据的，你可以在这里看到。 （5）发布/订阅 最后（但肯定不是最不重要的）是Redis的发布/订阅功能。发布/订阅的使用场景确实非常多。我已看见人们在社交网络连接中使用，还可作为基于发布/订阅的脚本触发器，甚至用Redis的发布/订阅功能来建立聊天系统！（不，这是真的，你可以去核实）。 Redis提供的所有特性中，我感觉这个是喜欢的人最少的一个，虽然它为用户提供如果此多功能。 redis的一些其他特点 （1）Redis是单进程单线程的 redis利用队列技术将并发访问变为串行访问，消除了传统数据库串行控制的开销 （2）读写分离模型 通过增加Slave DB的数量，读的性能可以线性增长。为了避免Master DB的单点故障，集群一般都会采用两台Master DB做双机热备，所以整个集群的读和写的可用性都非常高。 读写分离架构的缺陷在于，不管是Master还是Slave，每个节点都必须保存完整的数据，如果在数据量很大的情况下，集群的扩展能力还是受限于单个节点的存储能力，而且对于Write-intensive类型的应用，读写分离架构并不适合。 （3）数据分片模型 为了解决读写分离模型的缺陷，可以将数据分片模型应用进来。 可以将每个节点看成都是独立的master，然后通过业务实现数据分片。 结合上面两种模型，可以将每个master设计成由一个master和多个slave组成的模型。 （4）Redis的回收策略 volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-enviction（驱逐）：禁止驱逐数据 注意这里的6种机制，volatile和allkeys规定了是对已设置过期时间的数据集淘汰数据还是从全部数据集淘汰数据，后面的lru、ttl以及random是三种不同的淘汰策略，再加上一种no-enviction永不回收的策略。 使用策略规则： 如果数据呈现幂律分布，也就是一部分数据访问频率高，一部分数据访问频率低，则使用allkeys-lru 如果数据呈现平等分布，也就是所有的数据访问频率都相同，则使用allkeys-random mySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据 相关知识：redis 内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略。redis提供6种数据淘汰策略见上面一条 假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如果将它们全部找出来？ 使用keys指令可以扫出指定模式的key列表。 对方接着追问：如果这个redis正在给线上的业务提供服务，那使用keys指令会有什么问题？ 这个时候你要回答redis关键的一个特性：redis的单线程的。keys指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用scan指令，scan指令可以无阻塞的提取出指定模式的key列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长。 Redis 常见的性能问题都有哪些？如何解决？ Master写内存快照，save命令调度rdbSave函数，会阻塞主线程的工作，当快照比较大时对性能影响是非常大的，会间断性暂停服务，所以Master最好不要写内存快照。 Master AOF持久化，如果不重写AOF文件，这个持久化方式对性能的影响是最小的，但是AOF文件会不断增大，AOF文件过大会影响Master重启的恢复速度。Master最好不要做任何持久化工作，包括内存快照和AOF日志文件，特别是不要启用内存快照做持久化,如果数据比较关键，某个Slave开启AOF备份数据，策略为每秒同步一次。 Master调用BGREWRITEAOF重写AOF文件，AOF在重写的时候会占大量的CPU和内存资源，导致服务load过高，出现短暂服务暂停现象。 Redis主从复制的性能问题，为了主从复制的速度和连接的稳定性，Slave和Master最好在同一个局域网内 Redis有哪些数据结构？ 字符串String、字典Hash、列表List、集合Set、有序集合SortedSet。 如果你是Redis中高级用户，还需要加上下面几种数据结构HyperLogLog、Geo、Pub/Sub。 如果你说还玩过Redis Module，像BloomFilter，RedisSearch，Redis-ML，面试官得眼睛就开始发亮了。 使用过Redis分布式锁么，它是什么回事？ 先拿setnx来争抢锁，抢到之后，再用expire给锁加一个过期时间防止锁忘记了释放。 这时候对方会告诉你说你回答得不错，然后接着问如果在setnx之后执行expire之前进程意外crash或者要重启维护了，那会怎么样？ 这时候你要给予惊讶的反馈：唉，是喔，这个锁就永远得不到释放了。紧接着你需要抓一抓自己得脑袋，故作思考片刻，好像接下来的结果是你主动思考出来的，然后回答：我记得set指令有非常复杂的参数，这个应该是可以同时把setnx和expire合成一条指令来用的！对方这时会显露笑容，心里开始默念：摁，这小子还不错。 使用过Redis做异步队列么，你是怎么用的？ 一般使用list结构作为队列，rpush生产消息，lpop消费消息。当lpop没有消息的时候，要适当sleep一会再重试。 如果对方追问可不可以不用sleep呢？list还有个指令叫blpop，在没有消息的时候，它会阻塞住直到消息到来。 如果对方追问能不能生产一次消费多次呢？使用pub/sub主题订阅者模式，可以实现1:N的消息队列。 如果对方追问pub/sub有什么缺点？在消费者下线的情况下，生产的消息会丢失，得使用专业的消息队列如rabbitmq等。 如果对方追问redis如何实现延时队列？我估计现在你很想把面试官一棒打死如果你手上有一根棒球棍的话，怎么问的这么详细。但是你很克制，然后神态自若的回答道：使用sortedset，拿时间戳作为score，消息内容作为key调用zadd来生产消息，消费者用zrangebyscore指令获取N秒之前的数据轮询进行处理。 到这里，面试官暗地里已经对你竖起了大拇指。但是他不知道的是此刻你却竖起了中指，在椅子背后。 如果有大量的key需要设置同一时间过期，一般需要注意什么？ 如果大量的key过期时间设置的过于集中，到过期的那个时间点，redis可能会出现短暂的卡顿现象。一般需要在时间上加一个随机值，使得过期时间分散一些。 为什么Redis需要把所有数据放到内存中？ Redis为了达到最快的读写速度将数据都读到内存中，并通过异步的方式将数据写入磁盘。所以redis具有快速和数据持久化的特征。如果不将数据放在内存中，磁盘I/O速度为严重影响redis的性能。在内存越来越便宜的今天，redis将会越来越受欢迎。 如果设置了最大使用的内存，则数据已有记录数达到内存限值后不能继续插入新值。 Redis 持久化机制 bgsave做镜像全量持久化，aof做增量持久化。因为bgsave会耗费较长时间，不够实时，在停机的时候会导致大量丢失数据，所以需要aof来配合使用。在redis实例重启时，会使用bgsave持久化文件重新构建内存，再使用aof重放近期的操作指令来实现完整恢复重启之前的状态。 对方追问那如果突然机器掉电会怎样？取决于aof日志sync属性的配置，如果不要求性能，在每条写指令时都sync一下磁盘，就不会丢失数据。但是在高性能的要求下每次都sync是不现实的，一般都使用定时sync，比如1s1次，这个时候最多就会丢失1s的数据。 对方追问bgsave的原理是什么？你给出两个词汇就可以了，fork和cow。fork是指redis通过创建子进程来进行bgsave操作，cow指的是copy on write，子进程创建后，父子进程共享数据段，父进程继续提供读写服务，写脏的页面数据会逐渐和子进程分离开来。 Redis提供了哪几种持久化方式？ RDB持久化方式能够在指定的时间间隔能对你的数据进行快照存储。 AOF持久化方式记录每次对服务器写的操作，当服务器重启的时候会重新执行这些命令来恢复原始的数据，AOF命令以redis协议追加保存每次写的操作到文件末尾。Redis还能对AOF文件进行后台重写，使得AOF文件的体积不至于过大。 如果你只希望你的数据在服务器运行的时候存在，你也可以不使用任何持久化方式。 你也可以同时开启两种持久化方式， 在这种情况下， 当redis重启的时候会优先载入AOF文件来恢复原始的数据，因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整。 最重要的事情是了解RDB和AOF持久化方式的不同，让我们以RDB持久化方式开始。 如何选择合适的持久化方式？ 一般来说， 如果想达到足以媲美PostgreSQL的数据安全性， 你应该同时使用两种持久化功能。如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失，那么你可以只使用RDB持久化。 有很多用户都只使用AOF持久化，但并不推荐这种方式：因为定时生成RDB快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比AOF恢复的速度要快，除此之外， 使用RDB还可以避免之前提到的AOF程序的bug。 Pipeline有什么好处，为什么要用pipeline？ 可以将多次IO往返的时间缩减为一次，前提是pipeline执行的指令之间没有因果相关性。使用redis-benchmark进行压测的时候可以发现影响redis的QPS峰值的一个重要因素是pipeline批次指令的数目。 Redis的同步机制了解么？ Redis可以使用主从同步，从从同步。第一次同步时，主节点做一次bgsave，并同时将后续修改操作记录到内存buffer，待完成后将rdb文件全量同步到复制节点，复制节点接受完成后将rdb镜像加载到内存。加载完成后，再通知主节点将期间修改的操作记录同步到复制节点进行重放就完成了同步过程。 Redis 集群方案与实现 Redis Sentinal着眼于高可用，在master宕机时会自动将slave提升为master，继续提供服务。 Redis Cluster着眼于扩展性，在单个redis内存不足时，使用Cluster进行分片存储。 一个Redis实例最多能存放多少的keys？List、Set、Sorted Set他们最多能存放多少元素？ 理论上Redis可以处理多达232的keys，并且在实际中进行了测试，每个实例至少存放了2亿5千万的keys。我们正在测试一些较大的值。 任何list、set、和sorted set都可以放232个元素。 换句话说，Redis的存储极限是系统中的可用内存值。 Redis持久化数据和缓存怎么做扩容？ 如果Redis被当做缓存使用，使用一致性哈希实现动态扩容缩容。 如果Redis被当做一个持久化存储使用，必须使用固定的keys-to-nodes映射关系，节点的数量一旦确定不能变化。否则的话(即Redis节点需要动态变化的情况），必须使用可以在运行时进行数据再平衡的一套系统，而当前只有Redis集群可以做到这样。 Redis 为什么是单线程的 "},"zother3-java_interview/db-cache/sharding.html":{"url":"zother3-java_interview/db-cache/sharding.html","title":"Sharding","keywords":"","body":"分库分表 说说分库与分表设计 面对海量数据，例如，上千万甚至上亿的数据，查询一次所花费的时间会变长，甚至会造成数据库的单点压力。因此，分库与分表的目的在于，减小数据库的单库单表负担，提高查询性能，缩短查询时间。 分表概述 随着用户数的不断增加，以及数据量的不断增加，会使得单表压力越来越大，面对上千万甚至上亿的数据，查询一次所花费的时间会变长，如果有联合查询的情况下，甚至可能会成为很大的瓶颈。此外，MySQL 存在表锁和行锁，因此更新表数据可能会引起表锁或者行锁，这样也会导致其他操作等待，甚至死锁问题。 通过分表，可以减少数据库的单表负担，将压力分散到不同的表上，同时因为不同的表上的数据量少了，起到提高查询性能，缩短查询时间的作用，此外，可以很大的缓解表锁的问题。 分表策略可以归纳为垂直拆分和水平拆分。 垂直拆分，把表的字段进行拆分，即一张字段比较多的表拆分为多张表，这样使得行数据变小。一方面，可以减少客户端程序和数据库之间的网络传输的字节数，因为生产环境共享同一个网络带宽，随着并发查询的增多，有可能造成带宽瓶颈从而造成阻塞。另一方面，一个数据块能存放更多的数据，在查询时就会减少 I/O 次数。举个例子，假设用户表中有一个字段是家庭地址，这个字段是可选字段，在数据库操作的时候除了个人信息外，并不需要经常读取或是更改这个字段的值。在这种情况下，更建议把它拆分到另外一个表，从而提高性能。 如何设计好垂直拆分，我的建议： 将不常用的字段单独拆分到另外一张扩展表，例如前面讲解到的用户家庭地址，这个字段是可选字段，在数据库操作的时候除了个人信息外，并不需要经常读取或是更改这个字段的值。 将大文本的字段单独拆分到另外一张扩展表，例如 BLOB 和 TEXT 字符串类型的字段，以及 TINYBLOB、 MEDIUMBLOB、 LONGBLOB、 TINYTEXT、 MEDIUMTEXT、 LONGTEXT字符串类型。这样可以减少客户端程序和- 数据库之间的网络传输的字节数。 将不经常修改的字段放在同一张表中，将经常改变的字段放在另一张表中。举个例子，假设用户表的设计中，还存在“最后登录时间”字段，每次用户登录时会被更新。这张用户表会存在频繁的更新操作，此外，每次更新时会导致该表的查询缓存被清空。所以，可以把这个字段放到另一个表中，这样查询缓存会增加很多性能。 对于需要经常关联查询的字段，建议放在同一张表中。不然在联合查询的情况下，会带来数据库额外压力。 水平拆分，把表的行进行拆分。因为表的行数超过几百万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。水平拆分，有许多策略，例如，取模分表，时间维度分表，以及自定义 Hash 分表，例如用户 ID 维度分表等。在不同策略分表情况下，根据各自的策略写入与读取。 实际上，垂直拆分后的表依然存在单表数据量过大的问题，需要进行水平拆分。因此，实际情况中，水平拆分往往会和垂直拆分结合使用。假设，随着用户数的不断增加，用户表单表存在上千万的数据，这时可以把一张用户表的数据拆成多张用户表来存放。 常见的水平分表策略归纳起来，可以总结为随机分表和连续分表两种情况。例如，取模分表就属于随机分表，而时间维度分表则属于连续分表。 连续分表可以快速定位到表进行高效查询，大多数情况下，可以有效避免跨表查询。如果想扩展，只需要添加额外的分表就可以了，无需对其他分表的数据进行数据迁移。但是，连续分表有可能存在数据热点的问题，有些表可能会被频繁地查询从而造成较大压力，热数据的表就成为了整个库的瓶颈，而有些表可能存的是历史数据，很少需要被查询到。 随机分表是遵循规则策略进行写入与读取，而不是真正意义上的随机。通常，采用取模分表或者自定义 Hash 分表的方式进行水平拆分。随机分表的数据相对比较均匀，不容易出现热点和并发访问的瓶颈。但是，分表扩展需要迁移旧的数据。此外，随机分表比较容易面临跨表查询的复杂问题。 对于日志场景，可以考虑根据时间维度分表，例如年份维度分表或者月份维度分表，在日志记录表的名字中包含年份和月份的信息，例如 log_2017_01，这样可以在已经没有新增操作的历史表上做频繁地查询操作，而不会影响时间维度分表上新增操作。 对于海量用户场景，可以考虑取模分表，数据相对比较均匀，不容易出现热点和并发访问的瓶颈。 对于租户场景，可以考虑租户维度分表，不同的租户数据独立，而不应该在每张表中添加租户 ID，这是一个不错的选择。 分库概述 库内分表，仅仅是解决了单表数据过大的问题，但并没有把单表的数据分散到不同的物理机上，因此并不能减轻 MySQL 服务器的压力，仍然存在同一个物理机上的资源竞争和瓶颈，包括 CPU、内存、磁盘 IO、网络带宽等。 分库策略也可以归纳为垂直拆分和水平拆分。 垂直拆分，按照业务和功能划分，把数据分别放到不同的数据库中。举个例子，可以划分资讯库、百科库等。 水平拆分，把一张表的数据划分到不同的数据库，两个数据库的表结构一样。实际上，水平分库与水平分表类似，水平拆分有许多策略，例如，取模分库，自定义 Hash 分库等，在不同策略分库情况下，根据各自的策略写入与读取。举个例子，随着业务的增长，资讯库的单表数据过大，此时采取水平拆分策略，根据取模分库。 以上文字来源：服务端指南 数据存储篇 | MySQL（08） 分库与分表设计 分库与分表带来的分布式困境与应对之策 数据迁移与扩容问题 表关联问题 分页与排序问题 分布式事务问题 分布式全局唯一ID 选择合适的分布式主键方案 如何设计可以动态扩容缩容的分库分表方案？ 用过哪些分库分表中间件，有啥优点和缺点？讲一下你了解的分库分表中间件的底层实现原理？ "},"zother3-java_interview/design-pattern/":{"url":"zother3-java_interview/design-pattern/","title":"Index","keywords":"","body":"设计模式 简述一下你了解的设计模式。 所谓设计模式，就是一套被反复使用的代码设计经验的总结（情境中一个问题经过证实的一个解决方案）。使用设计模式是为了可重用代码、让代码更容易被他人理解、保证代码可靠性。设计模式使人们可以更加简单方便的复用成功的设计和体系结构。将已证实的技术表述成设计模式也会使新系统开发者更加容易理解其设计思路。 在GoF的《Design Patterns: Elements of Reusable Object-Oriented Software》中给出了三类: 创建型[对类的实例化过程的抽象化] 结构型[描述如何将类或对象结合在一起形成更大的结构] 行为型[对在不同的对象之间划分责任和算法的抽象化] 共23种设计模式，包括： Abstract Factory（抽象工厂模式） Builder（建造者模式） Factory Method（工厂方法模式） Prototype（原始模型模式） Singleton（单例模式） Facade（门面模式） Adapter（适配器模式） Bridge（桥梁模式） Composite（合成模式） Decorator（装饰模式） Flyweight（享元模式） Proxy（代理模式） Command（命令模式） Interpreter（解释器模式） Visitor（访问者模式） Iterator（迭代子模式） Mediator（调停者模式） Memento（备忘录模式） Observer（观察者模式） State（状态模式） Strategy（策略模式） Template Method（模板方法模式） Chain Of Responsibility（责任链模式） 面试被问到关于设计模式的知识时，可以拣最常用的作答，例如： 工厂模式：工厂类可以根据条件生成不同的子类实例，这些子类有一个公共的抽象父类并且实现了相同的方法，但是这些方法针对不同的数据进行了不同的操作（多态方法）。当得到子类的实例后，开发人员可以调用基类中的方法而不必考虑到底返回的是哪一个子类的实例。 代理模式：给一个对象提供一个代理对象，并由代理对象控制原对象的引用。实际开发中，按照使用目的的不同，代理可以分为：远程代理、虚拟代理、保护代理、Cache代理、防火墙代理、同步化代理、智能引用代理。 适配器模式：把一个类的接口变换成客户端所期待的另一种接口，从而使原本因接口不匹配而无法在一起使用的类能够一起工作。 模板方法模式：提供一个抽象类，将部分逻辑以具体方法或构造器的形式实现，然后声明一些抽象方法来迫使子类实现剩余的逻辑。不同的子类可以以不同的方式实现这些抽象方法（多态实现），从而实现不同的业务逻辑。 除此之外，还可以讲讲上面提到的门面模式、桥梁模式、单例模式、装潢模式（Collections工具类和I/O系统中都使用装潢模式）等，反正基本原则就是拣自己最熟悉的、用得最多的作答，以免言多必失。 设计模式在实际场景中的应用 Spring中用到了哪些设计模式 MyBatis中用到了哪些设计模式 你项目中有使用哪些设计模式 说说常用开源框架中设计模式使用分析 "},"zother3-java_interview/distributed/":{"url":"zother3-java_interview/distributed/","title":"Index","keywords":"","body":"分布式 分布式全局ID生成方案 参考：高并发分布式系统中生成全局唯一Id汇总 分布式事务 参考：浅谈分布式事务 session 分布式处理 第一种：粘性session。粘性Session是指将用户锁定到某一个服务器上（通过NG）。 第二种：服务器session复制。 第三种：session共享机制。使用分布式缓存方案比如memcached、Redis，但是要求Memcached或Redis必须是集群。 第四种：session持久化到数据库。 谈谈业务中使用分布式的场景 分布式锁的场景 分布是锁的实现方案 集群与负载均衡的算法与实现 说说分库与分表设计 分库与分表带来的分布式困境与应对之策 分布式寻址方式都有哪些算法知道一致性hash吗？你若userId取摸分片，那我要查一段连续时间里的数据怎么办？ 分布式缓存 1、redis和memcheched 什么区别为什么单线程的redis比多线程的memched效率要高啊? 2、redis有什么数据类型都在哪些场景下使用啊? 3、reids的主从复制是怎么实现的redis的集群模式是如何实现的呢redis的key是如何寻址的啊? 4、使用redis如何设计分布式锁?使用zk可以吗?如何实现啊这两种哪个效率更高啊?? 5、知道redis的持久化吗都有什么缺点优点啊? ?具体底层实现呢? 6、redis过期策略都有哪些LRU 写一下java版本的代码吧?? 分布式服务框架 1、说一下dubbo的实现过程注册中心挂了可以继续通信吗?? 2、zk原理知道吗zk都可以干什么Paxos算法知道吗?说一下原理和实现?? 3、dubbo支持哪些序列化协议?hessian 说一下hessian的数据结构PB知道吗为啥PB效率是最高的啊?? 4、知道netty吗'netty可以干嘛呀NIO,BIO,AIO 都是什么啊有什么区别啊? 5、dubbo复制均衡策略和高可用策略都有哪些啊动态代理策略呢? 6、为什么要进行系统拆分啊拆分不用dubbo可以吗'dubbo和thrift什么区别啊? 分布式消息队列 1、为什么使用消息队列啊消息队列有什么优点和缺点啊? 2、如何保证消息队列的高可用啊如何保证消息不被重复消费啊 3、kafka ，activemq,rabbitmq ，rocketmq都有什么优点，缺点啊??? 4、如果让你写一个消息队列，该如何进行架构设计啊?说一下你的思路 分布式搜索引擎 1、es的工作过程实现是如何的?如何实现分布式的啊 2、es在数据量很大的情况下( 数十亿级别)如何提高查询效率啊? 3、es的查询是一个怎么的工作过程?底层的lucence介绍一下呗倒排索引知道吗?es和mongdb什么区别啊都在什么场景下使用啊? 高并发高可用架构设计 1、如何设计一个高并发高可用系统 2、如何限流?工程中怎么做的，说一下具体实现 3、缓存如何使用的缓存使用不当会造成什么后果? 4、如何熔断啊?熔断框架都有哪些?具体实现原理知道吗? 5、如何降级如何进行系统拆分，如何数据库拆分???? 分布式架构原理 1、分布式架构演进过程 2、如何把应用从单机扩展到分布式 3、CDN加速静态文件访问 4、系统监控、容灾、存储动态扩容 5、架构设计及业务驱动划分 6、CAP、Base卢纶以及其应用 分布式架构策略 1、 分布式架构网络通信原理剖析 2、通信协议中的序列化和反序列化 3、基于框架RPC技术 Webservice/RMI/hessian 4、基于ZooKeeper实现分布式服务器动态上下线感知 5、深入分析ZooKeeper在disconfi配置中心的应用 6、深入分析ZooKeeper Zab协议及选举机制源码解读 7、Dubbo管理中心及监控平台安装部署 8、基于Dubbo的分布式系统架构实战 9、Dubbo容错机制及高扩展性分析 分布式架构中间件 1、 分布式消息通信ActiveMQ/kafka/rabbitmq 2、Redis主从复制原理及无磁盘复制分析 3、图解Redis中AOF和RDB持久化策略的原理 5、Session跨域共享以及企业级单点登录解决方案实战 6、分布式事务解决方案实战 7、高并发下的服务降级、限流实战 8、基于分布式架构下分布式锁的解决方案实战 9、分布式架构下实现分布式定时调度 分布式锁的应用场景、分布式锁的产生原因、基本概念 分布是锁的常见解决方案 分布式事务的常见解决方案 集群与负载均衡的算法与实现 说说分库与分表设计，可参考《数据库分库分表策略的具体实现方案》 分库与分表带来的分布式困境与应对之策 说说 CAP 定理、 BASE 理论 怎么考虑数据一致性问题 说说最终一致性的实现方案 请解释什么是C10K问题或者知道什么是C10K问题吗？ "},"zother3-java_interview/java-basic/basic.html":{"url":"zother3-java_interview/java-basic/basic.html","title":"Basic","keywords":"","body":"JAVA基本 String 是最基本的数据类型吗？ 不是。Java中的基本数据类型只有8个：byte、short、int、long、float、double、char、boolean；除了基本类型（primitive type）和枚举类型（enumeration type），剩下的都是引用类型（reference type）。 float f=3.4;是否正确？ 不正确。3.4是双精度数，将双精度型（double）赋值给浮点型（float）属于下转型（down-casting，也称为窄化）会造成精度损失，因此需要强制类型转换float f =(float)3.4; 或者写成float f =3.4F;。 short s1 = 1; s1 = s1 + 1;有错吗?short s1 = 1; s1 += 1;有错吗？ 对于short s1 = 1; s1 = s1 + 1;由于1是int类型，因此s1+1运算结果也是int 型，需要强制转换类型才能赋值给short型。而short s1 = 1; s1 += 1;可以正确编译，因为s1+= 1;相当于s1 = (short)(s1 + 1);其中有隐含的强制类型转换。 int和Integer有什么区别？ Java是一个近乎纯洁的面向对象编程语言，但是为了编程的方便还是引入了基本数据类型，但是为了能够将这些基本数据类型当成对象操作，Java为每一个基本数据类型都引入了对应的包装类型（wrapper class），int的包装类就是Integer，从Java 5开始引入了自动装箱/拆箱机制，使得二者可以相互转换。 Java 为每个原始类型提供了包装类型： 原始类型: boolean，char，byte，short，int，long，float，double 包装类型：Boolean，Character，Byte，Short，Integer，Long，Float，Double &和&&的区别？ &运算符有两种用法：(1)按位与；(2)逻辑与。&&运算符是短路与运算。逻辑与跟短路与的差别是非常巨大的，虽然二者都要求运算符左右两端的布尔值都是true整个表达式的值才是true。&&之所以称为短路运算是因为，如果&&左边的表达式的值是false，右边的表达式会被直接短路掉，不会进行运算。很多时候我们可能都需要用&&而不是&，例如在验证用户登录时判定用户名不是null而且不是空字符串，应当写为：username != null &&!username.equals(“”)，二者的顺序不能交换，更不能用&运算符，因为第一个条件如果不成立，根本不能进行字符串的equals比较，否则会产生NullPointerException异常。注意：逻辑或运算符（|）和短路或运算符（||）的差别也是如此。 Math.round(11.5) 等于多少？Math.round(-11.5)等于多少？ Math.round(11.5)的返回值是12，Math.round(-11.5)的返回值是-11。四舍五入的原理是在参数上加0.5然后进行下取整。 switch 是否能作用在byte 上，是否能作用在long,float 上，是否能作用在String上？ 在Java 5以前，switch(expr)中，expr只能是byte、short、char、int。从Java 5开始，Java中引入了枚举类型，expr也可以是enum类型，从Java 7开始，expr还可以是字符串（String），但是长整型（long）,浮点数（float）在目前所有的版本中都是不可以的。 两个对象值相同(x.equals(y) == true)，但却可有不同的hash code，这句话对不对？ 不对，如果两个对象x和y满足x.equals(y) == true，它们的哈希码（hash code）应当相同。 Java对于eqauls方法和hashCode方法是这样规定的： (1)如果两个对象相同（equals方法返回true），那么它们的hashCode值一定要相同； (2)如果两个对象的hashCode相同，它们并不一定相同。当然，你未必要按照要求去做，但是如果你违背了上述原则就会发现在使用容器时，相同的对象可以出现在Set集合中，同时增加新元素的效率会大大下降（对于使用哈希存储的系统，如果哈希码频繁的冲突将会造成存取性能急剧下降）。 补充：关于equals和hashCode方法，很多Java程序都知道，但很多人也就是仅仅知道而已，在Joshua Bloch的大作《Effective Java》（很多软件公司，《Effective Java》、《Java编程思想》以及《重构：改善既有代码质量》是Java程序员必看书籍，如果你还没看过，那就赶紧去亚马逊买一本吧）中是这样介绍equals方法的：首先equals方法必须满足自反性（x.equals(x)必须返回true）、对称性（x.equals(y)返回true时，y.equals(x)也必须返回true）、传递性（x.equals(y)和y.equals(z)都返回true时，x.equals(z)也必须返回true）和一致性（当x和y引用的对象信息没有被修改时，多次调用x.equals(y)应该得到同样的返回值），而且对于任何非null值的引用x，x.equals(null)必须返回false。 实现高质量的equals方法的诀窍包括： 使用==操作符检查”参数是否为这个对象的引用”； 使用instanceof操作符检查”参数是否为正确的类型”； 对于类中的关键属性，检查参数传入对象的属性是否与之相匹配； 编写完equals方法后，问自己它是否满足对称性、传递性、一致性； 重写equals时总是要重写hashCode； 不要将equals方法参数中的Object对象替换为其他的类型，在重写时不要忘掉@Override注解。 当一个对象被当作参数传递到一个方法后，此方法可改变这个对象的属性，并可返回变化后的结果，那么这里到底是值传递还是引用传递？ 是值传递。Java语言的方法调用只支持参数的值传递。当一个对象实例作为一个参数被传递到方法中时，参数的值就是对该对象的引用。对象的属性可以在被调用过程中被改变，但对对象引用的改变是不会影响到调用者的。C++和C#中可以通过传引用或传输出参数来改变传入的参数的值。 String和StringBuilder、StringBuffer的区别？ Java平台提供了两种类型的字符串：String和StringBuffer/StringBuilder，它们可以储存和操作字符串。其中String是只读字符串，也就意味着String引用的字符串内容是不能被改变的。而StringBuffer/StringBuilder类表示的字符串对象可以直接进行修改。StringBuilder是Java 5中引入的，它和StringBuffer的方法完全相同，区别在于它是在单线程环境下使用的，因为它的所有方面都没有被synchronized修饰，因此它的效率也比StringBuffer要高。 抽象的（abstract）方法是否可同时是静态的（static）,是否可同时是本地方法（native），是否可同时被synchronized修饰？ 都不能。抽象方法需要子类重写，而静态的方法是无法被重写的，因此二者是矛盾的。本地方法是由本地代码（如C代码）实现的方法，而抽象方法是没有实现的，也是矛盾的。synchronized和方法的实现细节有关，抽象方法不涉及实现细节，因此也是相互矛盾的。 阐述静态变量和实例变量的区别。 静态变量是被static修饰符修饰的变量，也称为类变量，它属于类，不属于类的任何一个对象，一个类不管创建多少个对象，静态变量在内存中有且仅有一个拷贝；实例变量必须依存于某一实例，需要先创建对象然后通过对象才能访问到它。静态变量可以实现让多个对象共享内存。 补充：在Java开发中，上下文类和工具类中通常会有大量的静态成员。 Object中有哪些公共方法？ equals() clone() getClass() notify(),notifyAll(),wait() toString() 是否可以从一个静态（static）方法内部发出对非静态（non-static）方法的调用？ 不可以，静态方法只能访问静态成员，因为非静态方法的调用要先创建对象，在调用静态方法时可能对象并没有被初始化。 深拷贝和浅拷贝的区别是什么？ 浅拷贝：被复制对象的所有变量都含有与原来的对象相同的值，而所有的对其他对象的引用仍然指向原来的对象。换言之，浅拷贝仅仅复制所考虑的对象，而不复制它所引用的对象。 深拷贝：被复制对象的所有变量都含有与原来的对象相同的值，而那些引用其他对象的变量将指向被复制过的新对象，而不再是原有的那些被引用的对象。换言之，深拷贝把要复制的对象所引用的对象都复制了一遍。 如何实现对象克隆？ 有两种方式： 实现Cloneable接口并重写Object类中的clone()方法； 实现Serializable接口，通过对象的序列化和反序列化实现克隆，可以实现真正的深度克隆。 代码如下： import java.io.ByteArrayInputStream; import java.io.ByteArrayOutputStream; import java.io.ObjectInputStream; import java.io.ObjectOutputStream; public class MyUtil { private MyUtil() { throw new AssertionError(); } public static T clone(T obj) throws Exception { ByteArrayOutputStream bout = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(bout); oos.writeObject(obj); ByteArrayInputStream bin = new ByteArrayInputStream(bout.toByteArray()); ObjectInputStream ois = new ObjectInputStream(bin); return (T) ois.readObject(); // 说明：调用ByteArrayInputStream或ByteArrayOutputStream对象的close方法没有任何意义 // 这两个基于内存的流只要垃圾回收器清理对象就能够释放资源，这一点不同于对外部资源（如文件流）的释放 } } 下面是测试代码： import java.io.Serializable; /** * 人类 * @author 骆昊 * */ class Person implements Serializable { private static final long serialVersionUID = -9102017020286042305L; private String name; // 姓名 private int age; // 年龄 private Car car; // 座驾 public Person(String name, int age, Car car) { this.name = name; this.age = age; this.car = car; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } public Car getCar() { return car; } public void setCar(Car car) { this.car = car; } @Override public String toString() { return \"Person [name=\" + name + \", age=\" + age + \", car=\" + car + \"]\"; } } /** * 小汽车类 * @author 骆昊 * */ class Car implements Serializable { private static final long serialVersionUID = -5713945027627603702L; private String brand; // 品牌 private int maxSpeed; // 最高时速 public Car(String brand, int maxSpeed) { this.brand = brand; this.maxSpeed = maxSpeed; } public String getBrand() { return brand; } public void setBrand(String brand) { this.brand = brand; } public int getMaxSpeed() { return maxSpeed; } public void setMaxSpeed(int maxSpeed) { this.maxSpeed = maxSpeed; } @Override public String toString() { return \"Car [brand=\" + brand + \", maxSpeed=\" + maxSpeed + \"]\"; } } class CloneTest { public static void main(String[] args) { try { Person p1 = new Person(\"Hao LUO\", 33, new Car(\"Benz\", 300)); Person p2 = MyUtil.clone(p1); // 深度克隆 p2.getCar().setBrand(\"BYD\"); // 修改克隆的Person对象p2关联的汽车对象的品牌属性 // 原来的Person对象p1关联的汽车不会受到任何影响 // 因为在克隆Person对象时其关联的汽车对象也被克隆了 System.out.println(p1); } catch (Exception e) { e.printStackTrace(); } } } 注意：基于序列化和反序列化实现的克隆不仅仅是深度克隆，更重要的是通过泛型限定，可以检查出要克隆的对象是否支持序列化，这项检查是编译器完成的，不是在运行时抛出异常，这种是方案明显优于使用Object类的clone方法克隆对象。让问题在编译的时候暴露出来总是优于把问题留到运行时。 String s = new String(“xyz”);创建了几个字符串对象？ 两个对象，一个是静态区的”xyz”，一个是用new创建在堆上的对象。 java中==和eqauls()的区别,equals()和`hashcode的区别 ==是运算符,用于比较两个变量是否相等,而equals是Object类的方法,用于比较两个对象是否相等.默认Object类的equals方法是比较两个对象的地址,此时和==的结果一样.换句话说:基本类型比较用==,比较的是他们的值.默认下,对象用==比较时,比较的是内存地址,如果需要比较对象内容,需要重写equal方法 a==b与a.equals(b)有什么区别 如果a 和b 都是对象，则 a==b 是比较两个对象的引用，只有当 a 和 b 指向的是堆中的同一个对象才会返回 true，而 a.equals(b) 是进行逻辑比较，所以通常需要重写该方法来提供逻辑一致性的比较。例如，String 类重写 equals() 方法，所以可以用于两个不同对象，但是包含的字母相同的比较。 接口是否可继承（extends）接口？抽象类是否可实现（implements）接口？抽象类是否可继承具体类（concrete class）？ 接口可以继承接口，而且支持多重继承。抽象类可以实现(implements)接口，抽象类可继承具体类也可以继承抽象类。 Java 中的final关键字有哪些用法？ (1)修饰类：表示该类不能被继承； (2)修饰方法：表示方法不能被重写； (3)修饰变量：表示变量只能一次赋值以后值不能被修改（常量）。 throw和throws的区别 throw用于主动抛出java.lang.Throwable 类的一个实例化对象，意思是说你可以通过关键字 throw 抛出一个 Error 或者 一个Exception，如：throw new IllegalArgumentException(“size must be multiple of 2″)。 throws 的作用是作为方法声明和签名的一部分，方法被抛出相应的异常以便调用者能处理。Java 中，任何未处理的受检查异常强制在 throws 子句中声明。 Error和Exception有什么区别？ Error表示系统级的错误和程序不必处理的异常，是恢复不是不可能但很困难的情况下的一种严重问题；比如内存溢出，不可能指望程序能处理这样的情况； Exception表示需要捕捉或者需要程序进行处理的异常，是一种设计或实现问题；也就是说，它表示如果程序运行正常，从不会发生的情况。 Java语言如何进行异常处理，关键字：throws、throw、try、catch、finally分别如何使用？ Java通过面向对象的方法进行异常处理，把各种不同的异常进行分类，并提供了良好的接口。在Java中，每个异常都是一个对象，它是Throwable类或其子类的实例。当一个方法出现异常后便抛出一个异常对象，该对象中包含有异常信息，调用这个对象的方法可以捕获到这个异常并可以对其进行处理。 Java的异常处理是通过5个关键词来实现的：try、catch、throw、throws和finally。 一般情况下是用try来执行一段程序，如果系统会抛出（throw）一个异常对象，可以通过它的类型来捕获（catch）它，或通过总是执行代码块（finally）来处理； try用来指定一块预防所有异常的程序； catch子句紧跟在try块后面，用来指定你想要捕获的异常的类型； throw语句用来明确地抛出一个异常； throws用来声明一个方法可能抛出的各种异常（当然声明异常时允许无病呻吟）； finally为确保一段代码不管发生什么异常状况都要被执行； try语句可以嵌套，每当遇到一个try语句，异常的结构就会被放入异常栈中，直到所有的try语句都完成。如果下一级的try语句没有对某种异常进行处理，异常栈就会执行出栈操作，直到遇到有处理这种异常的try语句或者最终将异常抛给JVM。 运行时异常与受检异常有何异同？ 异常表示程序运行过程中可能出现的非正常状态，运行时异常表示虚拟机的通常操作中可能遇到的异常，是一种常见运行错误，只要程序设计得没有问题通常就不会发生。受检异常跟程序运行的上下文环境有关，即使程序设计无误，仍然可能因使用的问题而引发。Java编译器要求方法必须声明抛出可能发生的受检异常，但是并不要求必须声明抛出未被捕获的运行时异常。 异常和继承一样，是面向对象程序设计中经常被滥用的东西，在Effective Java中对异常的使用给出了以下指导原则： 不要将异常处理用于正常的控制流（设计良好的API不应该强迫它的调用者为了正常的控制流而使用异常） 对可以恢复的情况使用受检异常，对编程错误使用运行时异常 避免不必要的使用受检异常（可以通过一些状态检测手段来避免异常的发生） 优先使用标准的异常 每个方法抛出的异常都要有文档 保持异常的原子性 不要在catch中忽略掉捕获到的异常 列出一些你常见的运行时异常？ ArithmeticException（算术异常） ClassCastException （类转换异常） IllegalArgumentException （非法参数异常） IndexOutOfBoundsException （下标越界异常） NullPointerException （空指针异常） SecurityException （安全异常） 阐述final、finally、finalize的区别 final：修饰符（关键字）有三种用法：如果一个类被声明为final，意味着它不能再派生出新的子类，即不能被继承，因此它和abstract是反义词。将变量声明为final，可以保证它们在使用中不被改变，被声明为final的变量必须在声明时给定初值，而在以后的引用中只能读取不可修改。被声明为final的方法也同样只能使用，不能在子类中被重写。 finally：通常放在try…catch…的后面构造总是执行代码块，这就意味着程序无论正常执行还是发生异常，这里的代码只要JVM不关闭都能执行，可以将释放外部资源的代码写在finally块中。 finalize：Object类中定义的方法，Java中允许使用finalize()方法在垃圾收集器将对象从内存中清除出去之前做必要的清理工作。这个方法是由垃圾收集器在销毁对象时调用的，通过重写finalize()方法可以整理系统资源或者执行其他清理工作。 java当中的四种引用 强引用,软引用,弱引用,虚引用.不同的引用类型主要体现在GC上: 强引用：如果一个对象具有强引用，它就不会被垃圾回收器回收。即使当前内存空间不足，JVM也不会回收它，而是抛出 OutOfMemoryError 错误，使程序异常终止。如果想中断强引用和某个对象之间的关联，可以显式地将引用赋值为null，这样一来的话，JVM在合适的时间就会回收该对象 软引用：在使用软引用时，如果内存的空间足够，软引用就能继续被使用，而不会被垃圾回收器回收，只有在内存不足时，软引用才会被垃圾回收器回收。 弱引用：具有弱引用的对象拥有的生命周期更短暂。因为当 JVM 进行垃圾回收，一旦发现弱引用对象，无论当前内存空间是否充足，都会将弱引用回收。不过由于垃圾回收器是一个优先级较低的线程，所以并不一定能迅速发现弱引用对象 虚引用：顾名思义，就是形同虚设，如果一个对象仅持有虚引用，那么它相当于没有引用，在任何时候都可能被垃圾回收器回收。 更多了解参见深入对象引用： http://blog.csdn.net/dd864140130/article/details/49885811 为什么要有不同的引用类型 不像C语言,我们可以控制内存的申请和释放,在Java中有时候我们需要适当的控制对象被回收的时机,因此就诞生了不同的引用类型,可以说不同的引用类型实则是对GC回收时机不可控的妥协.有以下几个使用场景可以充分的说明: 利用软引用和弱引用解决OOM问题：用一个HashMap来保存图片的路径和相应图片对象关联的软引用之间的映射关系，在内存不足时，JVM会自动回收这些缓存图片对象所占用的空间，从而有效地避免了OOM的问题. 通过软引用实现Java对象的高速缓存:比如我们创建了一Person的类，如果每次需要查询一个人的信息,哪怕是几秒中之前刚刚查询过的，都要重新构建一个实例，这将引起大量Person对象的消耗,并且由于这些对象的生命周期相对较短,会引起多次GC影响性能。此时,通过软引用和 HashMap 的结合可以构建高速缓存,提供性能. 内部类的作用 内部类可以有多个实例,每个实例都有自己的状态信息,并且与其他外围对象的信息相互独立.在单个外围类当中,可以让多个内部类以不同的方式实现同一接口,或者继承同一个类.创建内部类对象的时刻不依赖于外部类对象的创建.内部类并没有令人疑惑的”is-a”关系,它就像是一个独立的实体. 内部类提供了更好的封装,除了该外围类,其他类都不能访问 SimpleDateFormat是线程安全的吗？ 非常不幸，DateFormat 的所有实现，包括 SimpleDateFormat 都不是线程安全的，因此你不应该在多线程序中使用，除非是在对外线程安全的环境中使用，如 将 SimpleDateFormat 限制在 ThreadLocal 中。如果你不这么做，在解析或者格式化日期的时候，可能会获取到一个不正确的结果。因此，从日期、时间处理的所有实践来说，我强力推荐 joda-time 库。 如何格式化日期？ Java 中，可以使用 SimpleDateFormat 类或者 joda-time 库来格式日期。DateFormat 类允许你使用多种流行的格式来格式化日期。参见答案中的示例代码，代码中演示了将日期格式化成不同的格式，如 dd-MM-yyyy 或 ddMMyyyy。 说出几条 Java 中方法重载的最佳实践？ 下面有几条可以遵循的方法重载的最佳实践来避免造成自动装箱的混乱。 不要重载这样的方法：一个方法接收 int 参数，而另个方法接收 Integer 参数。 不要重载参数数量一致，而只是参数顺序不同的方法。 如果重载的方法参数个数多于 5 个，采用可变参数。 说说反射的用途及实现 反射机制是Java语言中一个非常重要的特性，它允许程序在运行时进行自我检查，同时也允许对其内部成员进行操作。 反射机制提供的功能主要有：得到一个对象所属的类；获取一个类的所有成员变量和方法；在运行时创建对象；在运行时调用对象的方法； 说说自定义注解的场景及实现 登陆、权限拦截、日志处理，以及各种 Java 框架，如 Spring，Hibernate，JUnit 提到注解就不能不说反射，Java 自定义注解是通过运行时靠反射获取注解。 实际开发中，例如我们要获取某个方法的调用日志，可以通过 AOP（动态代理机制）给方法添加切面，通过反射来获取方法包含的注解，如果包含日志注解，就进行日志记录。 反射的实现在 Java 应用层面上讲，是通过对 Class 对象的操作实现的，Class 对象为我们提供了一系列方法对类进行操作。在 JVM 这个角度来说，Class 文件是一组以 8 位字节为基础单位的二进制流，各个数据项目按严格的顺序紧凑的排列在 Class 文件中，里面包含了类、方法、字段等等相关数据。 通过对 Class 数据流的处理我们即可得到字段、方法等数据。 什么要重写hashcode()和equals()以及他们之间的区别与关系？ 可以参考： 为什么要重写hashCode()方法和equals()方法以及如何进行重写 Java hashCode() 和 equals()的若干问题解答 Java中equals()与hashCode()方法详解 "},"zother3-java_interview/java-basic/collections.html":{"url":"zother3-java_interview/java-basic/collections.html","title":"Collections","keywords":"","body":"集合 Java中的集合及其继承关系 关于集合的体系是每个人都应该烂熟于心的,尤其是对我们经常使用的List,Map的原理更该如此.这里我们看这张图即可: List、Set、Map是否继承自Collection接口？ List、Set 是，Map 不是。Map是键值对映射容器，与List和Set有明显的区别，而Set存储的零散的元素且不允许有重复元素（数学中的集合也是如此），List是线性结构的容器，适用于按数值索引访问元素的情形。 阐述ArrayList、Vector、LinkedList的存储性能和特性。 ArrayList 和Vector都是使用数组方式存储数据，此数组元素数大于实际存储的数据以便增加和插入元素，它们都允许直接按序号索引元素，但是插入元素要涉及数组元素移动等内存操作，所以索引数据快而插入数据慢。Vector中的方法由于添加了synchronized修饰，因此Vector是线程安全的容器，但性能上较ArrayList差，因此已经是Java中的遗留容器。 LinkedList使用双向链表实现存储（将内存中零散的内存单元通过附加的引用关联起来，形成一个可以按序号索引的线性结构，这种链式存储方式与数组的连续存储方式相比，内存的利用率更高），按序号索引数据需要进行前向或后向遍历，但是插入数据时只需要记录本项的前后项即可，所以插入速度较快。 Vector属于遗留容器（Java早期的版本中提供的容器，除此之外，Hashtable、Dictionary、BitSet、Stack、Properties都是遗留容器），已经不推荐使用，但是由于ArrayList和LinkedListed都是非线程安全的，如果遇到多个线程操作同一个容器的场景，则可以通过工具类Collections中的synchronizedList方法将其转换成线程安全的容器后再使用（这是对装潢模式的应用，将已有对象传入另一个类的构造器中创建新的对象来增强实现）。 Collection和Collections的区别？ Collection是一个接口，它是Set、List等容器的父接口；Collections是个一个工具类，提供了一系列的静态方法来辅助容器操作，这些方法包括对容器的搜索、排序、线程安全化等等。 List、Map、Set三个接口存取元素时，各有什么特点？ List以特定索引来存取元素，可以有重复元素。 Set不能存放重复元素（用对象的equals()方法来区分元素是否重复）。 Map保存键值对（key-value pair）映射，映射关系可以是一对一或多对一。 Set和Map容器都有基于哈希存储和排序树的两种实现版本，基于哈希存储的版本理论存取时间复杂度为O(1)，而基于排序树版本的实现在插入或删除元素时会按照元素或元素的键（key）构成排序树从而达到排序和去重的效果。 List和Set区别 Set是最简单的一种集合。集合中的对象不按特定的方式排序，并且没有重复对象。 HashSet： HashSet类按照哈希算法来存取集合中的对象，存取速度比较快 TreeSet ：TreeSet类实现了SortedSet接口，能够对集合中的对象进行排序。 List的特征是其元素以线性方式存储，集合中可以存放重复对象。 ArrayList() : 代表长度可以改变得数组。可以对元素进行随机的访问，向ArrayList()中插入与删除元素的速度慢。 LinkedList(): 在实现中采用链表数据结构。插入和删除速度快，访问速度慢。 LinkedHashMap和PriorityQueue的区别 PriorityQueue 是一个优先级队列,保证最高或者最低优先级的的元素总是在队列头部，但是 LinkedHashMap 维持的顺序是元素插入的顺序。当遍历一个 PriorityQueue 时，没有任何顺序保证，但是 LinkedHashMap 课保证遍历顺序是元素插入的顺序。 WeakHashMap与HashMap的区别是什么？ WeakHashMap 的工作与正常的 HashMap 类似，但是使用弱引用作为 key，意思就是当 key 对象没有任何引用时，key/value 将会被回收。 ArrayList和LinkedList的区别？ 最明显的区别是 ArrrayList底层的数据结构是数组，支持随机访问，而 LinkedList 的底层数据结构是双向循环链表，不支持随机访问。使用下标访问一个元素，ArrayList 的时间复杂度是 O(1)，而 LinkedList 是 O(n)。 相对于ArrayList，LinkedList的插入，添加，删除操作速度更快，因为当元素被添加到集合任意位置的时候，不需要像数组那样重新计算大小或者是更新索引。 LinkedList比ArrayList更占内存，因为LinkedList为每一个节点存储了两个引用，一个指向前一个元素，一个指向下一个元素。 ArrayList和Array有什么区别？ Array可以容纳基本类型和对象，而ArrayList只能容纳对象。 Array是指定大小的，而ArrayList大小是固定的 ArrayList与Vector区别 ArrayList和Vector在很多时候都很类似。 两者都是基于索引的，内部由一个数组支持。 两者维护插入的顺序，我们可以根据插入顺序来获取元素。 ArrayList和Vector的迭代器实现都是fail-fast的。 ArrayList和Vector两者允许null值，也可以使用索引值对元素进行随机访问。 以下是ArrayList和Vector的不同点。 Vector是同步的，而ArrayList不是。然而，如果你寻求在迭代的时候对列表进行改变，你应该使用CopyOnWriteArrayList。 ArrayList比Vector快，它因为有同步，不会过载。 ArrayList更加通用，因为我们可以使用Collections工具类轻易地获取同步列表和只读列表。 HashMap和Hashtable的区别 HashMap和Hashtable都实现了Map接口，因此很多特性非常相似。但是，他们有以下不同点： HashMap允许键和值是null，而Hashtable不允许键或者值是null。 Hashtable是同步的，而HashMap不是。因此，HashMap更适合于单线程环境，而Hashtable适合于多线程环境。 HashMap提供了可供应用迭代的键的集合，因此，HashMap是快速失败的。另一方面，Hashtable提供了对键的列举(Enumeration)。 一般认为Hashtable是一个遗留的类。 HashSet和HashMap区别 HashSet实现了Set接口，它不允许集合中有重复的值。它存储的是对象 HashMap实现了Map接口，Map接口对键值对进行映射。Map中不允许重复的键。Map接口有两个基本的实现，HashMap和TreeMap。 HashMap和ConcurrentHashMap的区别 ConcurrentHashMap对整个桶数组进行了分段，而HashMap则没有。 ConcurrentHashMap在每一个分段上都用锁进行保护，从而让锁的粒度更精细一些，并发性能更好，而HashMap没有锁机制，不是线程安全的。 引入ConcurrentHashMap是为了在同步集合HashTable之间有更好的选择，HashTable与HashMap、ConcurrentHashMap主要的区别在于HashMap不是同步的、线程不安全的和不适合应用于多线程并发环境下，而ConcurrentHashMap是线程安全的集合容器，特别是在多线程和并发环境中，通常作为Map的主要实现。 Comparator和Comparable的区别？ Comparable 接口用于定义对象的自然顺序，而 comparator 通常用于定义用户定制的顺序。Comparable 总是只有一个，但是可以有多个 comparator 来定义对象的顺序。 poll()方法和remove()方法区别？ poll() 和 remove() 都是从队列中取出一个元素，但是 poll() 在获取元素失败的时候会返回空，但是 remove() 失败的时候会抛出异常。 ArrayList、HashMa和LinkedList的默认空间是多少？扩容机制是什么 ArrayList 的默认大小是 10 个元素。扩容点规则是，新增的时候发现容量不够用了，就去扩容；扩容大小规则是：扩容后的大小= 原始大小+原始大小/2 + 1。 HashMap 的默认大小是16个元素（必须是2的幂）。扩容因子默认0.75，扩容机制.(当前大小 和 当前容量 的比例超过了 扩容因子，就会扩容，扩容后大小为 一倍。例如：初始大小为 16 ，扩容因子 0.75 ，当容量为12的时候，比例已经是0.75 。触发扩容，扩容后的大小为 32.) LinkedList 是一个双向链表，没有初始化大小，也没有扩容的机制，就是一直在前面或者后面新增就好。 private static final int DEFAULT_CAPACITY = 10; //from HashMap.java JDK 7 static final int DEFAULT_INITIAL_CAPACITY = 1 如何实现集合排序？ 你可以使用有序集合，如 TreeSet 或 TreeMap，你也可以使用有顺序的的集合，如 list，然后通过 Collections.sort() 来排序。 如何打印数组内容 你可以使用 Arrays.toString() 和 Arrays.deepToString() 方法来打印数组。由于数组没有实现 toString() 方法，所以如果将数组传递给 System.out.println() 方法，将无法打印出数组的内容，但是 Arrays.toString() 可以打印每个元素。 LinkedList的是单向链表还是双向？ 双向循环列表,具体实现自行查阅源码. TreeMap是实现原理 采用红黑树实现,具体实现自行查阅源码. 遍历ArrayList时如何正确移除一个元素 该问题的关键在于面试者使用的是 ArrayList 的 remove() 还是 Iterator 的 remove()方法。这有一段示例代码，是使用正确的方式来实现在遍历的过程中移除元素，而不会出现 ConcurrentModificationException 异常的示例代码。 什么是ArrayMap？它和HashMap有什么区别？ ArrayMap是Android SDK中提供的,非Android开发者可以略过。 ArrayMap是用两个数组来模拟map,更少的内存占用空间,更高的效率。 具体参考这篇文章:ArrayMap VS HashMap：http://lvable.com/?p=217%5D 如何决定选用HashMap还是TreeMap？ 对于在Map中插入、删除和定位元素这类操作，HashMap是最好的选择。然而，假如你需要对一个有序的key集合进行遍历，TreeMap是更好的选择。基于你的collection的大小，也许向HashMap中添加元素会更快，将map换为TreeMap进行有序key的遍历。 HashMap的实现原理 HashMap概述： HashMap是基于哈希表的Map接口的非同步实现。此实现提供所有可选的映射操作，并允许使用null值和null键。此类不保证映射的顺序，特别是它不保证该顺序恒久不变。 HashMap的数据结构： 在java编程语言中，最基本的结构就是两种，一个是数组，另外一个是模拟指针（引用），所有的数据结构都可以用这两个基本结构来构造的，HashMap也不例外。HashMap实际上是一个“链表散列”的数据结构，即数组和链表的结合体。 当我们往Hashmap中put元素时,首先根据key的hashcode重新计算hash值,根绝hash值得到这个元素在数组中的位置(下标),如果该数组在该位置上已经存放了其他元素,那么在这个位置上的元素将以链表的形式存放,新加入的放在链头,最先加入的放入链尾.如果数组中该位置没有元素,就直接将该元素放到数组的该位置上. 需要注意Jdk 1.8中对HashMap的实现做了优化,当链表中的节点数据超过八个之后,该链表会转为红黑树来提高查询效率,从原来的O(n)到O(logn) 也可以参考： 深入Java集合学习系列：HashMap的实现原理 深入理解HashMap 解决Hash冲突的方法有哪些 开放地址法、链地址法、再哈希法、建立公共溢出区等 参考： java 解决Hash(散列)冲突的四种方法--开放定址法(线性探测,二次探测,伪随机探测)、链地址法、再哈希、建立公共溢出区 Java 8中HashMap冲突解决 ConcurrentHashMap 的工作原理及代码实现 ConcurrentHashMap具体是怎么实现线程安全的呢，肯定不可能是每个方法加synchronized，那样就变成了HashTable。 从ConcurrentHashMap代码中可以看出，它引入了一个“分段锁”的概念，具体可以理解为把一个大的Map拆分成N个小的HashTable，根据key.hashCode()来决定把key放到哪个HashTable中。 在ConcurrentHashMap中，就是把Map分成了N个Segment，put和get的时候，都是现根据key.hashCode()算出放到哪个Segment中。 你了解Fail-Fast机制吗 Fail-Fast即我们常说的快速失败, 更多内容参看fail-fast机制：http://blog.csdn.net/chenssy/article/details/38151189 Fail-fast和Fail-safe有什么区别 Iterator的fail-fast属性与当前的集合共同起作用，因此它不会受到集合中任何改动的影响。Java.util包中的所有集合类都被设计为fail->fast的，而java.util.concurrent中的集合类都为fail-safe的。当检测到正在遍历的集合的结构被改变时，Fail-fast迭代器抛出ConcurrentModificationException，而fail-safe迭代器从不抛出ConcurrentModificationException。 说出几点 Java 中使用 Collections 的最佳实践 这是我在使用 Java 中 Collectionc 类的一些最佳实践： 使用正确的集合类，例如，如果不需要同步列表，使用 ArrayList 而不是 Vector。 优先使用并发集合，而不是对集合进行同步。并发集合提供更好的可扩展性。 使用接口代表和访问集合，如使用List存储 ArrayList，使用 Map 存储 HashMap 等等。 使用迭代器来循环集合。 使用集合的时候使用泛型。 BlockingQueue是什么？ Java.util.concurrent.BlockingQueue是一个队列，在进行检索或移除一个元素的时候，它会等待队列变为非空；当在添加一个元素时，它会等待队列中的可用空间。BlockingQueue接口是Java集合框架的一部分，主要用于实现生产者-消费者模式。我们不需要担心等待生产者有可用的空间，或消费者有可用的对象，因为它都在BlockingQueue的实现类中被处理了。Java提供了集中BlockingQueue的实现，比如ArrayBlockingQueue、LinkedBlockingQueue、PriorityBlockingQueue,、SynchronousQueue 队列和栈是什么，列出它们的区别？ 栈和队列两者都被用来预存储数据。java.util.Queue是一个接口，它的实现类在Java并发包中。队列允许先进先出（FIFO）检索元素，但并非总是这样。Deque接口允许从两端检索元素。 栈与队列很相似，但它允许对元素进行后进先出（LIFO）进行检索。 Stack是一个扩展自Vector的类，而Queue是一个接口。 多线程情况下HashMap死循环的问题 可以参考：疫苗：JAVA HASHMAP的死循环 HashMap出现Hash DOS攻击的问题 可以参考：HASH COLLISION DOS 问题 Java Collections和Arrays的sort方法默认的排序方法是什么？ 参考：Collections.sort()和Arrays.sort()排序算法选择 "},"zother3-java_interview/java-basic/":{"url":"zother3-java_interview/java-basic/","title":"Index","keywords":"","body":"Java基础 面向对象 JAVA基本 集合 多线程 JVM IO "},"zother3-java_interview/java-basic/io.html":{"url":"zother3-java_interview/java-basic/io.html","title":"Io","keywords":"","body":"NIO 解释一下java.io.Serializable接口 类通过实现 Java.io.Serializable 接口以启用其序列化功能。未实现此接口的类将无法使其任何状态序列化或反序列化。 IO操作最佳实践 使用有缓冲的IO类,不要单独读取字节或字符 使用NIO和NIO 2或者AIO,而非BIO 在finally中关闭流 使用内存映射文件获取更快的IO Java IO 分类 Java BIO： 同步并阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。 Java NIO ： 同步非阻塞，服务器实现模式为一个请求一个线程，即当一个连接创建后，不需要对应一个线程，这个连接会被注册到多路复用器上面，所以所有的连接只需要一个线程就可以搞定，当这个线程中的多路复用器进行轮询的时候，发现连接上有请求的话，才开启一个线程进行处理，也就是一个请求一个线程模式。BIO与NIO一个比较重要的不同，是我们使用BIO的时候往往会引入多线程，每个连接一个单独的线程；而NIO则是使用单线程或者只使用少量的多线程，每个连接共用一个线程。 Java AIO(NIO.2) ： 异步非阻塞，服务器实现模式为一个有效请求一个线程，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理。 说出 5 条 IO 的最佳实践 IO 对 Java 应用的性能非常重要。理想情况下，你不应该在你应用的关键路径上避免 IO 操作。下面是一些你应该遵循的 Java IO 最佳实践： 使用有缓冲区的 IO 类，而不要单独读取字节或字符。 使用 NIO 和 NIO2 在 finally 块中关闭流，或者使用 try-with-resource 语句。 使用内存映射文件获取更快的 IO。 BIO、NIO、AIO适用场景分析 BIO（同步并阻塞）方式适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4以前的唯一选择，但程序直观简单易理解。 NIO（同步非阻塞）方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持。 AIO（ 异步非阻塞）方式使用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK7开始支持。 Java NIO和IO的主要区别 面向流与面向缓冲. Java NIO和IO之间第一个最大的区别是，IO是面向流的，NIO是面向缓冲区的。Java IO面向流意味着每次从流中读一个或多个字节，直至读取所有字节，它们没有被缓存在任何地方。此外，它不能前后移动流中的数据。如果需要前后移动从流中读取的数据，需要先将它缓存到一个缓冲区。 Java NIO的缓冲导向方法略有不同。数据读取到一个它稍后处理的缓冲区，需要时可在缓冲区中前后移动。这就增加了处理过程中的灵活性。 阻塞与非阻塞IO Java IO的各种流是阻塞的。这意味着，当一个线程调用read() 或 write()时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了。 Java NIO的非阻塞模式，使一个线程从某通道发送请求读取数据，但是它仅能得到目前可用的数据，如果目前没有数据可用时，该线程可以继续做其他的事情。 非阻塞写也是如此。一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。线程通常将非阻塞IO的空闲时间用于在其它通道上执行IO操作，所以一个单独的线程现在可以管理多个输入和输出通道（channel）。 选择器（Selectors） Java NIO的选择器允许一个单独的线程来监视多个输入通道，你可以注册多个通道使用一个选择器，然后使用一个单独的线程来“选择”通道：这些通道里已经有可以处理的输入，或者选择已准备写入的通道。这种选择机制，使得一个单独的线程很容易来管理多个通道。 Java I/O库的两个设计模式 Java I/O库的总体设计是符合装饰模式和适配器模式的。如前所述，这个库中处理流的类叫流类。 装饰模式（Decorator）：在由InputStream、OutputStream、Reader和Writer代表的等级结构内部，有一些流处理器可以对另一些流处理器起到装饰作用，形成新的、具有改善了的功能的流处理器。 适配器模式（Adapter）：在由InputStream、OutputStream、Reader和Writer代表的等级结构内部，有一些流处理器是对其他类型的流处理器的适配。这就是适配器的应用。 "},"zother3-java_interview/java-basic/jvm.html":{"url":"zother3-java_interview/java-basic/jvm.html","title":"Jvm","keywords":"","body":"JVM JVN内存结构 方法区和对是所有线程共享的内存区域；而java栈、本地方法栈和程序员计数器是运行是线程私有的内存区域。 Java堆（Heap）,是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。 方法区（Method Area）,方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 程序计数器（Program Counter Register）,程序计数器（Program Counter Register）是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。 JVM栈（JVM Stacks）,与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 本地方法栈（Native Method Stacks）,本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native方法服务。 对象分配规则 对象优先分配在Eden区，如果Eden区没有足够的空间时，虚拟机执行一次Minor GC。 大对象直接进入老年代（大对象是指需要大量连续内存空间的对象）。这样做的目的是避免在Eden区和两个Survivor区之间发生大量的内存拷贝（新生代采用复制算法收集内存）。 长期存活的对象进入老年代。虚拟机为每个对象定义了一个年龄计数器，如果对象经过了1次Minor GC那么对象会进入Survivor区，之后每经过一次Minor GC那么对象的年龄加1，知道达到阀值对象进入老年区。 动态判断对象的年龄。如果Survivor区中相同年龄的所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象可以直接进入老年代。 空间分配担保。每次进行Minor GC时，JVM会计算Survivor区移至老年区的对象的平均大小，如果这个值大于老年区的剩余值大小则进行一次Full GC，如果小于检查HandlePromotionFailure设置，如果true则只进行Monitor GC,如果false则进行Full GC。 解释内存中的栈(stack)、堆(heap)和静态区(static area)的用法 通常我们定义一个基本数据类型的变量，一个对象的引用，还有就是函数调用的现场保存都使用内存中的栈空间；而通过new关键字和构造器创建的对象放在堆空间；程序中的字面量（literal）如直接书写的100、”hello”和常量都是放在静态区中。栈空间操作起来最快但是栈很小，通常大量的对象都是放在堆空间，理论上整个内存没有被其他进程使用的空间甚至硬盘上的虚拟内存都可以被当成堆空间来使用。 String str = new String(\"hello\"); 上面的语句中变量str放在栈上，用new创建出来的字符串对象放在堆上，而”hello”这个字面量放在静态区。 Perm Space中保存什么数据？会引起OutOfMemory吗？ Perm Space中保存的是加载class文件。 会引起，出现异常可以设置 -XX:PermSize 的大小。JDK 1.8后，字符串常量不存放在永久带，而是在堆内存中，JDK8以后没有永久代概念，而是用元空间替代，元空间不存在虚拟机中，二是使用本地内存。 详细查看Java8内存模型—永久代(PermGen)和元空间(Metaspace) 什么是类的加载 类的加载指的是将类的.class文件中的二进制数据读入到内存中，将其放在运行时数据区的方法区内，然后在堆区创建一个java.lang.Class对象，用来封装类在方法区内的数据结构。类的加载的最终产品是位于堆区中的Class对象，Class对象封装了类在方法区内的数据结构，并且向Java程序员提供了访问方法区内的数据结构的接口。 类加载器 启动类加载器：Bootstrap ClassLoader，负责加载存放在JDK\\jre\\lib(JDK代表JDK的安装目录，下同)下，或被-Xbootclasspath参数指定的路径中的，并且能被虚拟机识别的类库 扩展类加载器：Extension ClassLoader，该加载器由sun.misc.Launcher$ExtClassLoader实现，它负责加载DK\\jre\\lib\\ext目录中，或者由java.ext.dirs系统变量指定的路径中的所有类库（如javax.*开头的类），开发者可以直接使用扩展类加载器。 应用程序类加载器：Application ClassLoader，该类加载器由sun.misc.Launcher$AppClassLoader来实现，它负责加载用户类路径（ClassPath）所指定的类，开发者可以直接使用该类加载器 双亲委派机制：类加载器收到类加载请求，自己不加载，向上委托给父类加载，父类加载不了，再自己加载。优势就是避免Java核心API篡改。 如何⾃定义⼀个类加载器？你使⽤过哪些或者你在什么场景下需要⼀个⾃ 定义的类加载器吗？ 自定义类加载的意义： 加载特定路径的class文件 加载一个加密的网络class文件 热部署加载class文件 描述一下JVM加载class文件的原理机制？ JVM中类的装载是由类加载器（ClassLoader）和它的子类来实现的，Java中的类加载器是一个重要的Java运行时系统组件，它负责在运行时查找和装入类文件中的类。 由于Java的跨平台性，经过编译的Java源程序并不是一个可执行程序，而是一个或多个类文件。当Java程序需要使用某个类时，JVM会确保这个类已经被加载、连接（验证、准备和解析）和初始化。 类的加载是指把类的.class文件中的数据读入到内存中，通常是创建一个字节数组读入.class文件，然后产生与所加载类对应的Class对象。加载完成后，Class对象还不完整，所以此时的类还不可用。当类被加载后就进入连接阶段，这一阶段包括验证、准备（为静态变量分配内存并设置默认的初始值）和解析（将符号引用替换为直接引用）三个步骤。最后JVM对类进行初始化，包括：1)如果类存在直接的父类并且这个类还没有被初始化，那么就先初始化父类；2)如果类中存在初始化语句，就依次执行这些初始化语句。类的加载是由类加载器完成的，类加载器包括：根加载器（BootStrap）、扩展加载器（Extension）、系统加载器（System）和用户自定义类加载器（java.lang.ClassLoader的子类）。从Java 2（JDK 1.2）开始，类加载过程采取了父亲委托机制（PDM）。PDM更好的保证了Java平台的安全性，在该机制中，JVM自带的Bootstrap是根加载器，其他的加载器都有且仅有一个父类加载器。类的加载首先请求父类加载器加载，父类加载器无能为力时才由其子类加载器自行加载。JVM不会向Java程序提供对Bootstrap的引用。 下面是关于几个类加载器的说明： ootstrap：一般用本地代码实现，负责加载JVM基础核心类库（rt.jar）； Extension：从java.ext.dirs系统属性所指定的目录中加载类库，它的父加载器是Bootstrap； System：又叫应用类加载器，其父类是Extension。它是应用最广泛的类加载器。它从环境变量classpath或者系统属性java.class.path所指定的目录中记载类，是用户自定义加载器的默认父加载器。 Java对象创建过程 JVM遇到一条新建对象的指令时首先去检查这个指令的参数是否能在常量池中定义到一个类的符号引用。然后加载这个类（类加载过程在后边讲） 为对象分配内存。一种办法“指针碰撞”、一种办法“空闲列表”，最终常用的办法“本地线程缓冲分配(TLAB)” 将除对象头外的对象内存空间初始化为0 对对象头进行必要设置 类的生命周期 类的生命周期包括这几个部分，加载、连接、初始化、使用和卸载，其中前三部是类的加载的过程,如下图: 加载，查找并加载类的二进制数据，在Java堆中也创建一个java.lang.Class类的对象 连接，连接又包含三块内容：验证、准备、初始化。 1）验证，文件格式、元数据、字节码、符号引用验证； 2）准备，为类的静态变量分配内存，并将其初始化为默认值； 3）解析，把类中的符号引用转换为直接引用 初始化，为类的静态变量赋予正确的初始值 使用，new出对象程序中使用 卸载，执行垃圾回收 Java 中会存在内存泄漏吗，请简单描述。 理论上Java因为有垃圾回收机制（GC）不会存在内存泄露问题（这也是Java被广泛使用于服务器端编程的一个重要原因）；然而在实际开发中，可能会存在无用但可达的对象，这些对象不能被GC回收，因此也会导致内存泄露的发生。例如hibernate的Session（一级缓存）中的对象属于持久态，垃圾回收器是不会回收这些对象的，然而这些对象中可能存在无用的垃圾对象，如果不及时关闭（close）或清空（flush）一级缓存就可能导致内存泄露。下面例子中的代码也会导致内存泄露。 import java.util.Arrays; import java.util.EmptyStackException; public class MyStack { private T[] elements; private int size = 0; private static final int INIT_CAPACITY = 16; public MyStack() { elements = (T[]) new Object[INIT_CAPACITY]; } public void push(T elem) { ensureCapacity(); elements[size++] = elem; } public T pop() { if(size == 0) throw new EmptyStackException(); return elements[--size]; } private void ensureCapacity() { if(elements.length == size) { elements = Arrays.copyOf(elements, 2 * size + 1); } } } 上面的代码实现了一个栈（先进后出（FILO））结构，乍看之下似乎没有什么明显的问题，它甚至可以通过你编写的各种单元测试。 然而其中的pop方法却存在内存泄露的问题，当我们用pop方法弹出栈中的对象时，该对象不会被当作垃圾回收，即使使用栈的程序不再引用这些对象，因为栈内部维护着对这些对象的过期引用（obsolete reference）。在支持垃圾回收的语言中，内存泄露是很隐蔽的，这种内存泄露其实就是无意识的对象保持。 如果一个对象引用被无意识的保留起来了，那么垃圾回收器不会处理这个对象，也不会处理该对象引用的其他对象，即使这样的对象只有少数几个，也可能会导致很多的对象被排除在垃圾回收之外，从而对性能造成重大影响，极端情况下会引发Disk Paging（物理内存与硬盘的虚拟内存交换数据），甚至造成OutOfMemoryError。 GC是什么？为什么要有GC？ GC是垃圾收集的意思，内存处理是编程人员容易出现问题的地方，忘记或者错误的内存回收会导致程序或系统的不稳定甚至崩溃，Java提供的GC功能可以自动监测对象是否超过作用域从而达到自动回收内存的目的，Java语言没有提供释放已分配内存的显示操作方法。 Java程序员不用担心内存管理，因为垃圾收集器会自动进行管理。要请求垃圾收集，可以调用下面的方法之一：System.gc() 或Runtime.getRuntime().gc() ，但JVM可以屏蔽掉显示的垃圾回收调用。 垃圾回收可以有效的防止内存泄露，有效的使用可以使用的内存。垃圾回收器通常是作为一个单独的低优先级的线程运行，不可预知的情况下对内存堆中已经死亡的或者长时间没有使用的对象进行清除和回收，程序员不能实时的调用垃圾回收器对某个对象或所有对象进行垃圾回收。 在Java诞生初期，垃圾回收是Java最大的亮点之一，因为服务器端的编程需要有效的防止内存泄露问题，然而时过境迁，如今Java的垃圾回收机制已经成为被诟病的东西。移动智能终端用户通常觉得iOS的系统比Android系统有更好的用户体验，其中一个深层次的原因就在于Android系统中垃圾回收的不可预知性。 补充：垃圾回收机制有很多种，包括：分代复制垃圾回收、标记垃圾回收、增量垃圾回收等方式。标准的Java进程既有栈又有堆。栈保存了原始型局部变量，堆保存了要创建的对象。Java平台对堆内存回收和再利用的基本算法被称为标记和清除，但是Java对其进行了改进，采用“分代式垃圾收集”。这种方法会跟Java对象的生命周期将堆内存划分为不同的区域，在垃圾收集过程中，可能会将对象移动到不同区域： 伊甸园（Eden）：这是对象最初诞生的区域，并且对大多数对象来说，这里是它们唯一存在过的区域。 幸存者乐园（Survivor）：从伊甸园幸存下来的对象会被挪到这里。 终身颐养园（Tenured）：这是足够老的幸存对象的归宿。年轻代收集（Minor-GC）过程是不会触及这个地方的。当年轻代收集不能把对象放进终身颐养园时，就会触发一次完全收集（Major-GC），这里可能还会牵扯到压缩，以便为大对象腾出足够的空间。 与垃圾回收相关的JVM参数： -Xms / -Xmx — 堆的初始大小 / 堆的最大大小 -Xmn — 堆中年轻代的大小 -XX:-DisableExplicitGC — 让System.gc()不产生任何作用 -XX:+PrintGCDetails — 打印GC的细节 -XX:+PrintGCDateStamps — 打印GC操作的时间戳 -XX:NewSize / XX:MaxNewSize — 设置新生代大小/新生代最大大小 -XX:NewRatio — 可以设置老生代和新生代的比例 -XX:PrintTenuringDistribution — 设置每次新生代GC后输出幸存者乐园中对象年龄的分布 -XX:InitialTenuringThreshold / -XX:MaxTenuringThreshold：设置老年代阀值的初始值和最大值 -XX:TargetSurvivorRatio：设置幸存区的目标使用率 做GC时，⼀个对象在内存各个Space中被移动的顺序是什么？ 标记清除法，复制算法，标记整理、分代算法。 新生代一般采用复制算法 GC，老年代使用标记整理算法。 垃圾收集器：串行新生代收集器、串行老生代收集器、并行新生代收集器、并行老年代收集器。 CMS（Current Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器，它是一种并发收集器，采用的是Mark-Sweep算法。 详见 Java GC机制。 你知道哪些垃圾回收算法？ GC最基础的算法有三种： 标记 -清除算法、复制算法、标记-压缩算法，我们常用的垃圾回收器一般都采用分代收集算法。 标记-清除算法，“标记-清除”（Mark-Sweep）算法，如它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。 复制算法，“复制”（Copying）的收集算法，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。 标记-压缩算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存 分代收集算法，“分代收集”（Generational Collection）算法，把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。 更详细的内容参见深入理解垃圾回收算法：http://blog.csdn.net/dd864140130/article/details/50084471 垃圾回收器 Serial收集器，串行收集器是最古老，最稳定以及效率高的收集器，可能会产生较长的停顿，只使用一个线程去回收。 ParNew收集器，ParNew收集器其实就是Serial收集器的多线程版本。 Parallel收集器，Parallel Scavenge收集器类似ParNew收集器，Parallel收集器更关注系统的吞吐量。 Parallel Old 收集器，Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记－整理”算法 CMS收集器，CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。 G1收集器，G1 (Garbage-First)是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足GC停顿时间要求的同时,还具备高吞吐量性能特征 如何判断一个对象是否应该被回收 判断对象是否存活一般有两种方式： 引用计数：每个对象有一个引用计数属性，新增一个引用时计数加1，引用释放时计数减1，计数为0时可以回收。此方法简单，无法解决对象相互循环引用的问题。 可达性分析（Reachability Analysis）：从GC Roots开始向下搜索，搜索所走过的路径称为引用链。当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的，不可达对象。 JVM的永久代中会发生垃圾回收么？ 垃圾回收不会发生在永久代，如果永久代满了或者是超过了临界值，会触发完全垃圾回收(Full GC)。如果你仔细查看垃圾收集器的输出信息，就会发现永久代也是被回收的。这就是为什么正确的永久代大小对避免Full GC是非常重要的原因。请参考下Java8：从永久代到元数据区 (注：Java8中已经移除了永久代，新加了一个叫做元数据区的native内存区) 引用的分类 强引用：GC时不会被回收 软引用：描述有用但不是必须的对象，在发生内存溢出异常之前被回收 弱引用：描述有用但不是必须的对象，在下一次GC时被回收 虚引用（幽灵引用/幻影引用）:无法通过虚引用获得对象，用PhantomReference实现虚引用，虚引用用来在GC时返回一个通知。 调优命令 Sun JDK监控和故障处理命令有jps jstat jmap jhat jstack jinfo jps，JVM Process Status Tool,显示指定系统内所有的HotSpot虚拟机进程。 jstat，JVM statistics Monitoring是用于监视虚拟机运行时状态信息的命令，它可以显示出虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据。 jmap，JVM Memory Map命令用于生成heap dump文件 jhat，JVM Heap Analysis Tool命令是与jmap搭配使用，用来分析jmap生成的dump，jhat内置了一个微型的HTTP/HTML服务器，生成dump的分析结果后，可以在浏览器中查看 jstack，用于生成java虚拟机当前时刻的线程快照。 jinfo，JVM Configuration info 这个命令作用是实时查看和调整虚拟机运行参数。 调优工具 常用调优工具分为两类,jdk自带监控工具：jconsole和jvisualvm，第三方有：MAT(Memory Analyzer Tool)、GChisto。 jconsole，Java Monitoring and Management Console是从java5开始，在JDK中自带的java监控和管理控制台，用于对JVM中内存，线程和类等的监控 jvisualvm，jdk自带全能工具，可以分析内存快照、线程快照；监控内存变化、GC变化等。 MAT，Memory Analyzer Tool，一个基于Eclipse的内存分析工具，是一个快速、功能丰富的Java heap分析工具，它可以帮助我们查找内存泄漏和减少内存消耗 GChisto，一款专业分析gc日志的工具 jstack 是⼲什么的? jstat 呢？如果线上程序周期性地出现卡顿，你怀疑可 能是 GC 导致的，你会怎么来排查这个问题？线程⽇志⼀般你会看其中的什么 部分？ jstack 用来查询 Java 进程的堆栈信息。 jvisualvm 监控内存泄露，跟踪垃圾回收、执行时内存、cpu分析、线程分析。 详见Java jvisualvm简要说明，可参考 线上FullGC频繁的排查。 Minor GC与Full GC分别在什么时候发生？ 新生代内存不够用时候发生MGC也叫YGC，JVM内存不够的时候发生FGC 对象头，详细讲下 【Java对象解析】不得不了解的对象头 JVM源码分析之java对象头实现 JVM——深入分析对象的内存布局 你知道哪些或者你们线上使⽤什么GC策略？它有什么优势，适⽤于什么场景？ 参考：参考 触发JVM进行Full GC的情况及应对策略 你有没有遇到过OutOfMemory问题？你是怎么来处理这个问题的？处理 过程中有哪些收获？ permgen space、heap space 错误。 常见的原因 内存加载的数据量太大：一次性从数据库取太多数据； 集合类中有对对象的引用，使用后未清空，GC不能进行回收； 代码中存在循环产生过多的重复对象； 启动参数堆内存值小。 详见 Java 内存溢出（java.lang.OutOfMemoryError）的常见情况和处理方式总结。 JDK 1.8之后Perm Space有哪些变动? MetaSpace⼤⼩默认是⽆限的么? 还是你们会通过什么⽅式来指定⼤⼩？ JDK 1.8后用元空间替代了 Perm Space；字符串常量存放到堆内存中。 MetaSpace大小默认没有限制，一般根据系统内存的大小。JVM会动态改变此值。 -XX:MetaspaceSize：分配给类元数据空间（以字节计）的初始大小（Oracle逻辑存储上的初始高水位，the initial high-water-mark）。此值为估计值，MetaspaceSize的值设置的过大会延长垃圾回收时间。垃圾回收过后，引起下一次垃圾回收的类元数据空间的大小可能会变大。 -XX:MaxMetaspaceSize：分配给类元数据空间的最大值，超过此值就会触发Full GC，此值默认没有限制，但应取决于系统内存的大小。JVM会动态地改变此值。 StackOverflow异常有没有遇到过？⼀般你猜测会在什么情况下被触发？如何指定⼀个线程的堆栈⼤⼩？⼀般你们写多少？ 栈内存溢出，一般由栈内存的局部变量过爆了，导致内存溢出。出现在递归方法，参数个数过多，递归过深，递归没有出口。 "},"zother3-java_interview/java-basic/multithread.html":{"url":"zother3-java_interview/java-basic/multithread.html","title":"Multithread","keywords":"","body":"多线程 什么是进程，什么是线程，为什么需要多线程编程？ 进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动，是操作系统进行资源分配和调度的一个独立单位； 线程是进程的一个实体，是CPU调度和分派的基本单位，是比进程更小的能独立运行的基本单位。线程的划分尺度小于进程，这使得多线程程序的并发性高；进程在执行时通常拥有独立的内存单元，而线程之间可以共享内存。 使用多线程的编程通常能够带来更好的性能和用户体验，但是多线程的程序对于其他程序是不友好的，因为它可能占用了更多的CPU资源。当然，也不是线程越多，程序的性能就越好，因为线程之间的调度和切换也会浪费CPU时间。时下很时髦的Node.js就采用了单线程异步I/O的工作模式。 什么是线程安全 如果你的代码在多线程下执行和在单线程下执行永远都能获得一样的结果，那么你的代码就是线程安全的。 这个问题有值得一提的地方，就是线程安全也是有几个级别的： 不可变。像String、Integer、Long这些，都是final类型的类，任何一个线程都改变不了它们的值，要改变除非新创建一个，因此这些不可变对象不需要任何同步手段就可以直接在多线程环境下使用 绝对线程安全。不管运行时环境如何，调用者都不需要额外的同步措施。要做到这一点通常需要付出许多额外的代价，Java中标注自己是线程安全的类，实际上绝大多数都不是线程安全的，不过绝对线程安全的类，Java中也有，比方说CopyOnWriteArrayList、CopyOnWriteArraySet 相对线程安全。相对线程安全也就是我们通常意义上所说的线程安全，像Vector这种，add、remove方法都是原子操作，不会被打断，但也仅限于此，如果有个线程在遍历某个Vector、有个线程同时在add这个Vector，99%的情况下都会出现ConcurrentModificationException，也就是fail-fast机制。 线程非安全。这个就没什么好说的了，ArrayList、LinkedList、HashMap等都是线程非安全的类 编写多线程程序有几种实现方式？ Java 5以前实现多线程有两种实现方法：一种是继承Thread类；另一种是实现Runnable接口。 两种方式都要通过重写run()方法来定义线程的行为，推荐使用后者，因为Java中的继承是单继承，一个类有一个父类，如果继承了Thread类就无法再继承其他类了，显然使用Runnable接口更为灵活。 Java 5以后创建线程还有第三种方式：实现Callable接口，该接口中的call方法可以在线程执行结束时产生一个返回值。 synchronized关键字的用法？ synchronized关键字可以将对象或者方法标记为同步，以实现对对象和方法的互斥访问，可以用synchronized(对象) { … }定义同步代码块，或者在声明方法时将synchronized作为方法的修饰符。 简述synchronized 和java.util.concurrent.locks.Lock的异同？ Lock是Java 5以后引入的新的API，和关键字synchronized相比主要相同点：Lock 能完成synchronized所实现的所有功能；主要不同点：Lock有比synchronized更精确的线程语义和更好的性能，而且不强制性的要求一定要获得锁。synchronized会自动释放锁，而Lock一定要求程序员手工释放，并且最好在finally 块中释放（这是释放外部资源的最好的地方）。 当一个线程进入一个对象的synchronized方法A之后，其它线程是否可进入此对象的synchronized方法B？ 不能。其它线程只能访问该对象的非同步方法，同步方法则不能进入。因为非静态方法上的synchronized修饰符要求执行方法时要获得对象的锁，如果已经进入A方法说明对象锁已经被取走，那么试图进入B方法的线程就只能在等锁池（注意不是等待池哦）中等待对象的锁。 synchronized和ReentrantLock的区别 synchronized是和if、else、for、while一样的关键字，ReentrantLock是类，这是二者的本质区别。既然ReentrantLock是类，那么它就提供了比synchronized更多更灵活的特性，可以被继承、可以有方法、可以有各种各样的类变量，ReentrantLock比synchronized的扩展性体现在几点上： ReentrantLock可以对获取锁的等待时间进行设置，这样就避免了死锁 ReentrantLock可以获取各种锁的信息 ReentrantLock可以灵活地实现多路通知 另外，二者的锁机制其实也是不一样的:ReentrantLock底层调用的是Unsafe的park方法加锁，synchronized操作的应该是对象头中mark word. 举例说明同步和异步。 如果系统中存在临界资源（资源数量少于竞争资源的线程数量的资源），例如正在写的数据以后可能被另一个线程读到，或者正在读的数据可能已经被另一个线程写过了，那么这些数据就必须进行同步存取（数据库操作中的排他锁就是最好的例子）。当应用程序在对象上调用了一个需要花费很长时间来执行的方法，并且不希望让程序等待方法的返回时，就应该使用异步编程，在很多情况下采用异步途径往往更有效率。事实上，所谓的同步就是指阻塞式操作，而异步就是非阻塞式操作。 启动一个线程是调用run()还是start()方法？ 启动一个线程是调用start()方法，使线程所代表的虚拟处理机处于可运行状态，这意味着它可以由JVM 调度并执行，这并不意味着线程就会立即运行。run()方法是线程启动后要进行回调（callback）的方法。 为什么需要run()和start()方法，我们可以只用run()方法来完成任务吗？ 我们需要run()&start()这两个方法是因为JVM创建一个单独的线程不同于普通方法的调用，所以这项工作由线程的start方法来完成，start由本地方法实现，需要显示地被调用，使用这俩个方法的另外一个好处是任何一个对象都可以作为线程运行，只要实现了Runnable接口，这就避免因继承了Thread类而造成的Java的多继承问题。 什么是线程池（thread pool）？ 在面向对象编程中，创建和销毁对象是很费时间的，因为创建一个对象要获取内存资源或者其它更多资源。 在Java中更是如此，虚拟机将试图跟踪每一个对象，以便能够在对象销毁后进行垃圾回收。所以提高服务程序效率的一个手段就是尽可能减少创建和销毁对象的次数，特别是一些很耗资源的对象创建和销毁，这就是“池化资源”技术产生的原因。线程池顾名思义就是事先创建若干个可执行的线程放入一个池（容器）中，需要的时候从池中获取线程不用自行创建，使用完毕不需要销毁线程而是放回池中，从而减少创建和销毁线程对象的开销。 Java 5+中的Executor接口定义一个执行线程的工具。它的子类型即线程池接口是ExecutorService。要配置一个线程池是比较复杂的，尤其是对于线程池的原理不是很清楚的情况下，因此在工具类Executors面提供了一些静态工厂方法，生成一些常用的线程池，如下所示： newSingleThreadExecutor：创建一个单线程的线程池。这个线程池只有一个线程在工作，也就是相当于单线程串行执行所有任务。如果这个唯一的线程因为异常结束，那么会有一个新的线程来替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。 newFixedThreadPool：创建固定大小的线程池。每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到最大值就会保持不变，如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。 newCachedThreadPool：创建一个可缓存的线程池。如果线程池的大小超过了处理任务所需要的线程，那么就会回收部分空闲（60秒不执行任务）的线程，当任务数增加时，此线程池又可以智能的添加新线程来处理任务。此线程池不会对线程池大小做限制，线程池大小完全依赖于操作系统（或者说JVM）能够创建的最大线程大小。 newScheduledThreadPool：创建一个大小无限的线程池。此线程池支持定时以及周期性执行任务的需求。 newSingleThreadExecutor：创建一个单线程的线程池。此线程池支持定时以及周期性执行任务的需求。 线程的基本状态以及状态之间的关系？ 其中Running表示运行状态；Runnable表示就绪状态（万事俱备，只欠CPU）；Blocked表示阻塞状态；阻塞状态又有多种情况，可能是因为调用wait()方法进入等待池，也可能是执行同步方法或同步代码块进入等锁池，或者是调用了sleep()方法或join()方法等待休眠或其他线程结束，或是因为发生了I/O中断。 Java中如何实现序列化，有什么意义？ 序列化就是一种用来处理对象流的机制，所谓对象流也就是将对象的内容进行流化。可以对流化后的对象进行读写操作，也可将流化后的对象传输于网络之间。序列化是为了解决对象流读写操作时可能引发的问题（如果不进行序列化可能会存在数据乱序的问题）。 要实现序列化，需要让一个类实现Serializable接口，该接口是一个标识性接口，标注该类对象是可被序列化的，然后使用一个输出流来构造一个对象输出流并通过writeObject(Object)方法就可以将实现对象写出（即保存其状态）；如果需要反序列化则可以用一个输入流建立对象输入流，然后通过readObject方法从流中读取对象。序列化除了能够实现对象的持久化之外，还能够用于对象的深度克隆。 产生死锁的条件 互斥条件：一个资源每次只能被一个进程使用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放 不剥夺条件:进程已获得的资源，在末使用完之前，不能强行剥夺。 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。 什么是线程饿死，什么是活锁？ 线程饿死和活锁虽然不想是死锁一样的常见问题，但是对于并发编程的设计者来说就像一次邂逅一样。 当所有线程阻塞，或者由于需要的资源无效而不能处理，不存在非阻塞线程使资源可用。JavaAPI中线程活锁可能发生在以下情形： 当所有线程在程序中执行Object.wait(0)，参数为0的wait方法。程序将发生活锁直到在相应的对象上有线程调用Object.notify()或者Object.notifyAll()。 当所有线程卡在无限循环中。 什么导致线程阻塞 阻塞指的是暂停一个线程的执行以等待某个条件发生（如某资源就绪），学过操作系统的同学对它一定已经很熟悉了。Java 提供了大量方法来支持阻塞，下面让我们逐一分析。 方法 说明 sleep() sleep() 允许 指定以毫秒为单位的一段时间作为参数，它使得线程在指定的时间内进入阻塞状态，不能得到CPU 时间，指定的时间一过，线程重新进入可执行状态。 典型地，sleep() 被用在等待某个资源就绪的情形：测试发现条件不满足后，让线程阻塞一段时间后重新测试，直到条件满足为止 suspend() 和 resume() 两个方法配套使用，suspend()使得线程进入阻塞状态，并且不会自动恢复，必须其对应的resume() 被调用，才能使得线程重新进入可执行状态。典型地，suspend() 和 resume() 被用在等待另一个线程产生的结果的情形：测试发现结果还没有产生后，让线程阻塞，另一个线程产生了结果后，调用 resume() 使其恢复。 yield() yield() 使当前线程放弃当前已经分得的CPU 时间，但不使当前线程阻塞，即线程仍处于可执行状态，随时可能再次分得 CPU 时间。调用 yield() 的效果等价于调度程序认为该线程已执行了足够的时间从而转到另一个线程。 wait() 和 notify() 两个方法配套使用，wait() 使得线程进入阻塞状态，它有两种形式，一种允许 指定以毫秒为单位的一段时间作为参数，另一种没有参数，前者当对应的 notify() 被调用或者超出指定时间时线程重新进入可执行状态，后者则必须对应的 notify() 被调用. 怎么检测一个线程是否持有对象监视器 Thread类提供了一个holdsLock(Object obj)方法，当且仅当对象obj的监视器被某条线程持有的时候才会返回true，注意这是一个static方法，这意味着”某条线程”指的是当前线程。 请说出与线程同步以及线程调度相关的方法。 wait()：使一个线程处于等待（阻塞）状态，并且释放所持有的对象的锁； sleep()：使一个正在运行的线程处于睡眠状态，是一个静态方法，调用此方法要处理InterruptedException异常； notify()：唤醒一个处于等待状态的线程，当然在调用此方法的时候，并不能确切的唤醒某一个等待状态的线程，而是由JVM确定唤醒哪个线程，而且与优先级无关； notityAll()：唤醒所有处于等待状态的线程，该方法并不是将对象的锁给所有线程，而是让它们竞争，只有获得锁的线程才能进入就绪状态； sleep() 、join（）、yield（）有什么区别 sleep()方法给其他线程运行机会时不考虑线程的优先级，因此会给低优先级的线程以运行的机会；yield()方法只会给相同优先级或更高优先级的线程以运行的机会； 线程执行sleep()方法后转入阻塞（blocked）状态，而执行yield()方法后转入就绪（ready）状态； sleep()方法声明抛出InterruptedException，而yield()方法没有声明任何异常； sleep()方法比yield()方法（跟操作系统CPU调度相关）具有更好的可移植性。 wait(),notify()和suspend(),resume()之间的区别 初看起来它们与 suspend() 和 resume() 方法对没有什么分别，但是事实上它们是截然不同的。区别的核心在于，前面叙述的所有方法，阻塞时都不会释放占用的锁（如果占用了的话），而这一对方法则相反。上述的核心区别导致了一系列的细节上的区别。 首先，前面叙述的所有方法都隶属于 Thread 类，但是这一对却直接隶属于 Object 类，也就是说，所有对象都拥有这一对方法。初看起来这十分不可思议，但是实际上却是很自然的，因为这一对方法阻塞时要释放占用的锁，而锁是任何对象都具有的，调用任意对象的 wait() 方法导致线程阻塞，并且该对象上的锁被释放。而调用 任意对象的notify()方法则导致从调用该对象的 wait() 方法而阻塞的线程中随机选择的一个解除阻塞（但要等到获得锁后才真正可执行）。 其次，前面叙述的所有方法都可在任何位置调用，但是这一对方法却必须在 synchronized 方法或块中调用，理由也很简单，只有在synchronized 方法或块中当前线程才占有锁，才有锁可以释放。同样的道理，调用这一对方法的对象上的锁必须为当前线程所拥有，这样才有锁可以释放。因此，这一对方法调用必须放置在这样的 synchronized 方法或块中，该方法或块的上锁对象就是调用这一对方法的对象。若不满足这一条件，则程序虽然仍能编译，但在运行时会出现IllegalMonitorStateException 异常。 wait() 和 notify() 方法的上述特性决定了它们经常和synchronized关键字一起使用，将它们和操作系统进程间通信机制作一个比较就会发现它们的相似性：synchronized方法或块提供了类似于操作系统原语的功能，它们的执行不会受到多线程机制的干扰，而这一对方法则相当于 block 和wakeup 原语（这一对方法均声明为 synchronized）。它们的结合使得我们可以实现操作系统上一系列精妙的进程间通信的算法（如信号量算法），并用于解决各种复杂的线程间通信问题。 关于 wait() 和 notify() 方法最后再说明两点： 第一：调用 notify() 方法导致解除阻塞的线程是从因调用该对象的 wait() 方法而阻塞的线程中随机选取的，我们无法预料哪一个线程将会被选择，所以编程时要特别小心，避免因这种不确定性而产生问题。 第二：除了 notify()，还有一个方法 notifyAll() 也可起到类似作用，唯一的区别在于，调用 notifyAll() 方法将把因调用该对象的 wait() 方法而阻塞的所有线程一次性全部解除阻塞。当然，只有获得锁的那一个线程才能进入可执行状态。 谈到阻塞，就不能不谈一谈死锁，略一分析就能发现，suspend() 方法和不指定超时期限的 wait() 方法的调用都可能产生死锁。遗憾的是，Java 并不在语言级别上支持死锁的避免，我们在编程中必须小心地避免死锁。 以上我们对 Java 中实现线程阻塞的各种方法作了一番分析，我们重点分析了 wait() 和 notify() 方法，因为它们的功能最强大，使用也最灵活，但是这也导致了它们的效率较低，较容易出错。实际使用中我们应该灵活使用各种方法，以便更好地达到我们的目的。 为什么wait()方法和notify()/notifyAll()方法要在同步块中被调用 这是JDK强制的，wait()方法和notify()/notifyAll()方法在调用前都必须先获得对象的锁 wait()方法和notify()/notifyAll()方法在放弃对象监视器时有什么区别 wait()方法和notify()/notifyAll()方法在放弃对象监视器的时候的区别在于：wait()方法立即释放对象监视器，notify()/notifyAll()方法则会等待线程剩余代码执行完毕才会放弃对象监视器。 Runnable和Callable的区别 Runnable接口中的run()方法的返回值是void，它做的事情只是纯粹地去执行run()方法中的代码而已；Callable接口中的call()方法是有返回值的，是一个泛型，和Future、FutureTask配合可以用来获取异步执行的结果。 这其实是很有用的一个特性，因为多线程相比单线程更难、更复杂的一个重要原因就是因为多线程充满着未知性，某条线程是否执行了？某条线程执行了多久？某条线程执行的时候我们期望的数据是否已经赋值完毕？无法得知，我们能做的只是等待这条多线程的任务执行完毕而已。而Callable+Future/FutureTask却可以方便获取多线程运行的结果，可以在等待时间太长没获取到需要的数据的情况下取消该线程的任务。 Thread类的sleep()方法和对象的wait()方法都可以让线程暂停执行，它们有什么区别？ sleep()方法（休眠）是线程类（Thread）的静态方法，调用此方法会让当前线程暂停执行指定的时间，将执行机会（CPU）让给其他线程，但是对象的锁依然保持，因此休眠时间结束后会自动恢复。 wait()是Object类的方法，调用对象的wait()方法导致当前线程放弃对象的锁（线程暂停执行），进入对象的等待池（wait pool），只有调用对象的notify()方法（或notifyAll()方法）时才能唤醒等待池中的线程进入等锁池（lock pool），如果线程重新获得对象的锁就可以进入就绪状态。 线程的sleep()方法和yield()方法有什么区别？ sleep()方法给其他线程运行机会时不考虑线程的优先级，因此会给低优先级的线程以运行的机会；yield()方法只会给相同优先级或更高优先级的线程以运行的机会； 线程执行sleep()方法后转入阻塞（blocked）状态，而执行yield()方法后转入就绪（ready）状态； sleep()方法声明抛出InterruptedException，而yield()方法没有声明任何异常； sleep()方法比yield()方法（跟操作系统CPU调度相关）具有更好的可移植性。 为什么wait,nofity和nofityAll这些方法不放在Thread类当中 一个很明显的原因是JAVA提供的锁是对象级的而不是线程级的，每个对象都有锁，通过线程获得。如果线程需要等待某些锁那么调用对象中的wait()方法就有意义了。如果wait()方法定义在Thread类中，线程正在等待的是哪个锁就不明显了。简单的说，由于wait，notify和notifyAll都是锁级别的操作，所以把他们定义在Object类中因为锁属于对象。 怎么唤醒一个阻塞的线程 如果线程是因为调用了wait()、sleep()或者join()方法而导致的阻塞，可以中断线程，并且通过抛出InterruptedException来唤醒它；如果线程遇到了IO阻塞，无能为力，因为IO是操作系统实现的，Java代码并没有办法直接接触到操作系统。 什么是多线程的上下文切换 多线程的上下文切换是指CPU控制权由一个已经正在运行的线程切换到另外一个就绪并等待获取CPU执行权的线程的过程。 FutureTask是什么 这个其实前面有提到过，FutureTask表示一个异步运算的任务。FutureTask里面可以传入一个Callable的具体实现类，可以对这个异步运算的任务的结果进行等待获取、判断是否已经完成、取消任务等操作。当然，由于FutureTask也是Runnable接口的实现类，所以FutureTask也可以放入线程池中。 一个线程如果出现了运行时异常怎么办？ 如果这个异常没有被捕获的话，这个线程就停止执行了。另外重要的一点是：如果这个线程持有某个某个对象的监视器，那么这个对象监视器会被立即释放 Java当中有哪几种锁 自旋锁: 自旋锁在JDK1.6之后就默认开启了。基于之前的观察，共享数据的锁定状态只会持续很短的时间，为了这一小段时间而去挂起和恢复线程有点浪费，所以这里就做了一个处理，让后面请求锁的那个线程在稍等一会，但是不放弃处理器的执行时间，看看持有锁的线程能否快速释放。为了让线程等待，所以需要让线程执行一个忙循环也就是自旋操作。在jdk6之后，引入了自适应的自旋锁，也就是等待的时间不再固定了，而是由上一次在同一个锁上的自旋时间及锁的拥有者状态来决定 偏向锁: 在JDK1.之后引入的一项锁优化，目的是消除数据在无竞争情况下的同步原语。进一步提升程序的运行性能。 偏向锁就是偏心的偏，意思是这个锁会偏向第一个获得他的线程，如果接下来的执行过程中，改锁没有被其他线程获取，则持有偏向锁的线程将永远不需要再进行同步。偏向锁可以提高带有同步但无竞争的程序性能，也就是说他并不一定总是对程序运行有利，如果程序中大多数的锁都是被多个不同的线程访问，那偏向模式就是多余的，在具体问题具体分析的前提下，可以考虑是否使用偏向锁。 轻量级锁: 为了减少获得锁和释放锁所带来的性能消耗，引入了“偏向锁”和“轻量级锁”，所以在Java SE1.6里锁一共有四种状态，无锁状态，偏向锁状态，轻量级锁状态和重量级锁状态，它会随着竞争情况逐渐升级。锁可以升级但不能降级，意味着偏向锁升级成轻量级锁后不能降级成偏向锁 如何在两个线程间共享数据 通过在线程之间共享对象就可以了，然后通过wait/notify/notifyAll、await/signal/signalAll进行唤起和等待，比方说阻塞队列BlockingQueue就是为线程之间共享数据而设计的 如何正确的使用wait()?使用if还是while？ wait() 方法应该在循环调用，因为当线程获取到 CPU 开始执行的时候，其他条件可能还没有满足，所以在处理前，循环检测条件是否满足会更好。下面是一段标准的使用 wait 和 notify 方法的代码： synchronized (obj) { while (condition does not hold) obj.wait(); // (Releases lock, and reacquires on wakeup) ... // Perform action appropriate to condition } 什么是线程局部变量ThreadLocal 线程局部变量是局限于线程内部的变量，属于线程自身所有，不在多个线程间共享。Java提供ThreadLocal类来支持线程局部变量，是一种实现线程安全的方式。但是在管理环境下（如 web 服务器）使用线程局部变量的时候要特别小心，在这种情况下，工作线程的生命周期比任何应用变量的生命周期都要长。任何线程局部变量一旦在工作完成后没有释放，Java 应用就存在内存泄露的风险。 ThreadLoal的作用是什么？ 简单说ThreadLocal就是一种以空间换时间的做法在每个Thread里面维护了一个ThreadLocal.ThreadLocalMap把数据进行隔离，数据不共享，自然就没有线程安全方面的问题了. ThreadLocal 原理分析 ThreadLocal为解决多线程程序的并发问题提供了一种新的思路。ThreadLocal，顾名思义是线程的一个本地化对象，当工作于多线程中的对象使用ThreadLocal维护变量时，ThreadLocal为每个使用该变量的线程分配一个独立的变量副本，所以每一个线程都可以独立的改变自己的副本，而不影响其他线程所对应的副本。从线程的角度看，这个变量就像是线程的本地变量。 ThreadLocal类非常简单好用，只有四个方法，能用上的也就是下面三个方法： void set(T value)：设置当前线程的线程局部变量的值。 T get()：获得当前线程所对应的线程局部变量的值。 void remove()：删除当前线程中线程局部变量的值。 ThreadLocal是如何做到为每一个线程维护一份独立的变量副本的呢？在ThreadLocal类中有一个Map，键为线程对象，值是其线程对应的变量的副本，自己要模拟实现一个ThreadLocal类其实并不困难，代码如下所示： import java.util.Collections; import java.util.HashMap; import java.util.Map; public class MyThreadLocal { private Map map = Collections.synchronizedMap(new HashMap()); public void set(T newValue) { map.put(Thread.currentThread(), newValue); } public T get() { return map.get(Thread.currentThread()); } public void remove() { map.remove(Thread.currentThread()); } } 如果你提交任务时，线程池队列已满，这时会发生什么 如果你使用的LinkedBlockingQueue，也就是无界队列的话，没关系，继续添加任务到阻塞队列中等待执行，因为LinkedBlockingQueue可以近乎认为是一个无穷大的队列，可以无限存放任务；如果你使用的是有界队列比方说ArrayBlockingQueue的话，任务首先会被添加到ArrayBlockingQueue中，ArrayBlockingQueue满了，则会使用拒绝策略RejectedExecutionHandler处理满了的任务，默认是AbortPolicy。 为什么要使用线程池 避免频繁地创建和销毁线程，达到线程对象的重用。另外，使用线程池还可以根据项目灵活地控制并发的数目。 java中用到的线程调度算法是什么 抢占式。一个线程用完CPU之后，操作系统会根据线程优先级、线程饥饿情况等数据算出一个总的优先级并分配下一个时间片给某个线程执行。 Thread.sleep(0)的作用是什么 由于Java采用抢占式的线程调度算法，因此可能会出现某条线程常常获取到CPU控制权的情况，为了让某些优先级比较低的线程也能获取到CPU控制权，可以使用Thread.sleep(0)手动触发一次操作系统分配时间片的操作，这也是平衡CPU控制权的一种操作。 什么是CAS CAS，全称为Compare and Swap，即比较-替换。假设有三个操作数：内存值V、旧的预期值A、要修改的值B，当且仅当预期值A和内存值V相同时，才会将内存值修改为B并返回true，否则什么都不做并返回false。当然CAS一定要volatile变量配合，这样才能保证每次拿到的变量是主内存中最新的那个值，否则旧的预期值A对某条线程来说，永远是一个不会变的值A，只要某次CAS操作失败，永远都不可能成功 什么是乐观锁和悲观锁 乐观锁：乐观锁认为竞争不总是会发生，因此它不需要持有锁，将比较-替换这两个动作作为一个原子操作尝试去修改内存中的变量，如果失败则表示发生冲突，那么就应该有相应的重试逻辑。 悲观锁：悲观锁认为竞争总是会发生，因此每次对某资源进行操作时，都会持有一个独占的锁，就像synchronized，不管三七二十一，直接上了锁就操作资源了。 ConcurrentHashMap的并发度是什么？ ConcurrentHashMap的并发度就是segment的大小，默认为16，这意味着最多同时可以有16条线程操作ConcurrentHashMap，这也是ConcurrentHashMap对Hashtable的最大优势，任何情况下，Hashtable能同时有两条线程获取Hashtable中的数据吗？ ConcurrentHashMap的工作原理 ConcurrentHashMap在jdk 1.6和jdk 1.8实现原理是不同的. jdk 1.6: ConcurrentHashMap是线程安全的，但是与Hashtablea相比，实现线程安全的方式不同。Hashtable是通过对hash表结构进行锁定，是阻塞式的，当一个线程占有这个锁时，其他线程必须阻塞等待其释放锁。ConcurrentHashMap是采用分离锁的方式，它并没有对整个hash表进行锁定，而是局部锁定，也就是说当一个线程占有这个局部锁时，不影响其他线程对hash表其他地方的访问。 具体实现:ConcurrentHashMap内部有一个Segment jdk 1.8 在jdk 8中，ConcurrentHashMap不再使用Segment分离锁，而是采用一种乐观锁CAS算法来实现同步问题，但其底层还是“数组+链表->红黑树”的实现。 CyclicBarrier和CountDownLatch区别 这两个类非常类似，都在java.util.concurrent下，都可以用来表示代码运行到某个点上，二者的区别在于： CyclicBarrier的某个线程运行到某个点上之后，该线程即停止运行，直到所有的线程都到达了这个点，所有线程才重新运行；CountDownLatch则不是，某线程运行到某个点上之后，只是给某个数值-1而已，该线程继续运行 CyclicBarrier只能唤起一个任务，CountDownLatch可以唤起多个任务 CyclicBarrier可重用，CountDownLatch不可重用，计数值为0该CountDownLatch就不可再用了 java中的++操作符线程安全么？ 不是线程安全的操作。它涉及到多个指令，如读取变量值，增加，然后存储回内存，这个过程可能会出现多个线程交差 有三个线程T1，T2，T3，怎么确保它们按顺序执行？ 在多线程中有多种方法让线程按特定顺序执行，你可以用线程类的join()方法在一个线程中启动另一个线程，另外一个线程完成该线程继续执行。为了确保三个线程的顺序你应该先启动最后一个(T3调用T2，T2调用T1)，这样T1就会先完成而T3最后完成。 如何在Java中创建Immutable对象？ 这个问题看起来和多线程没什么关系， 但不变性有助于简化已经很复杂的并发程序。Immutable对象可以在没有同步的情况下共享，降低了对该对象进行并发访问时的同步化开销。可是Java没有@Immutable这个注解符，要创建不可变类，要实现下面几个步骤：通过构造方法初始化所有成员、对变量不要提供setter方法、将所有的成员声明为私有的，这样就不允许直接访问这些成员、在getter方法中，不要直接返回对象本身，而是克隆对象，并返回对象的拷贝。 你有哪些多线程开发良好的实践？ 给线程命名 最小化同步范围 优先使用volatile 尽可能使用更高层次的并发工具而非wait和notify()来实现线程通信,如BlockingQueue,Semeaphore 优先使用并发容器而非同步容器. 考虑使用线程池 可以创建Volatile数组吗？ Java 中可以创建 volatile类型数组，不过只是一个指向数组的引用，而不是整个数组。如果改变引用指向的数组，将会受到volatile 的保护，但是如果多个线程同时改变数组的元素，volatile标示符就不能起到之前的保护作用了 Volatile关键字的作用 一个非常重要的问题，是每个学习、应用多线程的Java程序员都必须掌握的。理解volatile关键字的作用的前提是要理解Java内存模型，这里就不讲Java内存模型了，可以参见第31点，volatile关键字的作用主要有两个： 多线程主要围绕可见性和原子性两个特性而展开，使用volatile关键字修饰的变量，保证了其在多线程之间的可见性，即每次读取到volatile变量，一定是最新的数据 代码底层执行不像我们看到的高级语言—-Java程序这么简单，它的执行是Java代码–>字节码–>根据字节码执行对应的C/C++代码–>C/C++代码被编译成汇编语言–>和硬件电路交互，现实中，为了获取更好的性能JVM可能会对指令进行重排序，多线程下可能会出现一些意想不到的问题。使用volatile则会对禁止语义重排序，当然这也一定程度上降低了代码执行效率 从实践角度而言，volatile的一个重要作用就是和CAS结合，保证了原子性，详细的可以参见java.util.concurrent.atomic包下的类，比如AtomicInteger。 volatile能使得一个非原子操作变成原子操作吗？ 一个典型的例子是在类中有一个 long 类型的成员变量。如果你知道该成员变量会被多个线程访问，如计数器、价格等，你最好是将其设置为 volatile。为什么？因为 Java 中读取 long 类型变量不是原子的，需要分成两步，如果一个线程正在修改该 long 变量的值，另一个线程可能只能看到该值的一半（前 32 位）。但是对一个 volatile 型的 long 或 double 变量的读写是原子。 一种实践是用 volatile 修饰 long 和 double 变量，使其能按原子类型来读写。double 和 long 都是64位宽，因此对这两种类型的读是分为两部分的，第一次读取第一个 32 位，然后再读剩下的 32 位，这个过程不是原子的，但 Java 中 volatile 型的 long 或 double 变量的读写是原子的。volatile 修复符的另一个作用是提供内存屏障（memory barrier），例如在分布式框架中的应用。简单的说，就是当你写一个 volatile 变量之前，Java 内存模型会插入一个写屏障（write barrier），读一个 volatile 变量之前，会插入一个读屏障（read barrier）。意思就是说，在你写一个 volatile 域时，能保证任何线程都能看到你写的值，同时，在写之前，也能保证任何数值的更新对所有线程是可见的，因为内存屏障会将其他所有写的值更新到缓存。 volatile类型变量提供什么保证？ volatile 主要有两方面的作用:1.避免指令重排2.可见性保证.例如，JVM 或者 JIT为了获得更好的性能会对语句重排序，但是 volatile 类型变量即使在没有同步块的情况下赋值也不会与其他语句重排序。 volatile 提供 happens-before 的保证，确保一个线程的修改能对其他线程是可见的。某些情况下，volatile 还能提供原子性，如读 64 位数据类型，像 long 和 double 都不是原子的(低32位和高32位)，但 volatile 类型的 double 和 long 就是原子的. Java 中，编写多线程程序的时候你会遵循哪些最佳实践？ 这是我在写Java 并发程序的时候遵循的一些最佳实践： 给线程命名，这样可以帮助调试。 最小化同步的范围，而不是将整个方法同步，只对关键部分做同步。 如果可以，更偏向于使用 volatile 而不是 synchronized。 使用更高层次的并发工具，而不是使用 wait() 和 notify() 来实现线程间通信，如 BlockingQueue，CountDownLatch 及 Semeaphore。 优先使用并发集合，而不是对集合进行同步。并发集合提供更好的可扩展性。 说出至少 5 点在 Java 中使用线程的最佳实践。 这个问题与之前的问题类似，你可以使用上面的答案。对线程来说，你应该： 对线程命名 将线程和任务分离，使用线程池执行器来执行 Runnable 或 Callable。 使用线程池 Java中如何获取到线程dump文件 死循环、死锁、阻塞、页面打开慢等问题，打线程dump是最好的解决问题的途径。所谓线程dump也就是线程堆栈，获取到线程堆栈有两步： 获取到线程的pid，可以通过使用jps命令，在Linux环境下还可以使用ps -ef | grep java 打印线程堆栈，可以通过使用jstack pid命令，在Linux环境下还可以使用kill -3 pid 另外提一点，Thread类提供了一个getStackTrace()方法也可以用于获取线程堆栈。这是一个实例方法，因此此方法是和具体线程实例绑定的，每次获取获取到的是具体某个线程当前运行的堆栈。 高并发、任务执行时间短的业务怎样使用线程池？并发不高、任务执行时间长的业务怎样使用线程池？并发高、业务执行时间长的业务怎样使用线程池？ 这是我在并发编程网上看到的一个问题，把这个问题放在最后一个，希望每个人都能看到并且思考一下，因为这个问题非常好、非常实际、非常专业。关于这个问题，个人看法是： 高并发、任务执行时间短的业务，线程池线程数可以设置为CPU核数+1，减少线程上下文的切换 并发不高、任务执行时间长的业务要区分开看： 假如是业务时间长集中在IO操作上，也就是IO密集型的任务，因为IO操作并不占用CPU，所以不要让所有的CPU闲下来，可以加大线程池中的线程数目，让CPU处理更多的业务 假如是业务时间长集中在计算操作上，也就是计算密集型任务，这个就没办法了，和（1）一样吧，线程池中的线程数设置得少一些，减少线程上下文的切换 并发高、业务执行时间长，解决这种类型任务的关键不在于线程池而在于整体架构的设计，看看这些业务里面某些数据是否能做缓存是第一步，增加服务器是第二步，至于线程池的设置，设置参考（2）。 业务执行时间长的问题，也可能需要分析一下，看看能不能使用中间件对任务进行拆分和解耦。 作业(进程)调度算法 先来先服务调度算法(FCFS) 每次调度都是从后备作业队列中选择一个或多个最先进入该队列的作业，将它们调入内存，为它们分配资源、创建进程，然后放入就绪队列。 短作业(进程)优先调度算法(SPF) 短作业优先(SJF)的调度算法是从后备队列中选择一个或若干个估计运行时间最短的作业，将它们调入内存运行。缺点:长作业的运行得不到保证 优先权调度算法(HPF) 当把该算法用于作业调度时，系统将从后备队列中选择若干个优先权最高的作业装入内存。当用于进程调度时，该算法是把处理机分配给就绪队列中优先权最高的进程，这时，又可进一步把该算法分成如下两种。 可以分为: 非抢占式优先权算法 抢占式优先权调度算法 高响应比优先调度算法(HRN) 每次选择高响应比最大的作业执行，响应比=(等待时间+要求服务时间)/要求服务时间。该算法同时考虑了短作业优先和先来先服务。 如果作业的等待时间相同，则要求服务的时间愈短，其优先权愈高，因而该算法有利于短作业。 当要求服务的时间相同时，作业的优先权决定于其等待时间，等待时间愈长，其优先权愈高，因而它实现的是先来先服务。 对于长作业，作业的优先级可以随等待时间的增加而提高，当其等待时间足够长时，其优先级便可升到很高，从而也可获得处理机。简言之，该算法既照顾了短作业，又考虑了作业到达的先后次序，不会使长作业长期得不到服务。因此，该算法实现了一种较好的折衷。当然，在利用该算法时，每要进行调度之前，都须先做响应比的计算，这会增加系统开销。 时间片轮转法（RR） 在早期的时间片轮转法中，系统将所有的就绪进程按先来先服务的原则排成一个队列，每次调度时，把CPU分配给队首进程，并令其执行一个时间片。时间片的大小从几ms到几百ms。当执行的时间片用完时，由一个计时器发出时钟中断请求，调度程序便据此信号来停止该进程的执行，并将它送往就绪队列的末尾；然后，再把处理机分配给就绪队列中新的队首进程，同时也让它执行一个时间片。这样就可以保证就绪队列中的所有进程在一给定的时间内均能获得一时间片的处理机执行时间。换言之，系统能在给定的时间内响应所有用户的请求。 多级反馈队列调度算法 它是目前被公认的一种较好的进程调度算法。 应设置多个就绪队列，并为各个队列赋予不同的优先级。第一个队列的优先级最高，第二个队列次之，其余各队列的优先权逐个降低。该算法赋予各个队列中进程执行时间片的大小也各不相同，在优先权愈高的队列中，为每个进程所规定的执行时间片就愈小。例如，第二个队列的时间片要比第一个队列的时间片长一倍，……，第i+1个队列的时间片要比第i个队列的时间片长一倍。 当一个新进程进入内存后，首先将它放入第一队列的末尾，按FCFS原则排队等待调度。当轮到该进程执行时，如它能在该时间片内完成，便可准备撤离系统；如果它在一个时间片结束时尚未完成，调度程序便将该进程转入第二队列的末尾，再同样地按FCFS原则等待调度执行；如果它在第二队列中运行一个时间片后仍未完成，再依次将它放入第三队列，……，如此下去，当一个长作业(进程)从第一队列依次降到第n队列后，在第n 队列便采取按时间片轮转的方式运行。 仅当第一队列空闲时，调度程序才调度第二队列中的进程运行；仅当第1～(i-1)队列均空时，才会调度第i队列中的进程运行。如果处理机正在第i队列中为某进程服务时，又有新进程进入优先权较高的队列(第1～(i-1)中的任何一个队列)，则此时新进程将抢占正在运行进程的处理机，即由调度程序把正在运行的进程放回到第i队列的末尾，把处理机分配给新到的高优先权进程。 讲讲线程池的实现原理 以下资源来源 首先要明确为什么要使用线程池，使用线程池会带来什么好处？ 线程是稀缺资源，不能频繁的创建。 应当将其放入一个池子中，可以给其他任务进行复用。 解耦作用，线程的创建于执行完全分开，方便维护。 创建一个线程池 以一个使用较多的 ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue workQueue, RejectedExecutionHandler handler); 为例： 其中的 corePoolSize 为线程池的基本大小。 maximumPoolSize 为线程池最大线程大小。 keepAliveTime 和 unit 则是线程空闲后的存活时间。 workQueue 用于存放任务的阻塞队列。 handler 当队列和最大线程池都满了之后的饱和策略。 处理流程 当提交一个任务到线程池时它的执行流程是怎样的呢？ 首先第一步会判断核心线程数有没有达到上限，如果没有则创建线程(会获取全局锁)，满了则会将任务丢进阻塞队列。 如果队列也满了则需要判断最大线程数是否达到上限，如果没有则创建线程(获取全局锁)，如果最大线程数也满了则会根据饱和策略处理。 常用的饱和策略有: 直接丢弃任务。 调用者线程处理。 丢弃队列中的最近任务，执行当前任务。 所以当线程池完成预热之后都是将任务放入队列，接着由工作线程一个个从队列里取出执行。 合理配置线程池 线程池并不是配置越大越好，而是要根据任务的熟悉来进行划分： 如果是 CPU 密集型任务应当分配较少的线程，比如 CPU 个数相当的大小。 如果是 IO 密集型任务，由于线程并不是一直在运行，所以可以尽可能的多配置线程，比如 CPU 个数 * 2 。 当是一个混合型任务，可以将其拆分为 CPU 密集型任务以及 IO 密集型任务，这样来分别配置。 synchronize 实现原理 以下资源来源 众所周知 Synchronize 关键字是解决并发问题常用解决方案，有以下三种使用方式: 同步普通方法，锁的是当前对象。 同步静态方法，锁的是当前 Class 对象。 同步块，锁的是 {} 中的对象。 实现原理： JVM 是通过进入、退出对象监视器( Monitor )来实现对方法、同步块的同步的。 具体实现是在编译之后在同步方法调用前加入一个 monitor.enter 指令，在退出方法和异常处插入 monitor.exit 的指令。 其本质就是对一个对象监视器( Monitor )进行获取，而这个获取过程具有排他性从而达到了同一时刻只能一个线程访问的目的。 而对于没有获取到锁的线程将会阻塞到方法入口处，直到获取锁的线程 monitor.exit 之后才能尝试继续获取锁。 流程图如下: synchronize 很多都称之为重量锁，JDK1.6 中对 synchronize 进行了各种优化，为了能减少获取和释放锁带来的消耗引入了偏向锁和轻量锁。 轻量锁 当代码进入同步块时，如果同步对象为无锁状态时，当前线程会在栈帧中创建一个锁记录(Lock Record)区域，同时将锁对象的对象头中 Mark Word 拷贝到锁记录中，再尝试使用 CAS 将 Mark Word 更新为指向锁记录的指针。 如果更新成功，当前线程就获得了锁。 如果更新失败 JVM 会先检查锁对象的 Mark Word 是否指向当前线程的锁记录。 如果是则说明当前线程拥有锁对象的锁，可以直接进入同步块。 不是则说明有其他线程抢占了锁，如果存在多个线程同时竞争一把锁，轻量锁就会膨胀为重量锁。 解锁 轻量锁的解锁过程也是利用 CAS 来实现的，会尝试锁记录替换回锁对象的 Mark Word 。如果替换成功则说明整个同步操作完成，失败则说明有其他线程尝试获取锁，这时就会唤醒被挂起的线程(此时已经膨胀为重量锁) 轻量锁能提升性能的原因是： 认为大多数锁在整个同步周期都不存在竞争，所以使用 CAS 比使用互斥开销更少。但如果锁竞争激烈，轻量锁就不但有互斥的开销，还有 CAS 的开销，甚至比重量锁更慢。 偏向锁 为了进一步的降低获取锁的代价，JDK1.6 之后还引入了偏向锁。 偏向锁的特征是:锁不存在多线程竞争，并且应由一个线程多次获得锁。 当线程访问同步块时，会使用 CAS 将线程 ID 更新到锁对象的 Mark Word 中，如果更新成功则获得偏向锁，并且之后每次进入这个对象锁相关的同步块时都不需要再次获取锁了。 释放锁 当有另外一个线程获取这个锁时，持有偏向锁的线程就会释放锁，释放时会等待全局安全点(这一时刻没有字节码运行)，接着会暂停拥有偏向锁的线程，根据锁对象目前是否被锁来判定将对象头中的 Mark Word 设置为无锁或者是轻量锁状态。 偏向锁可以提高带有同步却没有竞争的程序性能，但如果程序中大多数锁都存在竞争时，那偏向锁就起不到太大作用。可以使用 -XX:-userBiasedLocking=false 来关闭偏向锁，并默认进入轻量锁。 线程池的几种方式与使用场景 参考：线程池的种类，区别和使用场景 volatile 实现原理 禁止指令重排、刷新内存 参考： 【死磕Java并发】—–深入分析volatile的实现原理 深入分析Volatile的实现原理 synchronized 实现原理 （对象监视器） 说说 Semaphore 原理 Semaphore内部通过AQS的共享锁实现，使用了许可证(permits) 作为锁的计数。state保存剩余permits数量 获取 获取锁时将申请一个许可，如果剩余的许可足够分配，即state大于1的情况，通过自旋CAS操作设置state = state -1。否则返回失败 释放 释放锁的过程与获取锁过程相反 说说 Exchanger 原理 线程的生命周期 重入锁的概念 Java 1.5的JUC中提供了ReentrantLock这个可重入独占锁，该锁通过AQS独占锁实现，重入锁可在同一线程内多次获取。 获取 如果锁没有被获取，即state为0的情况下，通过CAS操作将state设置为1，并将当前线程设置为ExclusiveOwnerThread。 若锁已被获取，即state不为0的情况，首先判断ExclusiveOwnerThread是非为当前线程，如果为当前线程则将state加1，否则获取锁失败 释放 释放锁时检查ExclusiveOwnerThread和state，通过后将state减1。当state为0时锁将回到空闲状态，并设置ExclusiveOwnerThread为null，即锁可被任意线程获取 死锁问题 重入锁不可防止死锁，仅仅是同一线程可以重复获得锁。如释放不当仍然会引起死锁 如何检查死锁（通过jConsole检查死锁） AQS同步队列 什么是ABA问题，出现ABA问题JDK是如何解决的 乐观锁的业务场景及实现方式 "},"zother3-java_interview/java-basic/oop.html":{"url":"zother3-java_interview/java-basic/oop.html","title":"Oop","keywords":"","body":"面向对象 什么是对象 对象是系统中用来描述客观事物的一个实体，它是构成系统的一个基本单位。一个对象由一组属性和对这组属性进行操作的一组服务组成。 类的实例化可生成对象，一个对象的生命周期包括三个阶段：生成、使用、消除。 当不存在对一个对象的引用时，该对象成为一个无用对象。Java的垃圾收集器自动扫描对象的动态内存区，把没有引用的对象作为垃圾收集起来并释放。当系统内存用尽或调用System.gc()要求垃圾回收时，垃圾回收程与系统同步运行。 面向对象的特征 封装，继承和多态。 封装：面向对象最基础的一个特性，封装性，是指隐藏对象的属性和现实细节，仅对外提供公共访问方式。 封装的原则：将不需要对外提供的内容都隐藏（设置访问修饰符为“private”）起来。把属性都隐藏，仅提供公共方法对其访问，可以在访问方式中加入逻辑判断等语句。 继承：继承是从已有类得到继承信息创建新类的过程。提供继承信息的类被称为父类（超类、基类）；得到继承信息的类被称为子类（派生类）。 多态：多态性是指允许不同子类型的对象对同一消息作出不同的响应。简单的说就是用同样的对象引用调用同样的方法但是做了不同的事情。 多态性分为编译时的多态性和运行时的多态性。 运行时的多态是面向对象最精髓的东西，要实现多态需要做两件事： 1). 方法重写（子类继承父类并重写父类中已有的或抽象的方法） 2). 对象造型（用父类型引用引用子类型对象，这样同样的引用调用同样的方法就会根据子类对象的不同而表现出不同的行为）。 什么是类 类是具有相同属性和方法的一组对象的集合，它为属于该类的所有对象提供了统一的抽象描述，其内部包括属性和方法两个主要部分。在面向对象的编程语言中，类是一个独立的程序单位，它应该有一个类名并包括属性和方法两个主要部分。 Java中的类实现包括两个部分：类声明和类体。 多态的好处 多态的定义：指允许不同类的对象对同一消息做出响应。即同一消息可以根据发送对象的不同而采用多种不同的行为方式。 主要有以下优点: 可替换性:多态对已存在代码具有可替换性. 可扩充性:增加新的子类不影响已经存在的类结构. 接口性:多态是超类通过方法签名,向子类提供一个公共接口,由子类来完善或者重写它来实现的. 灵活性:它在应用中体现了灵活多样的操作，提高了使用效率 简化性:多态简化对应用软件的代码编写和修改过程，尤其在处理大量对象的运算和操作时，这个特点尤为突出和重要 代码中如何实现多态 实现多态主要有以下三种方式: 接口实现 继承父类重写方法 同一类中进行方法重载 虚拟机是如何实现多态的 动态绑定技术(dynamic binding),执行期间判断所引用对象的实际类型,根据实际类型调用对应的方法. 重载（Overload）和重写（Override）的区别。重载的方法能否根据返回类型进行区分？ 方法的重载和重写都是实现多态的方式，区别在于前者实现的是编译时的多态性，而后者实现的是运行时的多态性。 重载发生在一个类中，同名的方法如果有不同的参数列表（参数类型不同、参数个数不同或者二者都不同）则视为重载； 重写发生在子类与父类之间，重写要求子类被重写方法与父类被重写方法有相同的返回类型，比父类被重写方法更好访问，不能比父类被重写方法声明更多的异常（里氏代换原则）。重载对返回类型没有特殊的要求。 构造器不能被继承，因此不能被重写，但可以被重载。 父类的静态方法不能被子类重写。重写只适用于实例方法，不能用于静态方法，而子类当中含有和父类相同签名的静态方法，我们一般称之为隐藏，调用的方法为定义的类所有的静态方法。 构造器（constructor）是否可被重写（override）？ 构造器不能被继承，因此不能被重写，但可以被重载。 接口的意义 接口的意义用四个词就可以概括：规范，扩展，回调和安全。 抽象类的意义 抽象类的意义可以用三句话来概括: 为其他子类提供一个公共的类型 封装子类中重复定义的内容 定义抽象方法,子类虽然有不同的实现,但是定义是一致的 抽象类和接口有什么区别 抽象类和接口都不能够实例化，但可以定义抽象类和接口类型的引用。一个类如果继承了某个抽象类或者实现了某个接口都需要对其中的抽象方法全部进行实现，否则该类仍然需要被声明为抽象类。接口比抽象类更加抽象，因为抽象类中可以定义构造器，可以有抽象方法和具体方法，而接口中不能定义构造器而且其中的方法全部都是抽象方法。抽象类中的成员可以是private、默认、protected、public的，而接口中的成员全都是public的。抽象类中可以定义成员变量，而接口中定义的成员变量实际上都是常量。有抽象方法的类必须被声明为抽象类，而抽象类未必要有抽象方法。 访问修饰符public,private,protected,以及不写（默认）时的区别 修饰符 当前类 同包 子类 其他包 public √ √ √ √ protected √ √ √ × default √ √ × × private √ × × × 类的成员不写访问修饰时默认为default。默认对于同一个包中的其他类相当于公开（public），对于不是同一个包中的其他类相当于私有（private）。受保护（protected）对子类相当于公开，对不是同一包中的没有父子关系的类相当于私有。Java中，外部类的修饰符只能是public或默认，类的成员（包括内部类）的修饰符可以是以上四种。 简述一下面向对象的”六原则一法则”。 单一职责原则：一个类只做它该做的事情。 单一职责原则想表达的就是”高内聚”，写代码最终极的原则只有六个字”高内聚、低耦合”。所谓的高内聚就是一个代码模块只完成一项功能，在面向对象中，如果只让一个类完成它该做的事，而不涉及与它无关的领域就是践行了高内聚的原则，这个类就只有单一职责。我们都知道一句话叫”因为专注，所以专业”，一个对象如果承担太多的职责，那么注定它什么都做不好。一个好的软件系统，它里面的每个功能模块也应该是可以轻易的拿到其他系统中使用的，这样才能实现软件复用的目标。 开闭原则：软件实体应当对扩展开放，对修改关闭。 在理想的状态下，当我们需要为一个软件系统增加新功能时，只需要从原来的系统派生出一些新类就可以，不需要修改原来的任何一行代码。要做到开闭有两个要点： 1）抽象是关键，一个系统中如果没有抽象类或接口系统就没有扩展点； 2）封装可变性，将系统中的各种可变因素封装到一个继承结构中，如果多个可变因素混杂在一起，系统将变得复杂而换乱，如果不清楚如何封装可变性，可以参考《设计模式精解》一书中对桥梁模式的讲解的章节。 依赖倒转原则：面向接口编程。 该原则说得直白和具体一些就是声明方法的参数类型、方法的返回类型、变量的引用类型时，尽可能使用抽象类型而不用具体类型，因为抽象类型可以被它的任何一个子类型所替代，请参考下面的里氏替换原则。 里氏替换原则：任何时候都可以用子类型替换掉父类型。 关于里氏替换原则的描述，Barbara Liskov女士的描述比这个要复杂得多，但简单的说就是能用父类型的地方就一定能使用子类型。里氏替换原则可以检查继承关系是否合理，如果一个继承关系违背了里氏替换原则，那么这个继承关系一定是错误的，需要对代码进行重构。例如让猫继承狗，或者狗继承猫，又或者让正方形继承长方形都是错误的继承关系，因为你很容易找到违反里氏替换原则的场景。需要注意的是：子类一定是增加父类的能力而不是减少父类的能力，因为子类比父类的能力更多，把能力多的对象当成能力少的对象来用当然没有任何问题。 接口隔离原则：接口要小而专，绝不能大而全。 臃肿的接口是对接口的污染，既然接口表示能力，那么一个接口只应该描述一种能力，接口也应该是高度内聚的。例如，琴棋书画就应该分别设计为四个接口，而不应设计成一个接口中的四个方法，因为如果设计成一个接口中的四个方法，那么这个接口很难用，毕竟琴棋书画四样都精通的人还是少数，而如果设计成四个接口，会几项就实现几个接口，这样的话每个接口被复用的可能性是很高的。Java中的接口代表能力、代表约定、代表角色，能否正确的使用接口一定是编程水平高低的重要标识。 合成聚合复用原则：优先使用聚合或合成关系复用代码。 通过继承来复用代码是面向对象程序设计中被滥用得最多的东西，因为所有的教科书都无一例外的对继承进行了鼓吹从而误导了初学者，类与类之间简单的说有三种关系，Is-A关系、Has-A关系、Use-A关系，分别代表继承、关联和依赖。其中，关联关系根据其关联的强度又可以进一步划分为关联、聚合和合成，但说白了都是Has-A关系，合成聚合复用原则想表达的是优先考虑Has-A关系而不是Is-A关系复用代码，原因嘛可以自己从百度上找到一万个理由，需要说明的是，即使在Java的API中也有不少滥用继承的例子，例如Properties类继承了Hashtable类，Stack类继承了Vector类，这些继承明显就是错误的，更好的做法是在Properties类中放置一个Hashtable类型的成员并且将其键和值都设置为字符串来存储数据，而Stack类的设计也应该是在Stack类中放一个Vector对象来存储数据。记住：任何时候都不要继承工具类，工具是可以拥有并可以使用的，而不是拿来继承的。 迪米特法则：迪米特法则又叫最少知识原则，一个对象应当对其他对象有尽可能少的了解。 迪米特法则简单的说就是如何做到”低耦合”，门面模式和调停者模式就是对迪米特法则的践行。对于门面模式可以举一个简单的例子，你去一家公司洽谈业务，你不需要了解这个公司内部是如何运作的，你甚至可以对这个公司一无所知，去的时候只需要找到公司入口处的前台美女，告诉她们你要做什么，她们会找到合适的人跟你接洽，前台的美女就是公司这个系统的门面。再复杂的系统都可以为用户提供一个简单的门面，Java Web开发中作为前端控制器的Servlet或Filter不就是一个门面吗，浏览器对服务器的运作方式一无所知，但是通过前端控制器就能够根据你的请求得到相应的服务。调停者模式也可以举一个简单的例子来说明，例如一台计算机，CPU、内存、硬盘、显卡、声卡各种设备需要相互配合才能很好的工作，但是如果这些东西都直接连接到一起，计算机的布线将异常复杂，在这种情况下，主板作为一个调停者的身份出现，它将各个设备连接在一起而不需要每个设备之间直接交换数据，这样就减小了系统的耦合度和复杂度。 "},"zother3-java_interview/java-web/":{"url":"zother3-java_interview/java-web/","title":"Index","keywords":"","body":""},"zother3-java_interview/java-web/java-web-basic.html":{"url":"zother3-java_interview/java-web/java-web-basic.html","title":"Java Web Basic","keywords":"","body":"JSP 和 Servlet Servlet接口中有哪些方法？ Servlet接口定义了5个方法，其中前三个方法与Servlet生命周期相关： void init(ServletConfig config) throws ServletException void service(ServletRequest req, ServletResponse resp) throws ServletException, java.io.IOException void destory() java.lang.String getServletInfo() ServletConfig getServletConfig() Web容器加载Servlet并将其实例化后，Servlet生命周期开始，容器运行其init()方法进行Servlet的初始化；请求到达时调用Servlet的service()方法，service()方法会根据需要调用与请求对应的doGet或doPost等方法；当服务器关闭或项目被卸载时服务器会将Servlet实例销毁，此时会调用Servlet的destroy()方法。 转发（forward）和重定向（redirect）的区别？ forward是容器中控制权的转向，是服务器请求资源，服务器直接访问目标地址的URL，把那个URL 的响应内容读取过来，然后把这些内容再发给浏览器，浏览器根本不知道服务器发送的内容是从哪儿来的，所以它的地址栏中还是原来的地址。 redirect就是服务器端根据逻辑，发送一个状态码，告诉浏览器重新去请求那个地址，因此从浏览器的地址栏中可以看到跳转后的链接地址，很明显redirect无法访问到服务器保护起来资源，但是可以从一个网站redirect到其他网站。 forward更加高效，所以在满足需要时尽量使用forward（通过调用RequestDispatcher对象的forward()方法，该对象可以通过ServletRequest对象的getRequestDispatcher()方法获得），并且这样也有助于隐藏实际的链接；在有些情况下，比如需要访问一个其它服务器上的资源，则必须使用重定向（通过HttpServletResponse对象调用其sendRedirect()方法实现）。 JSP有哪些内置对象？作用分别是什么？ JSP有9个内置对象： request：封装客户端的请求，其中包含来自GET或POST请求的参数； response：封装服务器对客户端的响应； pageContext：通过该对象可以获取其他对象； session：封装用户会话的对象； application：封装服务器运行环境的对象； out：输出服务器响应的输出流对象； config：Web应用的配置对象； page：JSP页面本身（相当于Java程序中的this）； exception：封装页面抛出异常的对象。 JSP和Servlet是什么关系？ ervlet是一个特殊的Java程序，它运行于服务器的JVM中，能够依靠服务器的支持向浏览器提供显示内容。JSP本质上是Servlet的一种简易形式，JSP会被服务器处理成一个类似于Servlet的Java程序，可以简化页面内容的生成。Servlet和JSP最主要的不同点在于，Servlet的应用逻辑是在Java文件中，并且完全从表示层中的HTML分离开来。而JSP的情况是Java和HTML可以组合成一个扩展名为.jsp的文件。有人说，Servlet就是在Java中写HTML，而JSP就是在HTML中写Java代码，当然这个说法是很片面且不够准确的。JSP侧重于视图，Servlet更侧重于控制逻辑，在MVC架构模式中，JSP适合充当视图（view）而Servlet适合充当控制器（controller）。 讲解JSP中的四种作用域。 答：JSP中的四种作用域包括page、request、session和application，具体来说： page代表与一个页面相关的对象和属性。 request代表与Web客户机发出的一个请求相关的对象和属性。一个请求可能跨越多个页面，涉及多个Web组件；需要在页面显示的临时数据可以置于此作用域。 session代表与某个用户与服务器建立的一次会话相关的对象和属性。跟某个用户相关的数据应该放在用户自己的session中。 application代表与整个Web应用程序相关的对象和属性，它实质上是跨越整个Web应用程序，包括多个页面、请求和会话的一个全局作用域。 实现会话跟踪的技术有哪些？ 由于HTTP协议本身是无状态的，服务器为了区分不同的用户，就需要对用户会话进行跟踪，简单的说就是为用户进行登记，为用户分配唯一的ID，下一次用户在请求中包含此ID，服务器据此判断到底是哪一个用户。 1）URL 重写：在URL中添加用户会话的信息作为请求的参数，或者将唯一的会话ID添加到URL结尾以标识一个会话。 2） 设置表单隐藏域：将和会话跟踪相关的字段添加到隐式表单域中，这些信息不会在浏览器中显示但是提交表单时会提交给服务器。 这两种方式很难处理跨越多个页面的信息传递，因为如果每次都要修改URL或在页面中添加隐式表单域来存储用户会话相关信息，事情将变得非常麻烦。 3）cookie：cookie有两种，一种是基于窗口的，浏览器窗口关闭后，cookie就没有了；另一种是将信息存储在一个临时文件中，并设置存在的时间。当用户通过浏览器和服务器建立一次会话后，会话ID就会随响应信息返回存储在基于窗口的cookie中，那就意味着只要浏览器没有关闭，会话没有超时，下一次请求时这个会话ID又会提交给服务器让服务器识别用户身份。会话中可以为用户保存信息。会话对象是在服务器内存中的，而基于窗口的cookie是在客户端内存中的。如果浏览器禁用了cookie，那么就需要通过下面两种方式进行会话跟踪。当然，在使用cookie时要注意几点：首先不要在cookie中存放敏感信息；其次cookie存储的数据量有限（4k），不能将过多的内容存储cookie中；再者浏览器通常只允许一个站点最多存放20个cookie。当然，和用户会话相关的其他信息（除了会话ID）也可以存在cookie方便进行会话跟踪。 4）HttpSession：在所有会话跟踪技术中，HttpSession对象是最强大也是功能最多的。当一个用户第一次访问某个网站时会自动创建HttpSession，每个用户可以访问他自己的HttpSession。可以通过HttpServletRequest对象的getSession方法获得HttpSession，通过HttpSession的setAttribute方法可以将一个值放在HttpSession中，通过调用HttpSession对象的getAttribute方法，同时传入属性名就可以获取保存在HttpSession中的对象。与上面三种方式不同的是，HttpSession放在服务器的内存中，因此不要将过大的对象放在里面，即使目前的Servlet容器可以在内存将满时将HttpSession中的对象移到其他存储设备中，但是这样势必影响性能。添加到HttpSession中的值可以是任意Java对象，这个对象最好实现了Serializable接口，这样Servlet容器在必要的时候可以将其序列化到文件中，否则在序列化时就会出现异常。 过滤器有哪些作用和用法？ Java Web开发中的过滤器（filter）是从Servlet 2.3规范开始增加的功能，并在Servlet 2.4规范中得到增强。对Web应用来说，过滤器是一个驻留在服务器端的Web组件，它可以截取客户端和服务器之间的请求与响应信息，并对这些信息进行过滤。当Web容器接受到一个对资源的请求时，它将判断是否有过滤器与这个资源相关联。如果有，那么容器将把请求交给过滤器进行处理。在过滤器中，你可以改变请求的内容，或者重新设置请求的报头信息，然后再将请求发送给目标资源。当目标资源对请求作出响应时候，容器同样会将响应先转发给过滤器，在过滤器中你可以对响应的内容进行转换，然后再将响应发送到客户端。 常见的过滤器用途主要包括：对用户请求进行统一认证、对用户的访问请求进行记录和审核、对用户发送的数据进行过滤或替换、转换图象格式、对响应内容进行压缩以减少传输量、对请求或响应进行加解密处理、触发资源访问事件、对XML的输出应用XSLT等。 过滤器相关的接口主要有：Filter、FilterConfig和FilterChain。 监听器有哪些作用和用法？ Java Web开发中的监听器（listener）就是application、session、request三个对象创建、销毁或者往其中添加修改删除属性时自动执行代码的功能组件，如下所示： ServletContextListener：对Servlet上下文的创建和销毁进行监听。 ervletContextAttributeListener：监听Servlet上下文属性的添加、删除和替换。 HttpSessionAttributeListener：对Session对象中属性的添加、删除和替换进行监听。 ServletRequestListener：对请求对象的初始化和销毁进行监听。 ServletRequestAttributeListener：对请求对象属性的添加、删除和替换进行监听。 HttpSessionListener：对Session的创建和销毁进行监听。 补充： session的销毁有两种情况： session超时（可以在web.xml中通过/标签配置超时时间）； 通过调用session对象的invalidate()方法使session失效。 Servlet的生命周期 "},"zother3-java_interview/java-web/mybatis.html":{"url":"zother3-java_interview/java-web/mybatis.html","title":"Mybatis","keywords":"","body":"MyBatis #{}和${}的区别是什么？ ${}是Properties文件中的变量占位符，它可以用于标签属性值和sql内部，属于静态文本替换，比如${driver}会被静态替换为com.mysql.jdbc.Driver。#{}是sql的参数占位符，Mybatis会将sql中的#{}替换为?号，在sql执行前会使用PreparedStatement的参数设置方法，按序给sql的?号占位符设置参数值，比如ps.setInt(0, parameterValue)，#{item.name}的取值方式为使用反射从参数对象中获取item对象的name属性值，相当于param.getItem().getName()。 Xml映射文件中，除了常见的select|insert|updae|delete标签之外，还有哪些标签？ 还有很多其他的标签，\\、\\、\\、\\、\\，加上动态sql的9个标签，trim|where|set|foreach|if|choose|when|otherwise|bind等，其中\\为sql片段标签，通过\\标签引入sql片段，\\为不支持自增的主键生成策略标签。 最佳实践中，通常一个Xml映射文件，都会写一个Dao接口与之对应，请问，这个Dao接口的工作原理是什么？Dao接口里的方法，参数不同时，方法能重载吗？ Dao接口，就是人们常说的Mapper接口，接口的全限名，就是映射文件中的namespace的值，接口的方法名，就是映射文件中MappedStatement的id值，接口方法内的参数，就是传递给sql的参数。 Mapper接口是没有实现类的，当调用接口方法时，接口全限名+方法名拼接字符串作为key值，可唯一定位一个MappedStatement，举例：com.mybatis3.mappers.StudentDao.findStudentById，可以唯一找到namespace为com.mybatis3.mappers.StudentDao下面id = findStudentById的MappedStatement。在Mybatis中，每一个\\、\\、\\、\\标签，都会被解析为一个MappedStatement对象。 Dao接口里的方法，是不能重载的，因为是全限名+方法名的保存和寻找策略。 Dao接口的工作原理是JDK动态代理，Mybatis运行时会使用JDK动态代理为Dao接口生成代理proxy对象，代理对象proxy会拦截接口方法，转而执行MappedStatement所代表的sql，然后将sql执行结果返回。 Mybatis是如何进行分页的？分页插件的原理是什么？ Mybatis使用RowBounds对象进行分页，它是针对ResultSet结果集执行的内存分页，而非物理分页，可以在sql内直接书写带有物理分页的参数来完成物理分页功能，也可以使用分页插件来完成物理分页。 分页插件的基本原理是使用Mybatis提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的sql，然后重写sql，根据dialect方言，添加对应的物理分页语句和物理分页参数。 举例：select * from student，拦截sql后重写为：select t.* from （select * from student）t limit 0，10 简述Mybatis的插件运行原理，以及如何编写一个插件。 Mybatis仅可以编写针对ParameterHandler、ResultSetHandler、StatementHandler、Executor这4种接口的插件，Mybatis使用JDK的动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这4种接口对象的方法时，就会进入拦截方法，具体就是InvocationHandler的invoke()方法，当然，只会拦截那些你指定需要拦截的方法。 实现Mybatis的Interceptor接口并复写intercept()方法，然后在给插件编写注解，指定要拦截哪一个接口的哪些方法即可，记住，别忘了在配置文件中配置你编写的插件。 Mybatis是如何将sql执行结果封装为目标对象并返回的？都有哪些映射形式？ 第一种是使用\\标签，逐一定义列名和对象属性名之间的映射关系。 第二种是使用sql列的别名功能，将列别名书写为对象属性名，比如T_NAME AS NAME，对象属性名一般是name，小写，但是列名不区分大小写，Mybatis会忽略列名大小写，智能找到与之对应对象属性名，你甚至可以写成T_NAME AS NaMe，Mybatis一样可以正常工作。 有了列名与属性名的映射关系后，Mybatis通过反射创建对象，同时使用反射给对象的属性逐一赋值并返回，那些找不到映射关系的属性，是无法完成赋值的。 Mybatis能执行一对一、一对多的关联查询吗？都有哪些实现方式，以及它们之间的区别。 能，Mybatis不仅可以执行一对一、一对多的关联查询，还可以执行多对一，多对多的关联查询，多对一查询，其实就是一对一查询，只需要把selectOne()修改为selectList()即可；多对多查询，其实就是一对多查询，只需要把selectOne()修改为selectList()即可。 关联对象查询，有两种实现方式，一种是单独发送一个sql去查询关联对象，赋给主对象，然后返回主对象。另一种是使用嵌套查询，嵌套查询的含义为使用join查询，一部分列是A对象的属性值，另外一部分列是关联对象B的属性值，好处是只发一个sql查询，就可以把主对象和其关联对象查出来。 那么问题来了，join查询出来100条记录，如何确定主对象是5个，而不是100个？其去重复的原理是\\标签内的\\子标签，指定了唯一确定一条记录的id列，Mybatis根据\\列值来完成100条记录的去重复功能，\\可以有多个，代表了联合主键的语意。 同样主对象的关联对象，也是根据这个原理去重复的，尽管一般情况下，只有主对象会有重复记录，关联对象一般不会重复。 举例：下面join查询出来6条记录，一、二列是Teacher对象列，第三列为Student对象列，Mybatis去重复处理后，结果为1个老师6个学生，而不是6个老师6个学生。 t_id t_name s_id 1 teacher 38 1 teacher 39 1 teacher 40 1 teacher 41 1 teacher 42 1 teacher 43 Mybatis是否支持延迟加载？如果支持，它的实现原理是什么？ Mybatis仅支持association关联对象和collection关联集合对象的延迟加载，association指的就是一对一，collection指的就是一对多查询。在Mybatis配置文件中，可以配置是否启用延迟加载lazyLoadingEnabled=true|false。 它的原理是，使用CGLIB创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用a.getB().getName()，拦截器invoke()方法发现a.getB()是null值，那么就会单独发送事先保存好的查询关联B对象的sql，把B查询上来，然后调用a.setB(b)，于是a的对象b属性就有值了，接着完成a.getB().getName()方法的调用。这就是延迟加载的基本原理。 当然了，不光是Mybatis，几乎所有的包括Hibernate，支持延迟加载的原理都是一样的。 Mybatis的Xml映射文件中，不同的Xml映射文件，id是否可以重复？ 不同的Xml映射文件，如果配置了namespace，那么id可以重复；如果没有配置namespace，那么id不能重复；毕竟namespace不是必须的，只是最佳实践而已。 原因就是namespace+id是作为Map的key使用的，如果没有namespace，就剩下id，那么，id重复会导致数据互相覆盖。有了namespace，自然id就可以重复，namespace不同，namespace+id自然也就不同。 Mybatis都有哪些Executor执行器？它们之间的区别是什么？ Mybatis有三种基本的Executor执行器，SimpleExecutor、ReuseExecutor、BatchExecutor。 SimpleExecutor：每执行一次update或select，就开启一个Statement对象，用完立刻关闭Statement对象。 ReuseExecutor：执行update或select，以sql作为key查找Statement对象，存在就使用，不存在就创建，用完后，不关闭Statement对象，而是放置于Map内，供下一次使用。简言之，就是重复使用Statement对象。 BatchExecutor：执行update（没有select，JDBC批处理不支持select），将所有sql都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个Statement对象，每个Statement对象都是addBatch()完毕后，等待逐一执行executeBatch()批处理。与JDBC批处理相同。 作用范围：Executor的这些特点，都严格限制在SqlSession生命周期范围内。 Mybatis中如何指定使用哪一种Executor执行器？ 在Mybatis配置文件中，可以指定默认的ExecutorType执行器类型，也可以手动给DefaultSqlSessionFactory的创建SqlSession的方法传递ExecutorType类型参数。 Mybatis是否可以映射Enum枚举类？ Mybatis可以映射枚举类，不单可以映射枚举类，Mybatis可以映射任何对象到表的一列上。映射方式为自定义一个TypeHandler，实现TypeHandler的setParameter()和getResult()接口方法。TypeHandler有两个作用，一是完成从javaType至jdbcType的转换，二是完成jdbcType至javaType的转换，体现为setParameter()和getResult()两个方法，分别代表设置sql问号占位符参数和获取列查询结果。 Mybatis映射文件中，如果A标签通过include引用了B标签的内容，请问，B标签能否定义在A标签的后面，还是说必须定义在A标签的前面？ 虽然Mybatis解析Xml映射文件是按照顺序解析的，但是，被引用的B标签依然可以定义在任何地方，Mybatis都可以正确识别。 原理是，Mybatis解析A标签，发现A标签引用了B标签，但是B标签尚未解析到，尚不存在，此时，Mybatis会将A标签标记为未解析状态，然后继续解析余下的标签，包含B标签，待所有标签解析完毕，Mybatis会重新解析那些被标记为未解析的标签，此时再解析A标签时，B标签已经存在，A标签也就可以正常解析完成了。 简述Mybatis的Xml映射文件和Mybatis内部数据结构之间的映射关系？ Mybatis将所有Xml配置信息都封装到All-In-One重量级对象Configuration内部。在Xml映射文件中，\\标签会被解析为ParameterMap对象，其每个子元素会被解析为ParameterMapping对象。\\标签会被解析为ResultMap对象，其每个子元素会被解析为ResultMapping对象。每一个\\、\\、\\、\\标签均会被解析为MappedStatement对象，标签内的sql会被解析为BoundSql对象。 为什么说Mybatis是半自动ORM映射工具？它与全自动的区别在哪里？ Hibernate属于全自动ORM映射工具，使用Hibernate查询关联对象或者关联集合对象时，可以根据对象关系模型直接获取，所以它是全自动的。而Mybatis在查询关联对象或关联集合对象时，需要手动编写sql来完成，所以，称之为半自动ORM映射工具。 简单的说一下MyBatis的一级缓存和二级缓存？ Mybatis首先去缓存中查询结果集，如果没有则查询数据库，如果有则从缓存取出返回结果集就不走数据库。Mybatis内部存储缓存使用一个HashMap，key为hashCode+sqlId+Sql语句。value为从查询出来映射生成的java对象 Mybatis的二级缓存即查询缓存，它的作用域是一个mapper的namespace，即在同一个namespace中查询sql可以从缓存中获取数据。二级缓存是可以跨SqlSession的。 "},"zother3-java_interview/java-web/spring.html":{"url":"zother3-java_interview/java-web/spring.html","title":"Spring","keywords":"","body":"Spring 什么是Spring？ Spring是一个开源的Java EE开发框架。Spring框架的核心功能可以应用在任何Java应用程序中，但对Java EE平台上的Web应用程序有更好的扩展性。Spring框架的目标是使得Java EE应用程序的开发更加简捷，通过使用POJO为基础的编程模型促进良好的编程风格。 Spring有哪些优点？ 轻量级：Spring在大小和透明性方面绝对属于轻量级的，基础版本的Spring框架大约只有2MB。 控制反转(IOC)：Spring使用控制反转技术实现了松耦合。依赖被注入到对象，而不是创建或寻找依赖对象。 面向切面编程(AOP)： Spring支持面向切面编程，同时把应用的业务逻辑与系统的服务分离开来。 容器：Spring包含并管理应用程序对象的配置及生命周期。 MVC框架：Spring的web框架是一个设计优良的web MVC框架，很好的取代了一些web框架。 事务管理：Spring对下至本地业务上至全局业务(JAT)提供了统一的事务管理接口。 异常处理：Spring提供一个方便的API将特定技术的异常(由JDBC, Hibernate, 或JDO抛出)转化为一致的、Unchecked异常。 Spring 事务实现方式 编程式事务管理：这意味着你可以通过编程的方式管理事务，这种方式带来了很大的灵活性，但很难维护。 声明式事务管理：这种方式意味着你可以将事务管理和业务代码分离。你只需要通过注解或者XML配置管理事务。 Spring框架的事务管理有哪些优点 它为不同的事务API(如JTA, JDBC, Hibernate, JPA, 和JDO)提供了统一的编程模型。 它为编程式事务管理提供了一个简单的API而非一系列复杂的事务API(如JTA). 它支持声明式事务管理。 它可以和Spring 的多种数据访问技术很好的融合。 spring事务定义的传播规则 PROPAGATION_REQUIRED: 支持当前事务，如果当前没有事务，就新建一个事务。这是最常见的选择。 PROPAGATION_SUPPORTS: 支持当前事务，如果当前没有事务，就以非事务方式执行。 PROPAGATION_MANDATORY: 支持当前事务，如果当前没有事务，就抛出异常。 PROPAGATION_REQUIRES_NEW: 新建事务，如果当前存在事务，把当前事务挂起。 PROPAGATION_NOT_SUPPORTED: 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER: 以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED: 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则进行与PROPAGATION_REQUIRED类似的操作。 Spring 事务底层原理 划分处理单元——IoC 由于spring解决的问题是对单个数据库进行局部事务处理的，具体的实现首先用spring中的IoC划分了事务处理单元。并且将对事务的各种配置放到了ioc容器中（设置事务管理器，设置事务的传播特性及隔离机制）。 AOP拦截需要进行事务处理的类 Spring事务处理模块是通过AOP功能来实现声明式事务处理的，具体操作（比如事务实行的配置和读取，事务对象的抽象），用TransactionProxyFactoryBean接口来使用AOP功能，生成proxy代理对象，通过TransactionInterceptor完成对代理方法的拦截，将事务处理的功能编织到拦截的方法中。读取ioc容器事务配置属性，转化为spring事务处理需要的内部数据结构（TransactionAttributeSourceAdvisor），转化为TransactionAttribute表示的数据对象。 对事务处理实现（事务的生成、提交、回滚、挂起） spring委托给具体的事务处理器实现。实现了一个抽象和适配。适配的具体事务处理器：DataSource数据源支持、hibernate数据源事务处理支持、JDO数据源事务处理支持，JPA、JTA数据源事务处理支持。这些支持都是通过设计PlatformTransactionManager、AbstractPlatforTransaction一系列事务处理的支持。 为常用数据源支持提供了一系列的TransactionManager。 结合 PlatformTransactionManager实现了TransactionInterception接口，让其与TransactionProxyFactoryBean结合起来，形成一个Spring声明式事务处理的设计体系。 有没有遇到过Spring事务失效的情况？在什么情况下Spring的事务是失效的？ 参考：面试必备技能：JDK动态代理给Spring事务埋下的坑！ Spring MVC 运行流程 第一步：发起请求到前端控制器(DispatcherServlet) 第二步：前端控制器请求HandlerMapping查找 Handler（ 可以根据xml配置、注解进行查找） 第三步：处理器映射器HandlerMapping向前端控制器返回Handler 第四步：前端控制器调用处理器适配器去执行Handler 第五步：处理器适配器去执行Handler 第六步：Handler执行完成给适配器返回ModelAndView 第七步：处理器适配器向前端控制器返回ModelAndView（ModelAndView是springmvc框架的一个底层对象，包括Model和view） 第八步：前端控制器请求视图解析器去进行视图解析（根据逻辑视图名解析成真正的视图(jsp)） 第九步：视图解析器向前端控制器返回View 第十步：前端控制器进行视图渲染（ 视图渲染将模型数据(在ModelAndView对象中)填充到request域） 第十一步：前端控制器向用户响应结果 BeanFactory和ApplicationContext有什么区别？ ApplicationContext提供了一种解决文档信息的方法，一种加载文件资源的方式(如图片)，他们可以向监听他们的beans发送消息。另外，容器或者容器中beans的操作，这些必须以bean工厂的编程方式处理的操作可以在应用上下文中以声明的方式处理。应用上下文实现了MessageSource，该接口用于获取本地消息，实际的实现是可选的。 相同点：两者都是通过xml配置文件加载bean,ApplicationContext和BeanFacotry相比,提供了更多的扩展功能。 不同点：BeanFactory是延迟加载,如果Bean的某一个属性没有注入，BeanFacotry加载后，直至第一次使用调用getBean方法才会抛出异常；而ApplicationContext则在初始化自身是检验，这样有利于检查所依赖属性是否注入；所以通常情况下我们选择使用ApplicationContext。 什么是Spring Beans？ Spring Beans是构成Spring应用核心的Java对象。这些对象由Spring IOC容器实例化、组装、管理。这些对象通过容器中配置的元数据创建，例如，使用XML文件中定义的创建。 在Spring中创建的beans都是单例的beans。在bean标签中有一个属性为”singleton”,如果设为true，该bean是单例的，如果设为false，该bean是原型bean。Singleton属性默认设置为true。因此，spring框架中所有的bean都默认为单例bean。 说一下Spring中支持的bean作用域 Spring框架支持如下五种不同的作用域： singleton：在Spring IOC容器中仅存在一个Bean实例，Bean以单实例的方式存在。 prototype：一个bean可以定义多个实例。 request：每次HTTP请求都会创建一个新的Bean。该作用域仅适用于WebApplicationContext环境。 session：一个HTTP Session定义一个Bean。该作用域仅适用于WebApplicationContext环境。 globalSession：同一个全局HTTP Session定义一个Bean。该作用域同样仅适用于WebApplicationContext环境。 bean默认的scope属性是\"singleton\"。 Spring 的单例实现原理 Spring框架对单例的支持是采用单例注册表的方式进行实现的，而这个注册表的缓存是HashMap对象，如果配置文件中的配置信息不要求使用单例，Spring会采用新建实例的方式返回对象实例。 解释Spring框架中bean的生命周期 ApplicationContext容器中，Bean的生命周期流程如上图所示，流程大致如下： 1.首先容器启动后，会对scope为singleton且非懒加载的bean进行实例化， 2.按照Bean定义信息配置信息，注入所有的属性， 3.如果Bean实现了BeanNameAware接口，会回调该接口的setBeanName()方法，传入该Bean的id，此时该Bean就获得了自己在配置文件中的id， 4.如果Bean实现了BeanFactoryAware接口,会回调该接口的setBeanFactory()方法，传入该Bean的BeanFactory，这样该Bean就获得了自己所在的BeanFactory， 5.如果Bean实现了ApplicationContextAware接口,会回调该接口的setApplicationContext()方法，传入该Bean的ApplicationContext，这样该Bean就获得了自己所在的ApplicationContext， 6.如果有Bean实现了BeanPostProcessor接口，则会回调该接口的postProcessBeforeInitialzation()方法， 7.如果Bean实现了InitializingBean接口，则会回调该接口的afterPropertiesSet()方法， 8.如果Bean配置了init-method方法，则会执行init-method配置的方法， 9.如果有Bean实现了BeanPostProcessor接口，则会回调该接口的postProcessAfterInitialization()方法， 10.经过流程9之后，就可以正式使用该Bean了,对于scope为singleton的Bean,Spring的ioc容器中会缓存一份该bean的实例，而对于scope为prototype的Bean,每次被调用都会new一个新的对象，期生命周期就交给调用方管理了，不再是Spring容器进行管理了 11.容器关闭后，如果Bean实现了DisposableBean接口，则会回调该接口的destroy()方法， 12.如果Bean配置了destroy-method方法，则会执行destroy-method配置的方法，至此，整个Bean的生命周期结束 Resource 是如何被查找、加载的？ Resource 接口是 Spring 资源访问策略的抽象，它本身并不提供任何资源访问实现，具体的资源访问由该接口的实现类完成——每个实现类代表一种资源访问策略。 Spring 为 Resource 接口提供了如下实现类： UrlResource：访问网络资源的实现类。 ClassPathResource：访问类加载路径里资源的实现类。 FileSystemResource：访问文件系统里资源的实现类。 ServletContextResource：访问相对于 ServletContext 路径里的资源的实现类： InputStreamResource：访问输入流资源的实现类。 ByteArrayResource：访问字节数组资源的实现类。 这些 Resource 实现类，针对不同的的底层资源，提供了相应的资源访问逻辑，并提供便捷的包装，以利于客户端程序的资源访问。 解释自动装配的各种模式？ 自动装配提供五种不同的模式供Spring容器用来自动装配beans之间的依赖注入: no：默认的方式是不进行自动装配，通过手工设置ref 属性来进行装配bean。 byName：通过参数名自动装配，Spring容器查找beans的属性，这些beans在XML配置文件中被设置为byName。之后容器试图匹配、装配和该bean的属性具有相同名字的bean。 byType：通过参数的数据类型自动自动装配，Spring容器查找beans的属性，这些beans在XML配置文件中被设置为byType。之后容器试图匹配和装配和该bean的属性类型一样的bean。如果有多个bean符合条件，则抛出错误。 constructor：这个同byType类似，不过是应用于构造函数的参数。如果在BeanFactory中不是恰好有一个bean与构造函数参数相同类型，则抛出一个严重的错误。 autodetect：如果有默认的构造方法，通过 construct的方式自动装配，否则使用 byType的方式自动装配。 Spring中的依赖注入是什么？ 依赖注入作为控制反转(IOC)的一个层面，可以有多种解释方式。在这个概念中，你不用创建对象而只需要描述如何创建它们。你不必通过代码直接的将组件和服务连接在一起，而是通过配置文件说明哪些组件需要什么服务。之后IOC容器负责衔接。 有哪些不同类型的IOC(依赖注入)？ 构造器依赖注入：构造器依赖注入在容器触发构造器的时候完成，该构造器有一系列的参数，每个参数代表注入的对象。 Setter方法依赖注入：首先容器会触发一个无参构造函数或无参静态工厂方法实例化对象，之后容器调用bean中的setter方法完成Setter方法依赖注入。 你推荐哪种依赖注入？构造器依赖注入还是Setter方法依赖注入？ 你可以同时使用两种方式的依赖注入，最好的选择是使用构造器参数实现强制依赖注入，使用setter方法实现可选的依赖关系。 Spring IOC 如何实现 Spring中的 org.springframework.beans 包和 org.springframework.context包构成了Spring框架IoC容器的基础。 BeanFactory 接口提供了一个先进的配置机制，使得任何类型的对象的配置成为可能。ApplicationContex接口对BeanFactory（是一个子接口）进行了扩展，在BeanFactory的基础上添加了其他功能，比如与Spring的AOP更容易集成，也提供了处理message resource的机制（用于国际化）、事件传播以及应用层的特别配置，比如针对Web应用的WebApplicationContext。 org.springframework.beans.factory.BeanFactory 是Spring IoC容器的具体实现，用来包装和管理前面提到的各种bean。BeanFactory接口是Spring IoC 容器的核心接口。 Spring IoC容器是什么？ Spring IOC负责创建对象、管理对象(通过依赖注入)、整合对象、配置对象以及管理这些对象的生命周期。 IoC有什么优点？ IOC或依赖注入减少了应用程序的代码量。它使得应用程序的测试很简单，因为在单元测试中不再需要单例或JNDI查找机制。简单的实现以及较少的干扰机制使得松耦合得以实现。IOC容器支持勤性单例及延迟加载服务。 解释AOP模块 AOP模块用来开发Spring应用程序中具有切面性质的部分。该模块的大部分服务由AOP Aliance提供，这就保证了Spring框架和其他AOP框架之间的互操作性。另外，该模块将元数据编程引入到了Spring。 Spring面向切面编程(AOP) 面向切面编程（AOP）：允许程序员模块化横向业务逻辑，或定义核心部分的功能，例如日志管理和事务管理。 切面(Aspect) ：AOP的核心就是切面，它将多个类的通用行为封装为可重用的模块。该模块含有一组API提供 cross-cutting功能。例如,日志模块称为日志的AOP切面。根据需求的不同，一个应用程序可以有若干切面。在Spring AOP中，切面通过带有@Aspect注解的类实现。 通知(Advice)：通知表示在方法执行前后需要执行的动作。实际上它是Spring AOP框架在程序执行过程中触发的一些代码。Spring切面可以执行一下五种类型的通知: before(前置通知)：在一个方法之前执行的通知。 after(最终通知)：当某连接点退出的时候执行的通知（不论是正常返回还是异常退出）。 after-returning(后置通知)：在某连接点正常完成后执行的通知。 after-throwing(异常通知)：在方法抛出异常退出时执行的通知。 around(环绕通知)：在方法调用前后触发的通知。 切入点(Pointcut)：切入点是一个或一组连接点，通知将在这些位置执行。可以通过表达式或匹配的方式指明切入点。 引入：引入允许我们在已有的类上添加新的方法或属性。 目标对象：被一个或者多个切面所通知的对象。它通常是一个代理对象。也被称做被通知（advised）对象。 代理：代理是将通知应用到目标对象后创建的对象。从客户端的角度看，代理对象和目标对象是一样的。有以下几种代理： BeanNameAutoProxyCreator：bean名称自动代理创建器 DefaultAdvisorAutoProxyCreator：默认通知者自动代理创建器 Metadata autoproxying：元数据自动代理 织入：将切面和其他应用类型或对象连接起来创建一个通知对象的过程。织入可以在编译、加载或运行时完成。 Spring AOP 实现原理 实现AOP的技术，主要分为两大类： 一是采用动态代理技术，利用截取消息的方式，对该消息进行装饰，以取代原有对象行为的执行； 二是采用静态织入的方式，引入特定的语法创建“方面”，从而使得编译器可以在编译期间织入有关“方面”的代码。 Spring AOP 的实现原理其实很简单：AOP 框架负责动态地生成 AOP 代理类，这个代理类的方法则由 Advice和回调目标对象的方法所组成, 并将该对象可作为目标对象使用。AOP 代理包含了目标对象的全部方法，但AOP代理中的方法与目标对象的方法存在差异，AOP方法在特定切入点添加了增强处理，并回调了目标对象的方法。 Spring AOP使用动态代理技术在运行期织入增强代码。使用两种代理机制：基于JDK的动态代理（JDK本身只提供接口的代理）和基于CGlib的动态代理。 (1) JDK的动态代理 JDK的动态代理主要涉及java.lang.reflect包中的两个类：Proxy和InvocationHandler。其中InvocationHandler只是一个接口，可以通过实现该接口定义横切逻辑，并通过反射机制调用目标类的代码，动态的将横切逻辑与业务逻辑织在一起。而Proxy利用InvocationHandler动态创建一个符合某一接口的实例，生成目标类的代理对象。 其代理对象必须是某个接口的实现, 它是通过在运行期间创建一个接口的实现类来完成对目标对象的代理.只能实现接口的类生成代理,而不能针对类 (2)CGLib CGLib采用底层的字节码技术，为一个类创建子类，并在子类中采用方法拦截的技术拦截所有父类的调用方法，并顺势织入横切逻辑.它运行期间生成的代理对象是目标类的扩展子类.所以无法通知final、private的方法,因为它们不能被覆写.是针对类实现代理,主要是为指定的类生成一个子类,覆盖其中方法. 在spring中默认情况下使用JDK动态代理实现AOP,如果proxy-target-class设置为true或者使用了优化策略那么会使用CGLIB来创建动态代理.Spring　AOP在这两种方式的实现上基本一样．以JDK代理为例，会使用JdkDynamicAopProxy来创建代理，在invoke()方法首先需要织入到当前类的增强器封装到拦截器链中，然后递归的调用这些拦截器完成功能的织入．最终返回代理对象． http://zhengjianglong.cn/2015/12/12/Spring/spring-source-aop/ 如何自定义注解实现功能 SpringMVC启动流程 cgLib知道吗？他和jdk动态代理什么区别？手写一个jdk动态代理呗？ "},"zother3-java_interview/mic-service/":{"url":"zother3-java_interview/mic-service/","title":"Index","keywords":"","body":"微服务 微服务哪些框架 你怎么理解 RPC 框架 说说 RPC 的实现原理 说说 Dubbo 的实现原理 你怎么理解 RESTful 如何理解 RESTful API 的幂等性 可以参考:如何理解RESTful的幂等性 如何保证接口的幂等性 你怎么看待微服务 参考:服务端指南 | 微服务架构概述 微服务与SOA的区别 SOA (Service-Oriented Architecture，面向服务的架构)是一种面向服务的思维方式，它将应用程序的不同功能（服务）通过服务之间定义良好的接口和契约联系起来。SOA 核心思想是服务是一种可重复的业务，将其经过标准封装达到复用的目的。SOA 可以允许各种不同的技术来表达 SOA 的架构理念，而业界比较流行的实现是 WebService，其中 WebService 采用 HTTP 协议传输数据，采用 XML 格式封装数据。微服务架构和 SOA 的思想没有太大的差别，从实现的方式而言，微服务架构强调实现的轻量化，做到服务粒度更细。这里，微服务的“微”指的并不是服务，而实际上是应用粒度。为了更好地识别 SOA 与微服务架构之间的区别，我们来做一个横向对比。 方面 SOA 微服务架构 应用粒度 多个系统整合成一个服务，粒度大 一个系统拆分成多个服务，粒度小 服务架构 企业服务总线（ESB），集中式架构 服务自治，松散式架构 服务规模 服务规模较小 服务规模膨胀 服务部署 单体架构，业务耦合 功能独立，独立部署 总结下，微服务架构可以理解成 SOA 的升级版，强调实现的轻量化，做到服务粒度更细。随着敏捷开发、持续交付、虚拟化技术、DevOps 理论的实践，微服务架构越来越被重视与应用。 如何拆分服务 参考：如何拆分服务 微服务如何进行数据库管理 参考：论微服务的数据库管理 如何应对微服务的链式调用异常 参考：应对微服务的链式调用异常 对于快速追踪与定位问题 参考：如何快速追踪与定位问题 微服务的安全 参考：微服务的安全 说说服务的治理（怎么注册怎么发现） "},"zother3-java_interview/mq/basic.html":{"url":"zother3-java_interview/mq/basic.html","title":"Basic","keywords":"","body":"消息队列基础 消息队列的使用场景 消息的重发补偿解决思路 消息的幂等性解决思路 消息的堆积解决思路 自己如何实现消息队列 如何保证消息的有序性 如何解决消息队列丢失消息和重复消费问题 异步队列怎么实现 "},"zother3-java_interview/mq/":{"url":"zother3-java_interview/mq/","title":"Index","keywords":"","body":""},"zother3-java_interview/network-server/":{"url":"zother3-java_interview/network-server/","title":"Index","keywords":"","body":""},"zother3-java_interview/network-server/netty.html":{"url":"zother3-java_interview/network-server/netty.html","title":"Netty","keywords":"","body":"Netty 为什么选择 Netty 说说业务中，Netty 的使用场景 原生的 NIO 在 JDK 1.7 版本存在 epoll bug 什么是TCP 粘包/拆包 TCP粘包/拆包的解决办法 Netty 线程模型 说说 Netty 的零拷贝 Netty 内部执行流程 Netty 重连实现 "},"zother3-java_interview/network-server/network.html":{"url":"zother3-java_interview/network-server/network.html","title":"Network","keywords":"","body":"计算机网络 参考：计算机网络 说一下TCP/IP四层？ "},"zother3-java_interview/network-server/nginx.html":{"url":"zother3-java_interview/network-server/nginx.html","title":"Nginx","keywords":"","body":"Nginx 什么是Nginx？ Nginx是一个高性能的HTTP和反向代理服务器，也是一个IMAP/POP3/SMTP服务器 Nginx是一款轻量级的Web服务器/反向代理服务器及电子邮件（IMAP/POP3）代理服务器 目前使用的最多的web服务器或者代理服务器，像淘宝、新浪、网易、迅雷等都在使用 为什么要用Nginx？ 优点： 跨平台、配置简单 非阻塞、高并发连接：处理2-3万并发连接数，官方监测能支持5万并发 内存消耗小：开启10个nginx才占150M内存 成本低廉：开源 内置的健康检查功能：如果有一个服务器宕机，会做一个健康检查，再发送的请求就不会发送到宕机的服务器了。重新将请求提交到其他的节点上。 节省宽带：支持GZIP压缩，可以添加浏览器本地缓存 稳定性高：宕机的概率非常小 master/worker结构：一个master进程，生成一个或者多个worker进程 接收用户请求是异步的：浏览器将请求发送到nginx服务器，它先将用户请求全部接收下来，再一次性发送给后端web服务器，极大减轻了web服务器的压力 一边接收web服务器的返回数据，一边发送给浏览器客户端 网络依赖性比较低，只要ping通就可以负载均衡 可以有多台nginx服务器 事件驱动：通信机制采用epoll模型 为什么Nginx性能这么高？ 得益于它的事件处理机制： 异步非阻塞事件处理机制：运用了epoll模型，提供了一个队列，排队解决 Nginx是如何实现高并发的 service nginx start之后，然后输入#ps -ef|grep nginx，会发现Nginx有一个master进程和若干个worker进程，这些worker进程是平等的，都是被master fork过来的。在master里面，先建立需要listen的socket（listenfd），然后再fork出多个worker进程。当用户进入nginx服务的时候，每个worker的listenfd变的可读，并且这些worker会抢一个叫accept_mutex的东西，accept_mutex是互斥的，一个worker得到了，其他的worker就歇菜了。而抢到这个accept_mutex的worker就开始“读取请求--解析请求--处理请求”，数据彻底返回客户端之后（目标网页出现在电脑屏幕上），这个事件就算彻底结束。 nginx用这个方法是底下的worker进程抢注用户的要求，同时搭配“异步非阻塞”的方式，实现高并发量。 为什么不使用多线程？ 因为线程创建和上下文的切换非常消耗资源，线程占用内存大，上下文切换占用cpu也很高，采用epoll模型避免了这个缺点 Nginx是如何处理一个请求的呢？ 首先，nginx在启动时，会解析配置文件，得到需要监听的端口与ip地址，然后在nginx的master进程里面 先初始化好这个监控的socket(创建socket，设置addrreuse等选项，绑定到指定的ip地址端口，再listen) 然后再fork(一个现有进程可以调用fork函数创建一个新进程。由fork创建的新进程被称为子进程 )出多个子进程出来 然后子进程会竞争accept新的连接。此时，客户端就可以向nginx发起连接了。当客户端与nginx进行三次握手，与nginx建立好一个连接后 此时，某一个子进程会accept成功，得到这个建立好的连接的socket，然后创建nginx对连接的封装，即ngx_connection_t结构体 接着，设置读写事件处理函数并添加读写事件来与客户端进行数据的交换。最后，nginx或客户端来主动关掉连接，到此，一个连接就寿终正寝了 正向代理 一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器) 然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端才能使用正向代理 正向代理总结就一句话：代理端代理的是客户端 反向代理 反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求，发给内部网络上的服务器 并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器 反向代理总结就一句话：代理端代理的是服务端 动态资源、静态资源分离 动态资源、静态资源分离是让动态网站里的动态网页根据一定规则把不变的资源和经常变的资源区分开来，动静资源做好了拆分以后 我们就可以根据静态资源的特点将其做缓存操作，这就是网站静态化处理的核心思路 动态资源、静态资源分离简单的概括是：动态文件与静态文件的分离 为什么要做动、静分离？ 在我们的软件开发中，有些请求是需要后台处理的（如：.jsp,.do等等），有些请求是不需要经过后台处理的（如：css、html、jpg、js等等文件） 这些不需要经过后台处理的文件称为静态文件，否则动态文件。因此我们后台处理忽略静态文件。这会有人又说那我后台忽略静态文件不就完了吗 当然这是可以的，但是这样后台的请求次数就明显增多了。在我们对资源的响应速度有要求的时候，我们应该使用这种动静分离的策略去解决 动、静分离将网站静态资源（HTML，JavaScript，CSS，img等文件）与后台应用分开部署，提高用户访问静态代码的速度，降低对后台应用访问 这里我们将静态资源放到nginx中，动态资源转发到tomcat服务器中 负载均衡 负载均衡即是代理服务器将接收的请求均衡的分发到各服务器中 负载均衡主要解决网络拥塞问题，提高服务器响应速度，服务就近提供，达到更好的访问质量，减少后台服务器大并发压力。 性能瓶颈可能出现在哪 CPU太弱 worker数量比CPU核心数大太多，导致频繁上下文切换 连接数，包括最大连接数，和当前是否由太多连接占着茅坑不拉屎 解决方案 CPU affinity 提高连接数 正确设置worker数 Nginx几种常见的负载均衡策略 Nginx服务器上的Master和Worker进程分别是什么 使用“反向代理服务器”的优点是什么？ 参考：Nginx面试题 "},"zother3-java_interview/network-server/tomcat.html":{"url":"zother3-java_interview/network-server/tomcat.html","title":"Tomcat","keywords":"","body":"Tomcat Tomcat的基础架构 （Server、Service、Connector、Container） Tomcat如何加载Servlet的 Pipeline-Valve机制 "},"zother3-java_interview/security-performance/":{"url":"zother3-java_interview/security-performance/","title":"Index","keywords":"","body":""},"zother3-java_interview/security-performance/performance.html":{"url":"zother3-java_interview/security-performance/performance.html","title":"Performance","keywords":"","body":"性能 性能基准 性能优化到底是什么 性能的衡量纬度 "},"zother3-java_interview/security-performance/security.html":{"url":"zother3-java_interview/security-performance/security.html","title":"Security","keywords":"","body":"安全问题 安全要素与 STRIDE 威胁 参考：安全要素与 STRIDE 威胁 防范常见的 Web 攻击 参考：如何防范常见的Web攻击 服务端通信安全攻防 参考：服务端通信安全攻防详解 HTTPS 原理剖析 参考：HTTPS原理剖析与项目场景 HTTPS 降级攻击 参考：HTTPS 降级攻击的场景剖析与解决之道 "},"zother3-java_interview/software_engineering/uml.html":{"url":"zother3-java_interview/software_engineering/uml.html","title":"Uml","keywords":"","body":""},"zother3-java_interview/system/":{"url":"zother3-java_interview/system/","title":"Index","keywords":"","body":"操作系统 相关概念 同步异步，阻塞非阻塞 同步：等待执行完 异步：不等待执行完就开始执行其他的，例如多线程 阻塞：调用之后不一定会立即返回 非阻塞：调用之后，会立即返回 孤儿进程，僵尸进程 孤儿进程是父进程退出的，那个子进程叫做孤儿进程 僵尸进程是子进程退出了但是父进程没有wait的，那个退出的子进程是僵尸进程 操作系统调度算法 先来先服务 短作业优先 优先级调度 时间片轮转 多级反馈队列调度算法 Linux内核：完全公平调度算法 死锁出现条件 互斥 持有并等待 循环等待 没有抢占 线程和进程的区别 进程是程序执行的实体，是操作系统提供的一个抽象概念 线程是操作系统调度的最小单元 线程是在进程的空间内，同一进程内的所有线程共享进程的所有资源如fds，stack等 进程间通信方式 管道 文件 socket 共享内存(mmapped-file也算一种共享内存) 信号量 消息队列 fork之后子进程从父进程继承的东西 real user id, real group id, effective id, effective group id process group id session id controlling terminal set-user, set-group flags current working directory file mode creation mask environment memory mappings resource limits opened file descriptors fork之后父子进程的区别 the return value from fork parent pid pid fork之后只有调用fork的线程被保存，其他线程被销毁。但是其他属性如opened file descriptors，锁等会被保留 线程间同步方式 mutex read-write lock & lock condition variables spin locks barriers(内存屏障) 分页，分段 早期直接使用物理内存，缺点： 地址空间不隔离 内存使用效率低，一个程序执行时，需要将整个程序载入内存，由于程序的地址空间是连续的，如果忽然要执行C，可能就要将已有的程序A放到磁盘。 程序的运行地址不确定 虚拟地址 不直接使用物理内存，而是通过映射。这样解决了程序运行地址不确定的问题 分段：基本思路是把一段程序所需要的内存空间大小的虚拟空间映射到某个地址，然后从物理地址找一块一样大小的地址，进行映射。解决了上面所说的第一个和第三个问题。 A程序和B程序分别被映射到了不痛得物理空间区域，他们没有任何重叠 无论实际上被分配到哪一个物理区域，对于程序来说都是透明的，程序不需要关心物理地址的变化，只需要按照地址从0x00000000到0x00A000000写程序即可 分页。根据程序的局部性原理，一个程序运行时，总是频繁的用到其中一小段数据。分页的基本方法是把地址空间人为的分为固定的大小，每一页的大小由硬件决定，通常是4KB-4MB。当我们把进程的虚拟空间按照分页之后，就可以只加载其中一部分，从而提高了内存的使用效率。 页错误：当进程需要用到某个分页的内容，但是却不在内存里，硬件就会报页错误。 然后由操作系统接管进程，负责将需要的页加到内存然后继续后面的动作。 内核线程与用户线程的三种模型 一对一 一对多 多对多 "},"zother4-EasyJob/":{"url":"zother4-EasyJob/","title":"Zother 4 Easy Job","keywords":"","body":"互联网求职面试题、知识点和面经整理。祝你求职顺利。 编码相关 Java研发工程师知识点总结 基础算法和设计模式 剑指offer题解(Java & Scala)实现 项目结构 . ├── Algorithm │ ├── Sort.md │ ├── 一致性哈希算法.md │ └── 九种内部排序算法的Java实现及其性能测试.md ├── Database │ ├── DATABASE.md │ └── MySQL_PASS.md ├── Document │ ├── Git.pptx │ ├── core_article.md │ └── rabbitmq.pptx ├── Framework │ ├── netty.md │ ├── nginx.md │ ├── rabbitmq.md │ ├── redis.md │ ├── spark-streaming.md │ ├── spark.md │ ├── spring-aop.md │ ├── spring-ioc.md │ └── websocket.md ├── Git │ ├── Git.md │ └── git-share.docx ├── Java │ ├── JVM.md │ ├── Java泛型.md │ ├── Java序列化.md │ ├── Java移位符.md │ ├── Java注解入门.md │ ├── Java类加载器.md │ ├── Java注解之注解处理器.md │ ├── Json.md │ ├── Maven基础.md │ ├── Spring.md │ ├── Struts.md │ ├── URL_URI.md │ ├── 单例.md │ ├── 一致性Hash算法.md │ └── 字符编码.md ├── Network │ └── network.md ├── OS │ ├── Linux.md │ └── OS.md ├── QA │ └── Game-Test.md ├── Questions │ └── Java面试知识点总结.md ├── README.md └── Tools ├── idea.md └── 占用端口.md 参与贡献 提交你的Pull Request. 联系我 邮箱: xmusaber@163.com "},"zother4-EasyJob/Algorithm/Sort.html":{"url":"zother4-EasyJob/Algorithm/Sort.html","title":"Sort","keywords":"","body":"排序算法算法复习 1.插入排序 稳定 原理：从有序序列中选择合适的位置进行插入 复杂度：最好 - 最坏 - 平均 O(n) - O(n^2) - O(n^2) public void selectionSort(int[] a) { if (null ==a || a.length = 0 && temp 2.冒泡排序 稳定 原理：相邻两个元素比较大小进行交换，一趟冒泡后会有一个元素到达最终位置 复杂度：最好 - 最坏 - 平均 O(n) - O(n^2) - O(n^2) public void bubbleSort(int[] a) { if (null == a || a.length a[j+1]) { int temp = a[j]; a[j] = a[j+1]; a[j+1] = temp; flag = true; } } if (flag == false) { return; } } } 3.希尔排序(缩小增量排序) 不稳定 按步长进行分组，组内直接插入，缩小增量再次进行此步骤，增量为1时相当于一次直接插入。 复杂度：最好O(n) - 最坏O(n^s 1 public void shellSort(int[] a) { if (null == a || a.length 0; d/=2) { for (int i = d; i = 0 && temp 4.选择排序 不稳定 原理：每次从无序序列选择一个最小的 复杂度：最好O(n^2) - 最坏O(n^2) - 平均O(n^2) public void selectSort(int[] a) { if (null == a || a.length 5.快速排序 不稳定 原理：分治+递归 复杂度：最好O(nlgn) - 最坏O(n^2) - 平均O(nlgn) public void quickSort(int[] a, int low, int high) { if (low = pivot) { high--; } a[low] = a[high]; while (low 选取pivot的方式：固定基准元 随机基准 三数取中 快排的优化：针对随机数组+有序数组+重复数组 1.当待排序序列的长度分割到一定大小后，使用插入排序：效率提高一些，但是都解决不了重复数组的问题。 2.在一次分割结束后，可以把与Key相等的元素聚在一起，继续下次分割时，不用再对与key相等元素分割 6.归并排序 稳定 原理：两个有序序列的合并，方法：分治 + 递归 复杂度：最好O(nlgn) - 最坏O(nlgn) - 平均O(nlgn) public void mergeSort(int[] a, int low, int high) { int mid = (low + high) / 2; if (low 堆排序 原理：利用堆的特性 复杂度：O(nlogn) [平均 - 最好 - 最坏] // 堆排序 public void heapSort(int[] a) { if (null == a || a.length = 0; i--) { int temp = a[0]; a[0] = a[i]; a[i] = temp; adjustHeap(a, i, 0); } } // 建堆 private void buildMaxHeap(int[] a) { for (int i = a.length/2; i >= 0; i--) { adjustHeap(a, a.length, i); } } // 调整堆 private void adjustHeap(int[] a, int size, int parent) { int left = 2 * parent + 1; int right = 2 * parent + 2; int largest = parent; if (left a[largest]) { largest = left; } if (right a[largest]) { largest = right; } if (parent != largest) { int temp = a[parent]; a[parent] = a[largest]; a[largest] = temp; adjustHeap(a, size, largest); } } "},"zother4-EasyJob/Algorithm/一致性哈希算法.html":{"url":"zother4-EasyJob/Algorithm/一致性哈希算法.html","title":"对一致性哈希算法的深入研究","keywords":"","body":"一致性Hash算法 关于一致性Hash算法，在我之前的博文中已经有多次提到了，MemCache超详细解读一文中”一致性Hash算法”部分，对于为什么要使用一致性Hash算法、一致性Hash算法的算法原理做了详细的解读。 算法的具体原理这里再次贴上： 先构造一个长度为2^32的整数环（这个环被称为一致性Hash环），根据节点名称的Hash值（其分布为[0, 2^32-1]）将服务器节点放置在这个Hash环上，然后根据数据的Key值计算得到其Hash值（其分布也为[0, 2^32-1]），接着在Hash环上顺时针查找距离这个Key值的Hash值最近的服务器节点，完成Key到服务器的映射查找。 这种算法解决了普通余数Hash算法伸缩性差的问题，可以保证在上线、下线服务器的情况下尽量有多的请求命中原来路由到的服务器。 当然，万事不可能十全十美，一致性Hash算法比普通的余数Hash算法更具有伸缩性，但是同时其算法实现也更为复杂，本文就来研究一下，如何利用Java代码实现一致性Hash算法。在开始之前，先对一致性Hash算法中的几个核心问题进行一些探究。 数据结构的选取 一致性Hash算法最先要考虑的一个问题是：构造出一个长度为2^32的整数环，根据节点名称的Hash值将服务器节点放置在这个Hash环上。 那么，整数环应该使用何种数据结构，才能使得运行时的时间复杂度最低？首先说明一点，关于时间复杂度，常见的时间复杂度与时间效率的关系有如下的经验规则： O(1) 一般来说，前四个效率比较高，中间两个差强人意，后三个比较差（只要N比较大，这个算法就动不了了）。OK，继续前面的话题，应该如何选取数据结构，我认为有以下几种可行的解决方案。 1、解决方案一：排序+List 我想到的第一种思路是：算出所有待加入数据结构的节点名称的Hash值放入一个数组中，然后使用某种排序算法将其从小到大进行排序，最后将排序后的数据放入List中，采用List而不是数组是为了结点的扩展考虑。 之后，待路由的结点，只需要在List中找到第一个Hash值比它大的服务器节点就可以了，比如服务器节点的Hash值是[0,2,4,6,8,10]，带路由的结点是7，只需要找到第一个比7大的整数，也就是8，就是我们最终需要路由过去的服务器节点。 如果暂时不考虑前面的排序，那么这种解决方案的时间复杂度： （1）最好的情况是第一次就找到，时间复杂度为O(1) （2）最坏的情况是最后一次才找到，时间复杂度为O(N) 平均下来时间复杂度为O(0.5N+0.5)，忽略首项系数和常数，时间复杂度为O(N)。 但是如果考虑到之前的排序，我在网上找了张图，提供了各种排序算法的时间复杂度： ![排序算法复杂度](http://mmbiz.qpic.cn/mmbiz/eZzl4LXykQysaB4Qialvt1cNI5WIAp1D4WfCIOarhhLOoYxXoEfFOayaYcmEXfLQbetdibFctgvAHxjCOZWSzNWg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1) **2、解决方案二：遍历+List** 既然排序操作比较耗性能，那么能不能不排序？可以的，所以进一步的，有了第二种解决方案。 解决方案使用List不变，不过可以采用遍历的方式： （1）服务器节点不排序，其Hash值全部直接放入一个List中 （2）带路由的节点，算出其Hash值，由于指明了”顺时针”，因此遍历List，比待路由的节点Hash值大的算出差值并记录，比待路由节点Hash值小的忽略 （3）算出所有的差值之后，最小的那个，就是最终需要路由过去的节点 在这个算法中，看一下时间复杂度： 1、最好情况是只有一个服务器节点的Hash值大于带路由结点的Hash值，其时间复杂度是O(N)+O(1)=O(N+1)，忽略常数项，即O(N) 2、最坏情况是所有服务器节点的Hash值都大于带路由结点的Hash值，其时间复杂度是O(N)+O(N)=O(2N)，忽略首项系数，即O(N) 所以，总的时间复杂度就是O(N)。其实算法还能更改进一些：给一个位置变量X，如果新的差值比原差值小，X替换为新的位置，否则X不变。这样遍历就减少了一轮，不过经过改进后的算法时间复杂度仍为O(N)。 总而言之，这个解决方案和解决方案一相比，总体来看，似乎更好了一些。 **3、解决方案三：二叉查找树** 抛开List这种数据结构，另一种数据结构则是使用二叉查找树。 当然我们不能简单地使用二叉查找树，因为可能出现不平衡的情况。平衡二叉查找树有AVL树、红黑树等，这里使用红黑树，选用红黑树的原因有两点： 1、红黑树主要的作用是用于存储有序的数据，这其实和第一种解决方案的思路又不谋而合了，但是它的效率非常高 2、JDK里面提供了红黑树的代码实现TreeMap和TreeSet 另外，以TreeMap为例，TreeMap本身提供了一个tailMap(K fromKey)方法，支持从红黑树中查找比fromKey大的值的集合，但并不需要遍历整个数据结构。 使用红黑树，可以使得查找的时间复杂度降低为O(logN)，比上面两种解决方案，效率大大提升。 为了验证这个说法，我做了一次测试，从大量数据中查找第一个大于其中间值的那个数据，比如10000数据就找第一个大于5000的数据（模拟平均的情况）。看一下O(N)时间复杂度和O(logN)时间复杂度运行效率的对比： ![treemaptest](http://7xlkoc.com1.z0.glb.clouddn.com/treemaptest.png) 因为再大就内存溢出了，所以只测试到4000000数据。可以看到，数据查找的效率，TreeMap是完胜的，其实再增大数据测试也是一样的，红黑树的数据结构决定了任何一个大于N的最小数据，它都只需要几次至几十次查找就可以查到。 当然，明确一点，有利必有弊，根据我另外一次测试得到的结论是，为了维护红黑树，数据插入效率TreeMap在三种数据结构里面是最差的，且插入要慢上5~10倍。 Hash值重新计算 服务器节点我们肯定用字符串来表示，比如”192.168.1.1″、”192.168.1.2″，根据字符串得到其Hash值，那么另外一个重要的问题就是Hash值要重新计算，这个问题是我在测试String的hashCode()方法的时候发现的，不妨来看一下为什么要重新计算Hash值： /** * String的hashCode()方法运算结果查看 * @author 哓哓 * */ public class StringHashCodeTest { public static void main(String[] args) { System.out.println(\"192.168.0.0:111的哈希值：\" + \"192.168.0.0:1111\".hashCode()); System.out.println(\"192.168.0.1:111的哈希值：\" + \"192.168.0.1:1111\".hashCode()); System.out.println(\"192.168.0.2:111的哈希值：\" + \"192.168.0.2:1111\".hashCode()); System.out.println(\"192.168.0.3:111的哈希值：\" + \"192.168.0.3:1111\".hashCode()); System.out.println(\"192.168.0.4:111的哈希值：\" + \"192.168.0.4:1111\".hashCode()); } } 我们在做集群的时候，集群点的IP以这种连续的形式存在是很正常的。看一下运行结果为： 192.168.0.0:111的哈希值：1845870087 192.168.0.1:111的哈希值：1874499238 192.168.0.2:111的哈希值：1903128389 192.168.0.3:111的哈希值：1931757540 192.168.0.4:111的哈希值：1960386691 这个就问题大了，[0,2^32-1]的区间之中，5个HashCode值却只分布在这么小小的一个区间，什么概念？[0,2^32-1]中有4294967296个数字，而我们的区间只有122516605，从概率学上讲这将导致97%待路由的服务器都被路由到”192.168.0.1″这个集群点上，简直是糟糕透了！ 另外还有一个不好的地方：规定的区间是非负数，String的hashCode()方法却会产生负数（不信用”192.168.1.0:1111″试试看就知道了）。不过这个问题好解决，取绝对值就是一种解决的办法。 综上，String重写的hashCode()方法在一致性Hash算法中没有任何实用价值，得找个算法重新计算HashCode。这种重新计算Hash值的算法有很多，比如CRC32_HASH、FNV1_32_HASH、KETAMA_HASH等，其中KETAMA_HASH是默认的MemCache推荐的一致性Hash算法，用别的Hash算法也可以，比如FNV1_32_HASH算法的计算效率就会高一些。 一致性Hash算法实现版本1：不带虚拟节点 使用一致性Hash算法，尽管增强了系统的伸缩性，但是也有可能导致负载分布不均匀，解决办法就是使用虚拟节点代替真实节点，第一个代码版本，先来个简单的，不带虚拟节点。 下面来看一下不带虚拟节点的一致性Hash算法的Java代码实现： /** * 不带虚拟结点的一致性Hash算法 * @author 哓哓 * */ public class ConsistentHashWithoutVN { /** * 待加入Hash环的服务器列表 */ private static String[] servers = { \"192.168.0.0:111\", \"192.168.0.1:111\", \"192.168.0.2:111\", \"192.168.0.3:111\", \"192.168.0.4:111\" }; /** * key表示服务器的hash值，value表示服务器的名称 */ private static SortedMap sortedMap = new TreeMap<>(); /** * 程序初始化，将所有服务器加入集合 */ static { for (int i = 0; i > 7; hash += hash > 17; hash += hash subMap = sortedMap.tailMap(hash); // 顺时针的第一个Key Integer i = subMap.firstKey(); // 返回路由到的服务器名称 return subMap.get(i); } public static void main(String[] args) { String[] nodes = {\"127.0.0.1:1111\", \"221.226.0.1:2222\", \"10.211.0.1:3333\"}; for (int i = 0; i 可以运行一下看一下结果： [192.168.0.0:111]加入集群中, 其Hash值为575774686 [192.168.0.1:111]加入集群中, 其Hash值为8518713 [192.168.0.2:111]加入集群中, 其Hash值为1361847097 [192.168.0.3:111]加入集群中, 其Hash值为1171828661 [192.168.0.4:111]加入集群中, 其Hash值为1764547046 [127.0.0.1:1111]的hash值为380278925,被路由到的服务器为[192.168.0.0:111] [221.226.0.1:2222]的hash值为1493545632,被路由到的服务器为[192.168.0.4:111] [10.211.0.1:3333]的hash值为1393836017,被路由到的服务器为[192.168.0.4:111] 使用虚拟节点来改善一致性Hash算法 上面的一致性Hash算法实现，可以在很大程度上解决很多分布式环境下不好的路由算法导致系统伸缩性差的问题，但是会带来另外一个问题：负载不均。 比如说有Hash环上有A、B、C三个服务器节点，分别有100个请求会被路由到相应服务器上。现在在A与B之间增加了一个节点D，这导致了原来会路由到B上的部分节点被路由到了D上，这样A、C上被路由到的请求明显多于B、D上的，原来三个服务器节点上均衡的负载被打破了。某种程度上来说，这失去了负载均衡的意义，因为负载均衡的目的本身就是为了使得目标服务器均分所有的请求。 解决这个问题的办法是引入虚拟节点，其工作原理是：将一个物理节点拆分为多个虚拟节点，并且同一个物理节点的虚拟节点尽量均匀分布在Hash环上。采取这样的方式，就可以有效地解决增加或减少节点时候的负载不均衡的问题。 至于一个物理节点应该拆分为多少虚拟节点，下面可以先看一张图： 横轴表示需要为每台服务器扩展的虚拟节点倍数，纵轴表示的是实际物理服务器数。可以看出，物理服务器很少，需要更大的虚拟节点；反之物理服务器比较多，虚拟节点就可以少一些。比如有10台物理服务器，那么差不多需要为每台服务器增加100~200个虚拟节点才可以达到真正的负载均衡。 一致性Hash算法实现版本2：带虚拟节点 在理解了使用虚拟节点来改善一致性Hash算法的理论基础之后，就可以尝试开发代码了。编程方面需要考虑的问题是： 1、一个真实结点如何对应成为多个虚拟节点？ 2、虚拟节点找到后如何还原为真实结点？ 这两个问题其实有很多解决办法，我这里使用了一种简单的办法，给每个真实结点后面根据虚拟节点加上后缀再取Hash值，比如”192.168.0.0:111″就把它变成”192.168.0.0:111&&VN0″到”192.168.0.0:111&&VN4″，VN就是Virtual Node的缩写，还原的时候只需要从头截取字符串到”&&”的位置就可以了。 下面来看一下带虚拟节点的一致性Hash算法的Java代码实现： /** * 带虚拟结点的一致性Hash算法 * * @author 哓哓 * */ public class ConsistentHashWithVN { /** * 待加入Hash环的服务器列表 */ private static String[] servers = { \"192.168.0.0:111\", \"192.168.0.1:111\", \"192.168.0.2:111\", \"192.168.0.3:111\", \"192.168.0.4:111\" }; /** * 真实结点列表，考虑到服务器上线、下线的场景，即添加、删除的场景会比较频繁，这里使用LinkedList会更好 */ private static List realNodes = new LinkedList<>(); /** * key表示虚拟结点服务器的hash值，value表示虚拟结点服务器的名称 */ private static SortedMap virtualNodes = new TreeMap<>(); /** * 虚拟结点数目（一个真实结点对应VN_SUM个虚拟结点） */ private static final int VN_SUM = 5; /** * 加所有服务器加入集合 */ static { for (int i = 0; i > 7; hash += hash > 17; hash += hash subMap = virtualNodes.tailMap(hash); // 顺时针的第一个Key Integer i = subMap.firstKey(); // 截取 String virtualNode = subMap.get(i); // 返回路由到的服务器名称 return virtualNode.substring(0, virtualNode.indexOf(\"&\")); } public static void main(String[] args) { String[] nodes = { \"127.0.0.1:1111\", \"221.226.0.1:2222\", \"10.211.0.1:3333\", \"112.74.15.218:80\" }; for (int i = 0; i 关注一下运行结果： 虚拟节点[192.168.0.0:111&VN0]被添加, hash值为62550928 虚拟节点[192.168.0.0:111&VN1]被添加, hash值为45670134 虚拟节点[192.168.0.0:111&VN2]被添加, hash值为1069081239 虚拟节点[192.168.0.0:111&VN3]被添加, hash值为681260483 虚拟节点[192.168.0.0:111&VN4]被添加, hash值为345193220 虚拟节点[192.168.0.1:111&VN0]被添加, hash值为1014794997 虚拟节点[192.168.0.1:111&VN1]被添加, hash值为314112378 虚拟节点[192.168.0.1:111&VN2]被添加, hash值为1764217630 虚拟节点[192.168.0.1:111&VN3]被添加, hash值为1754008301 虚拟节点[192.168.0.1:111&VN4]被添加, hash值为1013081826 虚拟节点[192.168.0.2:111&VN0]被添加, hash值为1936519782 虚拟节点[192.168.0.2:111&VN1]被添加, hash值为1962355349 虚拟节点[192.168.0.2:111&VN2]被添加, hash值为1051508275 虚拟节点[192.168.0.2:111&VN3]被添加, hash值为1487794011 虚拟节点[192.168.0.2:111&VN4]被添加, hash值为1010967116 虚拟节点[192.168.0.3:111&VN0]被添加, hash值为1671479534 虚拟节点[192.168.0.3:111&VN1]被添加, hash值为803892279 虚拟节点[192.168.0.3:111&VN2]被添加, hash值为1986618297 虚拟节点[192.168.0.3:111&VN3]被添加, hash值为1068919486 虚拟节点[192.168.0.3:111&VN4]被添加, hash值为454720555 虚拟节点[192.168.0.4:111&VN0]被添加, hash值为232783560 虚拟节点[192.168.0.4:111&VN1]被添加, hash值为1097591827 虚拟节点[192.168.0.4:111&VN2]被添加, hash值为812889841 虚拟节点[192.168.0.4:111&VN3]被添加, hash值为1338995023 虚拟节点[192.168.0.4:111&VN4]被添加, hash值为1008393313 ===========路由映射============== [127.0.0.1:1111]的hash值为380278925,被路由到的服务器为[192.168.0.3:111] [221.226.0.1:2222]的hash值为1493545632,被路由到的服务器为[192.168.0.3:111] [10.211.0.1:3333]的hash值为1393836017,被路由到的服务器为[192.168.0.2:111] [112.74.15.218:80]的hash值为51269059,被路由到的服务器为[192.168.0.0:111] 从代码运行结果看，每个点路由到的服务器都是Hash值顺时针离它最近的那个服务器节点，没有任何问题。 通过采取虚拟节点的方法，一个真实结点不再固定在Hash环上的某个点，而是大量地分布在整个Hash环上，这样即使上线、下线服务器，也不会造成整体的负载不均衡。 后记 在写本文的时候，很多知识我也是边写边学，难免有很多写得不好、理解得不透彻的地方，而且代码整体也比较糙，未有考虑到可能的各种情况。抛砖引玉，一方面，写得不对的地方，还望网友朋友们指正；另一方面，后续我也将通过自己的工作、学习不断完善上面的代码。 转自五月的仓颉，代码自己重新实现过。 联系我 邮箱: xmusaber@163.com "},"zother4-EasyJob/Algorithm/九种内部排序算法的Java实现及其性能测试.html":{"url":"zother4-EasyJob/Algorithm/九种内部排序算法的Java实现及其性能测试.html","title":"九种内部排序算法的Java实现及其性能测试","keywords":"","body":"九种内部排序算法的Java实现及其性能测试 9种内部排序算法性能比较 第九种为java.util.Arrays.sort（改进的快速排序方法） 100000的随机数据集 200000的随机数据集 500000的随机数据集 结论：归并排序和堆排序维持O(nlgn)的复杂度，速率差不多，表现优异。固定基准的快排表现很是优秀。而通过使用一个循环完成按增量分组后的直接插入的希尔排序，测试效果显著。 冒泡，选择，直接插入都很慢，而冒泡效率是最低。 1.插入排序[稳定] 适用于小数组,数组已排好序或接近于排好序速度将会非常快 复杂度：O(n^2) - O(n) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] public void insertionSort(int[] a) { if (null == a || a.length = 0 && temp 2.希尔排序(缩小增量排序)[不稳定] 复杂度 平均 O(n^1.3) 最好O(n) 最差O(n^s)[1 内循环通过模拟并行的方式完成分组的内部直接插入排序，而不是一个一个分组分组的排，在10w的随机数据20w的随机数据均表现优异。 public void shellSort(int[] a) { if (null == a || a.length 0; d/=2) { // 从1B开始先和1A比较 然后2A与2B...然后再1C向前与同组的比较 for (int i = d; i =0 && temp 3.冒泡排序[稳定] 复杂度：O(n^2) - O(n) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] public void bubbleSort(int[] a) { if (null == a || a.length a[j+1]) { int temp = a[j]; a[j] = a[j+1]; a[j+1] = temp; flag = true; } } if (false == flag) { return; } } } 4.选择排序[不稳定] 原理：每次从无序序列选取最小的 复杂度：O(n^2) - O(n^2) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] public void selectSort(int[] a) { if (null == a || a.length 5.归并排序[稳定] 原理：采用分治法 复杂度：O(nlogn) - O(nlgn) - O(nlgn) - O(n)[平均 - 最好 - 最坏 - 空间复杂度] // 排序 public void mergeSort(int[] a, int low, int high) { int mid = (low + high) / 2; if (low 6.快速排序[不稳定] 原理：分治+递归 复杂度：O(nlgn) - O(nlgn) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 栈空间0(lgn) - O(n) // 固定基准 public void quickSort(int[] a, int low, int high) { if (null == a || a.length = pivot) { high--; } a[low] = a[high]; // 注意等于，否则死循环 while (low 7.堆排序[不稳定] 堆一般指二叉堆。 复杂度：O(nlogn) - O(nlgn) - O(nlgn) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 大顶堆实现从小到大的升序排列，小顶堆一般用于构造优先队列 public void heapSort(int[] a) { if (null == a || a.length = 0; i--) { int temp = a[0]; a[0] = a[i]; a[i] = temp; adjustHeap(a, i, 0); } } // 建堆 private void buildMaxHeap(int[] a) { int mid = a.length / 2; for (int i = mid; i >= 0; i--) { adjustHeap(a, a.length, i); } } // 递归调整堆 private void adjustHeap(int[] a, int size, int parent) { int left = 2 * parent + 1; int right = 2 * parent + 2; int largest = parent; if (left a[parent]) { largest = left; } if (right a[largest]) { largest = right; } if (parent != largest) { int temp = a[parent]; a[parent] = a[largest]; a[largest] = temp; adjustHeap(a, size, largest); } } 8.基数排序[稳定] 原理：分配加收集 复杂度： O(d(n+r)) r为基数d为位数 空间复杂度O(n+r) // 基数排序 public void radixSort(int[] a, int begin, int end, int digit) { // 基数 final int radix = 10; // 桶中的数据统计 int[] count = new int[radix]; int[] bucket = new int[end-begin+1]; // 按照从低位到高位的顺序执行排序过程 for (int i = 1; i = begin; j--) { int index = getDigit(a[j], i); bucket[count[index] - 1] = a[j]; count[index]--; } // 取出，此时已是对应当前位数有序的表 for (int j = 0; j = 2) { div += \"0\"; d--; } return x/Integer.parseInt(div) % 10; } } 排序代码地址 https://github.com/Lemonjing/TinyCoding/tree/master/src/main/java/com/ryan/sort 性能测试代码地址 https://github.com/Lemonjing/TinyCoding/tree/master/src/test/java/com/ryan/sort "},"zother4-EasyJob/Database/DATABASE.html":{"url":"zother4-EasyJob/Database/DATABASE.html","title":"DATABASE","keywords":"","body":"数据库 1.事务 说明 ：事务是恢复和并发控制的基本单位，是用户定义的一个操作序列。这些操作要么都做，要么都不做，是一个不可分割的工作单位。通过事务，逻辑相关的一组操作绑定在一起，以便服务器保持数据的完整性。 事务的特性：ACID A:原子性(Atomicity) 事务是数据库的逻辑工作单位，事务中包括的诸操作要么全做，要么全不做。 B:一致性(Consistency) 事务执行的结果必须是使数据库从一个一致性状态变到另一个一致性状态。一致性与原子性是密切相关的。 C:隔离性(Isolation) 一个事务的执行不能被其他事务干扰。 D:持续性/永久性(Durability) 一个事务一旦提交，它对数据库中数据的改变就应该是永久性的。 2.多个条件where 1=1 模糊查询%%通配符 3.手写一段sql语句，具体内容忘了，好像和limit有关 4.存储引擎的区别 InnoDB 是支持事务的存储引擎，其设计目标是面向在线事务处理的应用，其特点是行锁设计，支持外键，支持类似Oracle的非锁定读，默认读取操作不会产生锁。MySQL5.5以后是默认的存储引擎。还提供了插入缓存，二次写，预读等高性能和高可用的功能。 MyISAM 引擎不支持事务，表锁设计，支持全文索引，主要是面向OLAP数据库应用。 NDB是集群存储引擎，其数据全部放在内存中，因此主键查找的速度极快。 Memory将表中的数据存放在内存中，如果数据库重启或者发生奔溃，表中的数据都将消失。它使用于存储临时数据的临时表。默认采用哈希索引 5.sql注入原理 就是通过把SQL命令插入到Web 表单 提交或输入域名或页面请求的查询字符串，最终达到欺骗服务器执行恶意的SQL命令 1.猜表名，列名等 2.后台身份验证绕过漏洞 验证绕过漏洞就是'or'='or'后台绕过漏洞，利用的就是AND和OR的运算规则，从而造成后台脚本逻辑性错误. 防范： 1.永远不要信任用户的输入，要对用户的输入进行校验，可以通过正则表达式，或限制长度，对单引号和双\"-\"进行转换等。 2.永远不要使用动态拼装SQL，可以使用参数化的SQL或者直接使用存储过程进行数据查询存取。 3.永远不要使用管理员权限的数据库连接，为每个应用使用单独的权限有限的数据库连接。 4.不要把机密信息明文存放，请加密或者hash掉密码和敏感的信息。 5.应用的异常信息应该给出尽可能少的提示，最好使用自定义的错误信息对原始错误信息进行包装，把异常信息存放在独立的表中。 6.数据库范式 第一范式（1NF）：属性不可分。 第二范式（2NF）：符合1NF，并且，非主属性完全依赖于码。 第三范式（3NF）：符合2NF，并且，消除传递依赖 BCNF:符合3NF, 并且,没有任何属性完全函数依赖于非码的任何一组属性. 数据库索引 索引是一个单独存储在磁盘上的数据库结构，它们包含着对数据表里所有记录的引用指针，使用索引可以提高数据库特定数据的查询速度.索引是在存储引擎中实现的，因此每种存储引擎的索引不一定完全相同,并且每种存储引擎也不一定支持所有索引类型． 索引的存储类型有两种：BTREE和HASH,具体和表的存储引擎有关．MyISAM和InnoDB存储引擎只支持BTREE;MEMORY/HEAD存储索引可以支持HASH和BTREE索引． 索引的优点: 1.通过创建唯一索引，可以保证数据库表中每行数据的唯一性. 2.可以加快数据的查询速度． 3.在实现数据的参考完整性方面，可以加速表和表之间的连接． 索引的缺点： 1.创建索引和维护索引要耗费时间，并且随着数据量的增加耗费时间也增加． 2.索引需要占空间内存． 3.在对表中数据进行增加,删除和修改的时候，索引也需要动态维护，这样降低了数据维护速度． 索引分类 普通索引和唯一索引 数据库锁机制 数据库锁定机制简单来说就是数据库为了保证数据的一致性而使各种共享资源在被并发访问，访问变得有序所设计的一种规则。MySQL各存储引擎使用了三种类型（级别）的锁定机制：行级锁定，页级锁定和表级锁定。 表级锁定（table-level）：表级别的锁定是MySQL各存储引擎中最大颗粒度的锁定机制。该锁定机制最大的特点是实现逻辑非常简单，带来的系统负面影响最小。所以获取锁和释放锁的速度很快。由于表级锁一次会将整个表锁定，所以可以很好的避免困扰我们的死锁问题。当然，锁定颗粒度大所带来最大的负面影响就是出现锁定资源争用的概率也会最高，致使并大度大打折扣。表级锁分为读锁和写锁。 页级锁定（page-level）：页级锁定的特点是锁定颗粒度介于行级锁定与表级锁之间，所以获取锁定所需要的资源开销，以及所能提供的并发处理能力也同样是介于上面二者之间。另外，页级锁定和行级锁定一样，会发生死锁。 行级锁定（row-level）：行级锁定最大的特点就是锁定对象的颗粒度很小，也是目前各大数据库管理软件所实现的锁定颗粒度最小的。由于锁定颗粒度很小，所以发生锁定资源争用的概率也最小，能够给予应用程序尽可能大的并发处理能力而提高一些需要高并发应用系统的整体性能。虽然能够在并发处理能力上面有较大的优势，但是行级锁定也因此带来了不少弊端。由于锁定资源的颗粒度很小，所以每次获取锁和释放锁需要做的事情也更多，带来的消耗自然也就更大了。此外，行级锁定也最容易发生死锁。InnoDB的行级锁同样分为两种，共享锁和排他锁，同样InnoDB也引入了意向锁（表级锁）的概念，所以也就有了意向共享锁和意向排他锁，所以InnoDB实际上有四种锁，即共享锁（S）、排他锁（X）、意向共享锁（IS）、意向排他锁（IX）； 在MySQL数据库中，使用表级锁定的主要是MyISAM，Memory，CSV等一些非事务性存储引擎，而使用行级锁定的主要是Innodb存储引擎和NDBCluster存储引擎，页级锁定主要是BerkeleyDB存储引擎的锁定方式。 而意向锁的作用就是当一个事务在需要获取资源锁定的时候，如果遇到自己需要的资源已经被排他锁占用的时候，该事务可以需要锁定行的表上面添加一个合适的意向锁。如果自己需要一个共享锁，那么就在表上面添加一个意向共享锁。而如果自己需要的是某行（或者某些行）上面添加一个排他锁的话，则先在表上面添加一个意向排他锁。意向共享锁可以同时并存多个，但是意向排他锁同时只能有一个存在。 | | 共享锁（S）| 排他锁（X）| 意向共享锁（IS）| 意向排他锁（IX）| 共享锁（S） | 兼容 | 冲突 | 兼容 |冲突 排他锁（X） | 冲突 | 冲突 | 冲突 |冲突 意向共享锁（IS） | 兼容 | 冲突 | 兼容 |兼容 意向排他锁（IX） | 冲突 | 冲突 | 兼容 |兼容 参考地址：http://www.cnblogs.com/ggjucheng/archive/2012/11/14/2770445.html MyISAM 表锁优化建议： 1、缩短锁定时间 2、分离能并行的操作 3、合理利用读写优先级 乐观锁，悲观锁 悲观锁:它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制。悲观的缺陷是不论是页锁还是行锁，加锁的时间可能会很长，这样可能会长时间的限制其他用户的访问，也就是说悲观锁的并发访问性不好。 乐观锁（ Optimistic Locking ） :相对悲观锁而言，乐观锁假设认为数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则则拒绝更新并返回用户错误的信息，让用户决定如何去做。乐观锁由程序实现，不会存在死锁问题。它适用的场景也相对乐观。但乐观锁不能解决脏读的问题 悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。[1] 乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。[1] 乐观锁不能解决脏读的问题。 事务隔离机制 为什么？ 1.更新丢失 两个事务都同时更新一行数据，一个事务对数据的更新把另一个事务对数据的更新覆盖了。这是因为系统没有执行任何的锁操作，因此并发事务并没有被隔离开来。 2.脏读 一个事务读取到了另一个事务未提交的数据操作结果。这是相当危险的，因为很可能所有的操作都被回滚。 3.不可重复读 不可重复读（Non-repeatable Reads）：一个事务对同一行数据重复读取两次，但是却得到了不同的结果。 4、幻读：幻读与不可重复读类似。它发生在一个事务(T1)读取了几行数据，接着另一个并发事务(T2)插入了一些数据时。在随后的查询中，第一个事务(T1)就会发现多了一些原本不存在的记录 事物隔离级别： 未提交读(READ UNCOMMITTED):允许脏读 不允许更新丢失 提交读(READ COMMITTED):允许不可重复读 但不允许脏读 可重复读(REPEATABLE READ):禁止不可重复读和脏读 但可能出现幻读 可串行化(SERIZLIZABLE):它通过强制事务串行执行，不能并发的执行，避免了前面说的幻读的问题。 总结：隔离级别越高，越能保证事务的完整性和一致性，但对并发性能的影响也就越大。对于多数应用可以把隔离级别设置为ReadCommited，也就是授权读取，能够避免脏读，而保持较好的并发性能。 脏读 不可重复读 幻读可能性 加锁读 未提交读 YES YES YES NO 提交读 NO YES YES NO 可重复读 NO NO YES NO 可串行化 NO NO NO YES 数据库事务属性 事务是由一组SQL语句组成的逻辑处理单元，事务具有以下4个属性，通常简称为事务的ACID属性。 原子性（Atomicity）：事务是一个原子操作单元，其对数据的修改，要么全都执行，要么全都不执行。 一致性（Consistent）：在事务开始和完成时，数据都必须保持一致状态。这意味着所有相关的数据规则都必须应用于事务的修改，以保持数据的完整性；事务结束时，所有的内部数据结构（如B树索引或双向链表）也都必须是正确的。 隔离性（Isolation）：数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的“独立”环境执行。这意味着事务处理过程中的中间状态对外部是不可见的，反之亦然。 持久性（Durable）：事务完成之后，它对于数据的修改是永久性的，即使出现系统故障也能够保持。 数据库事务的几种粒度； 是否了解数据库的索引是如何实现的 MyISAM索引实现 MyISAM索引使用了B+Tree作为索引结构，叶子结点的data域存放的是数据记录的地址。MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。主索引和辅助索引的存储结构没有任何区别。 InnoDB索引实现 虽然InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。 第一个重大区别是InnoDB的数据文件本身就是索引文件。从上文知道，MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这种索引叫做聚集索引。这种索引叫做聚集索引。因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据。 第二个与MyISAM索引的不同是InnoDB的辅助索引data域存储相应记录主键的值而不是地址。换句话说，InnoDB的所有辅助索引都引用主键作为data域。 Memory索引实现 Memory索引适用于需要快速访问数据的场景，显示支持哈希索引。内部基于哈希表数据结构实现，只包含哈希值和行指针，对于每一行数据，存储引擎都会对所有的引擎列计算一个哈希码，在哈希表对应位置存放该行数据的指针或地址。为了解决多个hash冲突问题，哈希索引采用了链地址法来解决冲突问题。所以采用链表数组作为存储结构。这种索引结构十分紧凑，且具有很快的查询速度。但也存在一些问题，1.哈希表数据不是按照索引顺序存储的，所以无法用于排序。2.只能支持等值比较查询。3.存在冲突情况下查询速度变慢。 https://msdn.microsoft.com/zh-cn/library/dn133190.aspx 数据库连接池原理 背景 传统的数据库连接方式是，用户每次请求都要向数据库获取连接，而数据库连接的创建和关闭需要一定的开销。频繁的建立、关闭数据库，会极大的降低系统的性能，增大系统的开销，甚至成为系统的瓶颈。另外使用这种传统的模式，还必须管理数据库的每一个连接，以确保他们能正确关闭，如果出现程序异常而导致某些连接未能关闭。同时无节制的创建连接极易导致数据库服务器内存溢出。 原理 数据库连接池的基本思想就是为数据库连接建立一个“缓冲池”。预先在缓冲池中放入一定数量的连接，当需要建立数据库连接时，只需从“缓冲池”中取出一个，使用完毕之后再放回去。以及一套连接使用、分配、管理策略，使得该连接池中的连接可以得到高效、安全的复用，避免了数据库连接频繁建立、关闭的开销。我们可以通过设定连接池最大连接数来防止系统无尽的与数据库连接。 开源java连接池: 现在很多Web服务器(Weblogic, WebSphere, Tomcat)都提供了DataSoruce的实现，即连接池的实现。通常我们把DataSource的实现，按其英文含义称之为数据源，数据源中都包含了数据库连接池的实现。 1.C3P0 :是一个开放源代码的JDBC连接池，它在lib目录中与Hibernate一起发布,包括了实现jdbc3和jdbc2扩展规范说明的Connection 和Statement 池的DataSources 对象。参考网站: http://sourceforge.net/projects/c30/ 2.Proxool :是一个Java SQL Driver驱动程序，提供了对你选择的其它类型的驱动程序的连接池封装。可以非常简单的移植到现存的代码中。完全可配置。快速，成熟，健壮。可以透明地为你现存的JDBC驱动程序增加连接池功能。 参考网站: http://proxool.sourceforge.net 3.Jakarta DBCP :是一个依赖Jakarta commons-pool对象池机制的数据库连接池.DBCP可以直接的在应用程序用使用。参考网站: http://jakarta.apache.org/commons/dbcp/ 原理: http://www.uml.org.cn/sjjm/201004153.asp 实现: http://www.cnblogs.com/lihuiyy/archive/2012/02/14/2351768.html 连接池使用什么数据结构实现 链表 实现连接池: http://www.cnblogs.com/lihuiyy/archive/2012/02/14/2351768.html 四个表 记录成绩，每个大约十万条记录，如何找到成绩最好的同学 servlet的一些相关问题 webservice相关 mysql有那些存储引擎，分别有什么特点 join操作 LEFT JOIN 关键字会从左表 (Persons) 那里返回所有的行，即使在右表 (Orders) 中没有匹配的行。 RIGHT JOIN 关键字会从右表 (Orders) 那里返回所有的行，即使在左表 (Persons) 中没有匹配的行。 FULL JOIN 关键字会从左表 (Persons) 和右表 (Orders) 那里返回所有的行。如果 \"Persons\" 中的行在表 \"Orders\" 中没有匹配，或者如果 \"Orders\" 中的行在表 \"Persons\" 中没有匹配，这些行同样会列出。 INNER JOIN 关键字在表中存在至少一个匹配时返回行。如果 \"Persons\" 中的行在 \"Orders\" 中没有匹配，就不会列出这些行。 三大范式。 第一范式（无重复的列） 第二范式（属性完全依赖于主键） 定义：满足第一范式前提，当存在多个主键的时候，才会发生不符合第二范式的情况。比如有两个主键，不能存在这样的属性，它只依赖于其中一个主键，这就是不符合。通俗解释：任意一个字段都只依赖表中的同一个字段。 eg:比如不符合第二范式 学生证 名称 学生证号 学生证办理时间 借书证名称 借书证号 借书证办理时间 改成2张表如下 学生证表 学生证 学生证号 学生证办理时间 借书证表 借书证 借书证号 借书证办理时间 第三范式（属性不能传递依赖于主属性） 定义：满足第二范式前提，如果某一属性依赖于其他非主键属性，而其他非主键属性又依赖于主键，那么这个属性就是间接依赖于主键，这被称作传递依赖于主属性。 eg:爸爸资料表，不满足第三范式 爸爸 儿子 女儿 女儿的小熊 女儿的海绵宝宝 改成 爸爸信息表： 爸爸 儿子 女儿 女儿信息表 女儿 女儿的小熊 女儿的海绵宝宝 "},"zother4-EasyJob/Database/MySQL_PASS.html":{"url":"zother4-EasyJob/Database/MySQL_PASS.html","title":"My SQL PASS","keywords":"","body":"MySQL修改root密码 方法一：用set password命令 首先，登陆mysql mysql -u root -p 然后执行set password命令 set password for root@localhost = password('654321'); 上面例子，将root密码更改为654321 方法二：使用mysqladmin 格式为：mysqladmin -u用户名 -p旧密码 password 新密码 mysqladmin -uroot -p123456 password \"654321\" 上面例子，将root密码由123456更改为654321 方法三：更改mysql的user表 首先，登陆mysql mysql -uroot -p 然后操作mysql库的user表，进行update mysql> use mysql; mysql> update user set password=password('654321') where user='root' and host='localhost'; mysql> flush privileges; 方法四：忘记密码的情况下 首先停止mysql服务 1、service mysqld stop 以跳过授权的方式启动mysql 2、mysqld_safe --skip-grant-tables & 3、mysql -u root 操作mysql库的user表，进行update mysql> use mysql; mysql> update user set password=password('654321') where user='root' and host='localhost'; mysql> flush privileges; mysql> quit 重启mysql服务 重启mysql服务 service mysqld restart 开启远程mysql连接 步骤： 1、登入mysql 2、use mysql命令 3、GRANT ALL PRIVILEGES ON . TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION; 4、flush privileges 5、查看select host,user from user "},"zother4-EasyJob/Document/core_article.html":{"url":"zother4-EasyJob/Document/core_article.html","title":"Core Article","keywords":"","body":"核心知识点 Spring事务机制 Spring Bean生命周期 一致性Hash算法 大型网站架构分析 缓存、缓存算法简介 Redis学习日记 分布式Session的管理 "},"zother4-EasyJob/Framework/netty.html":{"url":"zother4-EasyJob/Framework/netty.html","title":"Netty","keywords":"","body":"Netty笔记整理 [x] Netty基础架构 [x] I/O模型 [x] 线程模型 [x] Reactor模型 [x] 心跳 [x] 整流 [x] 序列化 [x] 资源回收 [x] 粘连包解决方法 [x] 断线重连解决方法 [x] Future-Listener机制 [ ] 责任链机制 [x] ByteBuf And Reference-Count [x] Netty解包组包 [ ] 滑动窗口协议 Netty是一个高性能、异步事件驱动的NIO框架，它提供了对TCP、UDP和文件传输的支持，作为一个异步NIO框架，Netty的所有IO操作都是异步非阻塞的，通过Future-Listener机制，用户可以方便的主动获取或者通过通知机制获得IO操作结果。 作为当前最流行的NIO框架，Netty在互联网领域、大数据分布式计算领域、游戏行业、通信行业等获得了广泛的应用，一些业界著名的开源组件也基于Netty的NIO框架构建。 为什么选择Netty Netty是业界最流行的NIO框架之一，它的健壮性、功能、性能、可定制性和可扩展性在同类框架中都是首屈一指的，它已经得到成百上千的商用项目验证，例如Hadoop的RPC框架avro使用Netty作为底层通信框架；很多其他业界主流的RPC框架，也使用Netty来构建高性能的异步通信能力。 通过对Netty的分析，我们将它的优点总结如下: API使用简单，开发门槛低； 功能强大，预置了多种编解码功能，支持多种主流协议； 定制能力强，可以通过ChannelHandler对通信框架进行灵活地扩展； 性能高，通过与其他业界主流的NIO框架对比，Netty的综合性能最优； 成熟、稳定，Netty修复了已经发现的所有JDK NIO BUG，业务开发人员不需要再为NIO的BUG而烦恼； 社区活跃，版本迭代周期短，发现的BUG可以被及时修复，同时，更多的新功能会加入； 经历了大规模的商业应用考验，质量得到验证。在互联网、大数据、网络游戏、企业应用、电信软件等众多行业得到成功商用，证明了它已经完全能够满足不同行业的商业应用了。 Netty架构分析 Netty 采用了比较典型的三层网络架构进行设计，逻辑架构图如下所示： 第一层，Reactor 通信调度层，它由一系列辅助类完成，包括 Reactor 线程 NioEventLoop 以及其父类、NioSocketChannel/NioServerSocketChannel 以及其父 类、ByteBuffer 以及由其衍生出来的各种 Buffer、Unsafe以及其衍生出的各种内部类等。该层的主要职责就是监听网络的读写和连接操作，负责将网络层的数据 读取到内存缓冲区中，然后触发各种网络事件，例如连接创建、连接激活、读事 件、写事件等等，将这些事件触发到 PipeLine 中，由 PipeLine 充当的职责链来 进行后续的处理。 第二层,职责链 PipeLine，它负责事件在职责链中的有序传播，同时负责动态的 编排职责链，职责链可以选择监听和处理自己关心的事件，它可以拦截处理和向 后/向前传播事件，不同的应用的 Handler 节点的功能也不同，通常情况下，往往 会开发编解码 Hanlder 用于消息的编解码，它可以将外部的协议消息转换成内部 的 POJO 对象，这样上层业务侧只需要关心处理业务逻辑即可，不需要感知底层 的协议差异和线程模型差异，实现了架构层面的分层隔离。 第三层，业务逻辑处理层。可以分为两类: 纯粹的业务逻辑 处理，例如订单处理。 应用层协议管理，例如HTTP协议、FTP协议等。 接下来，我从影响通信性能的三个方面（I/O模型、线程调度模型、序列化方式）来谈谈Netty的架构。 I/O模型 传统同步阻塞I/O模式如下图所示: 它的弊端有很多： 性能问题：一连接一线程模型导致服务端的并发接入数和系统吞吐量受到极大限制； 可靠性问题：由于I/O操作采用同步阻塞模式，当网络拥塞或者通信对端处理缓慢会导致I/O线程被挂住，阻塞时间无法预测； 可维护性问题：I/O线程数无法有效控制、资源无法有效共享（多线程并发问题），系统可维护性差； 几种I/O模型的功能和特性对比: Netty的I/O模型基于非阻塞I/O实现，底层依赖的是JDK NIO框架的Selector。 Selector提供选择已经就绪的任务的能力。简单来讲，Selector会不断地轮询注册在其上的Channel，如果某个Channel上面有新的TCP连接接入、读和写事件，这个Channel就处于就绪状态，会被Selector轮询出来，然后通过SelectionKey可以获取就绪Channel的集合，进行后续的I/O操作。 一个多路复用器Selector可以同时轮询多个Channel，由于JDK1.5_update10版本（+）使用了epoll()代替传统的select实现，所以它并没有最大连接句柄1024/2048的限制。这也就意味着只需要一个线程负责Selector的轮询，就可以接入成千上万的客户端，这确实是个非常巨大的技术进步。 使用非阻塞I/O模型之后，Netty解决了传统同步阻塞I/O带来的性能、吞吐量和可靠性问题。 线程调度模型 常用的Reactor线程模型有三种，分别如下： Reactor单线程模型：Reactor单线程模型，指的是所有的I/O操作都在同一个NIO线程上面完成。对于一些小容量应用场景，可以使用单线程模型。 Reactor多线程模型：Rector多线程模型与单线程模型最大的区别就是有一组NIO线程处理I/O操作。主要用于高并发、大业务量场景。 主从Reactor多线程模型：主从Reactor线程模型的特点是服务端用于接收客户端连接的不再是个1个单独的NIO线程，而是一个独立的NIO线程池。利用主从NIO线程模型，可以解决1个服务端监听线程无法有效处理所有客户端连接的性能不足问题。 事实上，Netty的线程模型并非固定不变，通过在启动辅助类中创建不同的EventLoopGroup实例并通过适当的参数配置，就可以支持上述三种Reactor线程模型. 在大多数场景下，并行多线程处理可以提升系统的并发性能。但是，如果对于共享资源的并发访问处理不当，会带来严重的锁竞争，这最终会导致性能的下降。为了尽可能的避免锁竞争带来的性能损耗，可以通过串行化设计，即消息的处理尽可能在同一个线程内完成，期间不进行线程切换，这样就避免了多线程竞争和同步锁。 为了尽可能提升性能，Netty采用了串行无锁化设计，在I/O线程内部进行串行操作，避免多线程竞争导致的性能下降。表面上看，串行化设计似乎CPU利用率不高，并发程度不够。但是，通过调整NIO线程池的线程参数，可以同时启动多个串行化的线程并行运行，这种局部无锁化的串行线程设计相比一个队列-多个工作线程模型性能更优。 Reactor模型 Java NIO非堵塞技术实际是采取反应器模式，或者说是观察者(observer)模式为我们监察I/O端口，如果有内容进来，会自动通知我们，这样，我们就不必开启多个线程死等，从外界看，实现了流畅的I/O读写，不堵塞了。 同步和异步区别：有无通知（是否轮询） 堵塞和非堵塞区别：操作结果是否等待（是否马上有返回值），只是设计方式的不同 NIO 有一个主要的类Selector，这个类似一个观察者，只要我们把需要探知的socketchannel告诉Selector，我们接着做别的事情，当有事件发生时，他会通知我们，传回一组SelectionKey，我们读取这些Key，就会获得我们刚刚注册过的socketchannel，然后，我们从这个Channel中读取数据，接着我们可以处理这些数据。 反应器模式与观察者模式在某些方面极为相似：当一个主体发生改变时，所有依属体都得到通知。不过，观察者模式与单个事件源关联，而反应器模式则与多个事件源关联 。 一般模型 我们想象以下情形：长途客车在路途上，有人上车有人下车，但是乘客总是希望能够在客车上得到休息。 传统的做法是：每隔一段时间（或每一个站），司机或售票员对每一个乘客询问是否下车。 反应器模式做法是：汽车是乘客访问的主体（Reactor），乘客上车后，到售票员（acceptor）处登记，之后乘客便可以休息睡觉去了，当到达乘客所要到达的目的地后，售票员将其唤醒即可。 EventLoopGroup 对应于Reactor模式中的定时器的角色，不断地检索时候有事件可用(I/O线程-BOSS)，然后交给分离者将事件分发给对应的事件绑定的handler(WORK线程)。 ServerBootstrap b = new ServerBootstrap(); b.group(boss,//负责I/O操作 work//负责事件分发 ); 经验分享: 在客户端编程中经常容易出现在EVENTLOOP上做定时任务的，如果定时任务耗时很长或者存在阻塞，那么可能会将I/O操作挂起(因为要等到定时任务做完才能做别的操作)。解决方法:用独立的EventLoopGroup 序列化方式 影响序列化性能的关键因素总结如下： 序列化后的码流大小（网络带宽占用）； 序列化&反序列化的性能（CPU资源占用）; 并发调用的性能表现：稳定性、线性增长、偶现的时延毛刺等; 对Java序列化和二进制编码分别进行性能测试，编码100万次，测试结果表明：Java序列化的性能只有二进制编码的6.17%左右。 Netty默认提供了对Google Protobuf的支持，通过扩展Netty的编解码接口，用户可以实现其它的高性能序列化框架，例如Thrift的压缩二进制编解码框架。 不同的应用场景对序列化框架的需求也不同，对于高性能应用场景Netty默认提供了Google的Protobuf二进制序列化框架，如果用户对其它二进制序列化框架有需求，也可以基于Netty提供的编解码框架扩展实现。 推荐阅读项目源码: 二进制协议：SMPP，该项目可以学习动态二进制大小协议的解码、编码、心跳以及整流&数据完整性协议(滑动窗口协议)等； 文本协议:Motan,轻量级RPC，Future-Linstener模型，序列化框架Hessian2、fastjson； Netty架构剖析之可靠性 Netty面临的可靠性挑战： 作为RPC框架的基础网络通信框架，一旦故障将导致无法进行远程服务（接口）调用。 作为应用层协议的基础通信框架，一旦故障将导致应用协议栈无法正常工作。 网络环境复杂（例如手游或者推送服务的GSM/3G/WIFI网络），故障不可避免，业务却不能中断。 从应用场景看，Netty是基础的通信框架，一旦出现Bug，轻则需要重启应用，重则可能导致整个业务中断。它的可靠性会影响整个业务集群的数据通信和交换，在当今以分布式为主的软件架构体系中，通信中断就意味着整个业务中断，分布式架构下对通信的可靠性要求非常高。 从运行环境看，Netty会面临恶劣的网络环境，这就要求它自身的可靠性要足够好，平台能够解决的可靠性问题需要由Netty自身来解决，否则会导致上层用户关注过多的底层故障，这将降低Netty的易用性，同时增加用户的开发和运维成本。 Netty的可靠性是如此重要，它的任何故障都可能会导致业务中断，蒙受巨大的经济损失。因此，Netty在版本的迭代中不断加入新的可靠性特性来满足用户日益增长的高可靠和健壮性需求。 链路有效性检测 Netty提供的心跳检测机制分为三种： 读空闲，链路持续时间t没有读取到任何消息； 写空闲，链路持续时间t没有发送任何消息； 读写空闲，链路持续时间t没有接收或者发送任何消息。 当网络发生单通、连接被防火墙Hang住、长时间GC或者通信线程发生非预期异常时，会导致链路不可用且不易被及时发现。特别是异常发生在凌晨业务低谷期间，当早晨业务高峰期到来时，由于链路不可用会导致瞬间的大批量业务失败或者超时，这将对系统的可靠性产生重大的威胁。 从技术层面看，要解决链路的可靠性问题，必须周期性的对链路进行有效性检测。目前最流行和通用的做法就是心跳检测。 心跳检测机制分为三个层面： TCP层面的心跳检测，即TCP的Keep-Alive机制，它的作用域是整个TCP协议栈； 协议层的心跳检测，主要存在于长连接协议中。例如SMPP协议； 应用层的心跳检测，它主要由各业务产品通过约定方式定时给对方发送心跳消息实现。 Keep-Alive仅仅是TCP协议层会发送连通性检测包，但并不代表设置了Keep-Alive就是长连接了。 心跳检测的目的就是确认当前链路可用，对方活着并且能够正常接收和发送消息。做为高可靠的NIO框架，Netty也提供了基于链路空闲的心跳检测机制： 读空闲，链路持续时间t没有读取到任何消息； 写空闲，链路持续时间t没有发送任何消息； 读写空闲，链路持续时间t没有接收或者发送任何消息。 netty自带心跳处理Handler IdleStateHandler 代码片段: //心跳 ch.pipeline().addLast(\"idle\",new IdleStateHandler(0, 0, 55)); ch.pipeline().addLast(\"heartBeat\",new HeartBeatHandler()); public class HeartBeatHandler extends ChannelDuplexHandler { @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception { if (evt instanceof IdleStateEvent) { IdleStateEvent e = (IdleStateEvent) evt; if (e.state() == IdleState.ALL_IDLE) { CMPPActive active = new CMPPActive(); ctx.writeAndFlush(active); } } } } 补充： 客户端和服务端之间连接断开机制 TCP连接的建立需要三个分节(三次握手)，终止则需要四个分节。 某个应用进程首先调用close，称该端执行主动关闭(active close)。该端的TCP于是发送一个FIN分节，表示数据发送完毕。 接收到这个FIN分节的对端执行被动关闭(passive close) 。这个FIN由TCP确认。他的接收也作为一个文件结束符传递给接收端应用程序，因为FIN的接收意味着接收端应用程序在相应连接上再无额外数据可以收取; 一段时间后，接收到这个文件结束符的应用程序调用close管理他的套接字。这导致他的TCP也发送一个FIN。 接收这个最终FIN的原发送端TCP(执行主动关闭的那一端)确认这个FIN。 既然每个方向都需要一个FIN和一个ACK，因此通常需要4个分节。但是某些情况下步骤1的FIN随数据一起发送；另外，步骤2和步骤3发送的分节都处在执行被动关闭的那一端，有可能合并成一个分节发送。 TCP关闭连接时对应的状态图如下: 对于大量短连接的情况下，经常出现卡在FIN_WAIT2和TIMEWAIT状态的连接，等待系统回收，但是操作系统底层回收的时间频率很长，导致SOCKET被耗尽。解决方案如下: Linux平台: #!/bin/sh echo \"Now,config system parameters...\" echo \"#config for MWGATE\" >> /etc/sysctl.conf #当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击 echo 'net.ipv4.tcp_syncookies = 1' >> /etc/sysctl.conf #允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭； echo 'net.ipv4.tcp_tw_reuse = 1' >> /etc/sysctl.conf #TIM_WAIT_2也就是FIN_SYN_2的等待时间 echo 'net.ipv4.tcp_fin_timeout = 30' >> /etc/sysctl.conf #表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。 echo 'net.ipv4.tcp_tw_recycle = 1' >>/etc/sysctl.conf FILEMAX=$(cat /proc/sys/fs/file-max) PORTRANGE=\"net.ipv4.ip_local_port_range = 1024 $FILEMAX\" echo $PORTRANGE >> /etc/sysctl.conf echo \"#end config for MWGATE\" >> /etc/sysctl.conf echo '#config for MWGATE' >> /etc/security/limits.conf SOFTLIMIT=\"* soft nofile $FILEMAX\" HARDLIMIT=\"* hard nofile $FILEMAX\" echo \"$SOFTLIMIT\" >> /etc/security/limits.conf echo \"$HARDLIMIT\" >> /etc/security/limits.conf echo '#end config for MWGATE' >> /etc/security/limits.conf echo \"#config for MWGATE\" >> /etc/pam.d/login echo 'session required /lib/security/pam_limits.so' >> /etc/pam.d/login echo \"#end config for MWGATE\" >> /etc/pam.d/login sysctl -p echo \"#config for MWGATE\" >> /etc/profile echo 'ulimit -S -c unlimited > /dev/null 2>&1' >> /etc/profile echo \"#end config for MWGATE\" >> /etc/profile echo 'core-%p-%t' >> /proc/sys/kernel/core_pattern source /etc/profile echo \"Config is OK...\" Windows平台: Windows Registry Editor Version 5.00 [HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\Tcpip\\Parameters] \"MaxUserPort\"=dword:0000fffe \"TCPTimedWaitDelay\"=dword:00000005 TCP状态图 TCP/IP半关闭 从上述讲的TCP关闭的四个分节可以看出，被动关闭执行方，发送FIN分节的前提是TCP套接字对应应用程序调用close产生的。如果服务端有数据发送给客户端那么可能存在服务端在接受到FIN之后，需要将数据发送到客户端才能发送FIN字节。这种处于业务考虑的情形通常称为半关闭。 半关闭可能导致大量socket处于CLOSE_WAIT状态 谁负责关闭连接合理 连接关闭触发的条件通常分为如下几种: 数据发送完成(发送到对端并且收到响应)，关闭连接； 通信过程中产生异常； 特殊指令强制要求关闭连接； 对于第一种，通常关闭时机是，数据发送完成方发起(客户端触发居多)； 对于第二种，异常产生方触发(例如残包、错误数据等)发起。但是此种情况可能也导致压根无法发送FIN。 对于第三种，通常是用于运维等。由命令发起方产生。 流量整形 流量整形（Traffic Shaping）是一种主动调整流量输出速率的措施。Netty的流量整形有两个作用： 防止由于上下游网元性能不均衡导致下游网元被压垮，业务流程中断； 防止由于通信模块接收消息过快，后端业务线程处理不及时导致的“撑死”问题。 流量整形的原理示意图如下： 流量整形（Traffic Shaping）是一种主动调整流量输出速率的措施。一个典型应用是基于下游网络结点的TP指标来控制本地流量的输出。流量整形与流量监管的主要区别在于，流量整形对流量监管中需要丢弃的报文进行缓存——通常是将它们放入缓冲区或队列内，也称流量整形（Traffic Shaping，简称TS）。当令牌桶有足够的令牌时，再均匀的向外发送这些被缓存的报文。流量整形与流量监管的另一区别是，整形可能会增加延迟，而监管几乎不引入额外的延迟。 Netty支持两种流量整形模式： 全局流量整形：全局流量整形的作用范围是进程级的，无论你创建了多少个Channel，它的作用域针对所有的Channel。用户可以通过参数设置：报文的接收速率、报文的发送速率、整形周期。[GlobalChannelTrafficShapingHandler] 链路级流量整形：单链路流量整形与全局流量整形的最大区别就是它以单个链路为作用域，可以对不同的链路设置不同的整形策略。[ChannelTrafficShapingHandler针对于每个channel] 优雅停机 Netty的优雅停机三部曲： 不再接收新消息 退出前的预处理操作 资源的释放操作 Java的优雅停机通常通过注册JDK的ShutdownHook来实现，当系统接收到退出指令后，首先标记系统处于退出状态，不再接收新的消息，然后将积压的消息处理完，最后调用资源回收接口将资源销毁，最后各线程退出执行。 通常优雅退出需要有超时控制机制，例如30S，如果到达超时时间仍然没有完成退出前的资源回收等操作，则由停机脚本直接调用kill -9 pid，强制退出。 在实际项目中，Netty作为高性能的异步NIO通信框架，往往用作基础通信框架负责各种协议的接入、解析和调度等，例如在RPC和分布式服务框架中，往往会使用Netty作为内部私有协议的基础通信框架。 当应用进程优雅退出时，作为通信框架的Netty也需要优雅退出，主要原因如下： 尽快的释放NIO线程、句柄等资源； 如果使用flush做批量消息发送，需要将积攒在发送队列中的待发送消息发送完成； 正在write或者read的消息，需要继续处理； 设置在NioEventLoop线程调度器中的定时任务，需要执行或者清理 Netty架构剖析之安全性 Netty面临的安全挑战： 对第三方开放 作为应用层协议的基础通信框架 安全威胁场景分析： 对第三方开放的通信框架：如果使用Netty做RPC框架或者私有协议栈，RPC框架面向非授信的第三方开放，例如将内部的一些能力通过服务对外开放出去，此时就需要进行安全认证，如果开放的是公网IP，对于安全性要求非常高的一些服务，例如在线支付、订购等，需要通过SSL/TLS进行通信。 应用层协议的安全性：作为高性能、异步事件驱动的NIO框架，Netty非常适合构建上层的应用层协议。由于绝大多数应用层协议都是公有的，这意味着底层的Netty需要向上层提供通信层的安全传输功能。 SSL/TLS Netty安全传输特性： 支持SSL V2和V3 支持TLS 支持SSL单向认证、双向认证和第三方CA认证。 SSL单向认证流程图如下： Netty通过SslHandler提供了对SSL的支持，它支持的SSL协议类型包括：SSL V2、SSL V3和TLS。 单向认证：单向认证，即客户端只验证服务端的合法性，服务端不验证客户端。 双向认证：与单向认证不同的是服务端也需要对客户端进行安全认证。这就意味着客户端的自签名证书也需要导入到服务端的数字证书仓库中。 CA认证：基于自签名的SSL双向认证，只要客户端或者服务端修改了密钥和证书，就需要重新进行签名和证书交换，这种调试和维护工作量是非常大的。因此，在实际的商用系统中往往会使用第三方CA证书颁发机构进行签名和验证。我们的浏览器就保存了几个常用的CA_ROOT。每次连接到网站时只要这个网站的证书是经过这些CA_ROOT签名过的。就可以通过验证了。 可扩展的安全特性 通过Netty的扩展特性，可以自定义安全策略： IP地址黑名单机制 接入认证 敏感信息加密或者过滤机制 IP地址黑名单是比较常用的弱安全保护策略，它的特点就是服务端在与客户端通信的过程中，对客户端的IP地址进行校验，如果发现对方IP在黑名单列表中，则拒绝与其通信，关闭链路。 接入认证策略非常多，通常是较强的安全认证策略，例如基于用户名+密码的认证，认证内容往往采用加密的方式，例如Base64+AES等。 Netty架构剖析之扩展性 通过Netty的扩展特性，可以自定义安全策略： 线程模型可扩展 序列化方式可扩展 上层协议栈可扩展 提供大量的网络事件切面，方便用户功能扩展 Netty的架构可扩展性设计理念如下： 判断扩展点，事先预留相关扩展接口，给用户二次定制和扩展使用； 主要功能点都基于接口编程，方便用户定制和扩展。 粘连包解决方案 TCP粘包是指发送方发送的若干包数据到接收方接收时粘成一包，从接收缓冲区看，后一包数据的头紧接着前一包数据的尾。 出现粘包现象的原因是多方面的，它既可能由发送方造成，也可能由接收方造成。发送方引起的粘包是由TCP协议本身造成的，TCP为提高传输效率，发送方往往要收集到足够多的数据后才发送一包数据。若连续几次发送的数据都很少，通常TCP会根据优化算法把这些数据合成一包后一次发送出去，这样接收方就收到了粘包数据。接收方引起的粘包是由于接收方用户进程不及时接收数据，从而导致粘包现象。这是因为接收方先把收到的数据放在系统接收缓冲区，用户进程从该缓冲区取数据，若下一包数据到达时前一包数据尚未被用户进程取走，则下一包数据放到系统接收缓冲区时就接到前一包数据之后，而用户进程根据预先设定的缓冲区大小从系统接收缓冲区取数据，这样就一次取到了多包数据。 粘包情况有两种: 粘在一起的包都是完整的数据包; 粘在一起的包有不完整的包。 解决粘连包的方法大致分为如下三种: 发送方开启TCP_NODELAY； 接收方简化或者优化流程尽可能快的接收数据； 认为强制分包每次只读一个完整的包。 对于以上三种方式，第一种会加重网络负担，第二种治标不治本，第三种算比较合理的。 第三种又可以分两种方式: 每次都只读取一个完整的包，不如不足一个完整的包，就等下次再接收，如果缓冲区有N个包要接受，那么需要分N次才能接收完成； 有多少接收多少，将接収的数据缓存在一个临时的缓存中，交由后续的专门解码的线程/进程处理。 以上两种分包方式，如果强制关闭程序，数据会存在丢失，第一种数据丢失在接收缓冲区；第二种丢失在程序自身缓存。 Netty自带的几种粘连包解决方案: 1. DelimiterBasedFrameDecoder 2. FixedLengthFrameDecoder 3. LengthFieldBasedFrameDecoder Netty Client断网重连机制 对于长连接的程序断网重连几乎是程序的标配。断网重连具体可以分为两类: CONNECT失败，需要重连； 程序运行过程中断网、远程强制关闭连接、收到错误包必须重连； 对于第一种解决方案是: 实现ChannelFutureListener 用来启动时监测是否连接成功，不成功的话重试 private void doConnect() { Bootstrap b = ...; b.connect().addListener((ChannelFuture f) -> { if (!f.isSuccess()) { long nextRetryDelay = nextRetryDelay(...); f.channel().eventLoop().schedule(nextRetryDelay, ..., () -> { doConnect(); }); // or you can give up at some point by just doing nothing. } }); } 或者 public class ConnectionListener implements ChannelFutureListener { private Client client; public ConnectionListener(Client client) { this.client = client; } @Override public void operationComplete(ChannelFuture channelFuture) throws Exception { if (!channelFuture.isSuccess()) { System.out.println(\"Reconnect\"); //因为是建立网络连接所以可以共用EventLoop final EventLoop loop = channelFuture.channel().eventLoop(); loop.schedule(new Runnable() { @Override public void run() { client.createBootstrap(new Bootstrap(), loop); } }, 1L, TimeUnit.SECONDS); } } } 对于第二种: public Bootstrap createBootstrap(Bootstrap bootstrap, EventLoopGroup eventLoop) { if (bootstrap != null) { final MyInboundHandler handler = new MyInboundHandler(this); bootstrap.group(eventLoop); bootstrap.channel(NioSocketChannel.class); bootstrap.option(ChannelOption.SO_KEEPALIVE, true); bootstrap.handler(new ChannelInitializer() { @Override protected void initChannel(SocketChannel socketChannel) throws Exception { socketChannel.pipeline().addLast(handler); } }); bootstrap.remoteAddress(\"localhost\", 8888); bootstrap.connect().addListener(new ConnectionListener(this)); } return bootstrap; } public class MyInboundHandler extends SimpleChannelInboundHandler { private Client client; public MyInboundHandler(Client client) { this.client = client; } @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception { final EventLoop eventLoop = ctx.channel().eventLoop(); eventLoop.schedule(new Runnable() { @Override public void run() { client.createBootstrap(new Bootstrap(), eventLoop); } }, 1L, TimeUnit.SECONDS); super.channelInactive(ctx); } } Future-Listener机制 Future在netty中位于io.netty.concurrent包中，其依赖关系如下: 在并发编程中，我们通常会用到一组非阻塞的模型:Promise,Future,Callback。其中的Future表示一个可能还没有实际完成的异步任务的结果,针对这个结果添加Callback以便在执行任务成功或者失败后做出响应的操作。 而经由Promise交给执行者，任务执行者通过Promise可以标记任务完成或者失败。以上这套模型是很多异步非阻塞框架的基础。 具体的理解可参见JDK的FutureTask和Callable。JDK的实现版本，在获取最终结果的时候，不得不做一些阻塞的方法等待最终结果的到来。 Netty的Future机制是JDK机制的一个子版本，它支持给Future添加Listener，以方便EventLoop在任务调度完成之后调用。 这里借用Scala中Future机制来讲解一下Future、Callback、Promise具体是什么. 简介 Future提供了一套高效便捷的非阻塞并行操作管理方案。其基本思想很简单，所谓Future，指的是一类占位符对象，用于指代某些尚未完成的计算的结果。一般来说，由Future指代的计算都是并行执行的，计算完毕后可另行获取相关计算结果。以这种方式组织并行任务，便可以写出高效、异步、非阻塞的并行代码。 默认情况下，future和promise并不采用一般的阻塞操作，而是依赖回调进行非阻塞操作。为了在语法和概念层面更加简明扼要地使用这些回调。当然，future仍然支持阻塞操作——必要时，可以阻塞(sync、await、awaitUninterruptibly)等待future（不过并不鼓励这样做）。 Future 所谓Future，是一种用于指代某个尚未就绪的值的对象。而这个值，往往是某个计算过程的结果： 若该计算过程尚未完成，我们就说该Future未就位； 若该计算过程正常结束，或中途抛出异常，我们就说该Future已就位。 Future的就位分为两种情况： 当Future带着某个值就位时，我们就说该Future携带计算结果成功就位。 当Future因对应计算过程抛出异常而就绪，我们就说这个Future因该异常而失败。 Future的一个重要属性在于它只能被赋值一次。一旦给定了某个值或某个异常，future对象就变成了不可变对象——无法再被改写。 Callbacks(回调函数) Callback用于对计算的最终结果Future做一些后续的处理，以便我们能够用它来做一些有用的事。我们经常对计算结果感兴趣而不仅仅是它的副作用。 Promises 如果说futures是为了一个还没有存在的结果，而当成一种只读占位符的对象类型去创建，那么Promise就被认为是一个可写的，可以实现一个Future的单一赋值容器。这就是说，promise通过这种success方法可以成功去实现一个带有值的future。相反的，因为一个失败的promise通过failure方法就会实现一个带有异常的future。 Future和Promise的区别 Promise与Future的区别在于，Future是Promise的一个只读的视图，也就是说Future没有设置任务结果的方法，只能获取任务执行结果或者为Future添加回调函数。 参考文档: WiKi Scala 责任链机制 TODO ByteBuf ByteBuf类似与JDK的ByteBuffer，但是在ByteBuffer的基础上做了优化，例如去除了讨厌的flip()； ByteBuf对应的继承关系如下: ReferenceCount ByteBuf可以使用池化的ByteBuf(PooledHeapByteBuf)。详情参见官方文档Reference counted objects。 Basics of reference counting 初始状态下ByteBuf的reference count值是1； ByteBuf buf = ctx().alloc().buf(); assert buf.refCnt() == 1 当Realease ByteBuf的时候他的reference coutn值会减一。 //调用release()方法返回为True的唯一条件是引用计数变为0 boolean released = buf.release(); assert released; assert buf.refCnt() == 0 Dangling reference(指针悬挂) 对一个应用技术的对象进行访问会抛出IllegalReferenceCountException异常。 assert buf.refCnt() == 0; try { buf.writeLong(0xdeadbeef); throw new Error(\"should not reach here\"); } catch (IllegalReferenceCountExeception e) { // Expected } 增加应用计数 对于一个还未被销毁的对象可以使用retain()增加起引用计数值。 ByteBuf buf = ctx.alloc().directBuffer(); assert buf.refCnt() == 1; buf.retain(); assert buf.refCnt() == 2; boolean destroyed = buf.release(); assert !destroyed; assert buf.refCnt() == 1; 谁来销毁它 通常来说最后一个调用这个对象的人负责销毁这个对象。但也有很多特殊情况: 如果一个发送模块将数据传递给一个接收模块，那么发送模块不需要回收，而是将回收权利交给接收模块。 如果一个模块消费了引用计数对象，并且明确知道该对象不会被传递到别的模块，那么该模块必须销毁此引用计数对象。 下边是一个简单的例子: /**传递input到别的地方*/ public ByteBuf a(ByteBuf input) { input.writeByte(42); return input; } /**消费了Input并做了新的封装*/ public ByteBuf b(ByteBuf input) { try { output = input.alloc().directBuffer(input.readableBytes() + 1); output.writeBytes(input); output.writeByte(42); return output; } finally { input.release(); } } /**单纯消费并且无需传递*/ public void c(ByteBuf input) { System.out.println(input); input.release(); } public void main() { ... ByteBuf buf = ...; // This will print buf to System.out and destroy it. c(b(a(buf))); assert buf.refCnt() == 0; } Action Who should release? Who released? 1. main() creates buf buf→main() x 2. main() calls a() with buf buf→a() x 3. a() returns buf merely. buf→main() x 4. main() calls b() with buf buf→b() x 5. b() returns the copy of buf buf→b(), copy→main() b() releases buf 6. main() calls c() with copy copy→c() x 7. c() swallows copy copy→c() c() releases copy Reference-counting in ChannelHandler Inbound messages 当EventLoop读取到ByteBuf并且触发channelRead()事件，此时应该是跟pipeline相关的ChannelHandler销毁ByteBuf，因为，Handler消费了ByteBuf所以应该channelReadl()方法调用release()。 public void channelRead(ChannelHandlerContext ctx, Object msg) { ByteBuf buf = (ByteBuf) msg; try { ... } finally { buf.release(); } } 但是如果你将这个ByteBuf或者其他的任何引用计数的对象传递给下一个Handler，那么该handler不用销毁。 public void channelRead(ChannelHandlerContext ctx, Object msg) { ByteBuf buf = (ByteBuf) msg; ... ctx.fireChannelRead(buf); } 需要主意的是:在Netty中ByteBuf不是唯一一个引用计数类型。如果你在使用由Decoders生成的消息的时候，那么极有可能该对象也是引用计数的。 // Assuming your handler is placed next to `HttpRequestDecoder` public void channelRead(ChannelHandlerContext ctx, Object msg) { if (msg instanceof HttpRequest) { HttpRequest req = (HttpRequest) msg; ... } if (msg instanceof HttpContent) { HttpContent content = (HttpContent) msg; try { ... } finally { content.release(); } } } 如果你对到底该不该release()感到疑惑那么可以使用ReferenceCountUtil.release(): public void channelRead(ChannelHandlerContext ctx, Object msg) { try { ... } finally { ReferenceCountUtil.release(msg); } } 另外，可以考虑继承SimpleChannelHanlder，它对你收到的所有消息调用ReferenceCountUtil.release() Outbound messages 不同于Inbound Messages，Outbound Message是由引用程序创建的。理论上应该由netty在他们将数据写入到链路层。当时的调用write()方法的Handler也应该release()任何可能的引用计数对象。 // Simple-pass through public void write(ChannelHandlerContext ctx, Object message, ChannelPromise promise) { System.err.println(\"Writing: \" + message); ctx.write(message, promise); } // Transformation public void write(ChannelHandlerContext ctx, Object message, ChannelPromise promise) { if (message instanceof HttpContent) { // Transform HttpContent to ByteBuf. HttpContent content = (HttpContent) message; try { ByteBuf transformed = ctx.alloc().buffer(); .... ctx.write(transformed, promise); } finally { content.release(); } } else { // Pass non-HttpContent through. ctx.write(message, promise); } } Netty解包组包 对于TCP编程最常遇到的就是根据具体的协议进行组包或者解包。根据协议的不同大致可以分为如下几种类型: JAVA平台之间通过JAVA序列化进行解包组包(object->byte->object)； 固定长度的包结构(定长每个包都是M个字节的长度)； 带有明确分隔符协议的解包组包(例如HTTP协议\\r\\n\\r\\n)； 可动态扩展的协议，此种协议通常遵循消息头+消息体的机制，其中消息头的长度是固定的，消息体的长度根据具体业务的不同长度可能不同。例如(SMPP协议、CMPP协议); 序列化协议组包解包 可以使用的有: MessagePack Google Protobuf 跨语言 JBOSS Marshalling Kryo 短小精悍 Hessian2 跨语言 固定长度解包组包 参见Netty ： FixedLengthFrameDecoder 解包 MessageToByteEncoder 组包 带有分隔符协议的解包组包 参见Netty: DelimiterBasedFrameDecoder 解包 MessageToByteEncoder 组包 HTTP 参见Netty: io.netty.codec.http 消息头固定长度，消息体不固定长度协议解包组包 参见Netty: LengthFieldBasedFrameDecoder 需要注意的是： 对于解码的Handler必须做到在将ByteBuf解析成Object之后，需要将ByteBuf release()。 数据安全性之滑动窗口协议 我们假设一个场景，客户端每次请求服务端必须得到服务端的一个响应，由于TCP的数据发送和数据接收是异步的，就存在必须存在一个等待响应的过程。该过程根据实现方式不同可以分为一下几类(部分是错误案例): 每次发送一个数据包，然后进入休眠(sleep)或者阻塞(await)状态，直到响应回来或者超时，整个调用链结束。此场景是典型的一问一答的场景，效率极其低下； 读写分离，写模块只负责写，读模块则负责接收响应，然后做后续的处理。此种场景能尽可能的利用带宽进行读写。但是此场景不坐控速操作可能导致大量报文丢失或者重复发送。 实现类似于Windowed Protocol。此窗口是以上两种方案的折中版，即允许一定数量的批量发送，又能保证数据的完整性。 以下引用一段对滑动窗口协议作解释(PS.英文版，不想翻译了): 参见Twitter的com.cloudhopper.commons.util.window包。 A utility class to support \"windowed\" protocols that permit requests to be sent asynchronously and the responses to be processed at a later time. Responses may be returned in a different order than requests were sent. Windowed protocols generally provide high throughput over high latency links such as TCP/IP connections since they allow requests one after the other without waiting for a response before sending the next request. This allows the underlying TCP/IP socket to potentially buffer multiple requests in one packet. The \"window\" is the amount of unacknowledged requests that are permitted to be outstanding/unacknowledged at any given time. This implementation allows a max window size to be defined during construction. This represents the number of open \"slots\". When a response is received, it's up to the user of this class to make sure that response is added so that any threads waiting for a response are properly signaled. The life cycle of a request in a Window has 3 steps: Request offered If free slot exists then goto 2 If no free slot exists, offer now \"pending\" and block for specified time. May either timeout or if free slot opens, then goto 2 Request accepted (caller may optionally await() on returned future till completion) Request completed/done (either success, failure, or cancelled) 精彩问答(摘抄自 一篇文章，读懂Netty的高性能架构之道 ) 问：据我之前了解到，Java的NIO selector底层在Windows下的实现是起两个随机端口互联来监测连接或读写事件，在Linux上是利用管道实现的；我有遇到过这样的需求，需要占用很多个固定端口做服务端，如果在Windows下，利用NIO框架（Mina或Netty）就有可能会造成端口冲突，这种情况有什么好的解决方案吗？ Linux使用Pipe实现网络监听，Windows要启动端口。目前没有更好的办法，建议的方式是作为服务端的端口可以规划一个范围，然后根据节点和进程信息动态生成，如果发现端口冲突，可以在规划范围内基于算法重新生成一个新的端口。 问：能不能讲解一下Netty的串行无锁化设计，如何在串行和并行中达到最优？ 为了尽可能提升性能，Netty采用了串行无锁化设计，在IO线程内部进行串行操作，避免多线程竞争导致的性能下降。表面上看，串行化设计似乎CPU利用率不高，并发程度不够。但是，通过调整NIO线程池的线程参数，可以同时启动多个串行化的线程并行运行，这种局部无锁化的串行线程设计相比一个队列-多个工作线程模型性能更优。Netty的NioEventLoop读取到消息之后，直接调用ChannelPipeline的fireChannelRead(Object msg)，只要用户不主动切换线程，一直会由NioEventLoop调用到用户的Handler，期间不进行线程切换，这种串行化处理方式避免了多线程操作导致的锁的竞争，从性能角度看是最优的。 本文引自 https://github.com/jackoll8868/net-learn/edit/master/netty.MD "},"zother4-EasyJob/Framework/nginx.html":{"url":"zother4-EasyJob/Framework/nginx.html","title":"Nginx","keywords":"","body":"nginx 1. 反向代理和正向代理 正向代理 也就是传说中的代理,他的工作原理就像一个跳板，简单的说，我是一个用户，我访问不了某网站，但是我能访问一个代理服务器，这个代理服务器呢，他能访问那个我不能访问的网站，于是我先连上代理服务器，告诉他我需要那个无法访问网站的内容，代理服务器去取回来，然后返回给我。 正向代理 是一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端必须要进行一些特别的设置才能使用正向代理。 反向代理 例用户访问 http://www.test.com/readme，但www.test.com上并不存在readme页面，他是偷偷从另外一台服务器上取回来，然后作为自己的内容返回用户，但用户并不知情。这里所提到的 www.test.com 这个域名对应的服务器就设置了反向代理功能。 结论就是，反向代理正好相反，对于客户端而言它就像是原始服务器，并且客户端不需要进行任何特别的设置。客户端向反向代理的命名空间(name-space)中的内容发送普通请求，接着反向代理将判断向何处(原始服务器)转交请求，并将获得的内容返回给客户端，就像这些内容原本就是它自己的一样。 两者区别 从用途上来讲： 正向代理的典型用途是为在防火墙内的局域网客户端提供访问Internet的途径。正向代理还可以使用缓冲特性减少网络使用率。反向代理的典型用途是将防火墙后面的服务器提供给Internet用户访问。反向代理还可以为后端的多台服务器提供负载平衡，或为后端较慢的服务器提供缓冲服务。另外，反向代理还可以启用高级URL策略和管理技术，从而使处于不同web服务器系统的web页面同时存在于同一个URL空间下。 从安全性来讲： 正向代理允许客户端通过它访问任意网站并且隐藏客户端自身，因此你必须采取安全措施以确保仅为经过授权的客户端提供服务。反向代理对外都是透明的，访问者并不知道自己访问的是一个代理。 2. 负载均衡 upstream模块实现。 4种调度算法实现 轮询 每个请求按时间顺序逐一分配到不同的后端服务器 ip_hash 每个请求按访问IP的hash结果分配，这样来自同一个IP的访客固定访问一个后端服务器，有效解决了动态网页存在的session共享问题。 fair 这是比上面两个更加智能的负载均衡算法。此种算法可以依据页面大小和加载时间长短智能地进行负载均衡，也就是根据后端服务器的响应时间来分配请求，响应时间短的优先分配。 url_hash 此方法按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，可以进一步提高后端缓存服务器的效率。 3. 页面缓存（静态内容缓存） proxy_cache和proxy_cache_path等相关配置 4. URL重写 完成临时重定向或者永久向 5. 读写分离 web1 写操作（文件上传） web2 只读 6.跨域访问 跨域是指浏览器不能执行其他网站的脚本。 同源：域名、协议和端口必须相同。浏览器执行javascript时会检查这个脚本属于哪个页面，如果不是同源页面会拒绝执行。 请求转发和url重写欺骗浏览器和真实服务器实现跨域访问。 "},"zother4-EasyJob/Framework/rabbitmq.html":{"url":"zother4-EasyJob/Framework/rabbitmq.html","title":"Rabbitmq","keywords":"","body":"重要概念 1. ConnectionFactory、Connection、Channel都是RabbitMQ对外提供的API中最基本的对象。Connection是RabbitMQ的socket链接，它封装了socket协议相关部分逻辑。ConnectionFactory为Connection的制造工厂。 Channel是我们与RabbitMQ打交道的最重要的一个接口，我们大部分的业务操作是在Channel这个接口中完成的，包括定义Queue、定义Exchange、绑定Queue与Exchange、发布消息等。 2. 消息回执 在实际应用中，可能会发生消费者收到Queue中的消息，但没有处理完成就宕机（或出现其他意外）的情况，这种情况下就可能会导致消息丢失。为了避免这种情况发生，我们可以要求消费者在消费完消息后发送一个回执给RabbitMQ，RabbitMQ收到消息回执（Message acknowledgment）后才将该消息从Queue中移除；如果RabbitMQ没有收到回执并检测到消费者的RabbitMQ连接断开，则RabbitMQ会将该消息发送给其他消费者（如果存在多个消费者）进行处理。 3. Message durability Queue与Message都设置为可持久化的（durable） Queue的持久化 设置为持久化的队列，queue中的消息会在server本地硬盘存储一份，防止系统crash，数据丢失 设置为临时队列，queue中的数据在系统重启之后就会丢失 设置为自动删除的队列，当不存在用户连接到server，队列中的数据会被自动删除 4. prefetchCount来限制Queue每次发送给每个消费者的消息数 5. producer consumer producing consuming exchange queue bind 生产者在将消息发送给Exchange的时候，一般会指定一个routing key，来指定这个消息的路由规则routingkey 在绑定（Binding）Exchange与Queue的同时bindkey 6.direct fanout topic headers RPC MQ本身是基于异步的消息处理，前面的示例中所有的生产者（P）将消息发送到RabbitMQ后不会知道消费者（C）处理成功或者失败（甚至连有没有消费者来处理这条消息都不知道）。 但实际的应用场景中，我们很可能需要一些同步处理，需要同步等待服务端将我的消息处理完成后再进行下一步处理。这相当于RPC（Remote Procedure Call，远程过程调用）。在RabbitMQ中也支持RPC。 RabbitMQ中实现RPC的机制是： replyTo: 一个Queue名称，用于告诉服务器处理完成后将通知我的消息发送到这个Queue中 correlationId: 此次请求的标识号，服务器处理完成后需要将此属性返还，客户端将根据这个id了解哪条请求被成功执行了或执行失败 客户端发送请求（消息）时，在消息的属性（MessageProperties，在AMQP协议中定义了14中properties，这些属性会随着消息一起发送）中设置两个值replyTo（一个Queue名称，用于告诉服务器处理完成后将通知我的消息发送到这个Queue中）和correlationId（此次请求的标识号，服务器处理完成后需要将此属性返还，客户端将根据这个id了解哪条请求被成功执行了或执行失败） 服务器端收到消息并处理 服务器端处理完消息后，将生成一条应答消息到replyTo指定的Queue，同时带上correlationId属性 客户端之前已订阅replyTo指定的Queue，从中收到服务器的应答消息后，根据其中的correlationId属性分析哪条请求被执行了，根据执行结果进行后续业务处理 "},"zother4-EasyJob/Framework/redis.html":{"url":"zother4-EasyJob/Framework/redis.html","title":"Redis","keywords":"","body":"Redis学习日记 Redis 是一个开源（BSD许可）的，非关系型内存数据库，它可以用作数据库、缓存和消息中间件。 它支持多种类型的数据结构，如 字符串（strings）， 散列（hashes）， 列表（lists）， 集合（sets）， 有序集合（sorted sets） 与范围查询， bitmaps， hyperloglogs 和 地理空间（geospatial） 索引半径查询。 Redis 内置了 复制（replication），LUA脚本（Lua scripting）， LRU驱动事件（LRU eviction），事务（transactions） 和不同级别的 磁盘持久化（persistence）， 并通过 Redis哨兵（Sentinel）和自动 分区（Cluster）提供高可用性（high availability）。 Redis安装 Ubuntu上安装 $sudo apt-get update $sudo apt-get install redis-server 启动 Redis $redis-server 查看 redis 是否还在运行 $redis-cli 这将打开一个 Redis 提示符，如下图所示： redis 127.0.0.1:6379> 在上面的提示信息中：127.0.0.1 是本机的IP地址，6379是 Redis 服务器运行的端口。现在输入 PING 命令，如下图所示： redis 127.0.0.1:6379> ping PONG 这说明现在你已经成功地在计算机上安装了 Redis。 在Ubuntu上安装Redis桌面管理器 要在Ubuntu 上安装 Redis桌面管理，可以从 http://redisdesktop.com/download 下载包并安装它。 Redis 桌面管理器会给你用户界面来管理 Redis 键和数据。 数据类型 数据类型String 1. exists 存在1，否则0 append get set strlen 2. incr decr incrby decrby del 3. GETSET SETEX 过期时间 SETNX 如果指定的Key不存在，则设定该Key持有指定字符串Value，此时其效果等价于SET命令。相反，如果该Key已经存在，该命令将不做任何操作并返回。 SETRANGE GETRANGE 4. setbit getbit 5. mset mget msetnx 如果在这一批Keys中有任意一个Key已经存在了，那么该操作将全部回滚，即所有的修改都不会生效。 数据类型List 1. lpush e.g.lpush mykey a b c d e lpushx lrange e.g. lrange mykey 0 -1 2. lpop llen 3. lrem e.g. lrem mykey 2 a lset lindex ltrim linsert 4. rpush e.g.rpush mykey a b c d rpushx rpop e.g.rpop mykey rpoplpush 尾部弹出插入到另一个头部，原子操作 数据类型Hash 1. hset hget hdel hexists hlen hsetnx 2. hincrby 3. hgetall hkeys hvals hmget hmset 数据类型SET 1. sadd smembers scard 数量 sismember 2. spop srem srandmember smove 3. sdiff e.g.sdiff myset myset2 myset3 sdiffstore e.g.sdiffstore diffkey myset myset2 myset3 sinter e.g.sinter myset myset2 myset3 交集 sinterstore e.g.sinterstore interkey myset myset2 myset3 存储交集 sunion e.g.sunion myset myset2 myset3 并集 sunionstore e.g.sunionstore unionkey myset myset2 myset3 数据类型SortedSet 1. zadd zcard zcount zrem zincrby zscore zrange zrank 2. zrangebysocre zremrangebyrank zremrangebyscore 3. zrevrange zrevrangebyscore zrevrank Key命令 1. flushdb set sadd hset del e.g.del key1 key2 exists move keys hello* select 0切换数据库 rename 改名 改名的存在被覆盖，成功 renamenx 改名的存在操作无效 quit info dbsize flushdb 2. persist expire expireat ttl 3. type randomkey sort 事务 1. 事务被正常执行： #在Shell命令行下执行Redis的客户端工具。 /> redis-cli #在当前连接上启动一个新的事务。 redis 127.0.0.1:6379> multi OK #执行事务中的第一条命令，从该命令的返回结果可以看出，该命令并没有立即执行，而是存于事务的命令队列。 redis 127.0.0.1:6379> incr t1 QUEUED #又执行一个新的命令，从结果可以看出，该命令也被存于事务的命令队列。 redis 127.0.0.1:6379> incr t2 QUEUED #执行事务命令队列中的所有命令，从结果可以看出，队列中命令的结果得到返回。 redis 127.0.0.1:6379> exec 1) (integer) 1 2) (integer) 1 2. 事务中存在失败的命令： #开启一个新的事务。 redis 127.0.0.1:6379> multi OK #设置键a的值为string类型的3。 redis 127.0.0.1:6379> set a 3 QUEUED #从键a所关联的值的头部弹出元素，由于该值是字符串类型，而lpop命令仅能用于List类型，因此在执行exec命令时，该命令将会失败。 redis 127.0.0.1:6379> lpop a QUEUED #再次设置键a的值为字符串4。 redis 127.0.0.1:6379> set a 4 QUEUED #获取键a的值，以便确认该值是否被事务中的第二个set命令设置成功。 redis 127.0.0.1:6379> get a QUEUED #从结果中可以看出，事务中的第二条命令lpop执行失败，而其后的set和get命令均执行成功，这一点是Redis的事务与关系型数据库中的事务之间最为重要的差别。 redis 127.0.0.1:6379> exec 1) OK 2) (error) ERR Operation against a key holding the wrong kind of value 3) OK 4) \"4\" 3. 回滚事务： #为键t2设置一个事务执行前的值。 redis 127.0.0.1:6379> set t2 tt OK #开启一个事务。 redis 127.0.0.1:6379> multi OK #在事务内为该键设置一个新值。 redis 127.0.0.1:6379> set t2 ttnew QUEUED #放弃事务。 redis 127.0.0.1:6379> discard OK #查看键t2的值，从结果中可以看出该键的值仍为事务开始之前的值。 redis 127.0.0.1:6379> get t2 \"tt\" 4. WATCH命令和基于CAS的乐观锁： 在Redis的事务中，WATCH命令可用于提供CAS(check-and-set)功能。假设我们通过WATCH命令在事务执行之前监控了多个Keys，倘若在WATCH之后有任何Key的值发生了变化，EXEC命令执行的事务都将被放弃，同时返回Null multi-bulk应答以通知调用者事务执行失败。例如，我们再次假设Redis中并未提供incr命令来完成键值的原子性递增，如果要实现该功能，我们只能自行编写相应的代码。其伪码如下： val = GET mykey val = val + 1 SET mykey $val 以上代码只有在单连接的情况下才可以保证执行结果是正确的，因为如果在同一时刻有多个客户端在同时执行该段代码，那么就会出现多线程程序中经常出现的一种错误场景--竞态争用(race condition)。比如，客户端A和B都在同一时刻读取了mykey的原有值，假设该值为10，此后两个客户端又均将该值加一后set回Redis服务器，这样就会导致mykey的结果为11，而不是我们认为的12。为了解决类似的问题，我们需要借助WATCH命令的帮助，见如下代码： WATCH mykey val = GET mykey val = val + 1 MULTI SET mykey $val EXEC 和此前代码不同的是，新代码在获取mykey的值之前先通过WATCH命令监控了该键，此后又将set命令包围在事务中，这样就可以有效的保证每个连接在执行EXEC之前，如果当前连接获取的mykey的值被其它连接的客户端修改，那么当前连接的EXEC命令将执行失败。这样调用者在判断返回值后就可以获悉val是否被重新设置成功。 主从复制 一、Redis的Replication： 这里首先需要说明的是，在Redis中配置Master-Slave模式真是太简单了。相信在阅读完这篇Blog之后你也可以轻松做到。这里我们还是先列出一些理论性的知识，后面给出实际操作的案例。下面的列表清楚的解释了Redis Replication的特点和优势。 1). 同一个Master可以同步多个Slaves。 2). Slave同样可以接受其它Slaves的连接和同步请求，这样可以有效的分载Master的同步压力。因此我们可以将Redis的Replication架构视为图结构。 3). Master Server是以非阻塞的方式为Slaves提供服务。所以在Master-Slave同步期间，客户端仍然可以提交查询或修改请求。 4). Slave Server同样是以非阻塞的方式完成数据同步。在同步期间，如果有客户端提交查询请求，Redis则返回同步之前的数据。 5). 为了分载Master的读操作压力，Slave服务器可以为客户端提供只读操作的服务，写服务仍然必须由Master来完成。即便如此，系统的伸缩性还是得到了很大的提高。 6). Master可以将数据保存操作交给Slaves完成，从而避免了在Master中要有独立的进程来完成此操作。 二、Replication的工作原理： 在Slave启动并连接到Master之后，它将主动发送一个SYNC命令。此后Master将启动后台存盘进程，同时收集所有接收到的用于修改数据集的命令，在后台进程执行完毕后，Master将传送整个数据库文件到Slave，以完成一次完全同步。而Slave服务器在接收到数据库文件数据之后将其存盘并加载到内存中。此后，Master继续将所有已经收集到的修改命令，和新的修改命令依次传送给Slaves，Slave将在本次执行这些数据修改命令，从而达到最终的数据同步。 如果Master和Slave之间的链接出现断连现象，Slave可以自动重连Master，但是在连接成功之后，一次完全同步将被自动执行。 三、如何配置Replication： 见如下步骤： 1). 同时启动两个Redis服务器，可以考虑在同一台机器上启动两个Redis服务器，分别监听不同的端口，如6379和6380。 2). 在Slave服务器上执行一下命令： /> redis-cli -p 6380 #这里我们假设Slave的端口号是6380 redis 127.0.0.1:6380> slaveof 127.0.0.1 6379 #我们假设Master和Slave在同一台主机，Master的端口为6379 OK 上面的方式只是保证了在执行slaveof命令之后，redis_6380成为了redis_6379的slave，一旦服务(redis_6380)重新启动之后，他们之间的复制关系将终止。 如果希望长期保证这两个服务器之间的Replication关系，可以在redis_6380的配置文件中做如下修改： /> cd /etc/redis #切换Redis服务器配置文件所在的目录。 /> ls 6379.conf 6380.conf /> vi 6380.conf 将 # slaveof 改为 slaveof 127.0.0.1 6379 保存退出。这样就可以保证Redis_6380服务程序在每次启动后都会主动建立与Redis_6379的Replication连接了。 四、应用示例： 这里我们假设Master-Slave已经建立。 #启动master服务器。 [root@Stephen-PC redis]# redis-cli -p 6379 redis 127.0.0.1:6379> #情况Master当前数据库中的所有Keys。 redis 127.0.0.1:6379> flushdb OK #在Master中创建新的Keys作为测试数据。 redis 127.0.0.1:6379> set mykey hello OK redis 127.0.0.1:6379> set mykey2 world OK #查看Master中存在哪些Keys。 redis 127.0.0.1:6379> keys * 1) \"mykey\" 2) \"mykey2\" #启动slave服务器。 [root@Stephen-PC redis]# redis-cli -p 6380 #查看Slave中的Keys是否和Master中一致，从结果看，他们是相等的。 redis 127.0.0.1:6380> keys * 1) \"mykey\" 2) \"mykey2\" #在Master中删除其中一个测试Key，并查看删除后的结果。 redis 127.0.0.1:6379> del mykey2 (integer) 1 redis 127.0.0.1:6379> keys * 1) \"mykey\" #在Slave中查看是否mykey2也已经在Slave中被删除。 redis 127.0.0.1:6380> keys * 1) \"mykey\" 数据持久化 一、Redis提供了哪些持久化机制： 1). RDB持久化： 该机制是指在指定的时间间隔内将内存中的数据集快照写入磁盘。 2). AOF持久化: 该机制将以日志的形式记录服务器所处理的每一个写操作，在Redis服务器启动之初会读取该文件来重新构建数据库，以保证启动后数据库中的数据是完整的。 3). 无持久化： 我们可以通过配置的方式禁用Redis服务器的持久化功能，这样我们就可以将Redis视为一个功能加强版的memcached了。 4). 同时应用AOF和RDB。 二、RDB机制的优势和劣势： RDB存在哪些优势呢？ 1). 一旦采用该方式，那么你的整个Redis数据库将只包含一个文件，这对于文件备份而言是非常完美的。比如，你可能打算每个小时归档一次最近24小时的数据，同时还要每天归档一次最近30天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复。 2). 对于灾难恢复而言，RDB是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上。 3). 性能最大化。对于Redis的服务进程而言，在开始持久化时，它唯一需要做的只是fork出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行IO操作了。 4). 相比于AOF机制，如果数据集很大，RDB的启动效率会更高。 RDB又存在哪些劣势呢？ 1). 如果你想保证数据的高可用性，即最大限度的避免数据丢失，那么RDB将不是一个很好的选择。因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失。 2). 由于RDB是通过fork子进程来协助完成数据持久化工作的，因此，如果当数据集较大时，可能会导致整个服务器停止服务几百毫秒，甚至是1秒钟。 三、AOF机制的优势和劣势： AOF的优势有哪些呢？ 1). 该机制可以带来更高的数据安全性，即数据持久性。Redis中提供了3中同步策略，即每秒同步、每修改同步和不同步。事实上，每秒同步也是异步完成的，其效率也是非常高的，所差的是一旦系统出现宕机现象，那么这一秒钟之内修改的数据将会丢失。而每修改同步，我们可以将其视为同步持久化，即每次发生的数据变化都会被立即记录到磁盘中。可以预见，这种方式在效率上是最低的。至于无同步，无需多言，我想大家都能正确的理解它。 2). 由于该机制对日志文件的写入操作采用的是append模式，因此在写入过程中即使出现宕机现象，也不会破坏日志文件中已经存在的内容。然而如果我们本次操作只是写入了一半数据就出现了系统崩溃问题，不用担心，在Redis下一次启动之前，我们可以通过redis-check-aof工具来帮助我们解决数据一致性的问题。 3). 如果日志过大，Redis可以自动启用rewrite机制。即Redis以append模式不断的将修改数据写入到老的磁盘文件中，同时Redis还会创建一个新的文件用于记录此期间有哪些修改命令被执行。因此在进行rewrite切换时可以更好的保证数据安全性。 4). AOF包含一个格式清晰、易于理解的日志文件用于记录所有的修改操作。事实上，我们也可以通过该文件完成数据的重建。 AOF的劣势有哪些呢？ 1). 对于相同数量的数据集而言，AOF文件通常要大于RDB文件。 2). 根据同步策略的不同，AOF在运行效率上往往会慢于RDB。总之，每秒同步策略的效率是比较高的，同步禁用策略的效率和RDB一样高效。 四、其它： 1. Snapshotting: 缺省情况下，Redis会将数据集的快照dump到dump.rdb文件中。此外，我们也可以通过配置文件来修改Redis服务器dump快照的频率，在打开6379.conf文件之后，我们搜索save，可以看到下面的配置信息： save 900 1 #在900秒(15分钟)之后，如果至少有1个key发生变化，则dump内存快照。 save 300 10 #在300秒(5分钟)之后，如果至少有10个key发生变化，则dump内存快照。 save 60 10000 #在60秒(1分钟)之后，如果至少有10000个key发生变化，则dump内存快照。 2. Dump快照的机制： 1). Redis先fork子进程。 2). 子进程将快照数据写入到临时RDB文件中。 3). 当子进程完成数据写入操作后，再用临时文件替换老的文件。 3. AOF文件： 上面已经多次讲过，RDB的快照定时dump机制无法保证很好的数据持久性。如果我们的应用确实非常关注此点，我们可以考虑使用Redis中的AOF机制。对于Redis服务器而言，其缺省的机制是RDB，如果需要使用AOF，则需要修改配置文件中的以下条目： 将appendonly no改为appendonly yes 从现在起，Redis在每一次接收到数据修改的命令之后，都会将其追加到AOF文件中。在Redis下一次重新启动时，需要加载AOF文件中的信息来构建最新的数据到内存中。 4. AOF的配置： 在Redis的配置文件中存在三种同步方式，它们分别是： appendfsync always #每次有数据修改发生时都会写入AOF文件。 appendfsync everysec #每秒钟同步一次，该策略为AOF的缺省策略。 appendfsync no #从不同步。高效但是数据不会被持久化。 5. 如何修复坏损的AOF文件： 1). 将现有已经坏损的AOF文件额外拷贝出来一份。 2). 执行\"redis-check-aof --fix \"命令来修复坏损的AOF文件。 3). 用修复后的AOF文件重新启动Redis服务器。 6. Redis的数据备份： 在Redis中我们可以通过copy的方式在线备份正在运行的Redis数据文件。这是因为RDB文件一旦被生成之后就不会再被修改。Redis每次都是将最新的数据dump到一个临时文件中，之后在利用rename函数原子性的将临时文件改名为原有的数据文件名。因此我们可以说，在任意时刻copy数据文件都是安全的和一致的。鉴于此，我们就可以通过创建cron job的方式定时备份Redis的数据文件，并将备份文件copy到安全的磁盘介质中。 管道 一、请求应答协议和RTT： Redis是一种典型的基于C/S模型的TCP服务器。在客户端与服务器的通讯过程中，通常都是客户端率先发起请求，服务器在接收到请求后执行相应的任务，最后再将获取的数据或处理结果以应答的方式发送给客户端。在此过程中，客户端都会以阻塞的方式等待服务器返回的结果。见如下命令序列： Client: INCR X Server: 1 Client: INCR X Server: 2 Client: INCR X Server: 3 Client: INCR X Server: 4 在每一对请求与应答的过程中，我们都不得不承受网络传输所带来的额外开销。我们通常将这种开销称为RTT(Round Trip Time)。现在我们假设每一次请求与应答的RTT为250毫秒，而我们的服务器可以在一秒内处理100k的数据，可结果则是我们的服务器每秒至多处理4条请求。要想解决这一性能问题，我们该如何进行优化呢？ 二、管线(pipelining)： Redis在很早的版本中就已经提供了对命令管线的支持。在给出具体解释之前，我们先将上面的同步应答方式的例子改造为基于命令管线的异步应答方式，这样可以让大家有一个更好的感性认识。 Client: INCR X Client: INCR X Client: INCR X Client: INCR X Server: 1 Server: 2 Server: 3 Server: 4 从以上示例可以看出，客户端在发送命令之后，不用立刻等待来自服务器的应答，而是可以继续发送后面的命令。在命令发送完毕后，再一次性的读取之前所有命令的应答。这样便节省了同步方式中RTT的开销。 最后需要说明的是，如果Redis服务器发现客户端的请求是基于管线的，那么服务器端在接受到请求并处理之后，会将每条命令的应答数据存入队列，之后再发送到客户端。 "},"zother4-EasyJob/Framework/spark-streaming.html":{"url":"zother4-EasyJob/Framework/spark-streaming.html","title":"Spark Streaming","keywords":"","body":"Spark Streaming Spark Streaming介绍 http://spark.apache.org/docs/latest/streaming-programming-guide.html 一个简单例子 package com.didi; import org.apache.spark.SparkConf; import org.apache.spark.api.java.JavaPairRDD; import org.apache.spark.api.java.Optional; import org.apache.spark.api.java.function.Function3; import org.apache.spark.streaming.Durations; import org.apache.spark.streaming.State; import org.apache.spark.streaming.StateSpec; import org.apache.spark.streaming.api.java.*; import scala.Tuple2; import java.util.Arrays; import java.util.List; /** * Hello Spark. */ public class StreamingWordCount { public static void main(String[] args) throws Exception { SparkConf conf = new SparkConf() .setMaster(\"local[2]\") .setAppName(\"StreamingWordCount\"); JavaStreamingContext ssc = new JavaStreamingContext(conf, Durations.seconds(1)); ssc.checkpoint(\".\"); // initial state rdd List> tuples = Arrays.asList(new Tuple2(\"hello\", 100), new Tuple2(\"word\", 100)); JavaPairRDD initialRDD = ssc.sparkContext().parallelizePairs(tuples); JavaReceiverInputDStream lines = ssc.socketTextStream(\"localhost\", 9999); JavaDStream words = lines.flatMap(x -> Arrays.asList(x.split(\" \")).iterator()); JavaPairDStream pairs = words.mapToPair(x -> new Tuple2(x, 1)); // update function Function3, State, Tuple2> updateFunc = (word, count, state) -> { int sum = count.orElse(0) + (state.exists() ? state.get() : 0); Tuple2 output = new Tuple2<>(word, sum); state.update(sum); return output; }; //update in every batch JavaMapWithStateDStream> stateDstream = pairs.mapWithState(StateSpec.function(updateFunc).initialState(initialRDD)); //print stateDstream.print(); //start ssc.start(); ssc.awaitTermination(); } } "},"zother4-EasyJob/Framework/spark.html":{"url":"zother4-EasyJob/Framework/spark.html","title":"Spark","keywords":"","body":"Spark 本文环境：Hadoop2.7 Spark2.1.1 Spark知识点 简介:快速通用的集群计算平台 生态和组件:Spark Core、Spark SQL、Spark Streaming、Mlib、Graphx、Cluster Managers Spark和Hadoop的比较： Hadoop应用场景，离线处理，对时效性要求不高。 Spark应用场景，时效性要求高，机器学习。 其他：Spark是紧密集成的。 Spark安装 依赖 Spark2.0以上需要Scala2.11 RDD RDD(Resilient Distributed Datasets),弹性分布式数据集 常用方法：parallelize，foreach RDD创建方法： 1、已存在的集合传递给SparkContext的parallelize方法，测试用 2、textFile加载 RDD Transformation Map、FlatMap、Filter、distinct、union、intersection、subtract RDD Action 在RDD上计算出一个结果，把结果返回给driver program或保存在文件系统 reduce、count、save、collect、foreach reduce：接收一个函数，作用在RDD两个类型相同的元素上，返回新元素 collect：遍历rdd内容，向driver program返回rdd的内容（测试用） 大数据集时用saveAsTextFile保存 take（n）：返回RDD的n个元素 top（n）：根据默认的比较器返回top foreach：计算RDD的每个元素，但不返回本地，配合print友好打印数据 RDD的特性 1、血统关系图 Spark维护RDD之间的依赖关系和创建关系，称作血统关系图 Spark使用血统关系图来就计算每个RDD的需求和恢复丢失的数据 2、延迟计算 Spark对RDD的计算是在第一次使用action操作的时候 3、RDD.persist() 默认每次在RDD上进行action操作时，spark都重新计算RDD。 如果想重复使用一个RDD，可以使用RDD.persist() unpersist方法从缓存移除 级别 空间占用 cpu消耗 内存 硬盘 MEMORY_ONLY High Low Y N MEMORY_ONLY_SEQ Low High Y N DISK_ONLY Low High N Y MEMORY_AND_DISK High Medium Some Some MEMORY_AND_DISK_SEQ Low High Some Some KeyValue对RDD 创建：如map函数 Transformation：reduceByKey（相同key结合）、groupByKey（相同key的value分组）、combineByKey mapValues：函数作用于pairRDD的每个元素，对value操作，key不变 flatMapValues：符号化的时候使用，也是对value操作 keys：返回keys values：返回values sortByKey()：按照key排序的RDD combineByKey(): 参数createCombiner、mergeValue、mergeCombiners、partitioner，最常用的基于key的聚合函数，返回的类型可以与输入类型不一样 如果是新元素，使用createCombiner，如果是已存在的key，使用mergeValue 合计每个partition的结果的时候，使用mergeCombiners函数 详解combineByKey def combineByKeyC => C, mergeValue: (C, V) => C, mergeCombiners: (C, C) => C): RDD[(K, C)] def combineByKeyC => C, mergeValue: (C, V) => C, mergeCombiners: (C, C) => C, numPartitions: Int): RDD[(K, C)] def combineByKeyC => C, mergeValue: (C, V) => C, mergeCombiners: (C, C) => C, partitioner: Partitioner, mapSideCombine: Boolean = true, serializer: Serializer = null): RDD[(K, C)] 其中的参数： createCombiner：组合器函数，用于将V类型转换成C类型，输入参数为RDD[K,V]中的V,输出为C mergeValue：合并值函数，将一个C类型和一个V类型值合并成一个C类型，输入参数为(C,V)，输出为C mergeCombiners：合并组合器函数，用于将两个C类型值合并成一个C类型，输入参数为(C,C)，输出为C numPartitions：结果RDD分区数，默认保持原有的分区数 partitioner：分区函数,默认为HashPartitioner mapSideCombine：是否需要在Map端进行combine操作，类似于MapReduce中的combine，默认为true 举例理解： 假设我们要将一堆的各类水果给榨果汁，并且要求果汁只能是纯的，不能有其他品种的水果。那么我们需要一下几步： 1 定义我们需要什么样的果汁。 2 定义一个榨果汁机，即给定水果，就能给出我们定义的果汁。--相当于hadoop中的local combiner 3 定义一个果汁混合器，即能将相同类型的水果果汁给混合起来。--相当于全局进行combiner 那么对比上述三步，combineByKey的三个函数也就是这三个功能 1 createCombiner就是定义了v如何转换为c 2 mergeValue 就是定义了如何给定一个V将其与原来的C合并成新的C 3 就是定义了如何将相同key下的C给合并成一个C var rdd1 = sc.makeRDD(Array((\"A\",1),(\"A\",2),(\"B\",1),(\"B\",2),(\"C\",1))) rdd1.combineByKey( (v : Int) => List(v),　　　　　　　　　　　　　--将1 转换成 list(1) (c : List[Int], v : Int) => v :: c,　　　　　　　--将list(1)和2进行组合从而转换成list(1,2) (c1 : List[Int], c2 : List[Int]) => c1 ::: c2　　--将全局相同的key的value进行组合 ).collect res65: Array[(String, List[Int])] = Array((A,List(2, 1)), (B,List(2, 1)), (C,List(1))) "},"zother4-EasyJob/Framework/spring-aop.html":{"url":"zother4-EasyJob/Framework/spring-aop.html","title":"Spring Aop","keywords":"","body":"Spring本质系列之AOP 问题来源 我们在做系统设计的时候，一个非常重要的工作就是把一个大系统做分解， 按业务功能分解成一个个低耦合、高内聚的模块，就像这样： 但是分解以后就会发现有些很有趣的东西， 这些东西是通用的，或者是跨越多个模块的： 日志： 对特定的操作输出日志来记录 安全：在执行操作之前进行操作检查 性能：要统计每个方法的执行时间 事务：方法开始之前要开始事务， 结束后要提交或者回滚事务 等等.... 这些可以称为是非功能需求， 但他们是多个业务模块都需要的， 是跨越模块的， 把他们放到什么地方呢？ 最简单的办法就是把这些通用模块的接口写好， 让程序员在实现业务模块的时候去调用就可以了，码农嘛，辛苦一下也没什么。 这样做看起来没问题， 只是会产生类似这样的代码： 这样的代码也实现了功能，但是看起来非常的不爽， 那就是日志，性能，事务 相关的代码几乎要把真正的业务代码给淹没了。 不仅仅这一个类需要这么干， 其他类都得这么干， 重复代码会非常的多。 有经验的程序员还好， 新手忘记写这样的非业务代码简直是必然的。 设计模式：模板方法 用设计模式在某些情况下可以部分解决上面的问题，例如著名的模板方法： 在父类（BaseCommand）中已经把那些“乱七八糟“的非功能代码都写好了， 只是留了一个口子（抽象方法doBusiness()）让子类去实现。 子类变的清爽， 只需要关注业务逻辑就可以了。 调用也很简单，例如： BaseCommand cmd = ... 获得PlaceOrderCommand的实例... cmd.execute(); 但是这样方式的巨大缺陷就是父类会定义一切： 要执行哪些非功能代码， 以什么顺序执行等等 子类只能无条件接受，完全没有反抗余地。 如果有个子类， 根本不需要事务， 但是它也没有办法把事务代码去掉。 设计模式：装饰者 如果利用装饰者模式， 针对上面的问题，可以带来更大的灵活性： 现在让这个PlaceOrderCommand 能够打印日志，进行性能统计 Command cmd = new LoggerDecorator( new PerformanceDecorator( new PlaceOrderCommand())); cmd.execute(); 如果PaymentCommand 只需要打印日志，装饰一次就可以了： Command cmd = new LoggerDecorator( new PaymentCommand()); cmd.execute(); 可以使用任意数量装饰器，还可以以任意次序执行（严格意义上来说是不行的）， 是不是很灵活？ AOP 如果仔细思考一下就会发现装饰者模式的不爽之处: (1) 一个处理日志/性能/事务 的类为什么要实现 业务接口（Command）呢? (2) 如果别的业务模块，没有实现Command接口，但是也想利用日志/性能/事务等功能，该怎么办呢？ 最好把日志/安全/事务这样的代码和业务代码完全隔离开来，因为他们的关注点和业务代码的关注点完全不同 ，他们之间应该是正交的，他们之间的关系应该是这样的： 如果把这个业务功能看成一层层面包的话， 这些日志/安全/事务 像不像一个个“切面”(Aspect) ？ 如果我们能让这些“切面“能和业务独立， 并且能够非常灵活的“织入”到业务方法中， 那就实现了面向切面编程(AOP)！ 实现AOP 现在我们来实现AOP吧， 首先我们得有一个所谓的“切面“类(Aspect)， 这应该是一个普通的java 类 ， 不用实现什么“乱七八糟”的接口。 以一个事务类为例： 我们想达到的目的只这样的： 对于com.coderising这个包中所有类的execute方法， 在方法调用之前，需要执行Transaction.beginTx()方法， 在调用之后， 需要执行Transaction.commitTx()方法。 暂时停下脚步分析一下。 “对于com.coderising这个包中所有类的execute方法” ， 用一个时髦的词来描述就是切入点（PointCut） , 它可以是一个方法或一组方法（可以通过通配符来支持，你懂的） ”在方法调用之前/之后 ， 需要执行xxx“ , 用另外一个时髦的词来描述就是通知（Advice） 码农翻身认为，PointCut,Advice 这些词实在是不直观， 其实Spring的作者们也是这么想的 : These terms are not Spring-specific… unfortunately, AOP terminology is not particularly intuitive; however, it would be even more confusing if Spring used its own terminology. 当然，想描述这些规则， xml依然是不二之选： 注意：现在Transaction这个类和业务类在源代码层次上没有一点关系，完全隔离了。 隔离是一件好事情， 但是马上给我们带来了大麻烦 。 Java 是一门静态的强类型语言， 代码一旦写好， 编译成java class 以后 ，可以在运行时通过反射（Reflection）来查看类的信息， 但是想对类进行修改非常困难。 而AOP要求的恰恰就是在不改变业务类的源代码（其实大部分情况下你也拿不到）的情况下， 修改业务类的方法, 进行功能的增强，就像上面给所有的业务类增加事务支持。 为了突破这个限制，大家可以说是费尽心机， 现在基本是有这么几种技术： (1) 在编译的时候， 根据AOP的配置信息，悄悄的把日志，安全，事务等“切面”代码 和业务类编译到一起去。 (2) 在运行期，业务类加载以后， 通过Java动态代理技术为业务类生产一个代理类， 把“切面”代码放到代理类中， Java 动态代理要求业务类需要实现接口才行。 (3) 在运行期， 业务类加载以后， 动态的使用字节码构建一个业务类的子类，将“切面”逻辑加入到子类当中去, CGLIB就是这么做的。 Spring采用的就是(1) +(2) 的方式，限于篇幅，这里不再展开各种技术了， 不管使用哪一种方式， 在运行时，真正干活的“业务类”其实已经不是原来单纯的业务类了， 它们被AOP了 ！ 本文转载自“码农翻身”公众号 "},"zother4-EasyJob/Framework/spring-ioc.html":{"url":"zother4-EasyJob/Framework/spring-ioc.html","title":"Spring Ioc","keywords":"","body":"Spring 的本质系列之依赖注入 前言： Spring 这个轻量级的框架已经成为Web开发事实上的标准，阅读本篇文章之前希望你对OO,设计模式，单元测试，XML，反射等技术有一定了解。 概念：什么是IOC？ IoC(Inversion of Control)，意为控制反转，不是什么技术，而是一种设计思想。Ioc意味着将你设计好的对象交给容器控制，而不是传统的在你的对象内部直接控制。 如何理解好Ioc呢？理解好Ioc的关键是要明确“谁控制谁，控制什么，为何是反转（有反转就应该有正转了），哪些方面反转了”，那我们来深入分析一下： 谁控制谁，控制什么：传统Java程序设计，我们直接在对象内部通过new进行创建对象，是程序主动去创建依赖对象；而IoC是有专门一个容器来创建这些对象，即由Ioc容器来控制对象的创建；谁控制谁？当然是IoC 容器控制了对象；控制什么？那就是主要控制了外部资源获取（不只是对象包括比如文件等）。 为何是反转，哪些方面反转了：有反转就有正转，传统应用程序是由我们自己在对象中主动控制去直接获取依赖对象，也就是正转；而反转则是由容器来帮忙创建及注入依赖对象；为何是反转？因为由容器帮我们查找及注入依赖对象，对象只是被动的接受依赖对象，所以是反转；哪些方面反转了？依赖对象的获取被反转了。 1.对象的创建 面向对象的编程语言是用类(Class)来对现实世界进行抽象，在运行时这些类会生成对象(Object)。 当然，单独的一个或几个对象根本没办法完成复杂的业务， 实际的系统是由千千万万个对象组成的， 这些对象需要互相协作才能干活，例如对象A调用对象B的方法，那必然会提出一个问题：对象A怎么才能获得对象B的引用呢？ 最简单的办法无非是： 当对象A需要使用对象B的时候，把它给new 出来 ，这也是最常用的办法，java 不就是这么做的？例如： Apple a = new Apple(); 后来业务变复杂了， 抽象出了一个水果(Fruit)的类， 创建对象会变成这个样子： Fruit f1 = new Apple(); Fruit f2 = new Banana(); Fruit f3 = ...... 很自然的，类似下面的代码就会出现： 这样的代码如果散落在各处，维护起来将会痛苦不堪， 例如你新加一个水果的类型Orange, 那得找到系统中所有的这些创建Fruit的地方，进行修改， 这绝对是一场噩梦。 解决办法也很简单，前辈们早就总结好了：工厂模式 工厂模式，以及其他模式像抽象工厂， Builder模式提供的都是创建对象的方法。 这背后体现的都是“封装变化”的思想。 这些模式只是一些最佳实践而已： 起了一个名称、描述一下解决的问题、使用的范围和场景，码农们在项目中还得自己去编码实现他们。 2.解除依赖 我们再来看一个稍微复杂一点，更加贴近实际项目的例子： 一个订单处理类，它会被定时调用：查询数据库中订单的处理情况，必要时给下订单的用户发信。 看起来也没什么难度， 需要注意的是很多类一起协作了， 尤其是OrderProcessor , 它依赖于 OrderService 和 EmailService这两个服务，它获取依赖的方式就是通过单例方法。 如果你想对这个process方法进行单元测试--这也是很多优秀的团队要求的-- 麻烦就来了。 首先OrderService 确实会从真正的数据库中取得Order信息，你需要确保数据库中有数据， 数据库连接没问题，实际上如果数据库连接Container（例如Tomcat）管理的， 你没有Tomcat很难建立数据库连接。 其次这个EmailService 真的会对外发邮件， 你可不想对真正的用户发测试邮件，当然你可以修改数据库，把邮件地址改成假的，但那样很麻烦， 并且EmailService 会抛出一堆错误来，很不爽。 所有的这些障碍，最终会导致脆弱的单元测试： 速度慢， 不可重复，需要手工干预，不能独立运行。 想克服这些障碍，一个可行的办法就是不在方法中直接调用OrderService和EmailService的getInstance()方法， 而是把他们通过setter方法传进来。 通过这种方式，你的单元测试就可以构造一个假的OrderService 和假的EmailService 了。 例如OrderService 的冒牌货可以是MockOrderService , 它可以返回你想要的任何Order 对象， 而不是从数据库取。 MockEmailService 也不会真的发邮件， 而是把代码中试图发的邮件保存下来， 测试程序可以检查是否正确。 你的测试代码可能是这样的： 当然， 有经验的你马上就会意识到：需要把OrderService 和 EmailService 变成 接口或者抽象类， 这样才可以把Mock对象传进来。 这其实也遵循了面向对象编程的另外一个要求：对接口编程，而不是对实现编程。 3.Spring 依赖注入 啰啰嗦嗦说了这么多，快要和Spring扯上关系了。 上面的代码其实就是实现了一个依赖的注入，把两个冒牌货注入到业务类中(通过set方法)， 这个注入的过程是在一个测试类中通过代码完成的。 既然能把冒牌货注入进去，那毫无疑问肯定也能把一个正经的类安插进去，因为setter 方法接受的是接口，而不是具体类。 用这种方式来处理对象之间的依赖，会强迫你对接口编程，好处显而易见。 随着系统复杂度的增长，这样的代码会越来越多，最后也会变得难于维护。 能不能把各个类之间的依赖关系统一维护呢？ 能不能把系统做的更加灵活一点，用声明的方式而不是用代码的方式来描述依赖关系呢？ 肯定可以，在Java 世界里，如果想描述各种逻辑关系，XML是不二之选： 这个xml 挺容易理解的，但是仅仅有它还不够，还缺一个解析器（假设叫做XmlAppContext）来解析，处理这个文件，基本过程是： 解析xml, 获取各种元素 通过Java反射把各个bean 的实例创建起来：com.coderising.OrderProcessor, OrderServiceImpl, EmailServiceImpl. 还是通过Java反射调用OrderProcessor的两个方法：setOrderService(....) 和 setEmailService(...) 把orderService, emailService 实例 注入进去。 应用程序使用起来就简单了： XmlAppContext ctx = new XmlAppContext(\"c:\\\\bean.xml\"); OrderProcessor op = (OrderProcessor) ctx.getBean(\"order-processor\"); op.process(); 其实Spring的处理方式和上面说的非常类似，当然Spring 处理了更多的细节，例如不仅仅是setter方法注入， 还可以构造函数注入，init 方法，destroy方法等等，基本思想是一致的。 既然对象的创建过程和装配过程都是Spring做的，那Spring 在这个过程中就可以玩很多把戏了， 比如对你的业务类做点字节码级别的增强，搞点AOP什么的，这都不在话下了。 4.IoC vs DI “不要给我们打电话，我们会打给你的(don‘t call us, we‘ll call you)”这是著名的好莱坞原则。 在好莱坞，把简历递交给演艺公司后就只有回家等待。由演艺公司对整个娱乐项目完全控制，演员只能被动式的接受公司的差使,在需要的环节中，完成自己的演出。 这和软件开发有一定的相似性，演员们就像一个个Java Object, 最早的时候自己去创建自己所依赖的对象， 有了演艺公司（Spring容器）的介入，所有的依赖关系都是演艺公司搞定的，于是控制就翻转了 Inversion of Control, 简称IoC。 但是IoC这个词不能让人更加直观和清晰的理解背后所代表的含义，于是Martin Flower先生就创造了一个新词 : 依赖注入 (Dependency Injection，简称DI), 是不是更加贴切一点？ 总结 DI—Dependency Injection，即“依赖注入”：组件之间依赖关系由容器在运行期决定，形象的说，即由容器动态的将某个依赖关系注入到组件之中。依赖注入的目的并非为软件系统带来更多功能，而是为了提升组件重用的频率，并为系统搭建一个灵活、可扩展的平台。通过依赖注入机制，我们只需要通过简单的配置，而无需任何代码就可指定目标需要的资源，完成自身的业务逻辑，而不需要关心具体的资源来自何处，由谁实现。 理解DI的关键是：“谁依赖谁，为什么需要依赖，谁注入谁，注入了什么”，那我们来深入分析一下： 谁依赖于谁：当然是应用程序依赖于IoC容器； 为什么需要依赖：应用程序需要IoC容器来提供对象需要的外部资源； 谁注入谁：很明显是IoC容器注入应用程序某个对象，应用程序依赖的对象； 注入了什么：就是注入某个对象所需要的外部资源（包括对象、资源、常量数据）。 IoC和DI由什么关系呢？其实它们是同一个概念的不同角度描述，由于控制反转概念比较含糊（可能只是理解为容器控制对象这一个层面，很难让人想到谁来维护对象关系），所以2004年大师级人物Martin Fowler又给出了一个新的名字：“依赖注入”，相对IoC 而言，“依赖注入”明确描述了“被注入对象依赖IoC容器配置依赖对象”。 对于Spring Ioc这个核心概念，我相信每一个学习Spring的人都会有自己的理解。这种概念上的理解没有绝对的标准答案，仁者见仁智者见智。理解了IoC和DI的概念后，一切都将变得简单明了，剩下的工作只是在框架中堆积木而已。 "},"zother4-EasyJob/Framework/websocket.html":{"url":"zother4-EasyJob/Framework/websocket.html","title":"Websocket","keywords":"","body":"简介 WebSocket是HTML5新增的协议，它的目的是在浏览器和服务器之间建立一个不受限的双向通信的通道，比如说，服务器可以在任意时刻发送消息给浏览器。 为什么传统的HTTP协议不能做到WebSocket实现的功能？这是因为HTTP协议是一个请求－响应协议，请求必须先由浏览器发给服务器，服务器才能响应这个请求，再把数据发送给浏览器。换句话说，浏览器不主动请求，服务器是没法主动发数据给浏览器的。 这样一来，要在浏览器中搞一个实时聊天，在线炒股（不鼓励），或者在线多人游戏的话就没法实现了，只能借助Flash这些插件。 也有人说，HTTP协议其实也能实现啊，比如用轮询或者Comet。轮询是指浏览器通过JavaScript启动一个定时器，然后以固定的间隔给服务器发请求，询问服务器有没有新消息。这个机制的缺点一是实时性不够，二是频繁的请求会给服务器带来极大的压力。 Comet本质上也是轮询，但是在没有消息的情况下，服务器先拖一段时间，等到有消息了再回复。这个机制暂时地解决了实时性问题，但是它带来了新的问题：以多线程模式运行的服务器会让大部分线程大部分时间都处于挂起状态，极大地浪费服务器资源。另外，一个HTTP连接在长时间没有数据传输的情况下，链路上的任何一个网关都可能关闭这个连接，而网关是我们不可控的，这就要求Comet连接必须定期发一些ping数据表示连接“正常工作”。 以上两种机制都治标不治本，所以，HTML5推出了WebSocket标准，让浏览器和服务器之间可以建立无限制的全双工通信，任何一方都可以主动发消息给对方。 WebSocket协议 WebSocket并不是全新的协议，而是利用了HTTP协议来建立连接。我们来看看WebSocket连接是如何创建的。 首先，WebSocket连接必须由浏览器发起，因为请求协议是一个标准的HTTP请求，格式如下： GET ws://localhost:3000/ws/chat HTTP/1.1 Host: localhost Upgrade: websocket Connection: Upgrade Origin: http://localhost:3000 Sec-WebSocket-Key: client-random-string Sec-WebSocket-Version: 13 该请求和普通的HTTP请求有几点不同： GET请求的地址不是类似/path/，而是以ws://开头的地址； 请求头Upgrade: websocket和Connection: Upgrade表示这个连接将要被转换为WebSocket连接； Sec-WebSocket-Key是用于标识这个连接，并非用于加密数据； Sec-WebSocket-Version指定了WebSocket的协议版本。 随后，服务器如果接受该请求，就会返回如下响应： HTTP/1.1 101 Switching Protocols Upgrade: websocket Connection: Upgrade Sec-WebSocket-Accept: server-random-string 该响应代码101表示本次连接的HTTP协议即将被更改，更改后的协议就是Upgrade: websocket指定的WebSocket协议。 版本号和子协议规定了双方能理解的数据格式，以及是否支持压缩等等。如果仅使用WebSocket的API，就不需要关心这些。 现在，一个WebSocket连接就建立成功，浏览器和服务器就可以随时主动发送消息给对方。消息有两种，一种是文本，一种是二进制数据。通常，我们可以发送JSON格式的文本，这样，在浏览器处理起来就十分容易。 为什么WebSocket连接可以实现全双工通信而HTTP连接不行呢？实际上HTTP协议是建立在TCP协议之上的，TCP协议本身就实现了全双工通信，但是HTTP协议的请求－应答机制限制了全双工通信。WebSocket连接建立以后，其实只是简单规定了一下：接下来，咱们通信就不使用HTTP协议了，直接互相发数据吧。 安全的WebSocket连接机制和HTTPS类似。首先，浏览器用wss://xxx创建WebSocket连接时，会先通过HTTPS创建安全的连接，然后，该HTTPS连接升级为WebSocket连接，底层通信走的仍然是安全的SSL/TLS协议。 浏览器 很显然，要支持WebSocket通信，浏览器得支持这个协议，这样才能发出ws://xxx的请求。目前，支持WebSocket的主流浏览器如下： Chrome Firefox IE >= 10 Sarafi >= 6 Android >= 4.4 iOS >= 8 服务器 由于WebSocket是一个协议，服务器具体怎么实现，取决于所用编程语言和框架本身。Node.js本身支持的协议包括TCP协议和HTTP协议，要支持WebSocket协议，需要对Node.js提供的HTTPServer做额外的开发。已经有若干基于Node.js的稳定可靠的WebSocket实现，我们直接用npm安装使用即可。 注：本篇文章转载于廖雪峰websocket教程 "},"zother4-EasyJob/Git/Git.html":{"url":"zother4-EasyJob/Git/Git.html","title":"Git","keywords":"","body":"Git常用命令备忘 基本配置 git config --global user.name git config --global user.email 创建版本库 mkdir -> pwd -> git init 添加，查看状态，比较git add -> git status -> git diff 提交 git commit -m 'description' 显示历史记录 git log 带参git log --pretty=oneline 版本回退 git reset --hard HEAD~n 版本回退（指定的版本号） git reflog -> git reset --hard 版本号 工作区和暂存区 git add 把文件添加到暂存区 git commit把暂存区的所有内容提交到当前分支上 Git撤销修改和删除文件操作 1.撤销修改 方法： 第一：如果我知道要删掉那些内容的话，直接手动更改去掉那些需要的文件，然后add添加到暂存区，最后commit掉。 第二：我可以按以前的方法直接恢复到上一个版本。使用 git reset –hard HEAD^ 更好的方法： git checkout -- readme.txt丢弃工作区的修改（撤销） 命令 git checkout -– readme.txt 意思就是，把readme.txt文件在工作区做的修改全部撤销，这里有2种情况，如下： 1.readme.txt自动修改后，还没有放到暂存区，使用撤销修改就回到和版本库一模一样的状态。 2.另外一种是readme.txt已经放入暂存区了，接着又作了修改，撤销修改就回到添加暂存区后的状态。 2.删除文件 rm b.txt 接下来：直接commit或者git checkout -- filename撤销 远程仓库 创建SSHKey ssh-keygen -t rsa –C \"932191671@qq.com\" 1.创建远程库 git remote add origin https://github.com/lemongjing/testgit.git 把本地master分支的最新修改推送到 github上 git push -u origin master 2.远程库存在 git clone 创建与合并分支 每次提交，Git都把它们串成一条时间线，这条时间线就是一个分支。在Git里，这个分支叫主分支，即master分支。HEAD严格来说不是指向提交，而是指向master，master才是指向提交的，所以，HEAD指向的就是当前分支。 创建 git branch dev + git checkout dev = git checkout -b dev 创建并切换到dev 合并 在master分支下执行git merge dev然后删除dev git branch -d dev 总结 查看分支：git branch 创建分支：git branch name 切换分支：git checkout name 创建+切换分支：git checkout –b name 合并某分支到当前分支：git merge name 删除分支：git branch –d name 1.如何解决冲突 Git用 >>>>>> 标记出不同分支的内容，其中>>>>fenzhi是指fenzhi上修改的内容，我们可以修改下保存。 2.分支管理策略 通常合并分支时，git一般使用”Fast forward”模式，在这种模式下，删除分支后，会丢掉分支信息，现在我们来使用带参数 –no-ff来禁用”Fast forward”模式。首先我们来做demo演示下： 创建一个dev分支。 修改readme.txt内容。 添加到暂存区。 切换回主分支(master)。 合并dev分支，使用命令 git merge –no-ff -m “注释” dev 查看历史记录 禁用fast-forward模式 git merge –-no-ff -m \"注释\" dev 查看分支日志 git log --graph --pretty=oneline --abbrev-commit 工作现场 暂存工作现场git stash 列出工作现场：git stash list 工作现场还在，Git把stash内容存在某个地方了，但是需要恢复一下，可以使用如下2个方法： 1.git stash apply恢复，恢复后，stash内容并不删除，你需要使用命令git stash drop来删除。 2.另一种方式是使用git stash pop,恢复的同时把stash内容也删除了。 多人协作 创建本地分支与远程分支的链接 git branch --set-upstream-to=origin/ dev 例： git branch --set-upstream dev origin/dev 提交 git push origin dev 拉取 git pull 本地远程分支相关 查看远程分支git branch -a 删除本地分支：git branch -d name 例，删除了本地dev * master remotes/origin/dev remotes/origin/master 删除远程分支（两种方法） git push --delete origin dev git push origin :dev(冒号前面一个空格) "},"zother4-EasyJob/Java/Java序列化.html":{"url":"zother4-EasyJob/Java/Java序列化.html","title":"Java序列化","keywords":"","body":"Java序列化 对于一个存在Java虚拟机中的对象来说，其内部的状态只是保存在内存中。JVM退出之后，内存资源也就被释放，Java对象的内部状态也就丢失了。而在很多情况下，对象内部状态是需要被持久化的，将运行中的对象状态保存下来(最直接的方式就是保存到文件系统中)，在需要的时候可以还原，即使是在Java虚拟机退出的情况下。 对象序列化机制是Java内建的一种对象持久化方式，可以很容易实现在JVM中的活动对象与字节数组(流)之间进行转换，使用得Java对象可以被存储，可以被网络传输，在网络的一端将对象序列化成字节流，经过网络传输到网络的另一端，可以从字节流重新还原为Java虚拟机中的运行状态中的对象。 1.相关的接口 Java类中对象的序列化工作是通过ObjectOutputStream和ObjectInputStream来完成的。 Java代码 ObjectOutputStream(OutputStream out); void writeObject(Object obj);//将指定的对象的非transient，非static属性，写入ObjectOutputStream ObjectInputStream(InputStream in); Object readObject();//从指定的流中读取还原对象信息 只能使用readObject()|writeObject()方法对对象进行读写操作。除对象之外，Java中的基本类型和数组也可以被序列化,对于基本类型，可以使用readInt(),writeInt(), readDouble(),writeDouble()等类似的接口进行读写。 2.Serializable接口 对于任何需要被序列化的对象，都必须要实现接口Serializable,它只是一个标识接口，本身没有任何成员，只是用来标识说明当前的实现类的对象可以被序列化. 3.transient关键字 如果在类中的一些属性，希望在对象序列化过程中不被序列化，使用关键字transient标注修饰就可以.当对象被序列化时，标注为transient的成员属性将会自动跳过。 4.Java序列化中需要注意 (1).当一个对象被序列化时，只保存对象的非静态成员变量，不能保存任何的成员方法,静态的成员变量和transient标注的成员变量。 (2).如果一个对象的成员变量是一个对象，那么这个对象的数据成员也会被保存还原，而且会是递归的方式。 (3).如果一个可序列化的对象包含对某个不可序列化的对象的引用，那么整个序列化操作将会失败，并且会抛出一个NotSerializableException。可以将这个引用标记transient,那么对象仍然可以序列化。 5.一个综合实例 class Student implements Serializable{ private String name; private transient int age; private Course course; public void setCourse(Course course){ this.course = course; } public Course getCourse(){ return course; } public Student(String name, int age){ this.name = name; this.age = age; } public String toString(){ return \"Student Object name:\"+this.name+\" age:\"+this.age; } } class Course implements Serializable{ private static String courseName; private int credit; public Course(String courseName, int credit){ this.courseName = courseName; this.credit = credit; } public String toString(){ return \"Course Object courseName:\"+courseName +\" credit:\"+credit; } } 将对象写入文件，序列化 public class TestWriteObject{ public static void main(String[] args) { String filePath = \"C://obj.txt\"; ObjectOutputStream objOutput = null; Course c1 = new Course(\"C language\", 3); Course c2 = new Course(\"OS\", 4); Student s1 = new Student(\"king\", 25); s1.setCourse(c1); Student s2 = new Student(\"jason\", 23); s2.setCourse(c2); try { objOutput = new ObjectOutputStream(new FileOutputStream(filePath)); objOutput.writeObject(s1); objOutput.writeObject(s2); objOutput.writeInt(123); } catch (FileNotFoundException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); }finally{ try { objOutput.close(); } catch (IOException e) { e.printStackTrace(); } } System.out.println(\"Info:对象被写入\"+filePath); } 从文件中读取对象，反序列化 public class TestReadObject { public static void main(String[] args) { String filePath = \"C://obj.txt\"; ObjectInputStream objInput = null; Student s1 = null,s2 = null; int intVal = 0; try { objInput = new ObjectInputStream(new FileInputStream(filePath)); s1 = (Student)objInput.readObject(); s2 = (Student)objInput.readObject(); intVal = objInput.readInt(); } catch (FileNotFoundException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); }catch(ClassNotFoundException cnfe){ cnfe.printStackTrace(); }finally{ try { objInput.close(); } catch (IOException e) { e.printStackTrace(); } } System.out.println(\"Info:文件\"+filePath+\"中读取对象\"); System.out.println(s1); System.out.println(s1.getCourse()); System.out.println(s2); System.out.println(s2.getCourse()); System.out.println(intVal); } } 输出: [TestWriteObject] Info:对象被写入C://obj.txt [TestReadObjec] Info:文件C://obj.txt中读取对象 Info:文件C://obj.txt中读取对象 Student Object name:king age:0 Course Object courseName:null credit:3 Student Object name:jason age:0 Course Object courseName:null credit:4 123 可知Person中的age属性被标注为transient后，在序列化对象时，age属性就没有被序列化了; Course中的name属性被static后，Course的name静态属性就没有被序列化;虽然是序列化Person对象，但是Person所引用的Course对象也被初始化了。 "},"zother4-EasyJob/Java/Java泛型.html":{"url":"zother4-EasyJob/Java/Java泛型.html","title":"Java泛型","keywords":"","body":"泛型概念的提出（为什么需要泛型）？ 首先，我们看下下面这段简短的代码: public class GenericTest { public static void main(String[] args) { List list = new ArrayList(); list.add(\"qqyumidi\"); list.add(\"corn\"); list.add(100); for (int i = 0; i 定义了一个List类型的集合，先向其中加入了两个字符串类型的值，随后加入一个Integer类型的值。这是完全允许的，因为此时list默认的类型为Object类型。在之后的循环中，由于忘记了之前在list中也加入了Integer类型的值或其他编码原因，很容易出现类似于//1中的错误。因为编译阶段正常，而运行时会出现“java.lang.ClassCastException”异常。因此，导致此类错误编码过程中不易发现。 在如上的编码过程中，我们发现主要存在两个问题： 1.当我们将一个对象放入集合中，集合不会记住此对象的类型，当再次从集合中取出此对象时，改对象的编译类型变成了Object类型，但其运行时类型任然为其本身类型。 2.因此，//1处取出集合元素时需要人为的强制类型转化到具体的目标类型，且很容易出现“java.lang.ClassCastException”异常。 那么有没有什么办法可以使集合能够记住集合内元素各类型，且能够达到只要编译时不出现问题，运行时就不会出现“java.lang.ClassCastException”异常呢？答案就是使用泛型。 什么是泛型？ 泛型，即“参数化类型”。一提到参数，最熟悉的就是定义方法时有形参，然后调用此方法时传递实参。那么参数化类型怎么理解呢？顾名思义，就是将类型由原来的具体的类型参数化，类似于方法中的变量参数，此时类型也定义成参数形式（可以称之为类型形参），然后在使用/调用时传入具体的类型（类型实参）。 看着好像有点复杂，首先我们看下上面那个例子采用泛型的写法。 public class GenericTest { public static void main(String[] args) { /* List list = new ArrayList(); list.add(\"qqyumidi\"); list.add(\"corn\"); list.add(100); */ List list = new ArrayList(); list.add(\"qqyumidi\"); list.add(\"corn\"); //list.add(100); // 1 提示编译错误 for (int i = 0; i 采用泛型写法后，在//1处想加入一个Integer类型的对象时会出现编译错误，通过List，直接限定了list集合中只能含有String类型的元素，从而在//2处无须进行强制类型转换，因为此时，集合能够记住元素的类型信息，编译器已经能够确认它是String类型了。 结合上面的泛型定义，我们知道在List中，String是类型实参，也就是说，相应的List接口中肯定含有类型形参。且get()方法的返回结果也直接是此形参类型（也就是对应的传入的类型实参）。下面就来看看List接口的的具体定义： public interface List extends Collection { int size(); boolean isEmpty(); boolean contains(Object o); Iterator iterator(); Object[] toArray(); T[] toArray(T[] a); boolean add(E e); boolean remove(Object o); boolean containsAll(Collection c); boolean addAll(Collection c); boolean addAll(int index, Collection c); boolean removeAll(Collection c); boolean retainAll(Collection c); void clear(); boolean equals(Object o); int hashCode(); E get(int index); E set(int index, E element); void add(int index, E element); E remove(int index); int indexOf(Object o); int lastIndexOf(Object o); ListIterator listIterator(); ListIterator listIterator(int index); List subList(int fromIndex, int toIndex); } 我们可以看到，在List接口中采用泛型化定义之后，中的E表示类型形参，可以接收具体的类型实参，并且此接口定义中，凡是出现E的地方均表示相同的接受自外部的类型实参。 自然的，ArrayList作为List接口的实现类，其定义形式是： public class ArrayList extends AbstractList implements List, RandomAccess, Cloneable, java.io.Serializable { public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true; } public E get(int index) { rangeCheck(index); checkForComodification(); return ArrayList.this.elementData(offset + index); } //...省略掉其他具体的定义过程 } 由此，我们从源代码角度明白了为什么//1处加入Integer类型对象编译错误，且//2处get()到的类型直接就是String类型了。 自定义泛型接口、泛型类和泛型方法 从上面的内容中，大家已经明白了泛型的具体运作过程。也知道了接口、类和方法也都可以使用泛型去定义，以及相应的使用。是的，在具体使用时，可以分为泛型接口、泛型类和泛型方法。 自定义泛型接口、泛型类和泛型方法与上述Java源码中的List、ArrayList类似。如下，我们看一个最简单的泛型类和方法定义： public class GenericTest { public static void main(String[] args) { Box name = new Box(\"corn\"); System.out.println(\"name:\" + name.getData()); } } class Box { private T data; public Box() { } public Box(T data) { this.data = data; } public T getData() { return data; } } 在泛型接口、泛型类和泛型方法的定义过程中，我们常见的如T、E、K、V等形式的参数常用于表示泛型形参，由于接收来自外部使用时候传入的类型实参。那么对于不同传入的类型实参，生成的相应对象实例的类型是不是一样的呢？ public class GenericTest { public static void main(String[] args) { Box name = new Box(\"corn\"); Box age = new Box(712); System.out.println(\"name class:\" + name.getClass()); // com.qqyumidi.Box System.out.println(\"age class:\" + age.getClass()); // com.qqyumidi.Box System.out.println(name.getClass() == age.getClass()); // true } } 由此，我们发现，在使用泛型类时，虽然传入了不同的泛型实参，但并没有真正意义上生成不同的类型，传入不同泛型实参的泛型类在内存上只有一个，即还是原来的最基本的类型（本实例中为Box），当然，在逻辑上我们可以理解成多个不同的泛型类型。 究其原因，在于Java中的泛型这一概念提出的目的，导致其只是作用于代码编译阶段，在编译过程中，对于正确检验泛型结果后，会将泛型的相关信息擦出，也就是说，成功编译过后的class文件中是不包含任何泛型信息的。泛型信息不会进入到运行时阶段。 对此总结成一句话：泛型类型在逻辑上看以看成是多个不同的类型，实际上都是相同的基本类型。 类型通配符 接着上面的结论，我们知道，Box和Box实际上都是Box类型，现在需要继续探讨一个问题，那么在逻辑上，类似于Box和Box是否可以看成具有父子关系的泛型类型呢？ 为了弄清这个问题，我们继续看下下面这个例子: public class GenericTest { public static void main(String[] args) { Box name = new Box(99); Box age = new Box(712); getData(name); //The method getData(Box) in the type GenericTest is //not applicable for the arguments (Box) getData(age); // 1 } public static void getData(Box data){ System.out.println(\"data :\" + data.getData()); } } 我们发现，在代码//1处出现了错误提示信息：The method getData(Box) in the t ype GenericTest is not applicable for the arguments (Box)。显然，通过提示信息，我们知道Box在逻辑上不能视为Box的父类。那么，原因何在呢？ public class GenericTest { public static void main(String[] args) { Box a = new Box(712); Box b = a; // 1 Box f = new Box(3.14f); b.setData(f); // 2 } public static void getData(Box data) { System.out.println(\"data :\" + data.getData()); } } class Box { private T data; public Box() { } public Box(T data) { setData(data); } public T getData() { return data; } public void setData(T data) { this.data = data; } } 这个例子中，显然//1和//2处肯定会出现错误提示的。在此我们可以使用反证法来进行说明。 假设Box在逻辑上可以视为Box的父类，那么//1和//2处将不会有错误提示了，那么问题就出来了，通过getData()方法取出数据时到底是什么类型呢？Integer? Float? 还是Number？且由于在编程过程中的顺序不可控性，导致在必要的时候必须要进行类型判断，且进行强制类型转换。显然，这与泛型的理念矛盾，因此，在逻辑上Box不能视为Box的父类。 好，那我们回过头来继续看“类型通配符”中的第一个例子，我们知道其具体的错误提示的深层次原因了。那么如何解决呢？总部能再定义一个新的函数吧。这和Java中的多态理念显然是违背的，因此，我们需要一个在逻辑上可以用来表示同时是Box和Box的父类的一个引用类型，由此，类型通配符应运而生。 类型通配符一般是使用 ? 代替具体的类型实参。注意了，此处是类型实参，而不是类型形参！且Box在逻辑上是Box、Box...等所有Box的父类。由此，我们依然可以定义泛型方法，来完成此类需求。 public class GenericTest { public static void main(String[] args) { Box name = new Box(\"corn\"); Box age = new Box(712); Box number = new Box(314); getData(name); getData(age); getData(number); } public static void getData(Box data) { System.out.println(\"data :\" + data.getData()); } } 有时候，我们还可能听到类型通配符上限和类型通配符下限。具体有是怎么样的呢？ 在上面的例子中，如果需要定义一个功能类似于getData()的方法，但对类型实参又有进一步的限制：只能是Number类及其子类。此时，需要用到类型通配符上限。 public class GenericTest { public static void main(String[] args) { Box name = new Box(\"corn\"); Box age = new Box(712); Box number = new Box(314); getData(name); getData(age); getData(number); //getUpperNumberData(name); // 1 getUpperNumberData(age); // 2 getUpperNumberData(number); // 3 } public static void getData(Box data) { System.out.println(\"data :\" + data.getData()); } public static void getUpperNumberData(Box data){ System.out.println(\"data :\" + data.getData()); } } 此时，显然，在代码//1处调用将出现错误提示，而//2 //3处调用正常。 类型通配符上限通过形如Box形式定义，相对应的，类型通配符下限为Box形式，其含义与类型通配符上限正好相反，在此不作过多阐述了。 话外篇 本文中的例子主要是为了阐述泛型中的一些思想而简单举出的，并不一定有着实际的可用性。另外，一提到泛型，相信大家用到最多的就是在集合中，其实，在实际的编程过程中，自己可以使用泛型去简化开发，且能很好的保证代码质量。并且还要注意的一点是，Java中没有所谓的泛型数组一说。 对于泛型，最主要的还是需要理解其背后的思想和目的。 "},"zother4-EasyJob/Java/Java注解之注解处理器.html":{"url":"zother4-EasyJob/Java/Java注解之注解处理器.html","title":"Java注解之注解处理器","keywords":"","body":"深入理解Java注解 － 注解处理器 如果没有用来读取注解的方法和工作，那么注解也就不会比注释更有用处了。使用注解的过程中，很重要的一部分就是创建于使用注解处理器。Java5扩展了反射机制的API，以帮助程序员快速的构造自定义注解处理器。 注解处理器类库(java.lang.reflect.AnnotatedElement)： Java使用Annotation接口来代表程序元素前面的注解，该接口是所有Annotation类型的父接口。除此之外，Java在java.lang.reflect 包下新增了AnnotatedElement接口，该接口代表程序中可以接受注解的程序元素，该接口主要有如下几个实现类： Class：类定义 Constructor：构造器定义 Field：类的成员变量定义 Method：类的方法定义 Package：类的包定义 java.lang.reflect 包下主要包含一些实现反射功能的工具类，实际上，java.lang.reflect 包所有提供的反射API扩充了读取运行时Annotation信息的能力。当一个Annotation类型被定义为运行时的Annotation后，该注解才能是运行时可见，当class文件被装载时被保存在class文件中的Annotation才会被虚拟机读取。 AnnotatedElement 接口是所有程序元素（Class、Method和Constructor）的父接口，所以程序通过反射获取了某个类的AnnotatedElement对象之后，程序就可以调用该对象的如下四个个方法来访问Annotation信息： 方法1： T getAnnotation(Class annotationClass):返回改程序元素上存在的、指定类型的注解，如果该类型注解不存在，则返回null。 方法2：Annotation[] getAnnotations():返回该程序元素上存在的所有注解。 　　 方法3：boolean is AnnotationPresent(Class annotationClass):判断该程序元素上是否包含指定类型的注解，存在则返回true，否则返回false. 　　 方法4：Annotation[] getDeclaredAnnotations()：返回直接存在于此元素上的所有注释。与此接口中的其他方法不同，该方法将忽略继承的注解。（如果没有注解直接存在于此元素上，则返回长度为零的一个数组。）该方法的调用者可以随意修 改返回的数组；这不会对其他调用者返回的数组产生任何影响。 一个简单的注解处理器：　　 /***********注解声明***************/ /** * 水果名称注解 * @author taoxiaoran * */ @Target(ElementType.FIELD) @Retention(RetentionPolicy.RUNTIME) @Documented public @interface FruitName { String value() default \"\"; } /** * 水果颜色注解 * @author peida * */ @Target(ElementType.FIELD) @Retention(RetentionPolicy.RUNTIME) @Documented public @interface FruitColor { /** * 颜色枚举 * @author peida * */ public enum Color{ BULE,RED,GREEN}; /** * 颜色属性 * @return */ Color fruitColor() default Color.GREEN; } /** * 水果供应者注解 * @author peida * */ @Target(ElementType.FIELD) @Retention(RetentionPolicy.RUNTIME) @Documented public @interface FruitProvider { /** * 供应商编号 * @return */ public int id() default -1; /** * 供应商名称 * @return */ public String name() default \"\"; /** * 供应商地址 * @return */ public String address() default \"\"; } /***********注解使用***************/ public class Apple { @FruitName(\"Apple\") private String appleName; @FruitColor(fruitColor=Color.RED) private String appleColor; @FruitProvider(id=1,name=\"陕西红富士集团\",address=\"陕西省西安市延安路89号红富士大厦\") private String appleProvider; public void setAppleColor(String appleColor) { this.appleColor = appleColor; } public String getAppleColor() { return appleColor; } public void setAppleName(String appleName) { this.appleName = appleName; } public String getAppleName() { return appleName; } public void setAppleProvider(String appleProvider) { this.appleProvider = appleProvider; } public String getAppleProvider() { return appleProvider; } public void displayName(){ System.out.println(\"水果的名字是：苹果\"); } } /***********注解处理器***************/ public class FruitInfoUtil { public static void getFruitInfo(Class clazz){ String strFruitName=\" 水果名称：\"; String strFruitColor=\" 水果颜色：\"; String strFruitProvicer=\"供应商信息：\"; Field[] fields = clazz.getDeclaredFields(); for(Field field :fields){ if(field.isAnnotationPresent(FruitName.class)) { FruitName fruitName = (FruitName) field.getAnnotation(FruitName.class); strFruitName=strFruitName+fruitName.value(); System.out.println(strFruitName); } else if(field.isAnnotationPresent(FruitColor.class)) { FruitColor fruitColor= (FruitColor) field.getAnnotation(FruitColor.class); strFruitColor=strFruitColor+fruitColor.fruitColor().toString(); System.out.println(strFruitColor); } else if(field.isAnnotationPresent(FruitProvider.class)) { FruitProvider fruitProvider= (FruitProvider) field.getAnnotation(FruitProvider.class); strFruitProvicer=\" 供应商编号：\"+fruitProvider.id()+\" 供应商名称：\"+fruitProvider.name()+\" 供应商地址：\"+fruitProvider.address(); System.out.println(strFruitProvicer); } } } } /***********输出结果***************/ public class FruitRun { /** * @param args */ public static void main(String[] args) { FruitInfoUtil.getFruitInfo(Apple.class); } } ==================================== 水果名称：Apple 水果颜色：RED 供应商编号：1 供应商名称：陕西红富士集团 供应商地址：陕西省西安市延安路89号红富士大厦 Java注解的基础知识点导图 本文转自博客园 地址http://www.cnblogs.com/peida 本人略有修改。2015-11-2 "},"zother4-EasyJob/Java/Java注解入门.html":{"url":"zother4-EasyJob/Java/Java注解入门.html","title":"Java注解入门","keywords":"","body":"深入理解Java注解 － 注解入门 要深入学习注解，我们就必须能定义自己的注解，并使用注解，在定义自己的注解之前，我们就必须要了解Java为我们提供的元注解和相关定义注解的语法。 Java提供的元注解 Java5.0定义了4个标准的元注解：@Target @Retention @Documented @Inherited 1.@Target注解 作用：用于描述注解的使用范围（即：被描述的注解可以用在什么地方) @Target说明了Annotation所修饰的对象范围：Annotation可被用于 packages、types（类、接口、枚举、Annotation类型）、类型成员（方法、构造方法、成员变量、枚举值）、方法参数和本地变量（如循环变量、catch参数）。在Annotation类型的声明中使用了target可更加明晰其修饰的目标。 取值(ElementType)有： CONSTRUCTOR:用于描述构造器 FIELD:用于描述域 LOCAL_VARIABLE:用于描述局部变量 METHOD:用于描述方法 PACKAGE:用于描述包 PARAMETER:用于描述参数 TYPE:用于描述类、接口(包括注解类型) 或enum声明 使用事例 @Target(ElementType.TYPE) public @interface Table { /** * 数据表名称注解，默认值为类名称 * @return */ public String tableName() default \"className\"; } @Target(ElementType.FIELD) public @interface NoDBColumn { } 注解Table 可以用于注解类、接口(包括注解类型) 或enum声明,而注解NoDBColumn仅可用于注解类的成员变量。 2.@Retention注解 作用：表示需要在什么级别保存该注解信息，用于描述注解的生命周期（即：被描述的注解在什么范围内有效） @Retention定义了该Annotation被保留的时间长短：某些Annotation仅出现在源代码中，而被编译器丢弃；而另一些却被编译在class文件中；编译在class文件中的Annotation可能会被虚拟机忽略，而另一些在class被装载时将被读取（请注意并不影响class的执行，因为Annotation与class在使用上是被分离的）。使用这个meta-Annotation可以对Annotation的“生命周期”限制。 取值（RetentionPolicy）有： SOURCE:在源文件中有效（即源文件保留） CLASS:在class文件中有效（即class保留） RUNTIME:在运行时有效（即运行时保留） Retention meta-annotation类型有唯一的value作为成员，它的取值来自java.lang.annotation.RetentionPolicy的枚举类型值。 使用事例 @Target(ElementType.FIELD) @Retention(RetentionPolicy.RUNTIME) public @interface Column { public String name() default \"fieldName\"; public String setFuncName() default \"setField\"; public String getFuncName() default \"getField\"; public boolean defaultDBValue() default false; } Column注解的的RetentionPolicy的属性值是RUTIME,这样注解处理器可以通过反射，获取到该注解的属性值，从而去做一些运行时的逻辑处理 3.@Documented注解　 @Documented用于描述其它类型的annotation应该被作为被标注的程序成员的公共API，因此可以被例如javadoc此类的工具文档化。Documented是一个标记注解，没有成员。 使用事例 @Target(ElementType.FIELD) @Retention(RetentionPolicy.RUNTIME) @Documented public @interface Column { public String name() default \"fieldName\"; public String setFuncName() default \"setField\"; public String getFuncName() default \"getField\"; public boolean defaultDBValue() default false; } 4.@Inherited注解 @Inherited元注解是一个标记注解，@Inherited阐述了某个被标注的类型是被继承的。如果一个使用了@Inherited修饰的annotation类型被用于一个class，则这个annotation将被用于该class的子类。 注意：@Inherited annotation类型是被标注过的class的子类所继承。类并不从它所实现的接口继承annotation，方法并不从它所重载的方法继承annotation。 当@Inherited annotation类型标注的annotation的Retention是RetentionPolicy.RUNTIME，则反射API增强了这种继承性。如果我们使用java.lang.reflect去查询一个@Inherited annotation类型的annotation时，反射代码检查将展开工作：检查class和其父类，直到发现指定的annotation类型被发现，或者到达类继承结构的顶层。 使用事例 /** * * @author taoxiaoran * */ @Inherited public @interface Greeting { public enum FontColor{ BULE,RED,GREEN}; String name(); FontColor fontColor() default FontColor.GREEN; } 自定义注解 使用@interface自定义注解时，自动继承了java.lang.annotation.Annotation接口，由编译程序自动完成其他细节。在定义注解时，不能继承其他的注解或接口。@interface用来声明一个注解，其中的每一个方法实际上是声明了一个配置参数。方法的名称就是参数的名称，返回值类型就是参数的类型（返回值类型只能是基本类型、Class、String、enum）。可以通过default来声明参数的默认值。 定义注解格式： public @interface 注解名 {定义体} 注解参数的可支持数据类型： 所有基本数据类型（int,float,boolean,byte,double,char,long,short) String类型 Class类型 enum类型 Annotation类型 以上所有类型的数组 Annotation类型里面的参数该怎么设定: 只能用public或默认(default)这两个访问权修饰.例如,String value();这里把方法设为defaul默认类型；　 　 参数成员只能用基本类型byte,short,char,int,long,float,double,boolean八种基本数据类型和 String,Enum,Class,annotations等数据类型,以及这一些类型的数组.例如,String value();这里的参数成员就为String;　　 如果只有一个参数成员,最好把参数名称设为\"value\",后加小括号.例:下面的例子FruitName注解就只有一个参数成员。 简单的自定义注解和使用注解实例： package annotation; import java.lang.annotation.Documented; import java.lang.annotation.ElementType; import java.lang.annotation.Retention; import java.lang.annotation.RetentionPolicy; import java.lang.annotation.Target; /** * 水果名称注解 * @author taoxiaoran * */ @Target(ElementType.FIELD) @Retention(RetentionPolicy.RUNTIME) @Documented public @interface FruitName { String value() default \"\"; } package annotation; import java.lang.annotation.Documented; import java.lang.annotation.ElementType; import java.lang.annotation.Retention; import java.lang.annotation.RetentionPolicy; import java.lang.annotation.Target; /** * 水果颜色注解 * @author taoxiaoran * */ @Target(ElementType.FIELD) @Retention(RetentionPolicy.RUNTIME) @Documented public @interface FruitColor { /** * 颜色枚举 * @author taoxiaoran * */ public enum Color{ BULE,RED,GREEN}; /** * 颜色属性 * @return */ Color fruitColor() default Color.GREEN; } package annotation; import annotation.FruitColor.Color; public class Apple { @FruitName(\"Apple\") private String appleName; @FruitColor(fruitColor=Color.RED) private String appleColor; public void setAppleColor(String appleColor) { this.appleColor = appleColor; } public String getAppleColor() { return appleColor; } public void setAppleName(String appleName) { this.appleName = appleName; } public String getAppleName() { return appleName; } public void displayName(){ System.out.println(\"水果的名字是：苹果\"); } } 注解元素的默认值 注解元素必须有确定的值，要么在定义注解的默认值中指定，要么在使用注解时指定，非基本类型的注解元素的值不可为null。因此, 使用空字符串或0作为默认值是一种常用的做法。这个约束使得处理器很难表现一个元素的存在或缺失的状态，因为每个注解的声明中，所有元素都存在，并且都具有相应的值，为了绕开这个约束，我们只能定义一些特殊的值，例如空字符串或者负数，以此表示某个元素不存在，在定义注解时，这已经成为一个习惯用法。例如： package annotation; import java.lang.annotation.Documented; import java.lang.annotation.ElementType; import java.lang.annotation.Retention; import java.lang.annotation.RetentionPolicy; import java.lang.annotation.Target; /** * 水果供应者注解 * @author taoxiaoran * */ @Target(ElementType.FIELD) @Retention(RetentionPolicy.RUNTIME) @Documented public @interface FruitProvider { /** * 供应商编号 * @return */ public int id() default -1; /** * 供应商名称 * @return */ public String name() default \"\"; /** * 供应商地址 * @return */ public String address() default \"\"; } 定义了注解，并在需要的时候给相关类，类属性加上注解信息，如果没有响应的注解信息处理流程，注解可以说是没有实用价值。如何让注解真真的发挥作用，主要就在于注解处理方法，下一步我们将学习注解信息的获取和处理！ 本文转自博客园 地址http://www.cnblogs.com/peida 本人略有修改。2015-11-2 "},"zother4-EasyJob/Java/Java移位符.html":{"url":"zother4-EasyJob/Java/Java移位符.html","title":"Java移位符","keywords":"","body":"Java移位符 java中有三种移位运算符 >> :带符号右移,x >> 1,相当于x除以2,正数高位补0,负数高位补1 >>> :无符号右移,忽略符号位,空位都以0补齐 "},"zother4-EasyJob/Java/Java类加载器.html":{"url":"zother4-EasyJob/Java/Java类加载器.html","title":"Java类加载器","keywords":"","body":"类装载器ClassLoader 类装载器工作机制 类装载器就是寻找类的节码文件并构造出类在JVM内部表示对象的组件。在Java中，类装载器把一个类装入JVM中，要经过以下步骤： [1.]装载：查找和导入Class文件； [2.]链接：执行校验、准备和解析步骤，其中解析步骤是可以选择的： [2.1]校验：检查载入Class文件数据的正确性； [2.2]准备：给类的静态变量分配存储空间； [2.3]解析：将符号引用转成直接引用； [3.]初始化：对类的静态变量、静态代码块执行初始化工作。 类装载工作由ClassLoader及其子类负责，ClassLoader是一个重要的Java运行时系统组件，它负责在运行时查找和装入Class字节码文件。JVM在运行时会产生三个ClassLoader：根装载器、ExtClassLoader（扩展类装载器）和AppClassLoader（系统类装载器）。其中，根装载器不是ClassLoader的子类，它使用C++编写，因此我们在Java中看不到它，根装载器负责装载JRE的核心类库，如JRE目标下的rt.jar、charsets.jar等。ExtClassLoader和AppClassLoader都是ClassLoader的子类。其中ExtClassLoader负责装载JRE扩展目录ext中的JAR类包；AppClassLoader负责装载Classpath路径下的类包。 这三个类装载器之间存在父子层级关系，即根装载器是ExtClassLoader的父装载器，ExtClassLoader是AppClassLoader的父装载器。默认情况下，使用AppClassLoader装载应用程序的类，我们可以做一个实验： public class ClassLoaderTest { public static void main(String[] args) { ClassLoader loader = Thread.currentThread().getContextClassLoader(); System.out.println(\"current loader:\"+loader); System.out.println(\"parent loader:\"+loader.getParent()); System.out.println(\"grandparent loader:\"+loader.getParent(). getParent()); } } 运行以上代码，在控制台上将打出以下信息： current loader:sun.misc.Launcher$AppClassLoader@131f71a parent loader:sun.misc.Launcher$ExtClassLoader@15601ea //①根装载器在Java中访问不到，所以返回null grandparent loader:null 通过以上的输出信息，我们知道当前的ClassLoader是AppClassLoader，父ClassLoader是ExtClassLoader，祖父ClassLoader是根类装载器，因为在Java中无法获得它的句柄，所以仅返回null。 JVM装载类时使用“全盘负责委托机制”，“全盘负责”是指当一个ClassLoader装载一个类的时，除非显式地使用另一个ClassLoader，该类所依赖及引用的类也由这个ClassLoader载入；“委托机制”是指先委托父装载器寻找目标类，只有在找不到的情况下才从自己的类路径中查找并装载目标类。这一点是从安全角度考虑的，试想如果有人编写了一个恶意的基础类（如java.lang.String）并装载到JVM中将会引起多么可怕的后果。但是由于有了“全盘负责委托机制”，java.lang.String永远是由根装载器来装载的，这样就避免了上述事件的发生。 ClassLoader重要方法 在Java中，ClassLoader是一个抽象类，位于java.lang包中。下面对该类的一些重要接口方法进行介绍： Class loadClass(String name) name参数指定类装载器需要装载类的名字，必须使用全限定类名，如com.baobaotao. beans.Car。该方法有一个重载方法loadClass(String name ,boolean resolve)，resolve参数告诉类装载器是否需要解析该类。在初始化类之前，应考虑进行类解析的工作，但并不是所有的类都需要解析，如果JVM只需要知道该类是否存在或找出该类的超类，那么就不需要进行解析。 Class defineClass(String name, byte[] b, int off, int len) 将类文件的字节数组转换成JVM内部的java.lang.Class对象。字节数组可以从本地文件系统、远程网络获取。name为字节数组对应的全限定类名。 Class findSystemClass(String name) 从本地文件系统载入Class文件，如果本地文件系统不存在该Class文件，将抛出ClassNotFoundException异常。该方法是JVM默认使用的装载机制。 Class findLoadedClass(String name) 调用该方法来查看ClassLoader是否已装入某个类。如果已装入，那么返回java.lang.Class对象，否则返回null。如果强行装载已存在的类，将会抛出链接错误。 ClassLoader getParent() 获取类装载器的父装载器，除根装载器外，所有的类装载器都有且仅有一个父装载器，ExtClassLoader的父装载器是根装载器，因为根装载器非Java编写，所以无法获得，将返回null。 除JVM默认的三个ClassLoader以外，可以编写自己的第三方类装载器，以实现一些特殊的需求。类文件被装载并解析后，在JVM内将拥有一个对应的java.lang.Class类描述对象，该类的实例都拥有指向这个类描述对象的引用，而类描述对象又拥有指向关联ClassLoader的引用，如图所示。 每一个类在JVM中都拥有一个对应的java.lang.Class对象，它提供了类结构信息的描述。数组、枚举、注解以及基本Java类型（如int、double等），甚至void都拥有对应的Class对象。Class没有public的构造方法。Class对象是在装载类时由JVM通过调用类装载器中的defineClass()方法自动构造的。 类的初始化 类什么时候才被初始化 创建类的实例，也就是new一个对象 访问某个类或接口的静态变量，或者对该静态变量赋值 调用类的静态方法 反射（Class.forName(\"com.lyj.load\")） 初始化一个类的子类（会首先初始化子类的父类） JVM启动时标明的启动类，即文件名和类名相同的那个类 只有这6中情况才会导致类的类的初始化。 类的初始化步骤： 如果这个类还没有被加载和链接，那先进行加载和链接 假如这个类存在直接父类，并且这个类还没有被初始化（注意：在一个类加载器中，类只能初始化一次），那就初始化直接的父类（不适用于接口） 加入类中存在初始化语句（如static变量和static块），那就依次执行这些初始化语句。 "},"zother4-EasyJob/Java/Json.html":{"url":"zother4-EasyJob/Java/Json.html","title":"Json","keywords":"","body":"Json解析工具Jackson的注解 @JsonIgnoreProperties 此注解是类注解，作用是json序列化时将Java bean中的一些属性忽略掉，序列化和反序列化都受影响。 @JsonIgnore 此注解用于属性或者方法上（最好是属性上），作用和上面的@JsonIgnoreProperties一样。 @JsonFormat 此注解用于属性或者方法上（最好是属性上），可以方便的把Date类型直接转化为我们想要的模式，比如@JsonFormat(pattern = \"yyyy-MM-dd HH-mm-ss\") @JsonProperty 此注解用于属性上，作用是把该属性的名称序列化为另外一个名称，如把trueName属性序列化为name，@JsonProperty(\"name\")。 @JsonSerialize 此注解用于属性或者getter方法上，用于在序列化时嵌入我们自定义的代码，比如序列化一个double时在其后面限制两位小数点。 @JsonDeserialize 此注解用于属性或者setter方法上，用于在反序列化时可以嵌入我们自定义的代码，类似于上面的@JsonSerialize @JsonInclude(Include.NON_NULL) //将该标记放在属性上，如果该属性为NULL则不参与序列化 //如果放在类上边,那对这个类的全部属性起作用 其他可选项 //Include.ALWAYS 默认 //Include.NON_DEFAULT 属性为默认值不序列化 //Include.NON_EMPTY 属性为 空（“”） 或者为 NULL 都不序列化 //Include.NON_NULL 属性为NULL 不序列化 "},"zother4-EasyJob/Java/JVM.html":{"url":"zother4-EasyJob/Java/JVM.html","title":"JVM","keywords":"","body":"Java虚拟机 垃圾收集器 young区 1.Serial 关注点：stw的时间 复制算法 STW 2.ParNew 关注点：stw的时间 Serial的多线程版本 复制算法 STW 参数 -XX:ParallelGCThreads 并行线程数 3.Parallel Scavenge 关注点：达到一个可控制的吞吐量（吞吐量优先） 复制算法 所谓吞吐量就是CPU运行用户代码的时间和CPU消耗的时间的比值，即吞吐量=运行用户代码时间/(运行用户代码时间+垃圾收集时间)。 Paralled Scavenge 收集器提供了两个参数用于精确控制吞吐量。分别是控制最大垃圾收集停顿时间的-XX：MaxGCPauseMillis ,以及直接设置吞吐量大小的-XX：GCTimeRatio 参数。 参数 -XX:MaxGCPauseMillis -XX:GCTimeRatio -XX:+UseAdaptiveSizePolicy 自适应调节策略 old区 1.Serial Old Serial的老年代版本 JDK1.5之前可搭配Parallel Scavenge使用 在CMS发生Concurrent mode failure时使用 Mark-Compact 2.Parallel Old Parallel Scavenge的老年代版本 JDK1.6出现 搭配Parallel Scavenge使用 Mark-Compact 3.CMS -XX:+UseConcMarkSweepGC Mark-Sweep 过程：初始标记 并发标记 重新标记 并发清除 初始标记和重新标记会STW 耗时最长是并发标记和并发清除 默认回收线程数：CMS默认启动的回收线程数目是 (ParallelGCThreads + 3)/4)，可以通过-XX:ParallelCMSThreads=20来设定 优点：并发低停顿 缺点： 1、对CPU资源非常敏感 占用部分线程（CPU资源）导致应用程序变慢，吞吐量降低 默认线程数=（CPU数量+3）/4 2、无法处理浮动垃圾，可能出现Concurrent mode failure而导致另一次fullGC的产生 参数-XX:CMSInitiatingOccupancyFraction默认68% CMS触发百分比，CMS执行期间，预留空间无法满足，就会出现Concurrent mode failure失败，启动后备SerialOld的方案，停顿时间更长了。（所以CMSInitiatingOccupancyFraction不能设置的过大） 3、内存碎片 参数-XX:+UseCMSCompactAtFullCollection(停顿时间加长) 默认开启 -XX:CMSFullGCsBeforeCompaction(执行多少次不进行碎片整理的FullGC后进行一次带压缩的) 其他参数 初始标记的并行化-XX:+CMSParallelInitialMarkEnabled 为了减少第二次暂停的时间，开启并行remark: -XX:+CMSParallelRemarkEnabled,如果remark还是过长的话，可以开启-XX:+CMSScavengeBeforeRemark（在CMS GC前启动一次ygc，目的在于减少old gen对ygc gen的引用，降低remark时的开销-----一般CMS的GC耗时 80%都在remark阶段） 为了避免Perm区满引起的full gc，建议开启CMS回收Perm区选项： +CMSPermGenSweepingEnabled -XX:+CMSClassUnloadingEnabled -XX:+UseConcMarkSweepGC：设置年老代为并发收集。测试中配置这个以后，-XX:NewRatio=4的配置失效了，原因不明。所以，此时年轻代大小最好用-Xmn设置。 -XX:+UseParNewGC:设置年轻代为并行收集。可与CMS收集同时使用。JDK5.0以上，JVM会根据系统配置自行设置，所以无需再设置此值。 G1收集器 收集器组合开关选项 -XX:+UseSerialGC Serial+SerialOld -XX:+UseParNewGC ParNew+SerialOld -XX:+UseParallelGC ParallelScavenge+SerialOld -XX:+UseConcMarkSweepGC ParNew+CMS+SerialOld FullGC算法：单线程的Mark Sweep Compact(MSC) -XX:+UseParallelOldGC ParallelScavenge+Parallel Old FullGC算法：PS MarkSweep JVM参数 垃圾回收参数 -Xnoclassgc 是否对类进行回收 -verbose:class -XX:+TraceClassUnloading 查看类加载和卸载信息 -XX:SurvivorRatio Eden和其中一个survivor的比值 -XX:PretenureSizeThreshold 大对象进入老年代的阈值，Serial和ParNew生效 -XX:MaxTenuringThreshold 晋升老年代的对象年龄，默认15, CMS默认是4 -XX:HandlePromotionFailure 老年代担保 -XX:+UseAdaptiveSizePolicy动态调整Java堆中各个区域大小和进入老年代年龄 -XX:ParallelGCThreads 并行回收的线程数 -XX:MaxGCPauseMillis Parallel Scavenge参数，设置GC的最大停顿时间 -XX:GCTimeRatio Parallel Scavenge参数，GC时间占总时间的比率，默认99%，即1%的GC时间 -XX:CMSInitiatingOccupancyFraction，old区触发cms阈值，默认68% -XX:+UseCMSCompactAtFullCollection(CMS完成后是否进行一次碎片整理，停顿时间加长) -XX:CMSFullGCsBeforeCompaction(执行多少次不进行碎片整理的FullGC后进行一次带压缩的) -XX:+ScavengeBeforeFullGC，在fullgc前触发一次minorGC 垃圾回收统计信息 -XX:+PrintGC 输出GC日志 -verbose:gc等同于上面那个 -XX:+PrintGCDetails 输出GC的详细日志 堆大小设置 -Xmx:最大堆大小 -Xms:初始堆大小(最小内存值) -Xmn:年轻代大小 -XX:NewSize和-XX:MaxNewSize 新生代大小 -XX:SurvivorRatio:3 意思是年轻代中Eden区与两个Survivor区的比值。注意Survivor区有两个。如：3，表示Eden：Survivor=3：2，一个Survivor区占整个年轻代的1/5 -XX:NewRatio=4:设置年轻代（包括Eden和两个Survivor区）与年老代的比值（除去持久代）。设置为4，则年轻代与年老代所占比值为1：4，年轻代占整个堆栈的1/5 -Xss栈容量 默认256k -XX:PermSize永久代初始值 -XX:MaxPermSize 永久代最大值 生产环境参数配置(CMS-GC) Java [g|m|k] -Xmx[g|m|k] -XX:PermSize=[g|m|k] -XX:MaxPermSize=[g|m|k] -Xmn[g|m|k] -XX:+DisableExplicitGC -XX:SurvivorRatio= -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+CMSScavengeBeforeRemark -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction= -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xloggc:\"\" -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=100M -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=`date`.hprof -Dsun.net.inetaddr.ttl= -Djava.rmi.server.hostname= -Dcom.sun.management.jmxremote.port= -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false Java >= 8 -server -Xms[g|m|k] -Xmx[g|m|k] -XX:MaxMetaspaceSize=[g|m|k] -Xmn[g|m|k] -XX:+DisableExplicitGC -XX:SurvivorRatio= -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+CMSScavengeBeforeRemark -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction= -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xloggc:\"\" -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=100M -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=`date`.hprof -Dsun.net.inetaddr.ttl= -Djava.rmi.server.hostname= -Dcom.sun.management.jmxremote.port= -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false 理解GC日志 DefNew：Serial收集器新生代名称 - Tenured - Perm ParNew：ParNew收集器新生代名称 - PSYoungGen：Parallel Scavenge收集器新生代名称 JVM调优 GC的时间足够的小 GC的次数足够的少 发生Full GC的周期足够的长 前两个目前是相悖的，要想GC时间小必须要一个更小的堆，要保证GC次数足够少，必须保证一个更大的堆，我们只能取其平衡。 （1）针对JVM堆的设置，一般可以通过-Xms -Xmx限定其最小、最大值，为了防止垃圾收集器在最小、最大之间收缩堆而产生额外的时间，我们通常把最大、最小设置为相同的值 （2）年轻代和年老代将根据默认的比例（1：2）分配堆内存，可以通过调整二者之间的比率NewRadio来调整二者之间的大小，也可以针对回收代，比如年轻代，通过 -XX:newSize -XX:MaxNewSize来设置其绝对大小。同样，为了防止年轻代的堆收缩，我们通常会把-XX:newSize -XX:MaxNewSize设置为同样大小。-XX:PermSize，-XX:MaxPermSize设置为一样防止老年代收缩。 -XX:NewRatio=4:设置年轻代（包括Eden和两个Survivor区）与年老代的比值（除去持久代）。设置为4，则年轻代与年老代所占比值为1：4，年轻代占整个堆栈的1/5 -XX:MaxTenuringThreshold=0：设置垃圾最大年龄。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概论。 （3）年轻代和年老代设置多大才算合理？这个我问题毫无疑问是没有答案的，否则也就不会有调优。我们观察一下二者大小变化有哪些影响 年轻代老年代设置：整个JVM内存大小=年轻代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐年轻代配置为整个堆的3/8。 更大的年轻代必然导致更小的年老代，大的年轻代会延长普通GC的周期，但会增加每次GC的时间；小的年老代会导致更频繁的Full GC 更小的年轻代必然导致更大年老代，小的年轻代会导致普通GC很频繁，但每次的GC时间会更短；大的年老代会减少Full GC的频率 如何选择应该依赖应用程序对象生命周期的分布情况：如果应用存在大量的临时对象，应该选择更大的年轻代；如果存在相对较多的持久对象，年老代应该适当增大。但很多应用都没有这样明显的特性，在抉择时应该根据以下两点：（A）本着Full GC尽量少的原则，让年老代尽量缓存常用对象，JVM的默认比例1：2也是这个道理 （B）通过观察应用一段时间，看其他在峰值时年老代会占多少内存，在不影响Full GC的前提下，根据实际情况加大年轻代，比如可以把比例控制在1：1。但应该给年老代至少预留1/3的增长空间 （4）在配置较好的机器上（比如多核、大内存），可以为年老代选择并行收集算法： -XX:+UseParallelOldGC ，默认为Serial收集 （5）线程堆栈的设置：每个线程默认会开启1M的堆栈，用于存放栈帧、调用参数、局部变量等，对大多数应用而言这个默认值太了，一般256K就足用。理论上，在内存不变的情况下，减少每个线程的堆栈，可以产生更多的线程，但这实际上还受限于操作系统。 -Xss256k：设置每个线程的堆栈大小。JDK5.0以后每个线程堆栈大小为1M，以前每个线程堆栈大小为256K。更具应用的线程所需内存大小进行调整。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右。 （6）可以通过下面的参数打Heap Dump信息 -XX:HeapDumpPath -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/usr/aaa/dump/heap_trace.txt 通过下面参数可以控制OutOfMemoryError时打印堆的信息 -XX:+HeapDumpOnOutOfMemoryError 注：通过分析dump文件可以发现，每个1小时都会发生一次Full GC，经过多方求证，只要在JVM中开启了JMX服务，JMX将会1小时执行一次Full GC以清除引用. 性能分析工具 jps -m 主类的参数 -l 主类的全名，如果执行的是jar包，输出jar路径 -v 虚拟机参数 jstat 监视虚拟机运行状态信息，包括类装载、GC、运行期编译（JIT） 用于输出java程序内存使用情况，包括新生代、老年代、元数据区容量、垃圾回收情况 jstat -gcutil 52670 2000 5 进程号 2s输出一次一共5次 S0：幸存1区当前使用比例 S1：幸存2区当前使用比例 E：伊甸园区使用比例 O：老年代使用比例 M：元数据区使用比例 CCS：压缩使用比例 YGC：年轻代垃圾回收次数 YGCT：年轻代垃圾回收消耗时间 FGC：老年代垃圾回收次数 FGCT：老年代垃圾回收消耗时间 GCT：垃圾回收消耗总时间 jmap jmap：用于生成堆转储快照。一般称为dump或heapdump文件。 jmap -histo 3618 上述命令打印出进程ID为3618的内存情况，包括有哪些对象，对象的数量。但我们常用的方式是将指定进程的内存heap输出到外 部文件，再由专门的heap分析工具进行分析,例如mat（Memory Analysis Tool），所以我们常用的命令是： jmap -dump:live,format=b,file=heap.hprof 3618 -F 强制生成dump快照 jstack jstack：用户输出虚拟机当前时刻的线程快照，常用于定位因为某些线程问题造成的故障或性能问题。一般称为threaddump文件。 参数 -F当正常输出没有响应的时候强制打印栈信息，一般情况不需要使用 -l长列表. 打印关于锁的附加信息，一般情况不需要使用 -m 如果调用本地方法的话，可打印c/c++的堆栈 jinfo 查看和修改虚拟机参数 可视化工具 JConsole Java监视与管理控制台 VisualVM 多合一故障处理工具 Java Class文件类型前缀 Element Type Encoding boolean Z byte B char C double D float F int I long J short S class or interface Lclassname; [L代表了相应类型数组嵌套的层数 内存泄漏及解决方法 1.系统崩溃前的一些现象： 每次垃圾回收的时间越来越长，由之前的10ms延长到50ms左右，FullGC的时间也有之前的0.5s延长到4、5s FullGC的次数越来越多，最频繁时隔不到1分钟就进行一次FullGC 年老代的内存越来越大并且每次FullGC后年老代没有内存被释放 之后系统会无法响应新的请求，逐渐到达OutOfMemoryError的临界值。 2.生成堆的dump文件 通过JMX的MBean生成当前的Heap信息，大小为一个3G（整个堆的大小）的hprof文件，如果没有启动JMX可以通过Java的jmap命令来生成该文件。 3.分析dump文件 下面要考虑的是如何打开这个3G的堆信息文件，显然一般的Window系统没有这么大的内存，必须借助高配置的Linux。当然我们可以借助X-Window把Linux上的图形导入到Window。我们考虑用下面几种工具打开该文件： 1.Visual VM 2.IBM HeapAnalyzer 3.JDK 自带的Hprof工具 使用这些工具时为了确保加载速度，建议设置最大内存为6G。使用后发现，这些工具都无法直观地观察到内存泄漏，Visual VM虽能观察到对象大小，但看不到调用堆栈；HeapAnalyzer虽然能看到调用堆栈，却无法正确打开一个3G的文件。因此，我们又选用了Eclipse专门的静态内存分析工具：Mat。 4.分析内存泄漏 通过Mat我们能清楚地看到，哪些对象被怀疑为内存泄漏，哪些对象占的空间最大及对象的调用关系。针对本案，在ThreadLocal中有很多的JbpmContext实例，经过调查是JBPM的Context没有关闭所致。 另，通过Mat或JMX我们还可以分析线程状态，可以观察到线程被阻塞在哪个对象上，从而判断系统的瓶颈。 5.回归问题 Q：为什么崩溃前垃圾回收的时间越来越长？ A:根据内存模型和垃圾回收算法，垃圾回收分两部分：内存标记、清除（复制），标记部分只要内存大小固定，时间是不变的，变的是复制部分，因为每次垃圾回收都有一些回收不掉的内存，所以增加了复制量，导致时间延长。所以，垃圾回收的时间也可以作为判断内存泄漏的依据 Q：为什么Full GC的次数越来越多？ A：因此内存的积累，逐渐耗尽了年老代的内存，导致新对象分配没有更多的空间，从而导致频繁的垃圾回收 Q:为什么年老代占用的内存越来越大？ A:因为年轻代的内存无法被回收，越来越多地被Copy到年老代 调优方法 一切都是为了这一步，调优，在调优之前，我们需要记住下面的原则： 1、多数的Java应用不需要在服务器上进行GC优化； 2、多数导致GC问题的Java应用，都不是因为我们参数设置错误，而是代码问题； 3、在应用上线之前，先考虑将机器的JVM参数设置到最优（最适合） 4、减少创建对象的数量； 5、减少使用全局变量和大对象； 6、GC优化是到最后不得已才采用的手段； 7、在实际使用中，分析GC情况优化代码比优化GC参数要多得多； GC优化的目的有两个 1、将转移到老年代的对象数量降低到最小； 2、减少full GC的执行时间； 为了达到上面的目的，一般地，你需要做的事情有： 1、减少使用全局变量和大对象； 2、调整新生代的大小到最合适； 3、设置老年代的大小为最合适； 4、选择合适的GC收集器 "},"zother4-EasyJob/Java/Maven基础.html":{"url":"zother4-EasyJob/Java/Maven基础.html","title":"Maven基础","keywords":"","body":"Maven基础 1.什么是maven? Maven:基于项目对象模型pom的项目管理工具 2.常用命令 mvn compile 编译 mvn test 测试 mvn package 打包 mvn clean 删除target目录 mvn install 安装Jar包到本地仓库 mvn archetype:generate 自动建立项目骨架 3.目录结构 src-main-java src-test-java src-main-resources src-test-resources target pom.xml 4.仓库 本地仓库（用户的本地仓库） 中央仓库（Maven的中央仓库） 镜像仓库（Mirrors镜像仓库，在settings.xml中配置） 5.怎么向本地仓库导入官方仓库没有的Jar包 注:简单复制是不可行的。 正确方法是执行cmd命令： mvn install:install file -DgroupId=包名 -DartifactId=项目名 -Dversion=版本号 -Dpackage=Jar -Dfile=文件路径 一个简单的例子 e.g. mvn install:install file -DgroupId=com.tinymood -DartifactId=tinymood-core -Dversion=1.0.0 -Dpackage=Jar -Dfile=D:\\lib\\tinymood-core-1.0.0.jar pom.xml中引用 com.tinymood tinymood-core 1.0.0 "},"zother4-EasyJob/Java/Spring.html":{"url":"zother4-EasyJob/Java/Spring.html","title":"Spring","keywords":"","body":""},"zother4-EasyJob/Java/Struts.html":{"url":"zother4-EasyJob/Java/Struts.html","title":"Struts","keywords":"","body":"Struts框架 定义：Struts是流行和成熟的基于MVC设计模式的Web应用程序框架。 Model1 = JSP + JavaBean Model2 = JSP + Servlet + JavaBean Struts中action就是Controller，Struts2是webwork的升级同时吸收两者的优势，不是一个全新的框架。 Struts2：servlet2.4 jsp2.0 java5(注解) 搭建struts2的步骤： Jar -> 创建web项目 -> 完成配置 -> 创建action并测试 核心配置文件：web.xml struts.xml struts.properties Struts2不再与Servlet API耦合，无需传入HttpServletRequest和HttpServletResponse struts2提供了3种方式访问Servlet API ActionContext 实现Aware接口 ServletActionContext 动态方法调用 1.指定method属性 2.感叹号方式 3.通配符方式 -> 解决action太多的问题 多个配置文件的方式 方法：include包含的方式 吧默认Action - 解决无法匹配的Action 后缀名 struts.action.extension 接收参数 1.使用Action的属性接收 2.使用DomainModel接收 3.实现ModelDriven接口 代码地址:struts "},"zother4-EasyJob/Java/URL_URI.html":{"url":"zother4-EasyJob/Java/URL_URI.html","title":"URL URI","keywords":"","body":"URI和URL的区别 这两天在写代码的时候，由于涉及到资源的位置，因此，需要在Java Bean中定义一些字段，用来表示资源的位置，比如：imgUrl，logoUri等等。但是，每次定义的时候，心里都很纠结，是该用imgUrl还是imgUri呢？ 同样的，另外一个问题：String HttpServletRequest.getRequestURI()；和StringBuffer HttpServletRequest.getRequestURL();返回的内容有何不同？为什么会如此？ 带着这些问题到网上去搜了下，没发现让自己看了明白的解释，于是，想到了Java类库里有两个对应的类java.net.URI和java.net.URL，终于，在这两个类里的javadoc里找到了答案。 URIs, URLs, and URNs 首先，URI，是uniform resource identifier，统一资源标识符，用来唯一的标识一个资源。而URL是uniform resource locator，统一资源定位器，它是一种具体的URI，即URL可以用来标识一个资源，而且还指明了如何locate这个资源。而URN，uniform resource name，统一资源命名，是通过名字来标识资源，比如mailto:java-net@java.sun.com。也就是说，URI是以一种抽象的，高层次概念定义统一资源标识，而URL和URN则是具体的资源标识的方式。URL和URN都是一种URI。 在Java的URI中，一个URI实例可以代表绝对的，也可以是相对的，只要它符合URI的语法规则。而URL类则不仅符合语义，还包含了定位该资源的信息，因此它不能是相对的，schema必须被指定。 ok，现在回答文章开头提出的问题，到底是imgUrl好呢，还是imgUri好？显然，如果说imgUri是肯定没问题的，因为即使它实际上是url，那它也是uri的一种。那么用imgUrl有没有问题呢？此时则要看它的可能取值，如果是绝对路径，能够定位的，那么用imgUrl是没问题的，而如果是相对路径，那还是不要用ImgUrl的好。总之，用imgUri是肯定没问题的，而用imgUrl则要视实际情况而定。 第二个，从HttpServletRequest的javadoc中可以看出，getRequestURI返回一个String，“the part of this request’s URL from the protocol name up to the query string in the first line of the HTTP request”，比如“POST /some/path.html?a=b HTTP/1.1”，则返回的值为”/some/path.html”。现在可以明白为什么是getRequestURI而不是getRequestURL了，因为此处返回的是相对的路径。而getRequestURL返回一个StringBuffer，“The returned URL contains a protocol, server name, port number, and server path, but it does not include query string parameters.”，完整的请求资源路径，不包括querystring。 总结一下：URL是一种具体的URI，它不仅唯一标识资源，而且还提供了定位该资源的信息。URI是一种语义上的抽象概念，可以是绝对的，也可以是相对的，而URL则必须提供足够的信息来定位，所以，是绝对的，而通常说的relative URL，则是针对另一个absolute URL，本质上还是绝对的。 注：这里的绝对(absolute)是指包含scheme，而相对(relative)则不包含scheme。 URI抽象结构 [scheme:]scheme-specific-part[#fragment] [scheme:][//authority][path][?query][#fragment] authority为[user-info@]host[:port] 参考资料： http://docs.oracle.com/javase/1.5.0/docs/api/java/net/URI.html http://en.wikipedia.org/wiki/Uniform_Resource_Identifier http://docs.oracle.com/javaee/5/api/javax/servlet/http/HttpServletRequest.html ps: java.net.URL类不提供对标准RFC2396规定的特殊字符的转义，因此需要调用者自己对URL各组成部分进行encode。而java.net.URI则会提供转义功能。因此The recommended way to manage the encoding and decoding of URLs is to use java.net.URI. 可以使用URI.toURL()和URL.toURI()方法来对两个类型的对象互相转换。对于HTML FORM的url encode/decode可以使用java.net.URLEncoder和java.net.URLDecoder来完成，但是对URL对象不适用。 "},"zother4-EasyJob/Java/一致性Hash算法.html":{"url":"zother4-EasyJob/Java/一致性Hash算法.html","title":"一致性Hash算法","keywords":"","body":"一致性Hash算法 1.为什么要使用一致性Hash 解决余数Hash算法在服务器集群的路由映射中诸多问题。 1、余数Hash 由于HashCode随机性比较强，所以使用余数Hash路由算法就可以保证缓存数据在整个Cache服务器集群中有比较均衡的分布。 如果不考虑服务器集群的伸缩性，那么余数Hash算法几乎可以满足绝大多数的缓存路由需求，但是当分布式缓存集群需要扩容或者某些服务器下线或宕机的时候，就难办了。 比如说服务器扩容，更改服务器列表，仍然使用余数Hash，它会导致大量缓存无法命中。（其实不仅仅是无法命中，那些大量的无法命中的数据还在原缓存中在被移除前占据着内存）。大部分的业务数据度操作请求上事实上是通过缓存获取的，只有少量读操作会访问数据库，因此数据库的负载能力是以有缓存为前提而设计的。当大部分被缓存了的数据因为服务器扩容而不能正确读取时，这些数据访问的压力就落在了数据库的身上，这将大大超过数据库的负载能力，严重的可能会导致数据库宕机。 这个问题有解决方案，解决步骤为： （1）在网站访问量低谷，通常是深夜，技术团队加班，扩容、重启服务器 （2）通过模拟请求的方式逐渐预热缓存，使缓存服务器中的数据重新分布 2、一致性Hash 一致性Hash算法通过一个叫做一致性Hash环的数据结构实现Key到缓存服务器的Hash映射。 算法的具体原理这里再次贴上： 先构造一个长度为2^32的整数环（这个环被称为一致性Hash环），根据节点名称的Hash值（其分布为[0, 2^32-1]）将服务器节点放置在这个Hash环上，然后根据需要的缓存的数据的Key值计算其Hash值（其分布也为[0, 2^32-1]），接着在Hash环上顺时针查找距离这个Key值的Hash值最近的服务器节点，完成Key到服务器的映射查找。 采用一致性Hash算法，的确也会影响到服务器集群，但是影响的只是某一段而已，相比余数Hash算法影响了远超一半的影响率，这种影响要小得多。更重要的是，集群中缓存服务器节点越多，增加节点带来的影响越小，很好理解。换句话说，随着集群规模的增大，继续命中原有缓存数据的概率会越来越大，虽然仍然有小部分数据缓存在服务器中不能被读到，但是这个比例足够小，即使访问数据库，也不会对数据库造成致命的负载压力。 这种算法解决了普通余数Hash算法伸缩性差的问题，可以保证在上线、下线缓存服务器的情况下尽量有多的请求命中原来路由到的缓存服务器。 当然，万事不可能十全十美，一致性Hash算法比普通的余数Hash算法更具有伸缩性，但是同时其算法实现也更为复杂。 数据结构的选取 一致性Hash算法最先要考虑的一个问题是：构造出一个长度为232的整数环，根据节点名称的Hash值将服务器节点放置在这个Hash环上。 那么，整数环应该使用何种数据结构，才能使得运行时的时间复杂度最低？首先说明一点，关于时间复杂度，常见的时间复杂度与时间效率的关系有如下的经验规则： O(1) 一般来说，前四个效率比较高，中间两个差强人意，后三个比较差（只要N比较大，这个算法就动不了了）。OK，继续前面的话题，应该如何选取数据结构，我认为有以下几种可行的解决方案。 1、解决方案一：排序+List 我想到的第一种思路是：算出所有待加入数据结构的节点名称的Hash值放入一个数组中，然后使用某种排序算法将其从小到大进行排序，最后将排序后的数据放入List中，采用List而不是数组是为了结点的扩展考虑。 之后，待路由的结点，只需要在List中找到第一个Hash值比它大的服务器节点就可以了，比如服务器节点的Hash值是[0,2,4,6,8,10]，带路由的结点是7，只需要找到第一个比7大的整数，也就是8，就是我们最终需要路由过去的服务器节点。 如果暂时不考虑前面的排序，那么这种解决方案的时间复杂度： （1）最好的情况是第一次就找到，时间复杂度为O(1) （2）最坏的情况是最后一次才找到，时间复杂度为O(N) 平均下来时间复杂度为O(0.5N+0.5)，忽略首项系数和常数，时间复杂度为O(N)。 但是如果考虑到之前的排序，我在网上找了张图，提供了各种排序算法的时间复杂度。 看得出来，排序算法要么稳定但是时间复杂度高、要么时间复杂度低但不稳定，看起来最好的归并排序法的时间复杂度仍然有O(N * logN)，稍微耗费性能了一些。 2、解决方案二：遍历+List 既然排序操作比较耗性能，那么能不能不排序？可以的，所以进一步的，有了第二种解决方案。 解决方案使用List不变，不过可以采用遍历的方式： （1）服务器节点不排序，其Hash值全部直接放入一个List中 （2）待路由的节点，算出其Hash值，由于指明了”顺时针”，因此遍历List，比待路由的节点Hash值大的算出差值并记录，比待路由节点Hash值小的忽略 （3）算出所有的差值之后，最小的那个，就是最终需要路由过去的节点 在这个算法中，看一下时间复杂度： 1、最好情况是只有一个服务器节点的Hash值大于带路由结点的Hash值，其时间复杂度是O(N)+O(1)=O(N+1)，忽略常数项，即O(N) 2、最坏情况是所有服务器节点的Hash值都大于带路由结点的Hash值，其时间复杂度是O(N)+O(N)=O(2N)，忽略首项系数，即O(N) 所以，总的时间复杂度就是O(N)。其实算法还能更改进一些：给一个位置变量X，如果新的差值比原差值小，X替换为新的位置，否则X不变。这样遍历就减少了一轮，不过经过改进后的算法时间复杂度仍为O(N)。 总而言之，这个解决方案和解决方案一相比，总体来看，似乎更好了一些。 3、解决方案三：二叉查找树 抛开List这种数据结构，另一种数据结构则是使用二叉查找树。 当然我们不能简单地使用二叉查找树，因为可能出现不平衡的情况。平衡二叉查找树有AVL树、红黑树等，这里使用红黑树，选用红黑树的原因有两点： 1、红黑树主要的作用是用于存储有序的数据，这其实和第一种解决方案的思路又不谋而合了，但是它的效率非常高 2、JDK里面提供了红黑树的代码实现TreeMap和TreeSet 另外，以TreeMap为例，TreeMap本身提供了一个tailMap(K fromKey)方法，支持从红黑树中查找比fromKey大的值的集合，但并不需要遍历整个数据结构。 使用红黑树，可以使得查找的时间复杂度降低为O(logN)，比上面两种解决方案，效率大大提升。 为了验证这个说法，我做了一次测试，从大量数据中查找第一个大于其中间值的那个数据，比如10000数据就找第一个大于5000的数据（模拟平均的情况）。看一下O(N)时间复杂度和O(logN)时间复杂度运行效率的对比： 因为再大就内存溢出了，所以只测试到4000000数据。可以看到，数据查找的效率，TreeMap是完胜的，其实再增大数据测试也是一样的，红黑树的数据结构决定了任何一个大于N的最小数据，它都只需要几次至几十次查找就可以查到。 当然，明确一点，有利必有弊，根据我另外一次测试得到的结论是，为了维护红黑树，数据插入效率TreeMap在三种数据结构里面是最差的，且插入要慢上5~10倍。 Hash值重新计算 服务器节点我们肯定用字符串来表示，比如”192.168.1.1″、”192.168.1.2″，根据字符串得到其Hash值，那么另外一个重要的问题就是Hash值要重新计算，这个问题是我在测试String的hashCode()方法的时候发现的，不妨来看一下为什么要重新计算Hash值： /** * String的hashCode()方法运算结果查看 * @author 哓哓 * */ public class StringHashCodeTest { public static void main(String[] args) { System.out.println(\"192.168.0.0:111的哈希值：\" + \"192.168.0.0:1111\".hashCode()); System.out.println(\"192.168.0.1:111的哈希值：\" + \"192.168.0.1:1111\".hashCode()); System.out.println(\"192.168.0.2:111的哈希值：\" + \"192.168.0.2:1111\".hashCode()); System.out.println(\"192.168.0.3:111的哈希值：\" + \"192.168.0.3:1111\".hashCode()); System.out.println(\"192.168.0.4:111的哈希值：\" + \"192.168.0.4:1111\".hashCode()); } } 我们在做集群的时候，集群点的IP以这种连续的形式存在是很正常的。看一下运行结果为： 192.168.0.0:111的哈希值：1845870087 192.168.0.1:111的哈希值：1874499238 192.168.0.2:111的哈希值：1903128389 192.168.0.3:111的哈希值：1931757540 192.168.0.4:111的哈希值：1960386691 这个就问题大了，[0,2^32-1]的区间之中，5个HashCode值却只分布在这么小小的一个区间，什么概念？[0,2^32-1]中有4294967296个数字，而我们的区间只有122516605，从概率学上讲这将导致97%待路由的服务器都被路由到”192.168.0.1″这个集群点上，简直是糟糕透了！ 另外还有一个不好的地方：规定的区间是非负数，String的hashCode()方法却会产生负数（不信用”192.168.1.0:1111″试试看就知道了）。不过这个问题好解决，取绝对值就是一种解决的办法。 综上，String重写的hashCode()方法在一致性Hash算法中没有任何实用价值，得找个算法重新计算HashCode。这种重新计算Hash值的算法有很多，比如CRC32_HASH、FNV1_32_HASH、KETAMA_HASH等，其中KETAMA_HASH是默认的MemCache推荐的一致性Hash算法，用别的Hash算法也可以，比如FNV1_32_HASH算法的计算效率就会高一些。 一致性Hash算法：CRC32_HASH、KETAMA_HASH、FNV1_32_HASH、NATIVE_HASH、MYSQL_HASH 一致性Hash算法实现版本1：不带虚拟节点 使用一致性Hash算法，尽管增强了系统的伸缩性，但是也有可能导致负载分布不均匀，解决办法就是使用虚拟节点代替真实节点，第一个代码版本，先来个简单的，不带虚拟节点。 下面来看一下不带虚拟节点的一致性Hash算法的Java代码实现： /** * 不带虚拟结点的一致性Hash算法 * @author 哓哓 * */ public class ConsistentHashWithoutVN { /** * 待加入Hash环的服务器列表 */ private static String[] servers = { \"192.168.0.0:111\", \"192.168.0.1:111\", \"192.168.0.2:111\", \"192.168.0.3:111\", \"192.168.0.4:111\" }; /** * key表示服务器的hash值，value表示服务器的名称 */ private static SortedMap sortedMap = new TreeMap<>(); /** * 程序初始化，将所有服务器加入集合 */ static { for (int i = 0; i > 7; hash += hash > 17; hash += hash subMap = sortedMap.tailMap(hash); // 顺时针的第一个Key Integer i = subMap.firstKey(); // 返回路由到的服务器名称 return subMap.get(i); } public static void main(String[] args) { String[] nodes = {\"127.0.0.1:1111\", \"221.226.0.1:2222\", \"10.211.0.1:3333\"}; for (int i = 0; i 可以运行一下看一下结果： [192.168.0.0:111]加入集合中, 其Hash值为575774686 [192.168.0.1:111]加入集合中, 其Hash值为8518713 [192.168.0.2:111]加入集合中, 其Hash值为1361847097 [192.168.0.3:111]加入集合中, 其Hash值为1171828661 [192.168.0.4:111]加入集合中, 其Hash值为1764547046[127.0.0.1:1111]的hash值为380278925, 被路由到结点[192.168.0.0:111] [221.226.0.1:2222]的hash值为1493545632, 被路由到结点[192.168.0.4:111] [10.211.0.1:3333]的hash值为1393836017, 被路由到结点[192.168.0.4:111] 看到经过FNV1_32_HASH算法重新计算过后的Hash值，就比原来String的hashCode()方法好多了。从运行结果来看，也没有问题，三个点路由到的都是顺时针离他们Hash值最近的那台服务器上。 使用虚拟节点来改善一致性Hash算法 上面的一致性Hash算法实现，可以在很大程度上解决很多分布式环境下不好的路由算法导致系统伸缩性差的问题，但是会带来另外一个问题：负载不均。 比如说有Hash环上有A、B、C三个服务器节点，分别有100个请求会被路由到相应服务器上。现在在A与B之间增加了一个节点D，这导致了原来会路由到B上的部分节点被路由到了D上，这样A、C上被路由到的请求明显多于B、D上的，原来三个服务器节点上均衡的负载被打破了。某种程度上来说，这失去了负载均衡的意义，因为负载均衡的目的本身就是为了使得目标服务器均分所有的请求。 解决这个问题的办法是引入虚拟节点，其工作原理是：将一个物理节点拆分为多个虚拟节点，并且同一个物理节点的虚拟节点尽量均匀分布在Hash环上。采取这样的方式，就可以有效地解决增加或减少节点时候的负载不均衡的问题。 至于一个物理节点应该拆分为多少虚拟节点，下面可以先看一张图： 横轴表示需要为每台福利服务器扩展的虚拟节点倍数，纵轴表示的是实际物理服务器数。可以看出，物理服务器很少，需要更大的虚拟节点；反之物理服务器比较多，虚拟节点就可以少一些。比如有10台物理服务器，那么差不多需要为每台服务器增加100~200个虚拟节点才可以达到真正的负载均衡。 一致性Hash算法实现版本2：带虚拟节点 在理解了使用虚拟节点来改善一致性Hash算法的理论基础之后，就可以尝试开发代码了。编程方面需要考虑的问题是： 1、一个真实结点如何对应成为多个虚拟节点？ 2、虚拟节点找到后如何还原为真实结点？ 这两个问题其实有很多解决办法，我这里使用了一种简单的办法，给每个真实结点后面根据虚拟节点加上后缀再取Hash值，比如”192.168.0.0:111″就把它变成”192.168.0.0:111&&VN0″到”192.168.0.0:111&&VN4″，VN就是Virtual Node的缩写，还原的时候只需要从头截取字符串到”&&”的位置就可以了。 下面来看一下带虚拟节点的一致性Hash算法的Java代码实现： /** * 带虚拟结点的一致性Hash算法 * * @author 哓哓 * */ public class ConsistentHashWithVN { /** * 待加入Hash环的服务器列表 */ private static String[] servers = { \"192.168.0.0:111\", \"192.168.0.1:111\", \"192.168.0.2:111\", \"192.168.0.3:111\", \"192.168.0.4:111\" }; /** * 真实结点列表，考虑到服务器上线、下线的场景，即添加、删除的场景会比较频繁，这里使用LinkedList会更好 */ private static List realNodes = new LinkedList<>(); /** * key表示虚拟结点服务器的hash值，value表示虚拟结点服务器的名称 */ private static SortedMap virtualNodes = new TreeMap<>(); /** * 虚拟结点数目（一个真实结点对应VN_SUM个虚拟结点） */ private static final int VN_SUM = 5; /** * 加所有服务器加入集合 */ static { for (int i = 0; i > 7; hash += hash > 17; hash += hash subMap = virtualNodes.tailMap(hash); // 顺时针的第一个Key Integer i = subMap.firstKey(); // 截取 String virtualNode = subMap.get(i); // 返回路由到的服务器名称 return virtualNode.substring(0, virtualNode.indexOf(\"&\")); } public static void main(String[] args) { String[] nodes = { \"127.0.0.1:1111\", \"221.226.0.1:2222\", \"10.211.0.1:3333\", \"112.74.15.218:80\" }; for (int i = 0; i 关注一下运行结果： 虚拟节点[192.168.0.0:111&&VN0]被添加, hash值为1686427075 虚拟节点[192.168.0.0:111&&VN1]被添加, hash值为354859081 虚拟节点[192.168.0.0:111&&VN2]被添加, hash值为1306497370 虚拟节点[192.168.0.0:111&&VN3]被添加, hash值为817889914 虚拟节点[192.168.0.0:111&&VN4]被添加, hash值为396663629 虚拟节点[192.168.0.1:111&&VN0]被添加, hash值为1032739288 虚拟节点[192.168.0.1:111&&VN1]被添加, hash值为707592309 虚拟节点[192.168.0.1:111&&VN2]被添加, hash值为302114528 虚拟节点[192.168.0.1:111&&VN3]被添加, hash值为36526861 虚拟节点[192.168.0.1:111&&VN4]被添加, hash值为848442551 虚拟节点[192.168.0.2:111&&VN0]被添加, hash值为1452694222 虚拟节点[192.168.0.2:111&&VN1]被添加, hash值为2023612840 虚拟节点[192.168.0.2:111&&VN2]被添加, hash值为697907480 虚拟节点[192.168.0.2:111&&VN3]被添加, hash值为790847074 虚拟节点[192.168.0.2:111&&VN4]被添加, hash值为2010506136 虚拟节点[192.168.0.3:111&&VN0]被添加, hash值为891084251 虚拟节点[192.168.0.3:111&&VN1]被添加, hash值为1725031739 虚拟节点[192.168.0.3:111&&VN2]被添加, hash值为1127720370 虚拟节点[192.168.0.3:111&&VN3]被添加, hash值为676720500 虚拟节点[192.168.0.3:111&&VN4]被添加, hash值为2050578780 虚拟节点[192.168.0.4:111&&VN0]被添加, hash值为586921010 虚拟节点[192.168.0.4:111&&VN1]被添加, hash值为184078390 虚拟节点[192.168.0.4:111&&VN2]被添加, hash值为1331645117 虚拟节点[192.168.0.4:111&&VN3]被添加, hash值为918790803 虚拟节点[192.168.0.4:111&&VN4]被添加, hash值为1232193678 [127.0.0.1:1111]的hash值为380278925, 被路由到结点[192.168.0.0:111] [221.226.0.1:2222]的hash值为1493545632, 被路由到结点[192.168.0.0:111] [10.211.0.1:3333]的hash值为1393836017, 被路由到结点[192.168.0.2:111] 从代码运行结果看，每个点路由到的服务器都是Hash值顺时针离它最近的那个服务器节点，没有任何问题。 通过采取虚拟节点的方法，一个真实结点不再固定在Hash换上的某个点，而是大量地分布在整个Hash环上，这样即使上线、下线服务器，也不会造成整体的负载不均衡。 "},"zother4-EasyJob/Java/单例.html":{"url":"zother4-EasyJob/Java/单例.html","title":"单例","keywords":"","body":"如何正确地写出单例模式 单例模式算是设计模式中最容易理解，也是最容易手写代码的模式了吧。但是其中的坑却不少，所以也常作为面试题来考。本文主要对几种单例写法的整理，并分析其优缺点。很多都是一些老生常谈的问题，但如果你不知道如何创建一个线程安全的单例，不知道什么是双检锁，那这篇文章可能会帮助到你。 1.懒加载 线程不安全 当被问到要实现一个单例模式时，很多人的第一反应是写出如下的代码，包括教科书上也是这样教我们的。 public class Singleton { private static Singleton uniqueInstance; private Singleton (){} public static Singleton getInstance() { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } return uniqueInstance; } } 这段代码简单明了，而且使用了懒加载模式，但是却存在致命的问题。当有多个线程并行调用 getInstance() 的时候，就会创建多个实例。也就是说在多线程下不能正常工作。 2.懒加载 线程安全 为了解决上面的问题，最简单的方法是将整个 getInstance() 方法设为同步（synchronized）。 public static synchronized Singleton getInstance() { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } return uniqueInstance; } 虽然做到了线程安全，并且解决了多实例的问题，但是它并不高效。因为在任何时候只能有一个线程调用 getInstance() 方法。但是同步操作只需要在第一次调用时才被需要，即第一次创建单例实例对象时。这就引出了双重检验锁。 3.双重检查加锁 线程安全 双重检验加锁模式（double checked locking pattern），是一种使用同步块加锁的方法。程序员称其为双重检查锁，因为会有两次检查 uniqueInstance == null，一次是在同步块外，一次是在同步块内。为什么在同步块内还要再检验一次？因为可能会有多个线程一起进入同步块外的 if，如果在同步块内不进行二次检验的话就会生成多个实例了。 public static Singleton getSingleton() { if (uniqueInstance == null) { //Single Checked synchronized (Singleton.class) { if (uniqueInstance == null) { //Double Checked uniqueInstance = new Singleton(); } } } return uniqueInstance; } 这段代码看起来很完美，很可惜，它是有问题。主要在于uniqueInstance = new Singleton()这句，这并非是一个原子操作，事实上在 JVM 中这句话大概做了下面 3 件事情。 给 uniqueInstance 分配内存 调用 Singleton 的构造函数来初始化成员变量 将uniqueInstance对象指向分配的内存空间（执行完这步 uniqueInstance 就为非 null 了） 但是在 JVM 的即时编译器中存在指令重排序的优化。也就是说上面的第二步和第三步的顺序是不能保证的，最终的执行顺序可能是 1-2-3 也可能是 1-3-2。如果是后者，则在 3 执行完毕、2 未执行之前，被线程二抢占了，这时uniqueInstance已经是非 null 了（但却没有初始化），所以线程二会直接返回 uniqueInstance，然后使用，然后顺理成章地报错。 我们只需要将 uniqueInstance 变量声明成 volatile 就可以了。 public class Singleton { private volatile static Singleton uniqueInstance; //声明成 volatile private Singleton (){} public static Singleton getSingleton() { if (uniqueInstance == null) { synchronized (Singleton.class) { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } } } return uniqueInstance; } } 有些人认为使用 volatile 的原因是可见性，也就是可以保证线程在本地不会存有 uniqueInstance 的副本，每次都是去主内存中读取。但其实是不对的。使用 volatile 的主要原因是其另一个特性：禁止指令重排序优化。也就是说，在 volatile 变量的赋值操作后面会有一个内存屏障（生成的汇编代码上），读操作不会被重排序到内存屏障之前。比如上面的例子，取操作必须在执行完 1-2-3 之后或者 1-3-2 之后，不存在执行到 1-3 然后取到值的情况。从「先行发生原则」的角度理解的话，就是对于一个 volatile 变量的写操作都先行发生于后面对这个变量的读操作（这里的“后面”是时间上的先后顺序）。 但是特别注意在 Java 5 以前的版本使用了 volatile 的双检锁还是有问题的。其原因是 Java 5 以前的 JMM （Java 内存模型）是存在缺陷的，即使将变量声明成 volatile 也不能完全避免重排序，主要是 volatile 变量前后的代码仍然存在重排序问题。这个 volatile 屏蔽重排序的问题在 Java 5 中才得以修复，所以在这之后才可以放心使用 volatile。 相信你不会喜欢这种复杂又隐含问题的方式，当然我们有更好的实现线程安全的单例模式的办法。 4.急加载 static final field 线程安全 这种方法非常简单，因为单例的实例被声明成 static 和 final 变量了，在第一次加载类到内存中时就会初始化，所以创建实例本身是线程安全的。 public class Singleton{ //类加载时就初始化 private static final Singleton uniqueInstance = new Singleton(); private Singleton(){} public static Singleton getInstance(){ return uniqueInstance; } } 这种写法如果完美的话，就没必要在啰嗦那么多双检锁的问题了。缺点是它不是一种懒加载模式（lazy initialization），单例会在加载类后一开始就被初始化，即使客户端没有调用 getInstance()方法。饿汉式的创建方式在一些场景中将无法使用：譬如 Singleton 实例的创建是依赖参数或者配置文件的，在 getInstance() 之前必须调用某个方法设置参数给它，那样这种单例写法就无法使用了。 5.静态内部类 static nested class 线程安全 我比较倾向于使用静态内部类的方法，这种方法也是《Effective Java》上所推荐的。 public class Singleton { private static class SingletonHolder { private static final Singleton uniqueInstance = new Singleton(); } private Singleton (){} public static final Singleton getInstance() { return SingletonHolder.uniqueInstance; } } 这种写法仍然使用JVM本身机制保证了线程安全问题；由于 SingletonHolder 是私有的，除了 getInstance() 之外没有办法访问它，因此它是懒加载的；同时读取实例的时候不会进行同步，没有性能缺陷；也不依赖 JDK 版本。 枚举 Enum 线程安全 用枚举写单例实在太简单了！这也是它最大的优点。下面这段代码就是声明枚举实例的通常做法。 public enum EasySingleton{ INSTANCE; } 我们可以通过EasySingleton.INSTANCE来访问实例，这比调用getInstance()方法简单多了。创建枚举默认就是线程安全的，所以不需要担心double checked locking，而且还能防止反序列化导致重新创建新的对象。但是还是很少看到有人这样写，可能是因为不太熟悉吧。 总结 一般来说，单例模式有五种写法：懒加载、急加载、双重检查加锁锁、静态内部类、枚举。上述所说都是线程安全的实现，文章开头给出的第一种方法不算正确的写法。 就我个人而言，一般情况下直接使用急加载就好了，如果明确要求要懒加载（lazy initialization）会倾向于使用静态内部类，如果涉及到反序列化创建对象时会试着使用枚举的方式来实现单例。 代码打包 完整代码 Singleton Read More Double Checked Locking on Singleton Class in Java http://javarevisited.blogspot.sg/2012/07/why-enum-singleton-are-better-in-java.html How to create thread safe Singleton in Java 10 Singleton Pattern Interview questions in Java "},"zother4-EasyJob/Java/字符编码.html":{"url":"zother4-EasyJob/Java/字符编码.html","title":"字符编码","keywords":"","body":"常见字符集及其分类 字符集编码是指对多个字符（通常在几十到几万个不等）进行整合封装成一个文件所使用的编码，外部程序通过这种编码就可以从字符集文件中调用指定的字符。我们常见的计算机字体文件就使用了字符集编码，通过输入法输入文字或者浏览网页时都会通过指定的字符集编码从字体文件中调用字符。 1.以下是常见的字符集： ASCII及其扩展字符集 作用：表语英语及西欧语言。 位数：ASCII是用7位表示的，能表示128个字符；其扩展使用8位表示，表示256个字符。 范围：ASCII从00到7F，扩展从00到FF。 ISO-8859-1字符集 作用：扩展ASCII，表示西欧、希腊语等。 位数：8位， 范围：从00到FF，兼容ASCII字符集。 Latin1 ISO-8859-1的别名 GB2312字符集：1981年5月1日发布的简体中文汉字编码国家标准。GB2312对汉字采用双字节编码，收录7445个图形字符，其中包括6763个汉字。 BIG5字符集：台湾地区繁体中文标准字符集，采用双字节编码，共收录13053个中文字，1984年实施。 GBK字符集：1995年12月发布的汉字编码国家标准，是对GB2312编码的扩充，对汉字采用双字节编码。GBK字符集共收录21003个汉字，包含国家标准GB13000-1中的全部中日韩汉字，和BIG5编码中的所有汉字。 GB18030字符集：2000年3月17日发布的汉字编码国家标准，是对GBK编码的扩充，覆盖中文、日文、朝鲜语和中国少数民族文字，其中收录27484个汉字。GB18030字符集采用单字节、双字节和四字节三种方式对字符编码。兼容GBK和GB2312字符集。 UCS字符集 作用：国际标准 ISO 10646 定义了通用字符集 (Universal Character Set)。它是与UNICODE同类的组织，UCS-2和UNICODE兼容 位数：它有UCS-2和UCS-4两种格式，分别是2字节和4字节 范围：目前，UCS-4只是在UCS-2前面加了0×0000 Unicode字符集： 作用：国际标准字符集，它将世界650种语言的每个字符定义一个唯一的编码，以满足跨语言、跨平台的文本信息转换，兼容ISO-8859-1 位数：UNICODE字符集有多个编码方式，分别是UTF-8，UTF-16和UTF-32 2.如何判断字符集 2.1 字节序 首先说一下字节序对编码的影响，字节序分为Big Endian字节序和Little Endian字节序。不同的处理器可能不一样。所以，传输时需要告诉处理器当时的编码字节序。 Big Endian 大端存储：高位字节存在低地址，低字节存于高地址 Little Endian 小端存储：相反 举例：0x03AB 大端存储 0000:03 0001:AB 小端存储 0000:AB 0001:03 Java判断处理器是大端存储还是小端存储 public class JudgeCPUEndian { public static void main(String[] args) { if (ByteOrder.nativeOrder() == ByteOrder.BIG_ENDIAN) { System.out.println(\"BIG_ENDIAN\"); } else { System.out.println(\"LITTLE_ENDIAN\"); } } } 2.2 编码识别 UNICODE，根据前几个字节可以判断UNICODE字符集的各种编码，叫做Byte Order Mask方法BOM： UTF-8: EFBBBF UTF-16 Big Endian：FEFF UTF-16 Little Endian：FFFE UTF-32 Big Endian：0000FEFF UTF-32 Little Endian：FFFE0000 3.按所表示的文字分类 语言 字符集 正式名称 英语、西欧语 ASCII，ISO-8859-1 MBCS 多字节 简体中文 GB2312 MBCS 多字节 繁体中文 BIG5 MBCS 多字节 简繁中文 GBK MBCS 多字节 中文、日文及朝鲜语 GB18030 MBCS 多字节 各国语言 UNICODE，UCS DBCS 宽字节 "},"zother4-EasyJob/Network/network.html":{"url":"zother4-EasyJob/Network/network.html","title":"Network","keywords":"","body":"计算机网络复习知识点 基本知识点 1.OSI参考模型（七层体系结构） 物理层 - 数据链路层 - 网络层 - 运输层 - 会话层 - 表示层 - 应用层 2.TCP/IP体系结构（四层体系结构） 网络接口层 - 网际层IP - 运输层(TCP/UDP) - 应用层(FTP/SMTP/HTTP/TELNET...) 物理层 1.奈氏准则 理想低通信道最高码元传输速率 = 2W Baud（即每赫兹理想低通信道最高码元传输速率每秒2个码元） 2.香农定理 信道的极限传输速率C = Wlog2(1+S/N)，W为带宽，S为平均功率，N为高斯噪声功率，S/N也称为信噪比 3.物理层之下的传输媒体 双绞线 同轴电缆 光缆 4.调制器和解调器 调制器：基带数字信号波形转为模拟信号的波形 解调器：将经过调制器变换的模拟信号恢复成原来的数字信号 5.采样定理 只要采样频率不低于电话信号最高频率的2倍，就可以从采样脉冲信号无失真地恢复出原来的电话信号。 6.信道复用技术 频分复用FDM 时分复用TDM 码分多址CDMA：每个用户可以在同样的时间使用同样的频带进行通信。 7.单工 半双工 全双工 8.基带和宽带 基带信号指将数字0,1直接用两种不同电压来表示，然后运到线路上传输。宽带信号指将基带信号调制后形成频分复用模拟信号。 9.基带信号 - 曼彻斯特编码信号 - 差分曼彻斯特编码 曼彻斯特编码：一个码元拆2个，码元1前高后低，码元0相反 差分曼彻斯特编码：一个码元拆2个，码元1前半个与上一个码元的后半个一样，码元0相反。 数据链路层 1.链路和数据链路（除了一条物理链路外还需要加一些通信协议控制数据的传输） 2.数据链路层3个基本问题 帧定界 透明传输 差错检测 3.帧定界 帧的数据部分的上限 4.透明传输（数据的比特组合必须是不受限制的） 方法1：字节插入 SOH EOT ESC ESC x ESC y ESC z 方法2：比特插入 5.差错检测 误码率：传输错误的比特与传输总比特数的比率 CRC(Cyclic Redundancy Check)：循环冗余检验 CRC是检错方法并不能纠错，FCS（Frame Check Sequence）是冗余码。 计算冗余码（余数R）的方法：先补0（n个）再对生成多项式取模。 CRC只能表示以接近1的概率认为它没有差错。但不能做到可靠传输。可靠传输还需要确认和重传机制。 生成多项式P(X)：CRC-16，CRC-CCITT，CRC-32 6.停止等待协议：解决死锁使用超时计时器。对重复帧的处理：丢弃，再次发送确认帧。向上面的网络层提供了可靠传输。是一种ARQ的差错检测方法。 7.发送窗口大于1的ARQ：连续ARQ和选择重传ARQ 连续ARQ又称Go-back-N ARQ，意思是出现差错必须重传时，要回走n个帧，即使是已经传送正确的数据帧也要重传。（仅因这些正确帧那面有一个错误） 滑动窗口：（解决连续ARQ中一旦某个帧出错要重传之后许多帧，浪费时间，增大开销，因此我们对发出去但未被确认的帧的数目加以限制）包括发送窗口和接收窗口。为了减少开销，连续ARQ还执行捎带确认的机制。发送窗口Wt(n代表用几比特编号)，接收窗口Wr=1。 选择重传ARQ：需要缓存空间，一般不使用。 接收窗口Wr 这种收发两端窗口按规律向前移动，也称为滑动窗口协议。（连续ARQ，选择重传ARQ，停止等待都是） 6.PPP协议 ppp帧格式 字段 0x7E 0xFF 0x03 协议 信息部分 FCS 0x7E 解释 标志 地址 控制 0x0021 IP数据报 检验 标志 字节数 1 1 1 2 2 1 注：当协议字段为0x0021时，信息字段为IP数据报，不超过1500字节 字节填充：转义符为0x7D 0比特填充：5个连续的1插入1个0 7.HDLC协议：可以实现可靠传输 局域网 1.按网络拓扑结构分：星形网、环形网（令牌环形网）、总线网、树形网。 2.以太网 DIX EthernetV2以太网标准成为主流 802的数据链路层划分：逻辑链路控制LLC子层 + 媒体接入控制MAC子层 以太网提供的服务是不可靠的交付，即尽最大努力交付。因为采用无连接，数据帧不编号，也不要求对方确认。对有差错的帧的重传有高层来决定。 CSMA/CD协议(Carrier Sense Multiple Access with Collision Detection)：载波监听多点接入/碰撞检测 用于协调同一时间只能允许一台计算机发送信息，否则就会相互干扰的问题。 电磁波在1km电缆的传播时延约5us。使用CSMA/CD协议，一个站不能同时发送和接收，因此不能实现全双工。 争用期：2τ 51.2us 最短有效帧长为64字节 截断二进制指数退避算法：基本退避时间为2τ k=min{重传次数，10} r=random(0~2^k-1) 重传所需时延为r倍的基本退避时间 3.MAC帧格式（以太网V2） 说明 目的地址 源地址 类型 数据 FCS 字节数 6 6 2 46~1500 4 4.VLAN 局域网网段构成的与物理位置无关的逻辑组。克服了广播风暴 5.快速以太网参数a=τC/L。a为总线的单程传播时延和帧的发送时延之比 C为数据率 L为帧长，a远小于1，信道利用率越高 6.无线网CSMA/CA 城域网 网络层提供的服务：无连接的数据报和面向连接的虚电路 数据报优点：128字节作为一个分组，迅速经济。主机承担端到端的差错控制和流量控制。 虚电路：每个分组无需携带完整的目的地址，分组按顺序交付，网络负责差错控制和流量控制。 使用默认路由简化转发表。 拥塞：Σ对资源的需求 > 可用资源 拥塞控制是一个全局性的过程，涉及到所有主机路由器。 流量控制往往指端到端之间点对点通信量的控制。 拥塞过程：轻度拥塞 - 拥塞 - 死锁（直接死锁，重装死锁：因为路由器缓存引起的拥塞） ATM（异步传递方式）：建立在电路交换和分组交换的基础上一种面向连接的快速分组交换技术。 网络层（IP层） 1.中继（中间设备）： 物理层中继系统：转发器 数据链路层中继系统：网桥或桥接器 网络层中继系统：路由器 网络层以上的中继系统：网关。由于历史原因，TCP/IP的文献中将路由器称为网关。 2.协议：IP协议及与之配套的4个协议 ARP（Address Resolution Protocol）地址解析协议：IP地址 -> MAC地址 RARP（Reverse Address Resolution Protocol）逆地址解析协议：MAC地址 -> IP地址 ICMP（Internet Control Message Protocol）因特网控制报文协议 IGMP（Internet Group Management Protocol）因特网组管理协议 3.IP地址分类：1.基本分类的IP地址（两级） 2.子网的划分（三级） 3.无分类编址（构造超网） 4.IP地址使用范围： 网络类别 最大网络数 第一个可用的网络号 最后一个可用的网络号 每个网络的最大主机数 网络号固定位 A 2^7-2 1 126 2^24-2（16777214） 1 B 2^14 128.0 191.255 2^16-2（65534） 10 C 2^21 192.0.0 223.255.255 2^8-2（254） 110 网络类别 IP地址空间占比 A 50% B 25% C 12.5% 注：1.主机数减2的原因是因为主机号全0代表该IP地址是本主机所连接到的单个网络地址；主机号全1代表本网络上的所有主机。 注：2.A类地址网络数-2的原因是：第一，网络号全0是个保留地址代表本网络；第二，网络号为127（01111111）保留为本地软件环回测试。 注1：D类IP地址网络号固定位为1110 用于多播；E类IP地址网络号固定位为11110，为保留地址。 5.IP地址和硬件地址 网络层的IP数据报交给数据链路层被封装成MAC帧，MAC帧的源地址和目的地址变为硬件地址 6.ARP（IP地址 -> MAC地址） 使用IP地址并不能直接用来通信，因为IP地址只是主机在抽象的网络层中的地址。最终要传到数据链路层封装成MAC帧才能发送到实际的网络。 因此不管使用什么协议最终需要的还是硬件地址。 每个主机拥有一个ARP高速缓存（存放所在局域网内主机和路由器的IP地址到硬件地址的映射表） 举例：A发送B (1).A在ARP Cache中查到B的MAC地址，写入MAC帧发往此B (2).没查到，A向本局域网广播ARP请求分组，内容包括自己的地址映射和B的IP地址 (3).B发送ARP响应分组并写入A的映射（单播的方式） ARP Cache映射项目具有一个生存时间。 7.IP数据报格式 固定部分 20字节 版本 - 首部长度 - 服务类型 - 总长度 - 标识 - 标志 - 片偏移 - 生存时间 - 协议 - 首部检验和 - 源地址4B - 目的地址4B - 可变部分4B 标识：一个计数器，用来产生数据报的标识。 标志字段：3位 MF=1后面还有分片 DF=0允许分片 IP分片 MF DF 片偏移 协议：占1个字节 首部检验和：反码算数运算，不采用CRC 8.划分子网：IP地址：网络号 - （子网号是在两级分类编址中的主机号中划出的） 子网掩码：网络号子网号置1(相当于CIDR中的网络前缀)主机号置0，这样子网掩码与IP地址相与即可算出网络地址。 划分子网下的路由器转发算法： （1）从收到的数据报的首部提取目的IP地址D （2）先判断是否为直接交付。对与路由器相邻的子网进行检查。用各网络子网掩码与D相与，查看是否匹配。若是，直接交付（D 转化为物理地址，数据报封装成帧发送出去。）转发结束。若否，为间接交付，执行（3） （3）若路由表中有目的地址D的特定主机路由，则将数据报传给该路由，否则，执行（4） （4）对路由表的每一行（目的网络地址、子网掩码、下一跳地址），将其中的子网掩码与D相与，结果为N。若N与该行目的网络地址匹配，则 将该数据报传给该行指明的下一跳路由器，否则，执行（5） （5）若路由表中有一个默认路由，则传给默认路由，否则，执行（6） （6）报告分组转发出错 9.无分类编址CIDR（CLassless Inter-Domain Routing 无分类域间路由选择） 无分类的两级编址：格式 路由聚合也称为构造超网因为一个CIDR地址块可以表示很多地址。 最长前缀匹配：从匹配结果中选择具有最长网络前缀的路由 10.ICMP（Internet Control Message Protocol）因特网控制报文协议 包括ICMP差错报文和ICMP询问报文 ICMP差错报文的类型：终点不可达 源点抑制 时间超过 参数问题 改变路由 ICMP询问报文的类型：回送请求和回答 时间戳请求和回答 掩码地址请求和回答 路由器询问或通告 应用层服务Ping使用了ICMP回送请求和回答报文，是应用层直接使用网络层ICMP的例子。没有通过运输层的TCP或UDP 11.路由选择协议 自治系统AS：自治系统有权自主地决定在本系统类采用何种路由选择协议。 路由选择协议分为两类：内部网关协议IGP（Interior Gateway Protocol）主要有RIP和OSPF 外部网关协议EGP（External Gateway Protocol）主要是BGP-4 RIP（Routing Information Protocol）：分布式的基于距离向量的路由选择协议 RIP的距离也称为跳数，RIP允许一条路径上最多包含15个路由器，因此距离为16相当于不可达。 RIP仅和相邻路由交换信息，交换的是当前路由器知道的全部信息，即自己的路由表。按固定的时间间隔交换。 RIP协议是收敛的，所谓收敛指自治系统中的所有结点都得到正确的路由选择信息的过程。最终使得每一个路由器到 每一个目的网络的路由都是最短的。 优点：实习Ian简单，开销简单。缺点：“好消息传得快，坏消息传得慢”。 RIP使用运输层UDP（520端口）进行传送，因此RIP的位置在应用层。但转发用户数据报的过程在网络层完成的。 OSPF（Open Shortest Path First开放最短路径优先）：分布式的基于链路状态协议 1.使用洪范法向本自治系统内所有路由器发送信息。 2.发送的信息是与本路由器相邻的所有路由器的链路状态。 3.只有链路状态发生变化时，路由器才使用洪泛法向所有路由器发送信息（而RIP是不管网络拓扑有无变化，路由器间都要定期交换路由表信息） 由于频繁交换链路状态信息，因此所有路由器最终都能建立一个链路状态数据库，这个数据库实际上就是全网的拓扑结构图，在全网范围内是一致的。 OSPF更新过程收敛的快。 OSPF不用UDP而直接用IP数据报发送。可见OSPF的位置在网络层。 负载均衡：在代价相同的路径上分配通信量，这叫多路径间的负载均衡。 外部网关协议BGP（Border Gateway Protocol:边界网关协议）：基于路径向量路由选择协议。 目标：力求选择一条比较好的路由。并非寻找最佳。 BGP发言人需要建立TCP连接（端口179），然后在此连接上交换报文以建立BGP会话。 BGP发言人需要同时运行BGP协议和内部网关协议 BGP解决了基于距离向量路由选择算法中的“好消息传得快，坏消息传得慢”这一问题。 BGP-4一共4种报文：打开包文 更新报文 保活报文 通知报文 12.IGMP（因特网组管理协议） 13.虚拟专用网 专用地址 10.0.0.0 - 10.255.255.255 172.16.0.0 - 172.31.255.255 192.168.0.0 - 192.168.255.255 这些IP地址在本机构内部使用，也叫可重用地址。 隧道技术实现虚拟专用网VPN（Virtual Private Network） 分类:内联网VPN 外联网VPN 内联网外联网都采用TCP/IP 14.NAT（Network Address Translation） 使用本地地址的主机在和外界通信时都要在NAT路由器上将本地地址转换成全球地址。 15.解决IP地址耗尽的方法 （1）采用无分类编址CIDR，使IP地址分类更合理 （2）采用NAT转换 （3）IPv6 16.在网络层，IP数据报的首部检验和字段只检验首部是否出差错而不检验数据部分。 运输层 运输层提供应用进程间的逻辑通信。它使应用进程看见的就好像是在两个运输层实体间一条端到端的逻辑通信信道。 当运输层采用TCP时，尽管下面的网络是不可靠的，但这种逻辑通信信道相当于一条全双工的可靠信道。可以做到报文的无差错、 按序、无丢失、无重复。 单单面向连接只是可靠的必要条件，不充分。还需要其他措施，如确认重传，按序接收，无丢失无重复。 熟知端口： 20 FTP数据连接 21 FTP控制连接 22 SSH 23 TELNET 25 SMTP 53 DNS 69 TFTP 80 HTTP 161 SNMP UDP重要 UDP的优点： 1.发送之前无需建立连接，减小了开销和发送数据的时延 2.UDP不使用拥塞控制，不使用可靠交付，因此主机不需要维护复杂的参数表、连接状态表 3.UDP用户数据报只有8个字节的首部开销，而TCP要20字节。 4.由于没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（IP电话等实时应用要求源主机以恒定的速率发送数据） Table，使用TCP和UDP的应用 应用 应用层协议 运输层协议 名字转换 DNS UDP 文件传送 TFTP UDP 路由选择协议 RIP UDP IP地址配置 BOOTTP,DHCP UDP 网络管理 SNMP UDP 远程文件服务器 NFS UDP IP电话 专用协议 UDP 流式多媒体通信 专用协议 UDP 电子邮件 SMTP TCP 远程终端接入 TELNET TCP 万维网 HTTP TCP 文件传送 FTP TCP 注：TFTP：Trivial File Transfer Protocol UDP的过程： 1.服务器进程运行着，等待TFTP客户进程的服务请求。客户端TFTP进程启动时，向操作系统申请一个临时端口号，然后操作系统为该进程创建2个队列， 入队列和出队列。只要进程在执行，2个队列一直存在。 2.客户进程将报文发送到出队列中。UDP按报文在队列的先后顺序发送。在传送到IP层前给报文加上UDP首部，其中目的端口后为69。然后发给IP层。 出队列若溢出，则操作系统通知应用层TFTP客户进程暂停发送。 3.客户端收到来自IP层的报文时，UDP检查报文中目的端口号是否正确，若正确，放入入队列队尾，客户进程按先后顺序一一取走。若不正确，UDP丢弃该报文，并请ICMP发送”端口不可达“差错报文给服务器端。入队列可能会溢出，若溢出，UDP丢弃该报文，不通知对方。 服务器端类似。 UDP首部：源端口 - 目的端口 - 长度 - 检验和，每个字段22字节。 IP数据报检验和只检验IP数据报的首部，而UDP的检验和将首部和数据部分一起都检验。 TCP重要 TCP报文段是面向字节的数据流。 TCP首部：20字节固定首部 确认比特ACK，ACK=1 确认号字段才有效；同步比特SYN：SYN=1 ACK=0表示一个连接请求报文段；终止比特FIN，FIN=1时要求释放连接。 窗口：A根据TCP缓存空间的大小确定自己的接收窗口大小。A发送给B的窗口字段写入该值。作为B的发送窗口的上限。 选项：最大报文段长度MSS，MSS告诉对方TCP：我的缓存所能接收的报文段的数据字段的最大长度是MSS个字节。若主机未填写，默认为536字节。 TCP的可靠是使用了序号和确认。存放副本。 TCP报文段的发送时机：1.维持一个变量等于MSS，缓存达到就发送 2.发送端应用进程指明要发送，即TCP支持的PUSH操作。3.设定计时器 接收端根据自身资源情况控制发送端发送窗口的大小。 接收端窗口rwnd（receiver window）：接收端根据目前接收缓存大小设置的窗口值，是来自接收端的流量控制 拥塞窗口cwnd（congestion window）：是发送端根据自己估计的网络拥塞程度设置的窗口值，是来自发送端的流量控制 发送端的窗口=Min(rwnd, cwnd) 慢开始算法：cwnd设置为1个MSS，每收到一个确认，将cwnd+1，逐步增大cwnd，使分组注入网络的速率更加合理。 慢开始门限：ssthresh，当cwnd ssthresh，改用拥塞避免算法。 cwnd = ssthresh时，都可以。 拥塞避免算法使发送端的拥塞窗口每经过一个RTT增加一个MSS（而不管在此期间收到多少ACK），这样，拥塞窗口cwnd按线性规律增长，比 慢开始算法拥塞窗口增长速率缓慢很多。（加法增大） 无论是慢开始还是拥塞避免，只要发送端发现网络出现拥塞（根据是没有按时收到ACK或者收到重复ACK），就开始将慢开始门限ssthresh设置为 出现拥塞时拥塞窗口值的一半。拥塞窗口cwnd置为1，执行慢开始算法。(乘法减小) 上述TCP确认都是通过捎带确认执行的。 快重传：发送端一连收到三个重复的ACK,即可断定分组丢失，不必等待重传计数器，立即重传丢失的报文。 快恢复：当发送端收到3个重复的ACK时，乘法减小，ssthresh变为一半。但是cwnd不是置为1，而是ssthresh+3MSS。若收到的重复ACK 为n(n > 3)，则cwnd=ssthresh+nMSS.在使用快恢复算法时，慢开始算法只在TCP连接建立时使用。若收到了确认新的报文段的ACK，就将 TCP的重传机制 每发送一个报文段，就对这个报文段设置一次计时器。新的重传时间=γ*旧的重传时间。 TCP连接建立和释放的过程 SYN置1和FIN的报文段要消耗一个序号。 客户端连接状态变迁：CLOSED -> 主动打开,发送SYN=1 -> SYN_SENT -> 收到服务器的SYN=1和ACK时,发送三次握手的最后一个ACK -> ESTABLISHED -> 数据传送 -> 主动关闭 -> 发送FIN=1,等待确认ACK的到达 -> FIN_WAIT_1 -> 收到确认ACK后时，一向连接关闭 -> FIN_WAIT_2 ->收到服务器发送的FIN=1报文，响应，发送四次挥手的的最后一个确认ACK -> 这时另一条连接也关闭了，进入TIME_WAIT状态 -> 经过2倍报文寿命，TCP删除连接记录 -> 回到CLOSED状态 服务器端连接状态变迁：CLOSED -> 被动打开 -> LISTEN -> 收到SYN=1的报文，发送SYN=1和确认ACK -> 进入SYN_RCVD -> 收到三次握手 的最后一个确认ACK -> ESTABLISHED -> 数据传送 -> 数据传送完毕，收到FIN=1 -> 发送确认ACK并进入CLOSED_WAIT -> 发送FIN=1给客户端 -> LAST_ACK -> 收到客户端四次挥手的最后一个确认ACK -> 删除连接记录 -> 回到CLOSED状态 应用层 1.DNS 2.电子邮件：用户代理，邮件服务器，协议 其中SMTP用于发送邮件，POP3用于接收邮件。 3.HTTP请求报文和响应报文：开始行 首部行 实体主体 4.BOOTP和DHCP：Dynamic Host Configuration动态主机配置协议。需要IP地址的主机向DHCP服务器广播，源IP全0 目的主机IP全1，DHCP查找配置信息，若找到，返回，若找不到，从服务器IP地址池取一个地址分配给该计算机。 5.SNMP 简单网络管理协议 网络安全 常规密钥密码体制：加密密钥和解密密钥是相同的密码体制，又称对称密钥体制。DES（Data Encryption Standard）和IDEA（International Data Encryption Algorithm）。 公开密钥密码体制：为了解决常规密钥体制的密钥分配问题和对数字签名的需求。 RSA公开密钥密码体制 原理：根据数论，寻求两个大素数比较简单，而把两个大素数的乘积分解及其困难。 数字签名：需要保证一下三点 （1）接受者能够核实发送者对报文的签名 （2）发送者事后不能抵赖对报文的签名 （3）接收者不能伪造对报文的签名 防火墙：是由软件硬件构成的系统，用来在两个网络之间实施接入控制策略。 完，作者：Campanulaceae 微博 To Be Continued... "},"zother4-EasyJob/OS/OS.html":{"url":"zother4-EasyJob/OS/OS.html","title":"OS","keywords":"","body":"操作系统知识复习 1、操作系统两个最基本特征 并发和共享 2、操作系统的主要功能 处理机管理、存储器管理、设备管理和文件管理 处理机管理：主要是对进程的管理：进程控制、进程同步、进程通信、调度。 存储器管理：内存分配、内存保护、地址映射、内存扩充 设备管理：缓冲管理、设备分配、设备处理 文件管理：文件存储空间的管理、目录管理、文件的读/写管理和保护 进程 1、进程的特征 结构特征：程序段+数据段+PCB组成，也称之为进程实体。 动态性：进程的本质就是进程实体的一次执行过程。由创建而产生，由调度而执行，由撤销而消亡。 并发性：重要特征 独立性：独立运行，独立分配资源，独立接受调度的基本单位 异步性：进程按各自独立的不可预知的速度向前推进，或者说进程实体按异步方式运行。 2、进程的基本状态 就绪 执行 阻塞 增加挂起状态，增加创建状态和终止状态。 3、进程控制块PCB的作用 PCB记录了操作系统所需的用于描述进程的当前情况以及控制进程运行的全部信息。PCB是进程存在的唯一标志。 PCB中的信息： 进程标识符、处理机状态、进程调度信息、进程控制信息 PCB的组织方式：链接方式、索引方式 4、进程控制 引起进程创建：1、用户登录 2、作业调度 3、提供服务 4、应用请求 进程创建过程：1、申请空白PCB 2、分配资源 3、初始化PCB 4、新进程插入就绪队列 引起进程终止：正常结束 异常结束（越界错误、非法指令、运行超时、IO故障等） 进程终止的过程：1、根据被终止进程的标识符，从PCB集合检索出该进程的PCB，读取进程状态 2、若进程处于执行，终止其执行并设调度状态为真。 3、若进程还有子孙进程，应先将所有子孙进程终止。 4、将被终止进程的全部资源归还给父进程或系统 5、将被终止进程从队列中移出 进程阻塞和唤醒：1、请求系统服务 2、启动某种操作 3、新数据尚未到达 4、无新工作可做 block原语，wakeup原语 进程的挂起和激活：suspend和active原语 进程同步 遵循的规则： 空闲让进、忙则等待、有限等待、让权等待 信号量机制：整型信号量、记录型信号量、AND型信号量、 管程：一个管程定义了一个数据结构和能为并发进程所执行的一组操作，这组操作能同步进程和改变管程中的数据。 经典的进程同步问题： 生产者消费者问题：记录型信号量、AND型信号量（思想：将进程在整个运行过程中所需要的全部资源一次性分配给进程，待进程使用完后一起释放。如果请求的资源不能分配到进程，则一个也不分配） 哲学家进餐厅 读者-写者 进程间通信 信号量 管道:：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。 有名管道 (named pipe) ：有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。 共享内存 消息队列 套接字( socket ) ：套接字也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同及其间的进程通信。 线程 1、与进程的比较 第一，调度。在引入线程的OS中将线程作为调度的基本单位，而进程作为资源拥有的基本单位，使线程轻装上阵，提高系统的并发程度。同一进程中，线程的切换不会引起进程的切换，当从一个进程的线程切换到另一个进程的线程，将会引起进程切换 第二，并发性。在引入线程的OS中，不仅进程可以并发，一个进程内的多个线程也可以并发执行，从而提高系统的利用率和吞吐量。 第三，拥有资源。进程是拥有资源的基本单位，线程不拥有资源（其实也有一点必不可少的资源），线程可以访问进程的代码段、数据段以及进程所拥有的系统资源，如打开的文件、IO设备等。这些资源为进程内的所有线程共享 第四，系统开销。创建或者撤销进程时，系统要为之分配和回收进程控制块，分配回收资源，如内存和IO设备等。线程的切换只需要保存和设置少量寄存器内容，系统开销远小于进程。 2、线程的属性 一、轻型实体 二、独立调度的基本单位 三、可以并发执行 四、共享进程资源 3、线程间通信 互斥锁 条件变量 信号量机制 4、线程的实现方式 内核支持线程、用户级线程、组合的方式 调度算法 1.先来先服务FCFS：有利于长作业 2.短作业优先 3、优先级法 两类：非抢占式和抢占式 优先权的类型： 静态优先权0-7 0-255 创建进程时确定，程序运行期间不再改变 动态优先权：优先权可以随着进程的推进或随等待时间的增加而改变 具体算法： 高响应比优先调度算法 优先权 = （等待时间+要求服务时间）/要求服务时间 该算法既照顾了短作业，又考虑了作业到达的先后次序，不会使长作业长期得不到服务 时间片轮转 多级反馈队列调度算法：无需指明进程或作业的执行时间 原理：设置多个就绪队列，并赋予不同的优先级。第一个队列的优先级最高，第二个次之，其余的逐个降低。在优先级最高的队列中，每个进程规定的时间片越小，第二个队列时间片比第一个长一倍，第i+1个比第i个长一倍。 过程：当一个新进程进入内存后，首先放入第一队列的队尾，当轮到该进程执行时，若在时间片内完成，便可撤离系统。否则，调度程序便将此进程转入第二队列的队尾，按同样的策略执行。需要注意的是，仅当第一队列空闲时，调度程序才调度第二队列中的进程运行。其他队列类似。如果当前处理机正在为第i个队列某进程所使用，又有新进程进入优先级更高的队列，则新进程将抢占处理机，调度程序把正在运行的进程放回到第i个队列的末尾。 死锁 原因：1、竞争资源 2、进程间推进顺序非法 死锁的必要条件： 1.互斥条件：一段时间内某资源只由一个进程占用 2.请求和保持条件：保持了至少一个资源又提出新的资源请求，新资源被其他进程占有，此时请求线程阻塞又对自己获得的其他资源保持不放。 3.不剥夺条件：指进程已获得的资源未使用完之前，不能被剥夺，只能使用完自己释放。 4.环路等待：存在一个进程-资源的环形链。 处理死锁的方法： 预防死锁、避免死锁、检测死锁、解除死锁 预防和避免都属于事先预防的策略。可以通过破坏死锁的必要条件去实现。 检测死锁和解除死锁的做法是将进程从死锁状态中解脱出来。常用的做法是撤销或挂起一些进程，以便回收一些资源然后分配给阻塞的进程，使之转为就绪，继续运行。 解除死锁：1、剥夺资源 2、撤销进程（直接的做法是全部死锁进程撤销，或按某个顺序去撤销） 预防的方法： 摒弃请求和保持：采用这种方法时，系统规定所有进程在运行前必须一次性申请其需要的全部资源。有，分配，摒弃了请求条件。不足，不分配。摒弃了保持条件。 摒弃不剥夺：已经保持了某些资源又提出新的资源请求而不能满足时，必须释放保持的资源。摒弃了不剥夺。 摒弃环路等待：系统将资源线性排队并赋予不同序号，规定进程对资源的请求必须严格按照资源序号提出。 系统安全状态 银行家算法：Max Allocation Need Available 判断请求 安全性算法 存储器管理 连续分配 分区分配算法： 1、首次适应：分配时从空闲分区链首顺序查找，直到找到一个大小能满足要求的空闲分区为止，从中划出请求大小的内存，如果不能，此次分配失败。 2、循环首次适应：首次适应基础上改变而来，每次不是从链首查找而是从上次找到的空闲分区的下一个空闲分区开始查找。 3、最佳适应：是指每次把既能满足要求又是最小的空闲分区给作业，缺点会留下难以利用的小空间。 4、最坏适应：挑选一个最大的空闲分区给作业，缺点使存储器缺乏大的存储空间。 5、快速适应：是指将空闲分区根据容量大小分类，对每一类具有相同容量的所有空闲分区，单独设置一张空闲分区链表，内存中在设立一张索引表。该算法的优点是查找效率高，不易产生内存碎片，但空闲分区划分的越细，浪费越严重，这是一种以空间换时间的做法。 连续分配会有很多碎片，虽然可以通过紧凑的方式拼接成大空间，但必须付出很大开销。 离散分配：分页存储管理和分段存储管理 分页存储管理 分页：页号＋页内地址 页面（一个进程的逻辑地址空间划分若干大小的片，简称页面） 页表（页号 － 块号对） 地址变换机构：用于用户地址空间的逻辑地址到物理地址的转换。 基本地址变换机构 具有快表的地址变换机构 由于页表是存放在内存中的，CPU每次存取一个数据时，都要访问两次内存。第一次访问内存的页表，得到块号，再将块号与页内偏移量 拼接得到物理地址。第二次访问内存时才可以获得所需数据。提高地址变换速度。无需经过页表（第一次需要） 两级页表和多级页表 分段存储管理 满足用户和程序员的一系列需要。 1，方便编程 2，信息共享 3，信息保护 4，动态增长 5，动态连接 分段地址：段号 ＋ 段内地址 段表：进程查找段表找到每个段对应的内存区 地址变换机构：逻辑地址到物理地址的变换 分页和分段的区别 1，页是信息的物理单位，分页是为了实现离散分配方式，以减少内存零头，提高内存利用率，是满足系统管理的需要而不是用户。段是信息的逻辑单位，它含有一组意义相对完整的信息。分段是更好的满足用户需要。 2，页的大小固定且由系统决定。512B － 8KB。段的长度不确定。 3，分页的地址空间是一维的，段的地址空间是二维的，程序员在标识一个地址时既需要段号又需给出段内地址。 局部性原理 时间局部性：某条指令 空间局部性：附近的存储单元 虚拟存储器：请求调页＋页面置换 何处调页：文件区，对换区，页面共享（其他进程） 页面置换算法 最佳置换： 淘汰以后永不使用或最长未来时间内不使用的页面。 先进先出置换： 淘汰最先进入内存的页面 LRU置换算法： 最近最久未使用的页面。需要寄存器和栈的支持。 Clock算法：页面使用循环队列链接，页面被访问时将访问位置1，当需要换出时，检查访问位，是0换出，如果是1置为0，继续滞留内存一次。按FIFO检查下一个页面。检查到最后一个还是1时，回到头，检查第一个。 改进的Clock算法。 磁盘调度 磁盘调度的目标是使磁盘的平均寻道时间最短。 FCFS，先来先服务算法 SSTF，最短寻道时间优先 要求访问的磁道离当前磁头最近。 SCAN扫描算法：欲访问的磁道离当前磁道最近，同时考虑方向。要求访问的磁道离当前磁头最近，且在磁道之外，直到没有更外的磁道才向里移动。又称电梯算法。 CSCAN：循环扫描。当前进程请求某一磁道，进程等待。CSCAN规定磁头只能单向移动。到达最外磁道后，立即返回最内磁道。即将最小磁道紧接最大磁道构成循环。 LinuxIO模型 1、阻塞IO模型 以socket为例，在进程空间调用recvfrom，其系统调用知道数据包到达且被复制到应用进程的缓冲区或者发生错误才返回，在此期间一直等待，进程从调用recvfrom开始到它返回的整段时间内都是被阻塞的，因此称为阻塞IO 2、非阻塞IO模型 应用进程调用recvfrom，如果缓冲区没有数据直接返回EWOULDBLOCK错误。一般对非阻塞IO进行轮询，以确定是否有数据到来。 3、IO多路复用模型 Linux提供select/poll，通过将一个或多个fd传递给select或poll系统调用，阻塞在select上。select/poll顺序扫描fd是否就绪。 4、信号驱动IO 开启套接字接口信号驱动IO功能，并通过系统调用sigaction执行信号处理函数。当数据准备就绪时，为该进程生成SIGIO信号，通过信号回调通知应用程序调用recvfrom来读取数据，并通知主函数处理数据。 5、异步IO 告知内核启动某个操作，并让内核在整个操作完成后通知我们。它与信号驱动IO的区别在于信号驱动IO由内核通知我们何时可以开始IO操作。而异步IO模型由内核通知我们IO操作已经完成。 IO多路复用 当服务器需要处理多个客户端接入请求时，可以利用多线程或者IO多路复用处理。IO多路复用技术通过把多个IO的阻塞复用到同一个select的阻塞上，是的系统可以在单线程的情况下同时处理多个客户端的请求。与多线程相比，IO多路复用的优势在于开销小，节省了系统资源。 "},"zother4-EasyJob/OS/Linux.html":{"url":"zother4-EasyJob/OS/Linux.html","title":"Linux","keywords":"","body":"Linux 常用命令 0.mkdir -p 一次可以建立多个目录 复制、删除、移动： 1.复制：cp -a（带文件特性一起复制）, cp -i（覆盖时询问）, cp -r（目录） 2.删除：rm -f（忽略警告） rm -r（递归删除） 3.移动: mv 文件1 文件2 目录，mv 目录名 新目录名(目录重命名) 磁盘与目录的容量： 4.磁盘：df, df -i（以inode数量显示）, df -h(MB,GB), df -m(MB) 5.目录: du -sm(列出总量,以MB显示) 连接： 6.ln(硬链接) 7.ln -s（软链接） 压缩/解压缩: 7.压缩: gzip -v, bzip2 -z 8.解压缩: gzip -d, bzip2 -d tar打包： 8.打包为gzip文件：tar -zcvf 新建的文件名 旧文件 解压缩：tar -zxvf 文件 9.打包为bzip2：tar -jcvf 新建的文件名 旧文件 解压缩：tar -jxvf 文件 备份目录： 10.dump -0j -f /tmp/etc.dump.bz2 /etc 11.恢复：restore -t -f /tmp/etc.dump.bz2 管道命令 12.选取：cut -d ' ' -f 1（用于分隔字符） cut -c 12-（字符范围，用于排列整齐的信息） 13.grep 'root'（查找root）grep -v 'root'（反向选择）-n（输出行号）-i（不区分大小写） 14.sort（默认） sort -t ':' -k 3 15.uniq: 仅列出一个显示 16.wc: 统计字数字符数 e.g. wc -l（仅列出行） cat /etc/man.config | wc 17.uniq（仅列出一个显示） 18.tee: 双向重定向 last | tee last.list | cut -d ' ' -f 1 字符转换命令 tr, col, join, paste, expand 19.tr: e.g. tr -d（删除指定字符）tr [a-z] [A-Z]小写转大写 20.col -x：将断行符^M（tab）转换成对等的空格 21.join -t ':' /etc/passwd /etc/shadow（连接） 22.paste（直接两行贴在一起） 23.expand -t（tab键转空格键） 24.切割命令split split -b 300k /etc/termcap termcap(按文件大小切割) split -l 10 - lsroot（按行切割） 25.xargs：很多命令不支持管道命令，可以通过xargs来提供该命令引用stdin之用。 管道命令：cut、grep、sort、wc、uniq、tee、tr、col、join、paste、expand、split、xargs 26.dmesg：列出内核信息 正则表达式 sed管道命令 27.sed -[nefr] 动作 (sed用于一整行的处理) 动作：a新增 c替换（整行替换） d删除 i插入 p打印（与-n安静模式搭配） sed的替换功能（字符串的替换）：sed 's/被替换的字符串/新字符串/g ' 28.格式化打印printf 29.awk（倾向于将一行分成数个“字段”来处理） e.g. last -n 5 | awk '{print $1 \"\\t\" $3}' FS（目前的分隔字符，默认空格字符） NR（目前awk处理的是第几行） NF（每行拥有的字段总数） cat /etc/passwd | awk '{FS=\":\"} $3 例子： cat pay.txt | awk 'NR==1 {printf \"%10s %10s %10s %10s %10s\\n\",$1,$2,$3,$4,\"total\" } NR>=2 {total=$2+$3+$4;printf \"%10s %10d %10d %10d %10.2f\\n\", $1, $2, $3, $4, total}' 数据流重定向 标准输入：0，或>> 标准错误输出：2，2>或2>> 正确错误写入同一个文件：find /home -name .bashrc > list 2>&1 文件比较： 30.diff passwd.old passwd.new（按行来比较） 31.cmp passwd.old passwd.new（按字节来比较） 32.patch diff制作补丁文件 diff -Naur passwd.old passwd.new 更新旧文件：patch -p0 查看 【less】 功能：less命令可以对文件或其他输出进行分页显示，与more命令相似。退出按q。 参数详解： -a：在当前屏幕显示最后 -c：从顶部（从上到下）刷新屏幕，并显示文件内容。而不是通过底部滚动完成刷新; -f：强制打开文件，二进制文件显示时，不提示警告； -i：搜索时忽略大小写；除非搜索串中包含大写字母; -I：搜索时忽略大小写，除非搜索串中包含小写字母; -m：显示当前读取文件的百分比 -M：显示当前读取文件的百分比、行号及总行数； -N：在每行前输出行号 -p pattern:搜索日志文件中含有pattern的所有日志内容； -s：把连续多个空白行作为一个空白行显示 -Q：在终端下不响铃 扩展： U：向上 J：向下 g：跳到第一行 G：跳到最后一行 /pattern:搜索pattern q：退出less "},"zother4-EasyJob/QA/Game-Test.html":{"url":"zother4-EasyJob/QA/Game-Test.html","title":"Game Test","keywords":"","body":"什么是黑盒测试和白盒测试？ 任何工程产品（注意是任何工程产品）都可以使用以下两种方法之一进行测试。 黑盒测试：已知产品的功能设计规格，可以进行测试证明每个实现了的功能是否符合要求。 白盒测试：已知产品的内部工作过程，可以通过测试证明每种内部操作是否符合设计规格要求，所有内部成分是否以经过检查。 软件的黑盒测试意味着测试要在软件的接口处进行。这种方法是把测试对象看做一个黑盒子，测试人员完全不考虑程序内部的逻辑结构和内部特性，只依据程序的需求规格说明书，检查程序的功能是否符合它的功能说明。因此黑盒测试又叫功能测试或数据驱动测试。黑盒测试主要是为了发现以下几类错误： 1、是否有不正确或遗漏的功能？ 2、在接口上，输入是否能正确的接受？能否输出正确的结果？ 3、是否有数据结构错误或外部信息（例如数据文件）访问错误？ 4、性能上是否能够满足要求？ 5、是否有初始化或终止性错误？ 黑盒测试方法主要有等价类划分、边值分析、因—果图、错误推测等 软件的白盒测试是对软件的过程性细节做细致的检查。这种方法是把测试对象看做一个打开的盒子，它允许测试人员利用程序内部的逻辑结构及有关信息，设计或选择测试用例，对程序所有逻辑路径进行测试。通过在不同点检查程序状态，确定实际状态是否与预期的状态一致。“白盒”法全面了解程序内部逻辑结构、对所有逻辑路径进行测试。“白盒”法是穷举路径测试。在使用这一方案时，测试者必须检查程序的内部结构，从检查程序的逻辑着手，得出测试数据。因此白盒测试又称为结构测试或逻辑驱动测试。白盒测试主要是想对程序模块进行如下检查： 1、对程序模块的所有独立的执行路径至少测试一遍。 2、对所有的逻辑判定，取“真”与取“假”的两种情况都能至少测一遍。 3、在循环的边界和运行的界限内执行循环体。 4、测试内部数据结构的有效性，等等。 以上事实说明，软件测试有一个致命的缺陷，即测试的不完全、不彻底性。由于任何程序只能进行少量（相对于穷举的巨大数量而言）的有限的测试，在未发现错误时，不能说明程序中没有错误。 测试方案： 1：熟悉被测功能，需要从策划的角度来了解功能需求，也同时需要以玩家的角度来思考这个功能对玩家是否有必要。 2：设计测试方案的时候需要多考虑各种异常情况，例如是否存在被玩家刷奖励的情况，因为往往规则上设计的漏洞才是游戏最致命的伤害。 3：设计测试方案的时候是否考虑自己测试是出于黑盒测试还是其他方式，如果是灰盒测试，估计还需要跟技术上的同事了解清楚代码的实现，看是否有漏洞。 4：如果测试的只是大系统，需要考虑性能测试，例如当人数达到一定程度对系统的影响等。 设计测试方案是一条很长远的路，即使是有好多年测试经验的人也在摸索，需要慢慢积累。 如何对手机游戏进行优化？ 一般分为内存优化帧数优化还有体积优化 帧数优化可以考虑对一个message loop中的逻辑运算进行优化，比如可以考虑A*的剪枝。 使用工具对资源进行打包，使用TexturePacker等工具把多张资源合成一张图片。 采用png压缩工具，在打包图片之前对每张图片进行压缩，降低图片质量。 针对不同的平台使用特定的压缩格式的图片 如果项目中帧序列占的比较多，那么可以采用降帧的方式来优化。 缩放图片，将原来图片缩小为原来的70% ~ %80，再对图像进行放大 采用编辑器，将大图转化为拼接，那么就可以利用地图编辑器、动作编辑器等从而减少体积，降低内存的使用 如何在对游戏的“手感”进行改进： 游戏手感一般指的是打击感，那么我就在打击到一个游戏对象时，游戏对象要产生击退的效果，产生该对象被打击的感觉。 时间控制要恰当，要让某个对象（比如火球，拳头）打击到另一个游戏对象的时候，才产生击退效果，这就需要进行使用消息机制和回调来解决。 如何在数据库中存储一个人的所有装备？ 建立一个人物ID和装备ID的关系表。 将人物的所有装备的id序列化为一个JSON字符串存储为人物的一个字段。 这两个最大的区别是在修改装备时，第一个只会影响一条记录，当时第二个会影响所有装备，一旦出现bug还让玩家损失所有装备。两者各有利弊，根据使用场景自己权衡。 网易游戏的产品 PC端 梦幻西游2 天下3 精灵传说 新大话西游2 代理 魔兽世界 炉石传说 手游 梦幻西游 大话西游 slogan 游戏它不只是娱乐，你得到的也远不止快乐 手游排行榜 我想说的是，网易游戏取得这样的成绩，可以说是必然，也可以说是偶然。 必然的是因为这两款游戏强大的群众基础和运营超过十年的丰富经验。大话西游是超过梦幻西游更久的游戏。反正我小学的时候已经有人在玩这个游戏了。梦幻西游呢，国产网游的带头羊，鼎盛时期各项数据一直刷榜首，在线游戏玩家几百万之多。虽然现在这两个游戏因为各种原因已经处于萧条的衰落期，都在走下坡路，但是它们都是从02、03年开始就活跃在网游玩家群体，因为受众广、玩家认可度高、技术比较成熟等，是国内网游界的标志性代表作，在跨度近十年的网游飞速发展的时期里作为旗舰，影响了很大批次的国内网游的制作和成长，更加可以说是影响了80、90两代热衷于网游的人的一段青春。直到现在，要是聊起网游，还是可以在人群里听到“嗌，我也玩了很久梦幻大话哎！”拥有超过十年的游戏运营时间可以用来攒经验，这两个游戏的运行团队从中可以堆积起一个庞大的数据库和坚实的运营平台，他们深知玩家需要的是什么，市场需要的是什么，游戏需要的是什么，熟悉游戏运营的每个环节，能准确根据各种反馈数据来调整更新的内容。只要把手游市场的模式弄清楚，后期宣传好的话，这两个游戏不火都对不起我们十年青春。以上是必然。 偶然呢？我个人认为，手游是否能取得成功，关键在于他们吸引新玩家能做到什么程度。其实，端游的衰败，很大程度是因为对新玩家的吸引力变小了，可以说是游戏老化了。因为我没玩过手游，我无法对手游本身作评价，我说说我看到的。我身边几乎玩过端游的梦幻老玩家都多多少少玩了一段时间的手游。评价是比较好的，对端游鼎盛时期的还原程度比较高，画面比较精细趣致，玩起来比较流畅。其实呢，在这个版本推出之前，梦幻就出过一个版本的手游，但是它很大程度是为端游服务的，手游获得的物品和奖励可以通过端游兑现，而且可以用于扩大端游收益，基本上是没有非端游玩家参与进来的。但是，应该是从做这个旧手游，网易的手游研发团队得到了很大的提升，摸清楚了手游的运转和构建，为现在新版本的推出打下良好的基础。在我所知道的没接触过端游的手游版玩家中，有为了泡妞的男生（说明手游的玩家交流系统做的不错，玩家互动一直是梦幻做得好的地方），有觉得可爱才下载的萌妹子（画面做的加分也是很重要的），有被朋友拉近坑的新人（还是老玩家效应的延伸啊）。。。或许，手游发展的势头这样延续下去，即使端游失败了，梦幻会以另外一种形式继续存在于我们的世界中吧。离开梦幻是很多人很无奈的事情，但是在我看来也算是一种成长。能看到它换了一副躯壳，还是跟我一起成长着，还是很欣慰和高兴的。 冒烟测试和回归测试的区别： 　　冒烟测试是自由测试的一种。冒烟测试(smoketest)在测试中发现问题，找到了一个Bug，然后开发人员会来修复这个Bug。这时想知道这次修复是否真的解决了程序的Bug，或者是否会对其它模块造成影响，就需要针对此问题进行专门测试，这个过程就被称为SmokeTest。在很多情况下，做SmokeTest是开发人员在试图解决一个问题的时候，造成了其它功能模块一系列的连锁反应，原因可能是只集中考虑了一开始的那个问题，而忽略其它的问题，这就可能引起了新的Bug。SmokeTest优点是节省测试时间，防止build失败。缺点是覆盖率还是比较低。 　　回归测试是指修改了旧代码后，重新进行测试以确认修改没有引入新的错误或导致其他代码产生错误。自动回归测试将大幅降低系统测试、维护升级等阶段的成本。回归测试作为软件生命周期的一个组成部分，在整个软件测试过程中占有很大的工作量比重，软件开发的各个阶段都会进行多次回归测试。在渐进和快速迭代开发中，新版本的连续发布使回归测试进行的更加频繁，而在极端编程方法中，更是要求每天都进行若干次回归测试。因此，通过选择正确的回归测试策略来改进回归测试的效率和有效性是非常有意义的。 "},"zother4-EasyJob/Questions/java_summary.html":{"url":"zother4-EasyJob/Questions/java_summary.html","title":"Java Summary","keywords":"","body":"Java面试知识点总结 最近一次更新2017年12月08日 大纲 [x] 一、Java基础(语言、集合框架、OOP、设计模式等) [x] 二、Java高级(JavaEE、框架、服务器、工具等) [x] 三、多线程和并发 [x] 四、Java虚拟机 [x] 五、数据库(Sql、MySQL、Redis等) [x] 六、算法与数据结构 [x] 七、计算机网络 [x] 八、操作系统(OS基础、Linux等) [x] 九、其他 　一、Java基础（语言、集合框架、OOP、设计模式等） 1. HashMap和Hashtable的区别 [x] Hashtable是基于陈旧的Dictionary的Map接口的实现，而HashMap是基于哈希表的Map接口的实现 [x] 从方法上看，HashMap去掉了Hashtable的contains方法 [x] HashTable是同步的(线程安全)，而HashMap线程不安全，效率上HashMap更快 [x] HashMap允许空键值，而Hashtable不允许 [x] HashMap的iterator迭代器执行快速失败机制，也就是说在迭代过程中修改集合结构，除非调用迭代器自身的remove方法，否则以其他任何方式的修改都将抛出并发修改异常。而Hashtable返回的Enumeration不是快速失败的。 注：Fast-fail机制:在使用迭代器的过程中有其它线程修改了集合对象结构或元素数量,都将抛出ConcurrentModificationException，但是抛出这个异常是不保证的，我们不能编写依赖于此异常的程序。 2. java的线程安全 Vector、Stack、HashTable、ConcurrentHashMap、Properties 3. java集合框架(常用) Collection - List - ArrayList Collection - List - LinkedList Collection - List - Vector Collection - Queue - PriorityQueue Collection - List - Vector - Stack Collection - Set - HashSet Collection - Set - TreeSet Collection - Set - LinkedHashSet Map - HashMap Map - TreeMap Map - HashTable Map - LinkedHashMap Map - ConcurrentHashMap 3.1 List集合和Set集合 List中元素存取是有序的、可重复的；Set集合中元素是无序的，不可重复的。 CopyOnWriteArrayList:COW的策略，即写时复制的策略。适用于读多写少的并发场景 Set集合元素存取无序，且元素不可重复。 HashSet不保证迭代顺序，线程不安全；LinkedHashSet是Set接口的哈希表和链接列表的实现，保证迭代顺序，线程不安全。 TreeSet：可以对Set集合中的元素排序，元素以二叉树形式存放，线程不安全。 3.2 ArrayList、LinkedList、Vector的区别 首先它们均是List接口的实现。 ArrayList、LinkedList的区别 1.随机存取：ArrayList是基于可变大小的数组实现，LinkedList是链接列表的实现。这也就决定了对于随机访问的get和set的操作，ArrayList要优于LinkedList，因为LinkedList要移动指针。 2.插入和删除：LinkedList要好一些，因为ArrayList要移动数据，更新索引。 3.内存消耗：LinkedList需要更多的内存，因为需要维护指向后继结点的指针。 Vector从JDK 1.0起就存在，在1.2时改为实现List接口，功能与ArrayList类似，但是Vector具备线程安全。 3.3 Map集合 Hashtable:基于Dictionary类，线程安全，速度快。底层是哈希表数据结构。是同步的。 不允许null作为键，null作为值。 Properties:Hashtable的子类。用于配置文件的定义和操作，使用频率非常高，同时键和值都是字符串。 HashMap：线程不安全，底层是数组加链表实现的哈希表。允许null作为键，null作为值。HashMap去掉了contains方法。 注意：HashMap不保证元素的迭代顺序。如果需要元素存取有序，请使用LinkedHashMap TreeMap：可以用来对Map集合中的键进行排序。 ConcurrentHashMap:是JUC包下的一个并发集合。 3.4 为什么使用ConcurrentHashMap而不是HashMap或Hashtable？ HashMap的缺点：主要是多线程同时put时，如果同时触发了rehash操作，会导致HashMap中的链表中出现循环节点，进而使得后面get的时候，会死循环，CPU达到100%，所以在并发情况下不能使用HashMap。让HashMap同步：Map m = Collections.synchronizeMap(hashMap);而Hashtable虽然是同步的，使用synchronized来保证线程安全，但在线程竞争激烈的情况下HashTable的效率非常低下。因为当一个线程访问HashTable的同步方法时，其他线程访问HashTable的同步方法时，可能会进入阻塞或轮询状态。如线程1使用put进行添加元素，线程2不但不能使用put方法添加元素，并且也不能使用get方法来获取元素，所以竞争越激烈效率越低。 ConcurrentHashMap的原理： Hashtable容器在竞争激烈的并发环境下表现出效率低下的原因在于所有访问Hashtable的线程都必须竞争同一把锁，那假如容器里有多把锁，每一把锁用于锁容器其中一部分数据，那么当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效的提高并发访问效率，这就是ConcurrentHashMap所使用的锁分段技术，首先将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。 ConcurrentHashMap的结构： ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成。Segment是一种可重入互斥锁ReentrantLock，在ConcurrentHashMap里扮演锁的角色，HashEntry则用于存储键值对数据。一个ConcurrentHashMap里包含一个Segment数组，Segment的结构和HashMap类似，是一种数组和链表结构， 一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元素，当对某个HashEntry数组的数据进行修改时，必须首先获得它对应的Segment锁。 ConcurrentHashMap的构造、get、put操作： 构造函数：传入参数分别为 1、初始容量，默认16 2、装载因子 装载因子用于rehash的判定，就是当ConcurrentHashMap中的元素大于装载因子*最大容量时进行扩容，默认0.75 3、并发级别 这个值用来确定Segment的个数，Segment的个数是大于等于concurrencyLevel的第一个2的n次方的数。比如，如果concurrencyLevel为12，13，14，15，16这些数，则Segment的数目为16(2的4次方)。默认值为static final int DEFAULT_CONCURRENCY_LEVEL = 16;。理想情况下ConcurrentHashMap的真正的并发访问量能够达到concurrencyLevel，因为有concurrencyLevel个Segment，假如有concurrencyLevel个线程需要访问Map，并且需要访问的数据都恰好分别落在不同的Segment中，则这些线程能够无竞争地自由访问（因为他们不需要竞争同一把锁），达到同时访问的效果。这也是为什么这个参数起名为“并发级别”的原因。默认16. 初始化的一些动作： 初始化segments数组（根据并发级别得到数组大小ssize），默认16 初始化segmentShift和segmentMask（这两个全局变量在定位segment时的哈希算法里需要使用），默认情况下segmentShift为28，segmentMask为15 初始化每个Segment，这一步会确定Segment里HashEntry数组的长度. put操作： 1、判断value是否为null，如果为null，直接抛出异常。 2、key通过一次hash运算得到一个hash值。将得到hash值向右按位移动segmentShift位，然后再与segmentMask做&运算得到segment的索引j。即segmentFor方法 3、使用Unsafe的方式从Segment数组中获取该索引对应的Segment对象。向这个Segment对象中put值，这个put操作也基本是一样的步骤（通过&运算获取HashEntry的索引，然后set）。 get操作： 1、和put操作一样，先通过key进行hash确定应该去哪个Segment中取数据。 2、使用Unsafe获取对应的Segment，然后再进行一次&运算得到HashEntry链表的位置，然后从链表头开始遍历整个链表（因为Hash可能会有碰撞，所以用一个链表保存），如果找到对应的key，则返回对应的value值，如果链表遍历完都没有找到对应的key，则说明Map中不包含该key，返回null。 定位Segment的hash算法：(hash >>> segmentShift) & segmentMask 定位HashEntry所使用的hash算法：int index = hash & (tab.length - 1); 注： 1.tab为HashEntry数组 2.ConcurrentHashMap既不允许key为null 也不允许value为null 3.5 Collection 和 Collections的区别 Collection是集合类的上级接口，子接口主要有Set 和List、Queue Collections是针对集合类的一个辅助类，提供了操作集合的工具方法：一系列静态方法实现对各种集合的搜索、排序、线程安全化等操作。 3.6 Map、Set、List、Queue、Stack的特点与用法 Set集合类似于一个罐子，\"丢进\"Set集合里的多个对象之间没有明显的顺序。 List集合代表元素有序、可重复的集合，集合中每个元素都有其对应的顺序索引。 Stack是Vector提供的一个子类，用于模拟\"栈\"这种数据结构(LIFO后进先出) Queue用于模拟\"队列\"这种数据结构(先进先出 FIFO)。 Map用于保存具有\"映射关系\"的数据，因此Map集合里保存着两组值。 3.7 HashMap的工作原理 HashMap维护了一个Entry数组，Entry内部类有key,value，hash和next四个字段，其中next也是一个Entry类型。可以将Entry数组理解为一个个的散列桶。每一个桶实际上是一个单链表。当执行put操作时，会根据key的hashcode定位到相应的桶。遍历单链表检查该key是否已经存在，如果存在，覆盖该value，反之，新建一个新的Entry，并放在单链表的头部。当通过传递key调用get方法时，它再次使用key.hashCode()来找到相应的散列桶，然后使用key.equals()方法找出单链表中正确的Entry，然后返回它的值。 3.8 Map的实现类的介绍 HashMap基于散列表来的实现，即使用hashCode()进行快速查询元素的位置，显著提高性能。插入和查询“键值对”的开销是固定的。可以通过设置容量和装载因子，以调整容器的性能。 LinkedHashMap, 类似于HashMap,但是迭代遍历它时，保证迭代的顺序是其插入的次序，因为它使用链表维护内部次序。此外可以在构造器中设定LinkedHashMap，使之采用LRU算法。使没有被访问过的元素或较少访问的元素出现在前面，访问过的或访问多的出现在后面。这对于需要定期清理元素以节省空间的程序员来说，此功能使得程序员很容易得以实现。 TreeMap, 是基于红黑树的实现。同时TreeMap实现了SortedMap接口，该接口可以确保键处于排序状态。所以查看“键”和“键值对”时，所有得到的结果都是经过排序的，次序由自然排序或提供的Comparator决定。SortedMap接口拥有其他额外的功能，如：返回当前Map使用的Comparator比较器，firstKey()，lastKey(),headMap(toKey),tailMap(fromKey)以及可以返回一个子树的subMap()方法等。 WeakHashMap，表示弱键映射，WeakHashMap 的工作与正常的 HashMap 类似，但是使用弱引用作为 key，意思就是当 key 对象没有任何引用时，key/value 将会被回收。 ConcurrentHashMap， 在HashMap基础上分段锁机制实现的线程安全的HashMap。 IdentityHashMap 使用==代替equals() 对“键”进行比较的散列映射。专为解决特殊问题而设计。 HashTable：基于Dictionary类的Map接口的实现，它是线程安全的。 3.9 LinkedList 和 PriorityQueue 的区别 它们均是Queue接口的实现。拥有FIFO的特点，它们的区别在于排序行为。LinkedList 支持双向列表操作， PriorityQueue 按优先级组织的队列，元素的出队次序由元素的自然排序或者由Comparator比较器指定。 3.10 线程安全的集合类。Vector、Hashtable、Properties和Stack、ConcurrentHashMap 3.11 BlockingQueue java.util.concurrent.BlockingQueue是一个队列，在进行获取元素时，它会等待队列变为非空；当在添加一个元素时，它会等待队列中的可用空间。BlockingQueue接口是Java集合框架的一部分，主要用于实现生产者-消费者模式。我们不需要担心等待生产者有可用的空间，或消费者有可用的对象，因为它都在BlockingQueue的实现类中被处理了。Java提供了集中BlockingQueue的实现，比如ArrayBlockingQueue、LinkedBlockingQueue、PriorityBlockingQueue,、SynchronousQueue等。 3.12 如何对一组对象进行排序 如果需要对一个对象数组进行排序，我们可以使用Arrays.sort()方法。如果我们需要排序一个对象列表，我们可以使用Collections.sort()方法。排序时是默认根据元素的自然排序（使用Comparable）或使用Comparator外部比较器。Collections内部使用数组排序方法，所有它们两者都有相同的性能，只是Collections需要花时间将列表转换为数组。 4. ArrayList 无参构造 容量为10 ArrayList(Collections c)构造包含指定collection的元素的列表 ArrayList(int initialCapacity) 指定初始容量 5. final关键字 final修饰的变量是常量，必须进行初始化，可以显示初始化，也可以通过构造函数进行初始化，如果不初始化编译会报错。 6. 接口与抽象类 6.1 一个子类只能继承一个抽象类，但能实现多个接口 6.2 抽象类可以有构造方法，接口没有构造方法 6.3 抽象类可以有普通成员变量，接口没有普通成员变量 6.4 抽象类和接口都可有静态成员变量，抽象类中静态成员变量访问类型任意，接口只能public static final(默认) 6.5 抽象类可以没有抽象方法，抽象类可以有普通方法,接口中都是抽象方法 6.6 抽象类可以有静态方法，接口不能有静态方法 6.7 抽象类中的方法可以是public、protected;接口方法只有public abstract 7. 抽象类和最终类 抽象类可以没有抽象方法, 最终类可以没有最终方法 最终类不能被继承, 最终方法不能被重写(可以重载) 8.异常 相关的关键字 throw、throws、try...catch、finally throws 用在方法签名上, 以便抛出的异常可以被调用者处理 throw 方法内部通过throw抛出异常 try 用于检测包住的语句块, 若有异常, catch子句捕获并执行catch块 9. 关于finally finally不管有没有异常都要处理 当try和catch中有return时，finally仍然会执行，finally比return先执行 不管有木有异常抛出, finally在return返回前执行 finally是在return后面的表达式运算后执行的（此时并没有返回运算后的值，而是先把要返回的值保存起来，管finally中的代码怎么样，返回的值都不会改变，仍然是之前保存的值），所以函数返回值是在finally执行前确定的 注意：finally中最好不要包含return，否则程序会提前退出，返回值不是try或catch中保存的返回值 finally不执行的几种情况：程序提前终止如调用了System.exit, 病毒，断电 10. 受检查异常和运行时异常 10.1 粉红色的是受检查的异常(checked exceptions),其必须被try...catch语句块所捕获, 或者在方法签名里通过throws子句声明。受检查的异常必须在编译时被捕捉处理,命名为Checked Exception是因为Java编译器要进行检查, Java虚拟机也要进行检查, 以确保这个规则得到遵守。 常见的checked exception：ClassNotFoundException IOException FileNotFoundException EOFException 10.2 绿色的异常是运行时异常(runtime exceptions), 需要程序员自己分析代码决定是否捕获和处理,比如空指针,被0除... 常见的runtime exception：NullPointerException ArithmeticException ClassCastException IllegalArgumentException IllegalStateException IndexOutOfBoundsException NoSuchElementException 10.3 而声明为Error的，则属于严重错误，如系统崩溃、虚拟机错误、动态链接失败等，这些错误无法恢复或者不可能捕捉，将导致应用程序中断，Error不需要捕获。 11. this & super 11.1 super出现在父类的子类中。有三种存在方式 super.xxx(xxx为变量名或对象名)意思是获取父类中xxx的变量或引用 super.xxx(); (xxx为方法名)意思是直接访问并调用父类中的方法 super() 调用父类构造 注：super只能指代其直接父类 11.2 this() & super()在构造方法中的区别 调用super()必须写在子类构造方法的第一行, 否则编译不通过 super从子类调用父类构造, this在同一类中调用其他构造 均需要放在第一行 尽管可以用this调用一个构造器, 却不能调用2个 this和super不能出现在同一个构造器中, 否则编译不通过 this()、super()都指的对象,不可以在static环境中使用 本质this指向本对象的指针。super是一个关键字 12. 修饰符一览 修饰符 类内部 同一个包 子类 任何地方 private yes default yes yes protected yes yes yes public yes yes yes yes 13. 内部类和静态内部类 public class Enclosingone { public class Insideone {} public static class Insideone{} } public class Test { public static void main(String[] args) { // 构造内部类对象需要外部类的引用 Enclosingone.Insideone obj1 = new Enclosingone().new Insideone(); // 构造静态内部类的对象 Enclosingone.Insideone obj2 = new Enclosingone.Insideone(); } } 静态内部类不需要有指向外部类的引用。但非静态内部类需要持有对外部类的引用。非静态内部类能够访问外部类的静态和非静态成员。静态内部类不能访问外部类的非静态成员，只能访问外部类的静态成员。 14. 序列化 声明为static和transient类型的数据不能被序列化， 反序列化需要一个无参构造函数 序列化参见我的笔记Java-note-序列化.md 15.正则表达式 次数符号 * 0或多次 + 1或多次 ？0或1次 {n} 恰n次 {n,m} 从n到m次 其他符号 符号 等价形式 \\d [0-9] \\D [^0-9] \\w [a-zA-Z_0-9] \\W [^a-zA-Z_0-9] \\s [\\t\\n\\r\\f] \\S [^\\t\\n\\r\\f] . 任何字符 边界匹配器 行开头 ^ 行结尾 $ 单词边界 \\b 贪婪模式:最大长度匹配 非贪婪模式:匹配到结果就好,最短匹配 环视 字符 描述 匹配对象 . 单个任意字符 [...] 字符组 列出的任意字符 [^...] 未列出的任意字符 ^ caret 行的起始位置 $ dollar 行的结束位置 \\ 单词的结束位置 \\b 单词边界 \\B 非单词边界 (?=Expression) 顺序肯定环视 成功,如果右边能够匹配 (?!Expression) 顺序否定环视 成功,如果右边不能够匹配 (?举例:北京市(海淀区)(朝阳区)(西城区) Regex: .*(?=\\() 模式和匹配器的典型调用次序 把正则表达式编译到模式中 Pattern p = Pattern.compile(\"a*b\"); 创建给定输入与此模式的匹配器 Matcher m = p.matcher(\"aaab\"); 尝试将整个区域与此模式匹配 boolean b = m.matches(); 16. 面向对象的五大基本原则(solid) S单一职责SRP:Single-Responsibility Principle 一个类,最好只做一件事,只有一个引起它的变化。单一职责原则可以看做是低耦合,高内聚在面向对象原则的引申,将职责定义为引起变化的原因,以提高内聚性减少引起变化的原因。 O开放封闭原则OCP:Open-Closed Principle 软件实体应该是可扩展的,而不是可修改的。对扩展开放,对修改封闭 L里氏替换原则LSP:Liskov-Substitution Principle 子类必须能够替换其基类。这一思想表现为对继承机制的约束规范,只有子类能够替换其基类时,才能够保证系统在运行期内识别子类,这是保证继承复用的基础。 I接口隔离原则ISP:Interface-Segregation Principle 使用多个小的接口,而不是一个大的总接口 D依赖倒置原则DIP:Dependency-Inversion Principle 依赖于抽象。具体而言就是高层模块不依赖于底层模块,二者共同依赖于抽象。抽象不依赖于具体,具体依赖于抽象。 17. 面向对象设计其他原则 封装变化 少用继承 多用组合 针对接口编程 不针对实现编程 为交互对象之间的松耦合设计而努力 类应该对扩展开发 对修改封闭（开闭OCP原则） 依赖抽象，不要依赖于具体类（依赖倒置DIP原则） 密友原则：只和朋友交谈（最少知识原则，迪米特法则） 说明：一个对象应当对其他对象有尽可能少的了解，将方法调用保持在界限内，只调用属于以下范围的方法：该对象本身（本地方法）对象的组件 被当作方法参数传进来的对象 此方法创建或实例化的任何对象 别找我（调用我） 我会找你（调用你）（好莱坞原则） 一个类只有一个引起它变化的原因（单一职责SRP原则） 18. null可以被强制转型为任意类型的对象 19.代码执行次序 多个静态成员变量, 静态代码块按顺序执行 单个类中: 静态代码 -> main方法 -> 构造块 -> 构造方法 构造块在每一次创建对象时执行 涉及父类和子类的初始化过程 a.初始化父类中的静态成员变量和静态代码块 b.初始化子类中的静态成员变量和静态代码块 c.初始化父类的普通成员变量和构造代码块(按次序)，再执行父类的构造方法(注意父类构造方法中的子类方法覆盖) d.初始化子类的普通成员变量和构造代码块(按次序)，再执行子类的构造方法 20. 数组复制方法 for逐一复制 System.arraycopy() -> 效率最高native方法 Arrays.copyOf() -> 本质调用arraycopy clone方法 -> 返回Object[],需要强制类型转换 21. 多态 Java通过方法重写和方法重载实现多态 方法重写是指子类重写了父类的同名方法 方法重载是指在同一个类中，方法的名字相同，但是参数列表不同 22. Java文件 java文件可以包含多个类，唯一的限制就是：一个文件中只能有一个public类， 并且此public类必须与 文件名相同。而且这些类和写在多个文件中没有区别。 23. Java移位运算符 java中有三种移位运算符 :左移运算符,x >> :带符号右移,x >> 1,相当于x除以2,正数高位补0,负数高位补1 >>> :无符号右移,忽略符号位,空位都以0补齐 参见我的GitHub，Java移位符 24. 形参&实参 形式参数可被视为local variable.形参和局部变量一样都不能离开方法。只有在方法中使用，不会在方法外可见。 形式参数只能用final修饰符，其它任何修饰符都会引起编译器错误。但是用这个修饰符也有一定的限制，就是在方法中不能对参数做任何修改。不过一般情况下，一个方法的形参不用final修饰。只有在特殊情况下，那就是：方法内部类。一个方法内的内部类如果使用了这个方法的参数或者局部变量的话，这个参数或局部变量应该是final。 形参的值在调用时根据调用者更改，实参则用自身的值更改形参的值（指针、引用皆在此列），也就是说真正被传递的是实参。 25. IO流一览 26. 局部变量为什么要初始化 局部变量是指类方法中的变量，必须初始化。局部变量运行时被分配在栈中，量大，生命周期短，如果虚拟机给每个局部变量都初始化一下，是一笔很大的开销，但变量不初始化为默认值就使用是不安全的。出于速度和安全性两个方面的综合考虑，解决方案就是虚拟机不初始化，但要求编写者一定要在使用前给变量赋值。 27. Java语言的鲁棒性 Java在编译和运行程序时，都要对可能出现的问题进行检查，以消除错误的产生。它提供自动垃圾收集来进行内存管理，防止程序员在管理内存时容易产生的错误。通过集成的面向对象的例外处理机制，在编译时，Java揭示出可能出现但未被处理的异常，帮助程序员正确地进行选择以防止系统的崩溃。另外，Java在编译时还可捕获类型声明中的许多常见错误，防止动态运行时不匹配问题的出现。 28. Java语言特性 Java致力于检查程序在编译和运行时的错误 Java虚拟机实现了跨平台接口 类型检查帮助检查出许多开发早期出现的错误 Java自己操纵内存减少了内存出错的可能性 Java还实现了真数组，避免了覆盖数据的可能 29. 包装类的equals()方法不处理数据转型，必须类型和值都一样才相等。 30. 子类可以继承父类的静态方法！但是不能覆盖。因为静态方法是在编译时确定了，不能多态，也就是不能运行时绑定。 31. Java语法糖 Java7的switch用字符串 - hashcode方法 switch用于enum枚举 伪泛型 - List原始类型 自动装箱拆箱 - Integer.valueOf和Integer.intValue foreach遍历 - Iterator迭代器实现 条件编译 enum枚举类、内部类 可变参数 - 数组 断言语言 try语句中定义和关闭资源 32. Java 中应该使用什么数据类型来代表价格？ 如果不是特别关心内存和性能的话，使用BigDecimal，否则使用预定义精度的 double 类型。 33. 怎么将 byte 转换为 String？ 可以使用 String 接收 byte[] 参数的构造器来进行转换，需要注意的点是要使用的正确的编码，否则会使用平台默认编码，这个编码可能跟原来的编码相同，也可能不同。 34. Java 中怎样将 bytes 转换为 long 类型？ String接收bytes的构造器转成String，再Long.parseLong 35. 我们能将 int 强制转换为 byte 类型的变量吗？如果该值大于 byte 类型的范围，将会出现什么现象？ 是的，我们可以做强制转换，但是 Java 中 int 是 32 位的，而 byte 是 8 位的，所以，如果强制转化是，int 类型的高 24 位将会被丢弃，byte 类型的范围是从 -128 到 127。 36. 存在两个类，B 继承 A，C 继承 B，我们能将 B 转换为 C 么？如 C = (C) B； 可以，向下转型。但是不建议使用，容易出现类型转型异常. 37. 哪个类包含 clone 方法？是 Cloneable 还是 Object？ java.lang.Cloneable 是一个标示性接口，不包含任何方法，clone 方法在 Object 类中定义。并且需要知道 clone() 方法是一个本地方法，这意味着它是由 c 或 c++ 或 其他本地语言实现的。 38. Java 中 ++ 操作符是线程安全的吗？ 不是线程安全的操作。它涉及到多个指令，如读取变量值，增加，然后存储回内存，这个过程可能会出现多个线程交差。还会存在竞态条件（读取-修改-写入）。 39. a = a + b 与 a += b 的区别 += 隐式的将加操作的结果类型强制转换为持有结果的类型。如果两这个整型相加，如 byte、short 或者 int，首先会将它们提升到 int 类型，然后在执行加法操作。 byte a = 127; byte b = 127; b = a + b; // error : cannot convert from int to byte b += a; // ok （因为 a+b 操作会将 a、b 提升为 int 类型，所以将 int 类型赋值给 byte 就会编译出错） 40. 我能在不进行强制转换的情况下将一个 double 值赋值给 long 类型的变量吗？ 不行，你不能在没有强制类型转换的前提下将一个 double 值赋值给 long 类型的变量，因为 double 类型的范围比 long 类型更广，所以必须要进行强制转换。 41. 3*0.1 == 0.3 将会返回什么？true 还是 false？ false，因为有些浮点数不能完全精确的表示出来。 42. int 和 Integer 哪个会占用更多的内存？ Integer 对象会占用更多的内存。Integer 是一个对象，需要存储对象的元数据。但是 int 是一个原始类型的数据，所以占用的空间更少。 43. 为什么 Java 中的 String 是不可变的（Immutable）？ Java 中的 String 不可变是因为 Java 的设计者认为字符串使用非常频繁，将字符串设置为不可变可以允许多个客户端之间共享相同的字符串。 44. 我们能在 Switch 中使用 String 吗？ 从 Java 7 开始，我们可以在 switch case 中使用字符串，但这仅仅是一个语法糖。内部实现在 switch 中使用字符串的 hash code。 45. Java 中的构造器链是什么？ 当你从一个构造器中调用另一个构造器，就是Java 中的构造器链。这种情况只在重载了类的构造器的时候才会出现。 46. 枚举类 JDK1.5出现 每个枚举值都需要调用一次构造函数。 48. 什么是不可变对象（immutable object）？Java 中怎么创建一个不可变对象？ 不可变对象指对象一旦被创建，状态就不能再改变。任何修改都会创建一个新的对象，如 String、Integer及其它包装类。 如何在Java中写出Immutable的类？ 要写出这样的类，需要遵循以下几个原则： 1）immutable对象的状态在创建之后就不能发生改变，任何对它的改变都应该产生一个新的对象。 2）Immutable类的所有的属性都应该是final的。 3）对象必须被正确的创建，比如：对象引用在对象创建过程中不能泄露(leak)。 4）对象应该是final的，以此来限制子类继承父类，以避免子类改变了父类的immutable特性。 5）如果类中包含mutable类对象，那么返回给客户端的时候，返回该对象的一个拷贝，而不是该对象本身（该条可以归为第一条中的一个特例） 49. 我们能创建一个包含可变对象的不可变对象吗？ 是的，我们是可以创建一个包含可变对象的不可变对象的，你只需要谨慎一点，不要共享可变对象的引用就可以了，如果需要变化时，就返回原对象的一个拷贝。最常见的例子就是对象中包含一个日期对象的引用。 50. List和Set List 是一个有序集合，允许元素重复。它的某些实现可以提供基于下标值的常量访问时间，但是这不是 List 接口保证的。Set 是一个无序集合。 51. poll() 方法和 remove() 方法的区别？ poll() 和 remove() 都是从队列中取出一个元素，但是 poll() 在获取元素失败的时候会返回空，但是 remove() 失败的时候会抛出异常。 52. Java 中 LinkedHashMap 和 PriorityQueue 的区别是什么？ PriorityQueue 保证最高或者最低优先级的的元素总是在队列头部，但是 LinkedHashMap 维持的顺序是元素插入的顺序。当遍历一个 PriorityQueue 时，没有任何顺序保证，但是 LinkedHashMap 课保证遍历顺序是元素插入的顺序。 53. ArrayList 与 LinkedList 的区别？ 最明显的区别是 ArrrayList 底层的数据结构是数组，支持随机访问，而 LinkedList 的底层数据结构书链表，不支持随机访问。使用下标访问一个元素，ArrayList 的时间复杂度是 O(1)，而 LinkedList 是 O(n)。 54. 用哪两种方式来实现集合的排序？ 你可以使用有序集合，如 TreeSet 或 TreeMap，你也可以使用有顺序的的集合，如 List，然后通过 Collections.sort() 来排序。 55. Java 中怎么打印数组？ 你可以使用 Arrays.toString() 和 Arrays.deepToString() 方法来打印数组。由于数组没有实现 toString() 方法，所以如果将数组传递给 System.out.println() 方法，将无法打印出数组的内容，但是 Arrays.toString() 可以打印每个元素。 56. Java 中的 LinkedList 是单向链表还是双向链表？ 是双向链表，你可以检查 JDK 的源码。在 Eclipse，你可以使用快捷键 Ctrl + T，直接在编辑器中打开该类。 57. Java 中的 TreeMap 是采用什么树实现的？ Java 中的 TreeMap 是使用红黑树实现的。 58. Java 中的 HashSet，内部是如何工作的？ HashSet 的内部采用 HashMap来实现。由于 Map 需要 key 和 value，所以所有 key 的都有一个默认 value。类似于 HashMap，HashSet 不允许重复的 key，只允许有一个null key，意思就是 HashSet 中只允许存储一个 null 对象。 59. 写一段代码在遍历 ArrayList 时移除一个元素？ 该问题的关键在于面试者使用的是 ArrayList 的 remove() 还是 Iterator 的 remove()方法。后者是正确的方式不会出现 ConcurrentModificationException 异常。 60. 我们能自己写一个容器类，然后使用 for-each 循环吗？ 可以，你可以写一个自己的容器类。如果你想使用 Java 中增强的循环来遍历，你只需要实现 Iterable 接口。如果你实现 Collection 接口，默认就具有该属性。 61. ArrayList 和 HashMap 的默认大小是多数？ 在 Java 7 中，ArrayList 的默认大小是 10 个元素，HashMap 的默认大小是16个元素（必须是2的幂）。这就是 Java 7 中 ArrayList 和 HashMap 类的代码片段： // from ArrayList.java JDK 1.7 private static final int DEFAULT_CAPACITY = 10; //from HashMap.java JDK 7 static final int DEFAULT_INITIAL_CAPACITY = 1 62. 有没有可能两个不相等的对象有有相同的 hashcode？ 有可能，两个不相等的对象可能会有相同的 hashcode 值，这就是为什么在 hashmap 中会有冲突。相等 hashcode 值的规定只是说如果两个对象相等，必须有相同的hashcode 值，但是没有关于不相等对象的任何规定。 63. 两个相同的对象会有不同的的 hash code 吗？ 不能，根据 hash code 的规定，这是不可能的。 64. 我们可以在 hashcode() 中使用随机数字吗？ 不行，因为对象的 hashcode 值必须是相同的。 65. Java 中，Comparator 与 Comparable 有什么不同？ Comparable 接口用于定义对象的自然顺序，而 comparator 通常用于定义用户定制的顺序。Comparable 总是只有一个，但是可以有多个 comparator 来定义对象的顺序。 66. 为什么在重写 equals 方法的时候需要重写 hashCode 方法？ 因为有强制的规范指定需要同时重写 hashcode 与 equal 方法，许多容器类，如 HashMap、HashSet 都依赖于 hashcode 与 equals 的规定。 67. “a==b”和”a.equals(b)”有什么区别？ 如果 a 和 b 都是对象，则 a==b 是比较两个对象的引用，只有当 a 和 b 指向的是堆中的同一个对象才会返回 true，而 a.equals(b) 是进行逻辑比较，所以通常需要重写该方法来提供逻辑一致性的比较。例如，String 类重写 equals() 方法，所以可以用于两个不同对象，但是包含的字母相同的比较。 68. a.hashCode() 有什么用？与 a.equals(b) 有什么关系？ 简介：hashCode() 方法是相应对象整型的 hash 值。它常用于基于 hash 的集合类，如 Hashtable、HashMap、LinkedHashMap等等。它与 equals() 方法关系特别紧密。根据 Java 规范，两个使用 equal() 方法来判断相等的对象，必须具有相同的 hash code。 1、hashcode的作用 List和Set，如何保证Set不重复呢？通过迭代使用equals方法来判断，数据量小还可以接受，数据量大怎么解决？引入hashcode，实际上hashcode扮演的角色就是寻址，大大减少查询匹配次数。 2、hashcode重要吗 对于数组、List集合就是一个累赘。而对于HashMap, HashSet, Hashtable就异常重要了。 3、equals方法遵循的原则 对称性 若x.equals(y)true，则y.equals(x)true 自反性 x.equals(x)必须true 传递性 若x.equals(y)true,y.equals(z)true,则x.equals(z)必为true 一致性 只要x,y内容不变，无论调用多少次结果不变 其他 x.equals(null) 永远false，x.equals(和x数据类型不同)始终false 两者的关系 69. final、finalize 和 finally 的不同之处？ final 是一个修饰符，可以修饰变量、方法和类。如果 final 修饰变量，意味着该变量的值在初始化后不能被改变。Java 技术允许使用 finalize() 方法在垃圾收集器将对象从内存中清除出去之前做必要的清理工作。这个方法是由垃圾收集器在确定这个对象没有被引用时对这个对象调用的，但是什么时候调用 finalize 没有保证。finally 是一个关键字，与 try 和 catch 一起用于异常的处理。finally 块一定会被执行，无论在 try 块中是否有发生异常。 70. Java 中的编译期常量是什么？使用它又什么风险？ 变量也就是我们所说的编译期常量，这里的 public 可选的。实际上这些变量在编译时会被替换掉，因为编译器知道这些变量的值，并且知道这些变量在运行时不能改变。这种方式存在的一个问题是你使用了一个内部的或第三方库中的公有编译时常量，但是这个值后面被其他人改变了，但是你的客户端仍然在使用老的值，甚至你已经部署了一个新的jar。为了避免这种情况，当你在更新依赖 JAR 文件时，确保重新编译你的程序。 71. 说出几点 Java 中使用 Collections 的最佳实践 这是我在使用 Java 中 Collectionc 类的一些最佳实践： a）使用正确的集合类，例如，如果不需要同步列表，使用 ArrayList 而不是 Vector。 b）优先使用并发集合，而不是对集合进行同步。并发集合提供更好的可扩展性。 c）使用接口代表和访问集合，如使用List存储 ArrayList，使用 Map 存储 HashMap 等等。 d）使用迭代器来循环集合。 e）使用集合的时候使用泛型。 72. 静态内部类与顶级类有什么区别？ 一个公共的顶级类的源文件名称与类名相同，而嵌套静态类没有这个要求。一个嵌套类位于顶级类内部，需要使用顶级类的名称来引用嵌套静态类，如 HashMap.Entry 是一个嵌套静态类，HashMap 是一个顶级类，Entry是一个嵌套静态类。 73. Java 中，Serializable 与 Externalizable 的区别？ Serializable 接口是一个序列化 Java 类的接口，以便于它们可以在网络上传输或者可以将它们的状态保存在磁盘上，是 JVM 内嵌的默认序列化方式，成本高、脆弱而且不安全。Externalizable 允许你控制整个序列化过程，指定特定的二进制格式，增加安全机制。 74. 说出 JDK 1.7 中的三个新特性？ 虽然 JDK 1.7 不像 JDK 5 和 8 一样的大版本，但是，还是有很多新的特性，如 try-with-resource 语句，这样你在使用流或者资源的时候，就不需要手动关闭，Java 会自动关闭。Fork-Join 池某种程度上实现 Java 版的 Map-reduce。允许 Switch 中有 String 变量和文本。菱形操作符(<>)用于泛型推断，不再需要在变量声明的右边申明泛型，因此可以写出可读写更强、更简洁的代码。另一个值得一提的特性是改善异常处理，如允许在同一个 catch 块中捕获多个异常。 75. 说出 5 个 JDK 1.8 引入的新特性？ Java 8 在 Java 历史上是一个开创新的版本，下面 JDK 8 中 5 个主要的特性： Lambda 表达式，允许像对象一样传递匿名函数 Stream API，充分利用现代多核 CPU，可以写出很简洁的代码 Date 与 Time API，最终，有一个稳定、简单的日期和时间库可供你使用 扩展方法，现在，接口中可以有静态、默认方法。 重复注解，现在你可以将相同的注解在同一类型上使用多次。 下述包含 Java 面试过程中关于 SOLID 的设计原则，OOP 基础，如类，对象，接口，继承，多态，封装，抽象以及更高级的一些概念，如组合、聚合及关联。也包含了 GOF 设计模式的问题。 76. 接口是什么？为什么要使用接口而不是直接使用具体类？ 接口用于定义 API。它定义了类必须得遵循的规则。同时，它提供了一种抽象，因为客户端只使用接口，这样可以有多重实现，如 List 接口，你可以使用可随机访问的 ArrayList，也可以使用方便插入和删除的 LinkedList。接口中不允许普通方法，以此来保证抽象，但是 Java 8 中你可以在接口声明静态方法和默认普通方法。 77. Java 中，抽象类与接口之间有什么不同？ Java 中，抽象类和接口有很多不同之处，但是最重要的一个是 Java 中限制一个类只能继承一个类，但是可以实现多个接口。抽象类可以很好的定义一个家族类的默认行为，而接口能更好的定义类型，有助于后面实现多态机制 参见第六条。 78. 除了单例模式，你在生产环境中还用过什么设计模式？ 这需要根据你的经验来回答。一般情况下，你可以说依赖注入，工厂模式，装饰模式或者观察者模式，随意选择你使用过的一种即可。不过你要准备回答接下的基于你选择的模式的问题。 79. 你能解释一下里氏替换原则吗? 严格定义：如果对每一个类型为S的对象o1，都有类型为T的对象o2，使得以T定义的所有程序P在所有的对象用o1替换o2时，程序P的行为没有变化，那么类型S是类型T的子类型。 通俗表述：所有引用基类（父类）的地方必须能透明地使用其子类的对象。也就是说子类可以扩展父类的功能，但不能改变父类原有的功能。它包含以下4层含义： 子类可以实现父类的抽象方法，但不能覆盖父类的非抽象方法。 子类中可以增加自己特有的方法。 当子类的方法重载父类的方法时，方法的前置条件（即方法的形参）要比父类方法的输入参数更宽松。 当子类的方法实现父类的抽象方法时，方法的后置条件（即方法的返回值）要比父类更严格。 80.什么情况下会违反迪米特法则？为什么会有这个问题？ 迪米特法则建议“只和朋友说话，不要陌生人说话”，以此来减少类之间的耦合。 81. 适配器模式是什么？什么时候使用？ 适配器模式提供对接口的转换。如果你的客户端使用某些接口，但是你有另外一些接口，你就可以写一个适配去来连接这些接口。 82. 构造器注入和 setter 依赖注入，那种方式更好？ 每种方式都有它的缺点和优点。构造器注入保证所有的注入都被初始化，但是 setter 注入提供更好的灵活性来设置可选依赖。如果使用 XML 来描述依赖，Setter 注入的可读写会更强。经验法则是强制依赖使用构造器注入，可选依赖使用 setter 注入。 83. 依赖注入和工厂模式之间有什么不同？ 虽然两种模式都是将对象的创建从应用的逻辑中分离，但是依赖注入比工厂模式更清晰。通过依赖注入，你的类就是 POJO，它只知道依赖而不关心它们怎么获取。使用工厂模式，你的类需要通过工厂来获取依赖。因此，使用 DI 会比使用工厂模式更容易测试。 84. 适配器模式和装饰器模式有什么区别？ 虽然适配器模式和装饰器模式的结构类似，但是每种模式的出现意图不同。适配器模式被用于桥接两个接口，而装饰模式的目的是在不修改类的情况下给类增加新的功能。 85. 适配器模式和代理模式之前有什么不同？ 这个问题与前面的类似，适配器模式和代理模式的区别在于他们的意图不同。由于适配器模式和代理模式都是封装真正执行动作的类，因此结构是一致的，但是适配器模式用于接口之间的转换，而代理模式则是增加一个额外的中间层，以便支持分配、控制或智能访问。 86. 什么是模板方法模式？ 模板方法提供算法的框架，你可以自己去配置或定义步骤。例如，你可以将排序算法看做是一个模板。它定义了排序的步骤，但是具体的比较，可以使用 Comparable 或者其语言中类似东西，具体策略由你去配置。列出算法概要的方法就是众所周知的模板方法。 87. 什么时候使用访问者模式？ 访问者模式用于解决在类的继承层次上增加操作，但是不直接与之关联。这种模式采用双派发的形式来增加中间层。 88. 什么时候使用组合模式？ 组合模式使用树结构来展示部分与整体继承关系。它允许客户端采用统一的形式来对待单个对象和对象容器。当你想要展示对象这种部分与整体的继承关系时采用组合模式。 89. 继承和组合之间有什么不同？ 虽然两种都可以实现代码复用，但是组合比继承共灵活，因为组合允许你在运行时选择不同的实现。用组合实现的代码也比继承测试起来更加简单。 90. 描述 Java 中的重载和重写？ 重载和重写都允许你用相同的名称来实现不同的功能，但是重载是编译时活动，而重写是运行时活动。你可以在同一个类中重载方法，但是只能在子类中重写方法。重写必须要有继承。 91. OOP 中的 组合、聚合和关联有什么区别？ 如果两个对象彼此有关系，就说他们是彼此相关联的。组合和聚合是面向对象中的两种形式的关联。组合是一种比聚合更强力的关联。组合中，一个对象是另一个的拥有者，而聚合则是指一个对象使用另一个对象。如果对象 A 是由对象 B 组合的，则 A 不存在的话，B一定不存在，但是如果 A 对象聚合了一个对象 B，则即使 A 不存在了，B 也可以单独存在。 92. 给我一个符合开闭原则的设计模式的例子？ 开闭原则要求你的代码对扩展开放，对修改关闭。这个意思就是说，如果你想增加一个新的功能，你可以很容易的在不改变已测试过的代码的前提下增加新的代码。有好几个设计模式是基于开闭原则的，如策略模式，如果你需要一个新的策略，只需要实现接口，增加配置，不需要改变核心逻辑。一个正在工作的例子是 Collections.sort() 方法，这就是基于策略模式，遵循开闭原则的，你不需为新的对象修改 sort() 方法，你需要做的仅仅是实现你自己的 Comparator 接口。 93. 什么时候使用享元模式（蝇量模式）？ 享元模式通过共享对象来避免创建太多的对象。为了使用享元模式，你需要确保你的对象是不可变的，这样你才能安全的共享。JDK 中 String 池、Integer 池以及 Long 池都是很好的使用了享元模式的例子。 94. Java 中如何格式化一个日期？如格式化为 ddMMyyyy 的形式？ Java 中，可以使用 SimpleDateFormat 类或者 joda-time 库来格式日期。DateFormat 类允许你使用多种流行的格式来格式化日期。 95. Java 中，怎么在格式化的日期中显示时区？ pattern中加z yyyy-MM-dd HH:mm:ss.SSS Z 96. Java 中 java.util.Date 与 java.sql.Date 有什么区别？ java.sql.Date是针对SQL语句使用的，它只包含日期而没有时间部分,它们都有getTime方法返回毫秒数，自然就可以直接构建。java.util.Date 是 java.sql.Date 的父类，前者是常用的表示时间的类，我们通常格式化或者得到当前时间都是用他，后者之后在读写数据库的时候用他，因为PreparedStament的setDate()的第2参数和ResultSet的getDate()方法的第2个参数都是java.sql.Date。 97. Java 中，如何计算两个日期之间的差距？ public static int dateDiff(Date d1, Date d2) throws Exception { long n1 = d1.getTime(); long n2 = d2.getTime(); long diff = Math.abs(n1 - n2); diff /= 3600 * 1000 * 24; return diff; } 98. Java 中，如何将字符串 YYYYMMDD 转换为日期？ SimpleDateFormat的parse方法 99. 说出几条 Java 中方法重载的最佳实践？ 下面有几条可以遵循的方法重载的最佳实践来避免造成自动装箱的混乱。 a）不要重载这样的方法：一个方法接收 int 参数，而另个方法接收 Integer 参数。 b）不要重载参数数量一致，而只是参数顺序不同的方法。 c）如果重载的方法参数个数多于 5 个，采用可变参数。 100. 说出 5 条 IO 的最佳实践 IO 对 Java 应用的性能非常重要。理想情况下，你应该在你应用的关键路径上避免 IO 操作。下面是一些你应该遵循的 Java IO 最佳实践： a）使用有缓冲区的 IO 类，而不要单独读取字节或字符 b）使用 NIO 和 NIO2 c）在 finally 块中关闭流，或者使用 try-with-resource（Java7） 语句 d）使用内存映射文件获取更快的 IO 101. Object有哪些公用方法？ clone equals hashcode wait notify notifyall finalize toString getClass 除了clone和finalize其他均为public方法。 11个方法，wait被重载了两次 102. equals与==的区别 区别1. ==是一个运算符 equals是Object类的方法 区别2. 比较时的区别 a. 用于基本类型的变量比较时：==用于比较值是否相等，equals不能直接用于基本数据类型的比较，需要转换为其对应的包装类型。 b. 用于引用类型的比较时。==和equals都是比较栈内存中的地址是否相等 。相等为true 否则为false。但是通常会重写equals方法去实现对象内容的比较。 103. String、StringBuffer与StringBuilder的区别 第一点：可变和适用范围。String对象是不可变的，而StringBuffer和StringBuilder是可变字符序列。每次对String的操作相当于生成一个新的String对象，而对StringBuffer和StringBuilder的操作是对对象本身的操作，而不会生成新的对象，所以对于频繁改变内容的字符串避免使用String，因为频繁的生成对象将会对系统性能产生影响。 第二点：线程安全。String由于有final修饰，是immutable的，安全性是简单而纯粹的。StringBuilder和StringBuffer的区别在于StringBuilder不保证同步，也就是说如果需要线程安全需要使用StringBuffer，不需要同步的StringBuilder效率更高。 104. switch能否用String做参数 Java1.7开始支持，但实际这是一颗Java语法糖。除此之外，byte，short，long，枚举，boolean均可用于switch，只有浮点型不可以。 105. 封装、继承、多态 封装： 1.概念：就是把对象的属性和操作（或服务）结合为一个独立的整体，并尽可能隐藏对象的内部实现细节。 2.好处： (1)隐藏内部实现细节。 继承： 1.概念：继承是从已有的类中派生出新的类，新的类能吸收已有类的数据属性和行为，并能扩展新的能力 2.好处：提高代码的复用，缩短开发周期。 多态： 1.概念：多态（Polymorphism）按字面的意思就是“多种状态，即同一个实体同时具有多种形式。一般表现形式是程序在运行的过程中，同一种类型在不同的条件下表现不同的结果。多态也称为动态绑定，一般是在运行时刻才能确定方法的具体执行对象。 2.好处： 1）将接口和实现分开，改善代码的组织结构和可读性，还能创建可拓展的程序。 2）消除类型之间的耦合关系。允许将多个类型视为同一个类型。 3）一个多态方法的调用允许有多种表现形式 106. Comparable和Comparator接口区别 Comparator位于包java.util下，而Comparable位于包java.lang下 如果我们需要使用Arrays或Collections的排序方法对对象进行排序时，我们需要在自定义类中实现Comparable接口并重写compareTo方法，compareTo方法接收一个参数，如果this对象比传递的参数小，相等或大时分别返回负整数、0、正整数。Comparable被用来提供对象的自然排序。String、Integer实现了该接口。 Comparator比较器的compare方法接收2个参数，根据参数的比较大小分别返回负整数、0和正整数。 Comparator 是一个外部的比较器，当这个对象自然排序不能满足你的要求时，你可以写一个比较器来完成两个对象之间大小的比较。用 Comparator 是策略模式（strategy design pattern），就是不改变对象自身，而用一个策略对象（strategy object）来改变它的行为。 107. 与Java集合框架相关的有哪些最好的实践 （1）根据需要选择正确的集合类型。比如，如果指定了大小，我们会选用Array而非ArrayList。如果我们想根据插入顺序遍历一个Map，我们需要使用TreeMap。如果我们不想重复，我们应该使用Set。 （2）一些集合类允许指定初始容量，所以如果我们能够估计到存储元素的数量，我们可以使用它，就避免了重新哈希或大小调整。 （3）基于接口编程，而非基于实现编程，它允许我们后来轻易地改变实现。 （4）总是使用类型安全的泛型，避免在运行时出现ClassCastException。 （5）使用JDK提供的不可变类作为Map的key，可以避免自己实现hashCode()和equals()。 108. IO和NIO简述 1、简述 在以前的Java IO中，都是阻塞式IO，NIO引入了非阻塞式IO。 第一种方式：我从硬盘读取数据，然后程序一直等，数据读完后，继续操作。这种方式是最简单的，叫阻塞IO。 第二种方式：我从硬盘读取数据，然后程序继续向下执行，等数据读取完后，通知当前程序（对硬件来说叫中断，对程序来说叫回调），然后此程序可以立即处理数据，也可以执行完当前操作在读取数据。 2.流与块的比较 原来的 I/O 以流的方式处理数据，而 NIO 以块的方式处理数据。面向流 的 I/O 系统一次一个字节地处理数据。一个输入流产生一个字节的数据，一个输出流消费一个字节的数据。这样做是相对简单的。不利的一面是，面向流的 I/O 通常相当慢。 一个 面向块 的 I/O 系统以块的形式处理数据。每一个操作都在一步中产生或者消费一个数据块。按块处理数据比按(流式的)字节处理数据要快得多。但是面向块的 I/O 缺少一些面向流的 I/O 所具有的优雅性和简单性。 3.通道与流 Channel是一个对象，可以通过它读取和写入数据。通道与流功能类似，不同之处在于通道是双向的。而流只是在一个方向上移动(一个流必须是 InputStream 或者 OutputStream 的子类)， 而通道可以用于读、写或者同时用于读写。 4.缓冲区Buffer 在 NIO 库中，所有数据都是用缓冲区处理的。 Position: 表示下一次访问的缓冲区位置 Limit: 表示当前缓冲区存放的数据容量。 Capacity:表示缓冲区最大容量 flip()方法:读写模式切换 clear方法:它将 limit 设置为与 capacity 相同。它设置 position 为 0。 二、Java高级（JavaEE、框架、服务器、工具等） 1. Servlet 1.1 Servlet继承实现结构 Servlet (接口) --> init|service|destroy方法 GenericServlet(抽象类) --> 与协议无关的Servlet HttpServlet(抽象类) --> 实现了http协议 自定义Servlet --> 重写doGet/doPost 1.2 编写Servlet的步骤 继承HttpServlet 重写doGet/doPost方法 在web.xml中注册Servlet 1.3 Servlet生命周期 init:仅执行一次，负责装载servlet时初始化servlet对象 service:核心方法，一般get/post两种方式 destroy:停止并卸载servlet，释放资源 1.4 过程 客户端request请求 -> 服务器检查Servlet实例是否存在 -> 若存在调用相应service方法 -> 服务结束 客户端request请求 -> 服务器检查Servlet实例是否存在 -> 若不存在装载Servlet类并创建实例 -> 调用init初始化 -> 调用service -> 服务结束 1.5 doPost方法要抛出的异常:ServletExcception、IOException 1.6 Servlet容器什么时候装载装载Servlet web.xml中配置load-on-startup启动时装载 客户首次向Servlet发送请求 Servlet类文件被更新后, 重新装载Servlet 1.7 HttpServlet容器响应web客户请求流程 Web客户向servlet容器发出http请求 servlet容器解析Web客户的http请求 servlet容器创建一个HttpRequest对象, 封装http请求信息 servlet容器创建一个HttpResponse对象 servlet容器调用HttpServlet的service方法, 把HttpRequest和HttpResponse对象作为service方法的参数传给HttpServlet对象 HttpServlet调用httprequest的有关方法, 获取http请求信息 httpservlet调用httpresponse的有关方法, 生成响应数据 Servlet容器把HttpServlet的响应结果传给web客户 1.8 HttpServletRequest完成的一些功能 request.getCookie() request.getHeader(String s) request.getContextPath() request.getSession() HttpSession session = request.getSession(boolean create) 返回当前请求的会话 1.9 HttpServletResponse完成一些的功能 设http响应头 设置Cookie 输出返回数据 1.10 Servlet与JSP九大内置对象的关系 JSP对象 怎样获得 1. out -> response.getWriter 2. request -> Service方法中的req参数 3. response -> Service方法中的resp参数 4. session -> request.getSession 5. application -> getServletContext 6. exception -> Throwable 7. page -> this 8. pageContext -> PageContext 9. Config -> getServletConfig exception是JSP九大内置对象之一，其实例代表其他页面的异常和错误。只有当页面是错误处理页面时，即isErroePage为 true时，该对象才可以使用。 2. JSP JSP的前身就是Servlet 3. Tomcat 3.1 Tomcat容器的等级 Tomcat - Container - Engine - Host - Servlet - 多个Context(一个Context对应一个web工程)-Wrapper 4. struts struts可进行文件上传 struts基于MVC模式 struts让流程结构更清晰 struts有许多action类, 会增加类文件数目 5. Hibernate的7大鼓励措施 尽量使用many-to-one, 避免使用单项one-to-many 灵活使用单项one-to-many 不用一对一, 使用多对一代替一对一 配置对象缓存, 不使用集合对象 一对多使用bag, 多对一使用set 继承使用显示多态 消除大表, 使用二级缓存 6. Hibernate延迟加载 Hibernate2延迟加载实现：a)实体对象 b)集合（Collection） Hibernate3 提供了属性的延迟加载功能 当Hibernate在查询数据的时候，数据并没有存在与内存中，当程序真正对数据的操作时，对象才存在与内存中，就实现了延迟加载，他节省了服务器的内存开销，从而提高了服务器的性能。 hibernate使用Java反射机制，而不是字节码增强程序来实现透明性。 hibernate的性能非常好，因为它是个轻量级框架。映射的灵活性很出色。它支持各种关系数据库，从一对一到多对多的各种复杂关系。 7. Java 中，DOM 和 SAX 解析器有什么不同？ DOM 解析器将整个 XML 文档加载到内存来创建一棵 DOM 模型树，这样可以更快的查找节点和修改 XML 结构，而 SAX 解析器是一个基于事件的解析器，不会将整个 XML 文档加载到内存。由于这个原因，DOM 比 SAX 更快，也要求更多的内存，但不适合于解析大的 XML 文件。 8. Java 中，Maven 和 Ant 有什么区别？ 虽然两者都是构建工具，都用于创建 Java 应用，但是 Maven 做的事情更多，在基于“约定优于配置”的概念下，提供标准的 Java 项目结构，同时能为应用自动管理依赖（应用中所依赖的 JAR 文件）。 9. 解析XML不同方式对比 DOM、SAX、JDOM、DOM4J [x] DOM DOM树驻留内存 可以进行修改和写入,耗费内存。 步骤：创建DocumentBuilderFactory对象 -> 创建DocumentBuilder对象 -> Document document = db.parse(\"xml\") [x] SAX 事件驱动模式 获取一个SAXParserFactory工厂的实例 -> 根据该实例获取SAXParser -> 创建Handler对象 -> 调用SAXParser的parse方法解析 用于读取节点数据 不易编码 事件有顺序 很难同时访问xml的多处数据 [x] JDOM 创建一个SAXBuilder的对象 -> 创建一个输入流，加载xml文件 ->通过saxBuilder的build方法将输入流加载至saxBuilder并接收Document对象 使用具体类而不使用接口 [x] DOM4J 通过SAXReader的read方法加载xml文件并获取document对象 使用接口和抽象类，灵活性好，功能强大 10. Nginx相关 请参考我的Nginx 11. XML与JSON对比和区别 XML 1）应用广泛，可扩展性强，被广泛应用各种场合 2）读取、解析没有JSON快 3）可读性强，可描述复杂结构 JSON 1）结构简单，都是键值对 2）读取、解析速度快，很多语言支持 3）传输数据量小，传输速率大大提高 4）描述复杂结构能力较弱 JavaScript、PHP等原生支持，简化了读取解析。成为当前互联网时代普遍应用的数据结构。 三、多线程和并发 0. Java 中的 volatile 变量是什么 Java 语言提供了一种稍弱的同步机制,即volatile变量。但是volatile并不容易完全被正确、完整的理解。 一般来说，volatile具备2条语义，或者说2个特性。第一是保证volatile修饰的变量对所有线程的可见性，这里的可见性是指当一条线程修改了该变量，新值对于其它线程来说是立即可以得知的。而普通变量做不到这一点。 第二条语义是禁止指令重排序优化，这条语义在JDK1.5才被修复。 关于第一点：根据JMM，所有的变量存储在主内存，而每个线程还有自己的工作内存，线程的工作内存保存该线程使用到的变量的主内存副本拷贝，线程对变量的操作在工作内存中进行，不能直接读写主内存的变量。在volatile可见性这一点上，普通变量做不到的原因正因如此。比如，线程A修改了一个普通变量的值，然后向主内存进行回写，线程B在线程A回写完成后再从主内存读取，新变量才能对线程B可见。其实，按照虚拟机规范，volatile变量依然有工作内存的拷贝，要借助主内存来实现可见性。但由于volatile的特殊规则保证了新值能立即同步回主内存，以及每次使用从主内存刷新，以此保证了多线程操作volatile变量的可见性。 关于第二点：先说指令重排序，指令重排序是指CPU采用了允许将多条指令不按规定顺序分开发送给相应的处理单元处理，但并不是说任意重排，CPU需要正确处理指令依赖情况确保最终的正确结果，指令重排序是机器级的优化操作。那么为什么volatile要禁止指令重排序呢，又是如何去做的。举例，DCL（双重检查加锁）的单例模式。volatile修饰后，代码中将会插入许多内存屏障指令保证处理器不发生乱序执行。同时由于Happens-before规则的保证，在刚才的例子中写操作会发生在后续的读操作之前。 除了以上2点，volatile还保证对于64位long和double的读取是原子性的。因为在JMM中允许虚拟机对未被volatile修饰的64位的long和double读写操作分为2次32位的操作来执行，这也就是所谓的long和double的非原子性协定。 基于以上几点，我们知道volatile虽然有这些语义和特性在并发的情况下仍然不能保证线程安全。大部分情况下仍然需要加锁。 除非是以下2种情况，1.运算结果不依赖变量的当前值，或者能够确保只有单一线程修改变量的值；2.变量不需要与其他的状态变量共同参与不变约束。 1. volatile简述 Java 语言提供了一种稍弱的同步机制,即volatile变量.用来确保将变量的更新操作通知到其他线程,保证了新值能立即同步到主内存,以及每次使用前立即从主内存刷新。 当把变量声明为volatile类型后,编译器与运行时都会注意到这个变量是共享的。volatile修饰变量,每次被线程访问时强迫其从主内存重读该值,修改后再写回。保证读取的可见性,对其他线程立即可见。volatile的另一个语义是禁止指令重排序优化。但是volatile并不保证原子性，也就不能保证线程安全。 2. Java 中能创建 volatile 数组吗？ 能，Java 中可以创建 volatile 类型数组，不过只是一个指向数组的引用，而不是整个数组。我的意思是，如果改变引用指向的数组，将会受到 volatile 的保护，但是如果多个线程同时改变数组的元素，volatile 就不能起到之前的保护作用了。 3. volatile 能使得一个非原子操作变成原子操作吗？ 一个典型的例子是在类中有一个 long 类型的成员变量。如果你知道该成员变量会被多个线程访问，如计数器、价格等，你最好是将其设置为 volatile。为什么？因为 Java 中读取 long 类型变量不是原子的，需要分成两步，如果一个线程正在修改该 long 变量的值，另一个线程可能只能看到该值的一半（前 32 位）。但是对一个 volatile 型的 long 或 double 变量的读写是原子。 4. volatile 禁止指令重排序的底层原理 指令重排序，是指CPU允许多条指令不按程序规定的顺序分开发送给相应电路单元处理。但并不是说任意重排，CPU需要能正确处理指令依赖情况以正确的执行结果。volatile禁止指令重排序是通过内存屏障实现的，指令重排序不能把后面的指令重排序到内存屏障之前。由内存屏障保证一致性。注：该条语义在JDK1.5才得以修复，这点也是JDK1.5之前无法通过双重检查加锁来实现单例模式的原因。 5. volatile 类型变量提供什么保证？ volatile 变量提供有序性和可见性保证，例如，JVM 或者 JIT为了获得更好的性能会对语句重排序，但是 volatile 类型变量即使在没有同步块的情况下赋值也不会与其他语句重排序。 volatile 提供 happens-before 的保证，确保一个线程的修改能对其他线程是可见的。某些情况下，volatile 还能提供原子性，如读 64 位数据类型，像 long 和 double 都不是原子的，但 volatile 类型的 double 和 long 就是原子的。 volatile的使用场景： 运算结果不依赖变量的当前值，或者能够确保只有单一的线程修改该值 变量不需要与其他状态变量共同参与不变约束 6. volatile的性能 volatile变量的读操作性能消耗和普通变量差不多，但是写操作可能相对慢一些，因为它需要在本地代码中插入许多内存屏障指令以确保处理器不发生乱序执行。大多数情况下，volatile总开销比锁低，但我们要注意volatile的语义能否满足使用场景。 7. 10 个线程和 2 个线程的同步代码，哪个更容易写？ 从写代码的角度来说，两者的复杂度是相同的，因为同步代码与线程数量是相互独立的。但是同步策略的选择依赖于线程的数量，因为越多的线程意味着更大的竞争，所以你需要利用同步技术，如锁分离，这要求更复杂的代码和专业知识。 8. 你是如何调用 wait（）方法的？使用 if 块还是循环？为什么？ wait() 方法应该在循环调用，因为当线程获取到 CPU 开始执行的时候，其他条件可能还没有满足，所以在处理前，循环检测条件是否满足会更好。下面是一段标准的使用 wait 和 notify 方法的代码： // The standard idiom for using the wait method synchronized (obj) { while (condition does not hold) obj.wait(); // (Releases lock, and reacquires on wakeup) ... // Perform action appropriate to condition } 参见 Effective Java 第 69 条，获取更多关于为什么应该在循环中来调用 wait 方法的内容。 9. 什么是多线程环境下的伪共享（false sharing）？ 伪共享是多线程系统（每个处理器有自己的局部缓存）中一个众所周知的性能问题。伪共享发生在不同处理器的上的线程对变量的修改依赖于相同的缓存行，如下图所示： 伪共享问题很难被发现，因为线程可能访问完全不同的全局变量，内存中却碰巧在很相近的位置上。如其他诸多的并发问题，避免伪共享的最基本方式是仔细审查代码，根据缓存行来调整你的数据结构。 10. 线程的run方法和start方法 run方法 只是thread类的一个普通方法，若直接调用程序中依然只有主线程这一个线程，还要顺序执行，依然要等待run方法体执行完毕才可执行下面的代码。 start方法 用start方法来启动线程,是真正实现了多线程。调用thread类的start方法来启动一个线程，此时线程处于就绪状态，一旦得到cpu时间片，就开始执行run方法。 11. ReadWriteLock(读写锁) 写写互斥 读写互斥 读读并发, 在读多写少的情况下可以提高效率。 12. resume(继续挂起的线程)和suspend(挂起线程)一起用 13. wait与notify、notifyall一起用 14. sleep与wait的异同点 sleep是Thread类的静态方法, wait来自object类 sleep方法短暂停顿不释放锁, wait方法条件等待要释放锁，因为只有这样，其他等待的线程才能在满足条件时获取到该锁。 wait, notify, notifyall必须在同步代码块中使用, sleep可以在任何地方使用 都可以抛出InterruptedException 15. 让一个线程停止执行 异常 - 停止执行 休眠 - 停止执行 阻塞 - 停止执行 16. ThreadLocal简介 16.1 ThreadLocal解决了变量并发访问的冲突问题 当使用ThreadLocal维护变量时,ThreadLocal为每个使用该变量的线程提供独立的变量副本,每个线程都可以独立地改变自己的副本,而不会影响其它线程所对应的副本，是线程隔离的。线程隔离的秘密在于ThreadLocalMap类(ThreadLocal的静态内部类) 16.2 与synchronized同步机制的比较 首先，它们都是为了解决多线程中相同变量访问冲突问题。不过，在同步机制中，要通过对象的锁机制保证同一时间只有一个线程访问该变量。该变量是线程共享的, 使用同步机制要求程序缜密地分析什么时候对该变量读写，什么时候需要锁定某个对象， 什么时候释放对象锁等复杂的问题，程序设计编写难度较大, 是一种“以时间换空间”的方式。 而ThreadLocal采用了以“以空间换时间”的方式。 17. 线程局部变量原理 当使用ThreadLocal维护变量时，ThreadLocal为每个使用该变量的线程提供独立的变量副本，每个线程都可以独立地改变自己的副本，而不会影响其它线程所对应的副本，是线程隔离的。线程隔离的秘密在于ThreadLocalMap类(ThreadLocal的静态内部类) 线程局部变量是局限于线程内部的变量，属于线程自身所有，不在多个线程间共享。Java 提供 ThreadLocal 类来支持线程局部变量，是一种实现线程安全的方式。但是在管理环境下（如 web 服务器）使用线程局部变量的时候要特别小心，在这种情况下，工作线程的生命周期比任何应用变量的生命周期都要长。任何线程局部变量一旦在工作完成后没有释放，Java 应用就存在内存泄露的风险。 ThreadLocal的方法：void set(T value)、T get()以及T initialValue()。 ThreadLocal是如何为每个线程创建变量的副本的： 首先，在每个线程Thread内部有一个ThreadLocal.ThreadLocalMap类型的成员变量threadLocals，这个threadLocals就是用来存储实际的变量副本的，键值为当前ThreadLocal变量，value为变量副本（即T类型的变量）。初始时，在Thread里面，threadLocals为空，当通过ThreadLocal变量调用get()方法或者set()方法，就会对Thread类中的threadLocals进行初始化，并且以当前ThreadLocal变量为键值，以ThreadLocal要保存的副本变量为value，存到threadLocals。然后在当前线程里面，如果要使用副本变量，就可以通过get方法在threadLocals里面查找。 总结： 实际的通过ThreadLocal创建的副本是存储在每个线程自己的threadLocals中的 为何threadLocals的类型ThreadLocalMap的键值为ThreadLocal对象，因为每个线程中可有多个threadLocal变量，就像上面代码中的longLocal和stringLocal； 在进行get之前，必须先set，否则会报空指针异常；如果想在get之前不需要调用set就能正常访问的话，必须重写initialValue()方法 18. JDK提供的用于并发编程的同步器 Semaphore Java并发库的Semaphore可以很轻松完成信号量控制，Semaphore可以控制某个资源可被同时访问的个数，通过 acquire() 获取一个许可，如果没有就等待，而 release() 释放一个许可。 CyclicBarrier 主要的方法就是一个：await()。await()方法每被调用一次，计数加一，并阻塞住当前线程。当计数等于定义的大小时，阻塞解除，所有在此CyclicBarrier上面阻塞的线程开始运行并且计数重置为0。 CountDownLatch 直译过来就是倒计数(CountDown)门闩(Latch)。倒计数不用说，门闩的意思顾名思义就是阻止前进。在这里就是指 CountDownLatch.await() 方法在倒计数为0之前会阻塞当前线程。 19. 什么是 Busy spin？我们为什么要使用它？ Busy spin 是一种在不释放 CPU 的基础上等待事件的技术。它经常用于避免丢失 CPU 缓存中的数据（如果线程先暂停，之后在其他CPU上运行就会丢失）。所以，如果你的工作要求低延迟，并且你的线程目前没有任何顺序，这样你就可以通过循环检测队列中的新消息来代替调用 sleep() 或 wait() 方法。它唯一的好处就是你只需等待很短的时间，如几微秒或几纳秒。LMAX 分布式框架是一个高性能线程间通信的库，该库有一个 BusySpinWaitStrategy 类就是基于这个概念实现的，使用 busy spin 循环 EventProcessors 等待屏障。 20. Java 中怎么获取一份线程 dump 文件？ 在 Linux 下，你可以通过命令 kill -3 PID （Java 进程的进程 ID）来获取 Java 应用的 dump 文件。在 Windows 下，你可以按下 Ctrl + Break 来获取。这样 JVM 就会将线程的 dump 文件打印到标准输出或错误文件中，它可能打印在控制台或者日志文件中，具体位置依赖应用的配置。 21. Swing 是线程安全的？ 不是，Swing 不是线程安全的。你不能通过任何线程来更新 Swing 组件，如 JTable、JList 或 JPanel，事实上，它们只能通过 GUI 或 AWT 线程来更新。这就是为什么 Swing 提供 invokeAndWait() 和 invokeLater() 方法来获取其他线程的 GUI 更新请求。这些方法将更新请求放入 AWT 的线程队列中，可以一直等待，也可以通过异步更新直接返回结果。 22. 用 wait-notify 写一段代码来解决生产者-消费者问题？ 记住在同步块中调用 wait() 和 notify()方法，如果阻塞，通过循环来测试等待条件。 23. 用 Java 写一个线程安全的单例模式（Singleton）？ 当我们说线程安全时，意思是即使初始化是在多线程环境中，仍然能保证单个实例。Java 中，使用枚举作为单例类是最简单的方式来创建线程安全单例模式的方式。参见我整理的单例的文章6种单例模式的实现以及double check的剖析 24. Java 中，编写多线程程序的时候你会遵循哪些最佳实践？ 这是我在写Java 并发程序的时候遵循的一些最佳实践： a）给线程命名，这样可以帮助调试。 b）最小化同步的范围，而不是将整个方法同步，只对关键部分做同步。 c）如果可以，更偏向于使用 volatile 而不是 synchronized。 d）使用更高层次的并发工具，而不是使用 wait() 和 notify() 来实现线程间通信，如 BlockingQueue，CountDownLatch 及 Semeaphore。 e）优先使用并发集合，而不是对集合进行同步。并发集合提供更好的可扩展性。 25. 说出至少 5 点在 Java 中使用线程的最佳实践。 这个问题与之前的问题类似，你可以使用上面的答案。对线程来说，你应该： a）对线程命名 b）将线程和任务分离，使用线程池执行器来执行 Runnable 或 Callable。 c）使用线程池 26. 在多线程环境下，SimpleDateFormat 是线程安全的吗？ 不是，非常不幸，DateFormat 的所有实现，包括 SimpleDateFormat 都不是线程安全的，因此你不应该在多线程序中使用，除非是在对外线程安全的环境中使用，如将 SimpleDateFormat 限制在 ThreadLocal 中。如果你不这么做，在解析或者格式化日期的时候，可能会获取到一个不正确的结果。因此，从日期、时间处理的所有实践来说，我强力推荐 joda-time 库。 27. Happens-Before规则 程序次序规则 按控制流顺序先后发生 管程锁定规则 一个unlock操作先行发生于后面对同一个锁的lock操作 volatile变量规则 对一个volatile变量的写操作先行发生于后面对这个变量的读操作 线程启动规则 start方法先行发生于线程的每一个动作 线程中断规则 对线程的interrupt方法调用先行发生于被中断线程的代码检测到中断时间的发生 线程终止规则 线程内的所有操作都先行发生于对此线程的终止检测 对象终结规则 一个对象的初始化完成先行发生于它的finalize方法的开始 传递性 如果A先行发生于操作B，B先行发生于操作C，则A先行发生于操作C 28. 什么是线程 线程是操作系统能够进行运算调度的最小单位，它被包含在进程之中，是进程中的实际运作单位。程序员可以通过它进行多处理器编程，可以使用多线程对运算密集型任务提速。比如，如果一个线程完成一个任务要100 毫秒，那么用十个线程完成改任务只需 10 毫秒。Java在语言层面对多线程提供了很好的支持。 29. 线程和进程有什么区别 从概念上： 进程：一个程序对一个数据集的动态执行过程，是分配资源的基本单位。 线程：存在于进程内，是进程内的基本调度单位，共享进程的资源。 从执行过程中来看： 进程：拥有独立的内存单元，而多个线程共享内存，从而提高了应用程序的运行效率。 线程：每一个独立的线程，都有一个程序运行的入口、顺序执行序列、和程序的出口。但是线程不能够独立的执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。 从逻辑角度来看：（重要区别） 多线程的意义在于一个应用程序中，有多个执行部分可以同时执行。但是，操作系统并没有将多个线程看做多个独立的应用，来实现进程的调度和管理及资源分配。 简言之，一个程序至少有一个进程,一个进程至少有一个线程。进程是资源分配的基本单位，线程共享进程的资源。 30. 用 Runnable 还是 Thread Java 不支持类的多重继承，但允许你调用多个接口。所以如果你要继承其他类，当然是实现Runnable接口好了。 31. Java 中 Runnable 和 Callable 有什么不同 Runnable和 Callable 都代表那些要在不同的线程中执行的任务。Runnable 从 JDK1.0 开始就有了，Callable 是在 JDK1.5 增加的。它们的主要区别是 Callable 的 call () 方法可以返回值和抛出异常，而 Runnable 的 run ()方法没有这些功能。 32. Java 中 CyclicBarrier 和 CountDownLatch 有什么不同 它们都是JUC下的类，CyclicBarrier 和 CountDownLatch 都可以用来让一组线程等待其它线程。区别在于CountdownLatch计数无法被重置。如果需要重置计数，请考虑使用 CyclicBarrier。 33. Java 内存模型是什么 Java 内存模型规定和指引 Java 程序在不同的内存架构、CPU 和操作系统间有确定性地行为。它在多线程的情况下尤其重要。Java内存模型对一个线程所做的变动能被其它线程可见提供了保证，它们之间是先行发生关系。这个关系定义了一些规则让程序员在并发编程时思路更清晰。 线程内的代码能够按先后顺序执行，这被称为程序次序规则。 对于同一个锁，一个解锁操作一定要发生在时间上后发生的另一个锁定操作之前，也叫做管程锁定规则。 前一个对volatile的写操作在后一个volatile的读操作之前，也叫volatile变量规则。 一个线程内的任何操作必需在这个线程的 start ()调用之后，也叫作线程启动规则。 一个线程的所有操作都会在线程终止之前，线程终止规则。 一个对象的终结操作必需在这个对象构造完成之后，也叫对象终结规则。 a先行于b，b先行于c，传递性 34. 什么是线程安全？Vector 是一个线程安全类吗 如果你的代码所在的进程中有多个线程在同时运行，而这些线程可能会同时运行这段代码。如果每次运行结果和单线程运行的结果是一样的，而且其他的变量的值也和预期的是一样的，就是线程安全的。一个线程安全的计数器类的同一个实例对象在被多个线程使用的情况下也不会出现计算失误。很显然你可以将集合类分成两组，线程安全和非线程安全的。Vector 是用同步方法来实现线程安全的，而和它相似的 ArrayList 不是线程安全的。 35. Java 中什么是竞态条件？ 举个例子说明。 竞态条件会导致程序在并发情况下出现一些 bugs。多线程对一些资源的竞争的时候就会产生竞态条件，如果首先要执行的程序竞争失败排到后面执行了，那么整个程序就会出现一些不确定的 bugs。这种 bugs 很难发现而且会重复出现，因为线程间的随机竞争。几类竞态条件check-and-act、读取-修改-写入、put-if-absent。 36. Java 中如何停止一个线程 当 run () 或者 call () 方法执行完的时候线程会自动结束，如果要手动结束一个线程，你可以用 volatile 布尔变量来退出 run ()方法的循环或者是取消任务来中断线程。其他情形：异常 - 停止执行 休眠 - 停止执行 阻塞 - 停止执行 37. 一个线程运行时发生异常会怎样 简单的说，如果异常没有被捕获该线程将会停止执行。Thread.UncaughtExceptionHandler 是用于处理未捕获异常造成线程突然中断情况的一个内嵌接口。当一个未捕获异常将造成线程中断的时候 JVM 会使用 Thread.getUncaughtExceptionHandler ()来查询线程的 UncaughtExceptionHandler 并将线程和异常作为参数传递给 handler 的 uncaughtException ()方法进行处理。 38. 如何在两个线程间共享数据？ 通过共享对象来实现这个目的，或者是使用像阻塞队列这样并发的数据结构 39. Java 中 notify 和 notifyAll 有什么区别 notify ()方法不能唤醒某个具体的线程，所以只有一个线程在等待的时候它才有用武之地。而 notifyAll ()唤醒所有线程并允许他们争夺锁确保了至少有一个线程能继续运行。 40. 为什么 wait, notify 和 notifyAll 这些方法不在 thread 类里面 一个很明显的原因是 JAVA 提供的锁是对象级的而不是线程级的。如果线程需要等待某些锁那么调用对象中的 wait ()方法就有意义了。如果 wait ()方法定义在 Thread 类中，线程正在等待的是哪个锁就不明显了。简单的说，由于 wait，notify 和 notifyAll 都是锁级别的操作，所以把他们定义在 Object 类中因为锁属于对象。 41. 什么是 FutureTask？ 在 Java 并发程序中 FutureTask 表示一个可以取消的异步运算。它有启动和取消运算、查询运算是否完成和取回运算结果等方法。只有当运算完成的时候结果才能取回，如果运算尚未完成 get 方法将会阻塞。一个 FutureTask 对象可以对调用了 Callable 和 Runnable 的对象进行包装，由于 FutureTask 也是调用了 Runnable 接口所以它可以提交给 Executor 来执行。 42. Java 中 interrupted 和 isInterruptedd 方法的区别 interrupted是静态方法，isInterruptedd是一个普通方法 如果当前线程被中断（没有抛出中断异常，否则中断状态就会被清除），你调用interrupted方法，第一次会返回true。然后，当前线程的中断状态被方法内部清除了。第二次调用时就会返回false。如果你刚开始一直调用isInterrupted，则会一直返回true，除非中间线程的中断状态被其他操作清除了。也就是说isInterrupted 只是简单的查询中断状态，不会对状态进行修改。 43. 为什么 wait 和 notify 方法要在同步块中调用 如果不这么做，代码会抛出 IllegalMonitorStateException异常。还有一个原因是为了避免 wait 和 notify 之间产生竞态条件。 44. 为什么你应该在循环中检查等待条件？ 处于等待状态的线程可能会收到错误警报和伪唤醒，如果不在循环中检查等待条件，程序就会在没有满足结束条件的情况下退出。因此，当一个等待线程醒来时，不能认为它原来的等待状态仍然是有效的，在 notify 方法调用之后和等待线程醒来之前这段时间它可能会改变。这就是在循环中使用 wait 方法效果更好的原因。 45. Java 中的同步集合与并发集合有什么区别 同步集合与并发集合都为多线程和并发提供了合适的线程安全的集合，不过并发集合的可扩展性更高。在 Java1.5 之前程序员们只有同步集合来用且在多线程并发的时候会导致争用，阻碍了系统的扩展性。Java1.5加入了并发集合像 ConcurrentHashMap，不仅提供线程安全还用锁分离和内部分区等现代技术提高了可扩展性。它们大部分位于JUC包下。 46. 什么是线程池？ 为什么要使用它？ 创建线程要花费昂贵的资源和时间，如果任务来了才创建线程那么响应时间会变长，而且一个进程能创建的线程数有限。为了避免这些问题，在程序启动的时候就创建若干线程来响应处理，它们被称为线程池，里面的线程叫工作线程。从 JDK1.5 开始，Java API 提供了 Executor 框架让你可以创建不同的线程池。比如单线程池，每次处理一个任务；数目固定的线程池或者是缓存线程池（一个适合很多生存期短的任务的程序的可扩展线程池）。 47. 如何写代码来解决生产者消费者问题？ 在现实中你解决的许多线程问题都属于生产者消费者模型，就是一个线程生产任务供其它线程进行消费，你必须知道怎么进行线程间通信来解决这个问题。比较低级的办法是用 wait 和 notify 来解决这个问题，比较赞的办法是用 Semaphore 或者 BlockingQueue 来实现生产者消费者模型。 48.如何避免死锁？ 死锁是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。这是一个严重的问题，因为死锁会让你的程序挂起无法完成任务，死锁的发生必须满足以下四个条件： 互斥条件：一个资源每次只能被一个进程使用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件：进程已获得的资源，在末使用完之前，不能强行剥夺。 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。 避免死锁最简单的方法就是阻止循环等待条件，将系统中所有的资源设置标志位、排序，规定所有的进程申请资源必须以一定的顺序（升序或降序）做操作来避免死锁。 49. Java 中活锁和死锁有什么区别？ 活锁和死锁类似，不同之处在于处于活锁的线程或进程的状态是不断改变的，活锁可以认为是一种特殊的饥饿。一个现实的活锁例子是两个人在狭小的走廊碰到，两个人都试着避让对方好让彼此通过，但是因为避让的方向都一样导致最后谁都不能通过走廊。简单的说就是，活锁和死锁的主要区别是前者进程的状态可以改变但是却不能继续执行。 50. 怎么检测一个线程是否拥有锁 在 java.lang.Thread 中有一个方法叫 holdsLock，当且仅当当前线程拥有某个具体对象的锁时它返回true。 51. 你如何在 Java 中获取线程堆栈 在 Linux 下，你可以通过命令 kill -3 PID （Java 进程的进程 ID）来获取 Java 应用的 dump 文件。在 Windows 下，你可以按下 Ctrl + Break 来获取。这样 JVM 就会将线程的 dump 文件打印到标准输出或错误文件中，它可能打印在控制台或者日志文件中，具体位置依赖应用的配置。 52.Java 中 synchronized 和 ReentrantLock 有什么不同 Java 在过去很长一段时间只能通过 synchronized 关键字来实现互斥，它有一些缺点。比如你不能扩展锁之外的方法或者块边界，尝试获取锁时不能中途取消等。Java 5 通过 Lock 接口提供了更复杂的控制来解决这些问题。 ReentrantLock 类实现了 Lock，它拥有与 synchronized 相同的并发性和内存语义且它还具有可扩展性。 53.有三个线程 T1，T2，T3，怎么确保它们按顺序执行 可以用线程类的 join ()方法。具体操作是在T3的run方法中调用t2.join()，让t2执行完再执行t3；T2的run方法中调用t1.join()，让t1执行完再执行t2。这样就按T1，T2，T3的顺序执行了 54.Thread 类中的 yield 方法有什么作用 Yield 方法可以暂停当前正在执行的线程对象，让其它有相同优先级的线程执行。它是一个静态方法而且只保证当前线程放弃 CPU 占用而不能保证使其它线程一定能占用 CPU，执行 yield的线程有可能在进入到暂停状态后马上又被执行。 55.Java 中 ConcurrentHashMap 的并发度是什么 ConcurrentHashMap 把实际 map 划分成若干部分来实现它的可扩展性和线程安全。这种划分是使用并发度获得的，它是 ConcurrentHashMap 类构造函数的一个可选参数，默认值为 16，这样在多线程情况下就能避免争用。 56.Java 中 Semaphore是什么 JUC下的一种新的同步类，它是一个计数信号。从概念上讲，Semaphore信号量维护了一个许可集合。acquire获取许可，release释放一个许可，从而可能释放一个正在阻塞的获取者。但是，不使用实际的许可对象，Semaphore 只对可用许可的号码进行计数，并采取相应的行动。信号量常常用于多线程的代码中，比如数据库连接池。 57.如果你提交任务时，线程池队列已满。会发会生什么？ 这个问题问得很狡猾，许多程序员会认为该任务会阻塞直到线程池队列有空位。事实上如果一个任务不能被调度执行那么 ThreadPoolExecutor’s submit ()方法将会抛出一个 RejectedExecutionException 异常。 58.Java 线程池中 submit () 和 execute ()方法有什么区别 两个方法都可以向线程池提交任务，execute ()方法的返回类型是 void，它定义在 Executor 接口中， 而 submit ()方法可以返回持有计算结果的 Future 对象，它定义在 ExecutorService 接口中，它扩展了 Executor 接口，其它线程池类像 ThreadPoolExecutor 和 ScheduledThreadPoolExecutor 都有这些方法。 59.什么是阻塞式方法？ 阻塞式方法是指程序会一直等待该方法完成期间不做其他事情，ServerSocket 的 accept ()方法就是一直等待客户端连接。这里的阻塞是指调用结果返回之前，当前线程会被挂起，直到得到结果之后才会返回。此外，还有异步和非阻塞式方法在任务完成前就返回。 60.Swing 是线程安全的吗？ 你可以很肯定的给出回答，Swing 不是线程安全的。你不能通过任何线程来更新 Swing 组件，如 JTable、JList 或 JPanel，事实上，它们只能通过 GUI 或 AWT 线程来更新。这就是为什么 Swing 提供 invokeAndWait() 和 invokeLater() 方法来获取其他线程的 GUI 更新请求。这些方法将更新请求放入 AWT 的线程队列中，可以一直等待，也可以通过异步更新直接返回结果。 61.Java 中 invokeAndWait 和 invokeLater 有什么区别 这两个方法是 Swing API 提供给 Java 开发者用来从当前线程而不是事件派发线程更新 GUI 组件用的。InvokeAndWait ()同步更新 GUI 组件，比如一个进度条，一旦进度更新了，进度条也要做出相应改变。如果进度被多个线程跟踪，那么就调用 invokeAndWait ()方法请求事件派发线程对组件进行相应更新。而 invokeLater ()方法是异步调用更新组件的。 62.Swing API 中那些方法是线程安全的？ 虽然Swing不是线程安全的但是有一些方法是可以被多线程安全调用的。如repaint ()， revalidate ()。 JTextComponent 的 setText ()方法和 JTextArea 的 insert () 和 append () 方法也是线程安全的。 63.如何在 Java 中创建 Immutable 对象 Immutable 对象可以在没有同步的情况下共享，降低了对该对象进行并发访问时的同步化开销。可是 Java 没有@Immutable 这个注解符，要创建不可变类，要实现下面几个步骤：通过构造方法初始化所有成员、对变量不要提供 setter 方法、将所有的成员声明为私有的，这样就不允许直接访问这些成员、在 getter 方法中，不要直接返回对象本身，而是克隆对象，并返回对象的拷贝。 64.Java 中的 ReadWriteLock 是什么？ 一般而言，读写锁是用来提升并发程序性能的锁分离技术的成果。Java 中的 ReadWriteLock 是 Java 5 中新增的一个接口，一个 ReadWriteLock 维护一对关联的锁，一个用于只读操作一个用于写。在没有写线程的情况下一个读锁可能会同时被多个读线程持有。写锁是独占的，你可以使用 JDK 中的 ReentrantReadWriteLock 来实现这个规则，它最多支持 65535 个写锁和 65535 个读锁。 65.多线程中的忙循环是什么? 忙循环就是程序员用循环让一个线程等待，不像传统方法 wait ()， sleep () 或 yield () 它们都放弃了 CPU 控制，而忙循环不会放弃 CPU，它就是在运行一个空循环。这么做的目的是为了保留 CPU 缓存，在多核系统中，一个等待线程醒来的时候可能会在另一个内核运行，这样会重建缓存。为了避免重建缓存和减少等待重建的时间就可以使用它了。 66.volatile 变量和 atomic 变量有什么不同 volatile 变量和 atomic 变量看起来很像，但功能却不一样。volatile 变量可以确保先行关系，即写操作会发生在后续的读操作之前， 但它并不能保证原子性。例如用 volatile 修饰 count 变量那么 count++ 操作并不是原子性的。而 AtomicInteger 类提供的 atomic 方法可以让这种操作具有原子性如 getAndIncrement ()方法会原子性的进行增量操作把当前值加一，其它数据类型和引用变量也可以进行相似操作。 67.如果同步块内的线程抛出异常会发生什么？ 无论你的同步块是正常还是异常退出的，里面的线程都会释放锁，所以对比锁接口我更喜欢同步块，因为它不用我花费精力去释放锁，该功能可以在 finally block 里释放锁实现。 68.如何在 Java 中创建线程安全的 Singleton 5种，急加载，同步方法，双检锁，静态内部类，枚举 69.如何强制启动一个线程？ 这个问题就像是如何强制进行 Java 垃圾回收，目前还没有觉得方法，虽然你可以使用 System.gc ()来进行垃圾回收，但是不保证能成功。在 Java 里面没有办法强制启动一个线程，它是被线程调度器控制着且 Java 没有公布相关的 API。 70.Java 中的 fork join 框架是什么？ fork join 框架是 JDK7 中出现的一款高效的工具，Java 开发人员可以通过它充分利用现代服务器上的多处理器。它是专门为了那些可以递归划分成许多子模块设计的，目的是将所有可用的处理能力用来提升程序的性能。fork join 框架一个巨大的优势是它使用了工作窃取算法，可以完成更多任务的工作线程可以从其它线程中窃取任务来执行。 71.Java 多线程中调用 wait () 和 sleep ()方法有什么不同？ Java 程序中 wait 和 sleep 都会造成某种形式的暂停，它们可以满足不同的需要。wait ()方法意味着条件等待，如果等待条件为真且其它线程被唤醒时它会释放锁，而 sleep ()方法仅仅释放 CPU 资源或者让当前线程短暂停顿，但不会释放锁。 72.可重入锁 可重入锁：如果当前线程已经获得了某个监视器对象所持有的锁，那么该线程在该方法中调用另外一个同步方法也同样持有该锁。 public synchrnozied void test() { xxxxxx; test2(); } public synchronized void test2() { yyyyy; } 在上面代码段中，执行 test 方法需要获得当前对象作为监视器的对象锁，但方法中又调用了 test2 的同步方法。 如果锁是具有可重入性的话，那么该线程在调用 test2 时并不需要再次获得当前对象的锁，可以直接进入 test2 方法进行操作。 如果锁是不具有可重入性的话，那么该线程在调用test2前会等待当前对象锁的释放，实际上该对象锁已被当前线程所持有，不可能再次获得。 如果锁是不具有可重入性特点的话，那么线程在调用同步方法、含有锁的方法时就会产生死锁。 73. 同步方法和同步代码块 同步方法默认用this或者当前类class对象作为锁； 同步代码块可以选择以什么来加锁，比同步方法要更细颗粒度，我们可以选择只同步会发生同步问题的部分代码而不是整个方法。 四、Java虚拟机 0. 对哪些区域回收 Java运行时数据区域：程序计数器、JVM栈、本地方法栈、方法区和堆。 由于程序计数器、JVM栈、本地方法栈3个区域随线程而生随线程而灭，对这几个区域内存的回收和分配具有确定性。而方法区和堆则不一样，程序需要在运行时才知道创建哪些对象，对这部分内存的分配是动态的，GC关注的也就是这部分内存。 1. 主动GC 调用system.gc() Runtime.getRuntime.gc() 2. 垃圾回收 释放那些不在持有任何引用的对象的内存 3. 怎样判断是否需要收集 引用计数法：对象没有任何引用与之关联(无法解决循环引用) ext：Python使用引用计数法 可达性分析法：通过一组称为GC Root的对象为起点,从这些节点向下搜索，如果某对象不能从这些根对象的一个(至少一个)所到达,则判定该对象应当回收。 ext：可作为GCRoot的对象：虚拟机栈中引用的对象。方法区中类静态属性引用的对象，方法区中类常量引用的对象，本地方法栈中JNI引用的对象 4.对象的自我救赎 即使在可达性算法中判定为不可达时，也并非一定被回收。对象存在自我救赎的可能。要真正宣告对象的死亡，需要经历2次标记的过程。如果对象经过可达性分析法发现不可达时，对象将被第一次标记被进行筛选，筛选的条件是此对象是否有必要执行finalize方法。如果对象没有重写finalize方法或finalize方法已经被JVM调用过，则判定为不需要执行。 如果对象被判定为需要执行finalize方法，该对象将被放置在一个叫做F-Queue的队列中，JVM会建立一个低优先级的线程执行finalize方法，如果对象想要完成自我救赎需要在finalize方法中与引用链上的对象关联，比如把自己也就是this赋值给某个类变量。当GC第二次对F-Queue中对象标记时，该对象将被移出“即将回收”的集合，完成自我救赎。简言之，finalize方法是对象逃脱死亡命运的最后机会，并且任何对象的finalize方法只会被JVM调用一次。 5.垃圾回收算法 Mark-Sweep法：标记清除法 容易产生内存碎片，导致分配较大对象时没有足够的连续内存空间而提前出发GC。这里涉及到另一个问题，即对象创建时的内存分配，对象创建内存分配主要有2种方法，分别是指针碰撞法和空闲列表法。指针碰撞法：使用的内存在一侧，空闲的在另一侧，中间使用一个指针作为分界点指示器，对象内存分配时只要指针向空闲的移动对象大小的距离即可。 空闲列表法：使用的和空闲的内存相互交错无法进行指针碰撞，JVM必须维护一个列表记录哪些内存块可用，分配时从列表中找出一个足够的分配给对象，并更新列表记录。所以，当采用Mark-Sweep算法的垃圾回收器时，内存分配通常采用空闲列表法。 Copy法：将内存分为2块，每次使用其中的一块，当一块满了，将存活的对象复制到另一块，把使用过的那一块一次性清除。显然，Copy法解决了内存碎片的问题，但算法的代价是内存缩小为原来的一半。现代的垃圾收集器对新生代采用的正是Copy算法。但通常不执行1:1的策略，HotSpot虚拟机默认Eden区Survivor区8:1。每次使用Eden和其中一块Survivor区。也就是说新生代可用内存为新生代内存空间的90%。 Mark-Compact法：标记整理法。它的第一阶段与Mark-Sweep法一样，但不直接清除，而是将存活对象向一端移动，然后清除端边界以外的内存，这样也不存在内存碎片。 分代收集算法：将堆内存划分为新生代，老年代，根据新生代老年代的特点选取不同的收集算法。因为新生代对象大多朝生夕死，而老年代对象存活率高，没有额外空间进行分配担保，通常对新生代执行复制算法，老年代执行Mark-Sweep算法或Mark-Compact算法。 6.垃圾收集器 通常来说，新生代老年代使用不同的垃圾收集器。新生代的垃圾收集器有Serial（单线程）、ParNew（Serial的多线程版本）、ParallelScavenge（吞吐量优先的垃圾收集器），老年代有SerialOld（单线程老年代）、ParallelOld（与ParallelScavenge搭配的多线程执行标记整理算法的老年代收集器）、CMS（标记清除算法，容易产生内存碎片，可以开启内存整理的参数），以及当前最先进的垃圾收集器G1，G1通常面向服务器端的垃圾收集器，在我自己的Java应用程序中通过-XX:+PrintGCDetails，发现自己的垃圾收集器是使用了ParallelScavenge + ParallelOld的组合。 7. 不同垃圾回收算法对比 标记清除法(Mark-Sweeping):易产生内存碎片 复制回收法(Copying)：为了解决Mark-Sweep法而提出,内存空间减至一半 标记压缩法(Mark-Compact):为了解决Copying法的缺陷,标记后移动到一端再清除 分代回收法(GenerationalCollection):新生代对象存活周期短,需要大量回收对象,需要复制的少,执行copy算法;老年代对象存活周期相对长,回收少量对象,执行mark-compact算法.新生代划分：较大的eden区 和 2个survivor区 8. 内存分配 新生代的三部分 |Eden Space|From Space|To Space|，对象主要分配在新生代的Eden区 大对象直接进入老年代 大对象比如大数组直接进入老年代，可通过虚拟机参数-XX：PretenureSizeThreshold参数设置 长期存活的对象进入老年代 ext：虚拟机为每个对象定义一个年龄计数器，如果对象在Eden区出生并经过一次MinorGC仍然存活，将其移入Survivor的To区，GC完成标记互换后，相当于存活的对象进入From区，对象年龄加1，当增加到默认15岁时，晋升老年代。可通过-XX：MaxTenuringThreshold设置 GC的过程：GC开始前，对象只存在于Eden区和From区，To区逻辑上始终为空。对象分配在Eden区，Eden区空间不足，发起MinorGC，将Eden区所有存活的对象复制到To区，From区存活的对象根据年龄判断去向，若到达年龄阈值移入老年代，否则也移入To区，GC完成后Eden区和From区被清空，From区和To区标记互换。对象每在Survivor区躲过一次MinorGC年龄加一。MinorGC将重复这样的过程，直到To区被填满，To区满了以后，将把所有对象移入老年代。 动态对象年龄判定 suvivor区相同年龄对象总和大于suvivor区空间的一半,年龄大于等于该值的对象直接进入老年代 空间分配担保 在MinorGC开始前，虚拟机检查老年代最大可用连续空间是否大于新生代所有对象总空间，如果成立，MinorGC可以确保是安全的。否则，虚拟机会查看HandlePromotionFailure设置值是否允许担保失败，如果允许，继续查看老年代最大可用连续空间是否大于历次晋升到老年代对象的平均大小，如果大于则尝试MinorGC，尽管这次MinorGC是有风险的。如果小于，或者HandlerPromotionFailure设置不允许，则要改为FullGC。 新生代的回收称为MinorGC,对老年代的回收成为MajorGC又名FullGC 9. 关于GC的虚拟机参数 GC相关 -XX:NewSize和-XX:MaxNewSize 新生代大小 -XX:SurvivorRatio Eden和其中一个survivor的比值 -XX：PretenureSizeThreshold 大对象进入老年代的阈值 -XX:MaxTenuringThreshold 晋升老年代的对象年龄 收集器组合开关选项： -XX:+UseSerialGC Serial+SerialOld -XX:+UseParNewGC ParNew+SerialOld -XX:+UseParallelGC ParallelScavenge+SerialOld -XX:+UseConcMarkSweepGC ParNew+CMS+SerialOld -XX:+UseParallelOldGC ParallelScavenge+Parallel Old 堆大小设置 -Xmx:最大堆大小 -Xms:初始堆大小(最小内存值) -Xmn:年轻代大小 -XXSurvivorRatio:3 意思是Eden:Survivor=3:2 -Xss栈容量 垃圾回收统计信息 -XX:+PrintGC 输出GC日志 -XX:+PrintGCDetails 输出GC的详细日志 10. 方法区的回收 方法区通常会与永久代划等号，实际上二者并不等价，只不过是HotSpot虚拟机设计者用永久代实现方法区，并将GC分代扩展至方法区。 永久代垃圾回收通常包括两部分内容：废弃常量和无用的类。常量的回收与堆区对象的回收类似，当没有其他地方引用该字面量时，如果有必要，将被清理出常量池。 判定无用的类的3个条件： 1.该类的所有实例都已经被回收，也就是说堆中不存在该类的任何实例 2.加载该类的ClassLoader已经被回收 3.该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 当然，这也仅仅是判定，不代表立即卸载该类。 11. JVM工具 命令行 jps(jvm processor status)虚拟机进程状况工具 jstat(jvm statistics monitoring)统计信息监视 jinfo(configuration info for java)配置信息工具 jmap(memory map for java)Java内存映射工具 jhat(JVM Heap Analysis Tool)虚拟机堆转储快照分析工具 jstack(Stack Trace for Java)Java堆栈跟踪工具 HSDIS：JIT生成代码反汇编 可视化 JConsole(Java Monitoring and Management Console):Java监视与管理控制台 VisualVM(All-in-one Java Troubleshooting Tool):多合一故障处理工具 12. JVM内存结构 堆:新生代和年老代 方法区(非堆):持久代, 代码缓存, 线程共享 JVM栈:中间结果,局部变量,线程隔离 本地栈:本地方法(非Java代码) 程序计数器 ：线程私有，每个线程都有自己独立的程序计数器，用来指示下一条指令的地址 注：持久代Java8消失, 取代的称为元空间(本地堆内存的一部分) 13. JVM的方法区 与堆一样，是线程共享的区域。方法区中存储：被虚拟机加载的类信息，常量，静态变量，JIT编译后的代码等数据。参见我是一个Java Class。 14. Java类加载器 一个jvm中默认的classloader有Bootstrap ClassLoader、Extension ClassLoader、App ClassLoader，分别各司其职： Bootstrap ClassLoader(引导类加载器) 负责加载java基础类，主要是 %JRE_HOME/lib/目录下的rt.jar、resources.jar、charsets.jar等 Extension ClassLoader(扩展类加载器) 负责加载java扩展类，主要是 %JRE_HOME/lib/ext目录下的jar等 App ClassLoader(系统类加载器) 负责加载当前java应用的classpath中的所有类。 classloader 加载类用的是全盘负责委托机制。 所谓全盘负责，即是当一个classloader加载一个Class的时候，这个Class所依赖的和引用的所有 Class也由这个classloader负责载入，除非是显式的使用另外一个classloader载入。 所以，当我们自定义的classloader加载成功了com.company.MyClass以后，MyClass里所有依赖的class都由这个classLoader来加载完成。 15. 64 位 JVM 中，int 的长度是多大？ Java 中，int 类型变量的长度是一个固定值，与平台无关，都是 32 位。意思就是说，在 32 位 和 64 位 的Java 虚拟机中，int 类型的长度是相同的。 16. Serial 与 Parallel GC之间的不同之处？ Serial 与 Parallel 在GC执行的时候都会引起 stop-the-world。它们之间主要不同 serial 收集器是默认的复制收集器，执行 GC 的时候只有一个线程，而 parallel 收集器使用多个 GC 线程来执行。 17.Java 中 WeakReference 与 SoftReference的区别？ Java中一共有四种类型的引用。StrongReference、 SoftReference、 WeakReference 以及 PhantomReference。 StrongReference：Java 的默认引用实现, 它会尽可能长时间的存活于 JVM 内，当没有任何对象指向它时将会被GC回收 SoftReference：尽可能长时间保留引用，直到JVM内存不足，适合某些缓存应用 WeakReference：顾名思义, 是一个弱引用, 当所引用的对象在 JVM 内不再有强引用时, 下一次将被GC回收 PhantomReference：它是最弱的一种引用关系，也无法通过PhantomReference取得对象的实例。仅用来当该对象被回收时收到一个通知 虽然 WeakReference 与 SoftReference 都有利于提高 GC 和 内存的效率，但是 WeakReference ，一旦失去最后一个强引用，就会被 GC 回收，而 SoftReference 会尽可能长的保留引用直到 JVM 内存不足时才会被回收(虚拟机保证), 这一特性使得 SoftReference 非常适合缓存应用。 18. WeakHashMap 是怎么工作的？ WeakHashMap 的工作与正常的 HashMap 类似，但是使用弱引用作为 key，意思就是当 key 对象没有任何引用时，key/value 将会被回收。 19. JVM 选项 -XX:+UseCompressedOops 有什么作用？为什么要使用？ 当你将你的应用从 32 位的 JVM 迁移到 64 位的 JVM 时，由于对象的指针从 32 位增加到了 64 位，因此堆内存会突然增加，差不多要翻倍。这也会对 CPU 缓存（容量比内存小很多）的数据产生不利的影响。因为，迁移到 64 位的 JVM 主要动机在于可以指定最大堆大小，通过压缩 OOP 可以节省一定的内存。通过 -XX:+UseCompressedOops 选项，JVM 会使用 32 位的 OOP，而不是 64 位的 OOP。 20. 怎样通过 Java 程序来判断 JVM 是 32 位 还是 64 位？ 你可以检查某些系统属性如 sun.arch.data.model 或 os.arch 来获取该信息。 21. 32 位 JVM 和 64 位 JVM 的最大堆内存分别是多数？ 理论上说上 32 位的 JVM 堆内存可以到达 2^32，即 4GB，但实际上会比这个小很多。不同操作系统之间不同，如 Windows 系统大约 1.5 GB，Solaris 大约 3GB。64 位 JVM允许指定最大的堆内存，理论上可以达到 2^64，这是一个非常大的数字，实际上你可以指定堆内存大小到 100GB。甚至有的 JVM，如 Azul，堆内存到 1000G 都是可能的。 22. JRE、JDK、JVM 及 JIT 之间有什么不同？ JRE 代表 Java 运行时（Java run-time），是运行 Java 应用所必须的。JDK 代表 Java 开发工具（Java development kit），是 Java 程序的开发工具，如 Java 编译器，它也包含 JRE。JVM 代表 Java 虚拟机（Java virtual machine），它的责任是运行 Java 应用。JIT 代表即时编译（Just In Time compilation），当代码执行的次数超过一定的阈值时，会将 Java 字节码转换为本地代码，如，主要的热点代码会被准换为本地代码，这样有利大幅度提高 Java 应用的性能。 23. 解释 Java 堆空间及 GC？ 当通过 Java 命令启动 Java 进程的时候，会为它分配内存。内存的一部分用于创建堆空间，当程序中创建对象的时候，就从对空间中分配内存。GC 是 JVM 内部的一个后台进程，回收无效对象的内存用于将来的分配。 24. 你能保证 GC 执行吗？ 不能，虽然你可以调用 System.gc() 或者 Runtime.getRuntime().gc()，但是没有办法保证 GC 的执行。 25. 怎么获取 Java 程序使用的内存？堆使用的百分比？ 可以通过 java.lang.Runtime 类中与内存相关方法来获取剩余的内存，总内存及最大堆内存。通过这些方法你也可以获取到堆使用的百分比及堆内存的剩余空间。Runtime.freeMemory() 方法返回剩余空间的字节数，Runtime.totalMemory() 方法总内存的字节数，Runtime.maxMemory() 返回最大内存的字节数。 26. Java 中堆和栈有什么区别？ JVM 中堆和栈属于不同的内存区域，使用目的也不同。栈常用于保存方法帧和局部变量，而对象总是在堆上分配。栈通常都比堆小，也不会在多个线程之间共享，而堆被整个 JVM 的所有线程共享。 27. JVM调优 使用工具Jconsol、VisualVM、JProfiler等 堆信息查看 可查看堆空间大小分配（年轻代、年老代、持久代分配） 提供即时的垃圾回收功能 垃圾监控（长时间监控回收情况） 查看堆内类、对象信息查看：数量、类型等 对象引用情况查看 有了堆信息查看方面的功能，我们一般可以顺利解决以下问题： 年老代年轻代大小划分是否合理 内存泄漏 垃圾回收算法设置是否合理 线程监控 线程信息监控：系统线程数量。 线程状态监控：各个线程都处在什么样的状态下 Dump线程详细信息：查看线程内部运行情况 死锁检查 热点分析 CPU热点：检查系统哪些方法占用的大量CPU时间 内存热点：检查哪些对象在系统中数量最大（一定时间内存活对象和销毁对象一起统计） 快照 系统两个不同运行时刻，对象（或类、线程等）的不同 举例说，我要检查系统进行垃圾回收以后，是否还有该收回的对象被遗漏下来的了。那么，我可以在进行垃圾回收前后，分别进行一次堆情况的快照，然后对比两次快照的对象情况。 内存泄漏检查 年老代堆空间被占满 持久代被占满 堆栈溢出 线程堆栈满 系统内存被占满 28. Java中有内存泄漏吗？ 内存泄露的定义: 当某些对象不再被应用程序所使用,但是由于仍然被引用而导致垃圾收集器不能释放。 内存泄漏的原因：对象的生命周期不同。比如说对象A引用了对象B. A的生命周期比B的要长得多，当对象B在应用程序中不会再被使用以后, 对象 A 仍然持有着B的引用. (根据虚拟机规范)在这种情况下GC不能将B从内存中释放。这种情况很可能会引起内存问题，倘若A还持有着其他对象的引用,那么这些被引用的(无用)对象也不会被回收,并占用着内存空间。甚至有可能B也持有一大堆其他对象的引用。这些对象由于被B所引用,也不会被垃圾收集器所回收，所有这些无用的对象将消耗大量宝贵的内存空间。并可能导致内存泄漏。 怎样防止： 1、当心集合类, 比如HashMap, ArrayList等,因为这是最容易发生内存泄露的地方.当集合对象被声明为static时,他们的生命周期一般和整个应用程序一样长。 29. OOM解决办法 内存溢出的空间：Permanent Generation和Heap Space，也就是永久代和堆区 1、永久代的OOM 解决办法有2种： a.通过虚拟机参数-XX：PermSize和-XX：MaxPermSize调整永久代大小 b.清理程序中的重复的Jar文件，减少类的重复加载 2、堆区的溢出 发生这种问题的原因是java虚拟机创建的对象太多，在进行垃圾回收之间，虚拟机分配的到堆内存空间已经用满了，与Heap Space的size有关。解决这类问题有两种思路： 检查程序，看是否存在死循环或不必要地重复创建大量对象，定位原因，修改程序和算法。 通过虚拟机参数-Xms和-Xmx设置初始堆和最大堆的大小 30. DirectMemory直接内存 直接内存并不是Java虚拟机规范定义的内存区域的一部分，但是这部分内存也被频繁使用，而且也可能导致OOM异常的出现。 JDK1.4引入了NIO，这是一种基于通道和缓冲区的非阻塞IO模式，它可以使用Native函数库分配直接堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作，使得在某些场合显著提高性能，因为它避免了在Java堆和本地堆之间来回复制数据。 31. Java 中堆和栈有什么不同 每个线程都有自己的栈内存，用于存储本地变量，方法参数和栈调用，一个线程中存储的变量对其它线程是不可见的。而堆是所有线程共享的一片公用内存区域。对象都在堆里创建，为了提升效率线程会从堆中弄一个缓存到自己的栈，如果多个线程使用该变量就可能引发问题，这时 volatile 变量就可以发挥作用了，它要求线程从主存中读取变量的值。 32. 双亲委派模型中的方法 findLoadedClass(),LoadClass(),findBootstrapClassOrNull(),findClass(),resolveClass() 33. IO模型 一般来说 I/O 模型可以分为：同步阻塞，同步非阻塞，异步阻塞，异步非阻塞 四种IO模型 同步阻塞 IO ： 在此种方式下，用户进程在发起一个 IO 操作以后，必须等待 IO 操作的完成，只有当真正完成了 IO 操作以后，用户进程才能运行。 JAVA传统的 IO 模型属于此种方式！ 同步非阻塞 IO: 在此种方式下，用户进程发起一个 IO 操作以后可返回做其它事情，但是用户进程需要时不时的询问 IO 操作是否就绪，这就要求用户进程不停的去询问，从而引入不必要的 CPU 资源浪费。其中目前 JAVA 的 NIO 就属于同步非阻塞 IO 。 异步阻塞 IO ： 此种方式下是指应用发起一个 IO 操作以后，不等待内核 IO 操作的完成，等内核完成 IO 操作以后会通知应用程序，这其实就是同步和异步最关键的区别，同步必须等待或者主动的去询问 IO 是否完成，那么为什么说是阻塞的呢？因为此时是通过 select 系统调用来完成的，而 select 函数本身的实现方式是阻塞的，而采用 select 函数有个好处就是它可以同时监听多个文件句柄，从而提高系统的并发性！ 异步非阻塞 IO: 在此种模式下，用户进程只需要发起一个 IO 操作然后立即返回，等 IO 操作真正的完成以后，应用程序会得到 IO 操作完成的通知，此时用户进程只需要对数据进行处理就好了，不需要进行实际的 IO 读写操作，因为 真正的 IO读取或者写入操作已经由 内核完成了。目前 Java7的AIO正是此种类型。 BIO即同步阻塞IO，适用于连接数目较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4之前的唯一选择，但程序直观、简单、易理解。 NIO即同步非阻塞IO，适用于连接数目多且连接比较短的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持。 AIO即异步非阻塞IO，适用于连接数目多且连接比较长的架构，如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK1.7开始支持 34. 类加载器按照层次，从顶层到底层，分别加载哪些类？ 启动类加载器：负责将存放在JAVA_HOME/lib下的，或者被－Xbootclasspath参数所指定的路径中的，并且是虚拟机识别的类库加载到虚拟机内存中。启动类加载器无法被Java程序直接引用。 扩展类加载器：这个加载器负责加载JAVA_HOME/lib/ext目录中的，或者被java.ext.dirs系统变量所指定的路径中的所有类库，开发者可以直接使用扩展类加载器 应用程序类加载器：这个加载器是ClassLoader中getSystemClassLoader()方法的返回值，所以一般也称它为系统类加载器。它负责加载用户类路径（Classpath）上所指定的类库，可直接使用这个加载器，如果应用程序没有自定义自己的类加载器，一般情况下这个就是程序中默认的类加载器 实现自己的加载器 只需要继承ClassLoader，并覆盖findClass方法。 在调用loadClass方法时，会先根据委派模型在父加载器中加载，如果加载失败，则会调用自己的findClass方法来完成加载 五、数据库（Sql、MySQL、Redis等） 1. Statement 1.1 基本内容 Statement是最基本的用法, 不传参, 采用字符串拼接，存在注入漏洞 PreparedStatement传入参数化的sql语句, 同时检查合法性, 效率高可以重用, 防止sql注入 CallableStatement接口扩展PreparedStatement，用来调用存储过程 BatchedStatement用于批量操作数据库，BatchedStatement不是标准的Statement类 public interface CallableStatement extends PreparedStatement public interface PreparedStatement extends Statement 1.2 Statement与PrepareStatement的区别 创建时的区别Statement statement = conn.createStatement(); PreparedStatement preStatement = conn.prepareStatement(sql); 执行的时候 ResultSet rSet = statement.executeQuery(sql); ResultSet pSet = preStatement.executeQuery(); 由上可以看出，PreparedStatement有预编译的过程，已经绑定sql，之后无论执行多少遍，都不会再去进行编译，而 statement 不同，如果执行多遍，则相应的就要编译多少遍sql，所以从这点看，preStatement 的效率会比 Statement要高一些 安全性 PreparedStatement是预编译的，所以可以有效的防止SQL注入等问题 代码的可读性和可维护性 PreparedStatement更胜一筹 2. 游标 3. 列出 5 个应该遵循的 JDBC 最佳实践 有很多的最佳实践，你可以根据你的喜好来例举。下面是一些更通用的原则： a）使用批量的操作来插入和更新数据 b）使用 PreparedStatement 来避免 SQL 异常，并提高性能 c）使用数据库连接池 d）通过列名来获取结果集，不要使用列的下标来获取 4. 数据库索引的实现 数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。 B树： 一棵m阶B树(balanced tree of order m)是一棵平衡的m路搜索树。它或者是空树，或者是满足下列性质的树： 1、根结点至少有两个子女； 2、每个非根节点所包含的关键字个数 j 满足：┌m/2┐ - 1 由于B-Tree的特性，在B-Tree中按key检索数据的算法非常直观：首先从根节点进行二分查找，如果找到则返回对应节点的data，否则对相应区间的指针指向的节点递归进行查找，直到找到节点或找到null指针，前者查找成功，后者查找失败。 一个度为d的B-Tree，设其索引N个key，则其树高h的上限为logd((N+1)/2)，检索一个key，其查找节点个数的渐进复杂度为O(logdN)。从这点可以看出，B-Tree是一个非常有效率的索引数据结构。 B+树： B-Tree有许多变种，其中最常见的是B+Tree，例如MySQL就普遍使用B+Tree实现其索引结构。 B+树是B树的变形，它把所有的data都放在叶子结点中，只将关键字和子女指针保存于内结点，内结点完全是索引的功能。 与B-Tree相比，B+Tree有以下不同点： 1、每个节点的指针上限为2d而不是2d+1。 2、内节点不存储data，只存储key；叶子节点存储data不存储指针。 一般在数据库系统或文件系统中使用的B+Tree结构都在经典B+Tree的基础上进行了优化，增加了顺序访问指针。 在B+Tree的每个叶子节点增加一个指向相邻叶子节点的指针 例如图4中如果要查询key为从18到49的所有数据记录，当找到18后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。 为什么B树（B+树）？ 一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。 这涉及到磁盘存取原理、局部性原理和磁盘预读。 先从B-Tree分析，根据B-Tree的定义，可知检索一次最多需要访问h个节点。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。 为了达到这个目的，在实际实现B-Tree还需要使用如下技巧： 每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。 B-Tree中一次检索最多需要h-1次I/O（根节点常驻内存），渐进复杂度为O(h)=O(logdN)。一般实际应用中，出度d是非常大的数字，通常超过100，因此h非常小（通常不超过3）。 综上所述，用B-Tree作为索引结构效率是非常高的。 而红黑树这种结构，h明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进复杂度也为O(h)，效率明显比B-Tree差很多。 至于B+Tree为什么更适合外存索引，原因和内节点出度d有关。 由于B+Tree内节点去掉了data域，因此可以拥有更大的出度，拥有更好的性能。 六、算法与数据结构 1. 二叉搜索树:(Binary Search Tree又名：二叉查找树,二叉排序树)它或者是一棵空树,或者是具有下列性质的二叉树： 若它的左子树不空,则左子树上所有结点的值均小于它的根结点的值；若它的右子树不空,则右子树上所有结点的值均大于它的根结点的值；它的左、右子树也分别为二叉搜索树。 2. RBT红黑树 二叉搜索树:(Binary Search Tree又名：二叉查找树，二叉排序树)它或者是一棵空树,或者是具有下列性质的二叉树： 若它的左子树不空,则左子树上所有结点的值均小于它的根结点的值；若它的右子树不空,则右子树上所有结点的值均大于它的根结点的值；它的左、右子树也分别为二叉搜索树。 红黑树是一棵二叉搜索树，它在每个结点上增加一个存储位来表示结点的颜色，可以是RED或BLACK。通过对任何一条从根到叶子的简单路径上各个结点的颜色进行约束，红黑树没有一条路径会比其他路径长出2倍，所以红黑树是近似平衡的，使得红黑树的查找、插入、删除等操作的时间复杂度最坏为O(log n)，但需要注意到在红黑树上执行插入或删除后将不在满足红黑树性质，恢复红黑树的属性需要少量(O(log n))的颜色变更(实际是非常快速的)和不超过三次树旋转(对于插入操作是两次)。虽然插入和删除很复杂，但操作时间仍可以保持为 O(log n) 次。具体如何保证？引出红黑树的5个性质。 红黑树的5个性质：满足以下五个性质的二叉搜索树 每个结点或是红色的或是黑色的 根结点是黑色的 每个叶结点是黑色的 如果一个结点是红色的,则它的两个子结点是黑色的 对于每个结点,从该结点到其后代叶结点的简单路径上,均包含相同数目的黑色结点 插入操作： 由于性质的约束，插入的结点都是红色的。插入时性质1、3始终保持。破坏性质2当且仅当当前插入结点为根节点。变一下颜色即可。如果是破坏性质4或5，则需要旋转和变色来继续满足红黑树的性质。下面说一说插入的几种情况，约定当前插入结点为N，其父结点为P，叔叔为U，祖父为G 情形1：树空，直接插入违反性质1，将红色改黑。 情形2：N的父结点为黑，不必修改，直接插入 从情形3开始的情形假定N结点的父结点P为红色，所以存在G，并且G为黑色。且N存在一个叔叔结点U，尽管U可能为叶结点。 情形3：P为红，U为红（G结点一定存在且为黑）这里不论P是G的左孩子还是右孩子；不论N是P的左孩子还是右孩子。 首先把P、U改黑，G改红，并以G作为一个新插入的红结点重新进行各种情况的检查，若一路检索至根节点还未结束，则将根结点变黑。 情形4：P为红，U为黑或不存在（G结点一定存在且为黑），且P为G的左孩子，N为P的左孩子（或者P为G的右孩子，N为P的右孩子，保证同向的）。 P、G右旋并将P、G变相反色。因为P取代之前黑G的位置，所以P变黑可以理解，而G变红是为了不违反性质5。 情形5：P为红，U为黑或不存在，且P为G的左孩子，N为P的右孩子（或P为G的右孩子，N为P的左孩子，保证是反向的），对N，P进行一次左旋转换为情形4 删除操作比插入复杂一些，但最多不超过三次旋转可以让红黑树恢复平衡。 其他 黑高从某个结点x出发(不含x)到达一个叶结点的任意一条简单路径上的黑色结点个数称为该结点的黑高。红黑树的黑高为其根结点的黑高。 一个具有n个内部结点的红黑树的高度h 结点的属性(五元组):color key left right p 动态集合操作最坏时间复杂度为O(lgn) 3. 排序算法 稳定排序:插入排序、冒泡排序、归并排序、基数排序 插入排序[稳定] 适用于小数组,数组已排好序或接近于排好序速度将会非常快 复杂度：O(n^2) - O(n) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 归并排序[稳定] 采用分治法 复杂度：O(nlogn) - O(nlgn) - O(nlgn) - O(n)[平均 - 最好 - 最坏 - 空间复杂度] 冒泡排序[稳定] 复杂度：O(n^2) - O(n) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 基数排序 分配+收集[稳定] 复杂度： O(d(n+r)) r为基数d为位数 空间复杂度O(n+r) 树排序[不稳定] 应用：TreeSet的add方法、TreeMap的put方法 不支持相同元素,没有稳定性问题 复杂度：平均最差O(nlogn) 堆排序(就地排序)[不稳定] 复杂度：O(nlogn) - O(nlgn) - O(nlgn) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 快速排序[不稳定] 复杂度：O(nlgn) - O(nlgn) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 栈空间 O(lgn) - O(n) 选择排序[不稳定] 复杂度：O(n^2) - O(n^2) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 希尔排序[不稳定] 复杂度 小于O(n^2) 平均 O(nlgn) 最差O(n^s)[1 九大内部排序算法代码及性能分析参见我的GitHub 4. 查找与散列 4.1 散列函数设计 直接定址法:f(key) = a*key+b 简单、均匀,不易产生冲突。但需事先知道关键字的分布情况,适合查找表较小且连续的情况,故现实中并不常用 除留余数法:f(key) = key mod p (p DJBX33A算法(time33哈希算法hash = hash*33+(unsigned int)str[i]; 平方取中法 折叠法 更多.... 4.2 冲突处理 闭散列(开放地址方法):要求装填因子a较小，闭散列方法把所有记录直接存储在散列表中 线性探测:易产生堆积现象(基地址不同堆积在一起) 二次探测:f(key) = (f(key)+di) % m di=1^2,-1^2,2^2,-2^2...可以消除基本聚集 随机探测:f(key) = (f(key)+di),di采用随机函数得到,可以消除基本聚集 双散列:避免二次聚集 开散列(链地址法):原地处理 同义词记录存储在一个单链表中,散列表中子存储单链表的头指针。 优点:无堆积 事先无需确定表长 删除结点易于实现 装载因子a>=1,缺点:需要额外空间 5. 跳表 为什么选择跳表？ 目前经常使用的平衡数据结构有：B树，红黑树，AVL树，Splay Tree等。想象一下，给你一张草稿纸，一只笔，一个编辑器，你能立即实现一颗红黑树，或者AVL树出来吗？ 很难吧，这需要时间，要考虑很多细节，要参考一堆算法与数据结构之类的树，还要参考网上的代码，相当麻烦。用跳表吧，跳表是一种随机化的数据结构，目前开源软件 Redis 和 LevelDB 都有用到它，它的效率和红黑树以及 AVL 树不相上下，但跳表的原理相当简单，只要你能熟练操作链表，就能去实现一个 SkipList。 跳跃表是一种随机化数据结构，基于并联的链表，其效率可比拟于二叉查找树(对于大多数操作需要O(log n)平均时间)，并且对并发算法友好。 Skip list(跳表）是一种可以代替平衡树的数据结构，默认是按照Key值升序的。SkipList让已排序的数据分布在多层链表中，以0-1随机数决定一个数据的向上攀升与否，是一种“空间来换取时间”的一个算法，在每个节点中增加了指向下一层的指针，在插入、删除、查找时可以忽略一些不可能涉及到的结点，从而提高了效率。 在Java的API中已经有了实现：分别是 ConcurrentSkipListMap(在功能上对应HashTable、HashMap、TreeMap) ； ConcurrentSkipListSet(在功能上对应HashSet) SkipList的性质 (1) 由很多层结构组成，level是通过一定的概率随机产生的 (2) 每一层都是一个有序的链表，默认是升序 (3) 最底层(Level 1)的链表包含所有元素 (4) 如果一个元素出现在Level i 的链表中，则它在Level i 之下的链表也都会出现 (5) 每个节点包含两个指针，一个指向同一链表中的下一个元素，一个指向下面一层的元素 时间复杂度O(lgn) 最坏O(2lgn) Java实现参见我的GitHub Repo Algorithm 6. AVL树 1.LL型 在某一节点的左孩子的左子树上插入一个新的节点，使得该节点不再平衡。 举例 A B Ar Bl Br 在Bl下插入N，执行一次右旋即可，即把B变为父结点，原来的根节点A变为B的左孩子，B的右子树变为A的左子树。 2.RR型 与LL型是对称的，执行一次左旋即可。 3.LR型 指在AVL树某一结点左孩子的右子树上插入一个结点，使得该节点不在平衡。这时需要两次旋转，先左旋再右旋。 4.RL型 与LR对称，执行一次右旋，再执行一次左旋。 删除 1、被删的节点是叶子节点 将该节点直接从树中删除，并利用递归的特点和高度的变化，反向推算其父节点和祖先节点是否失衡。 2、被删的节点只有左子树或只有右子树 将左子树（右子树）替代原有节点的位置，并利用递归的特点和高度的变化，反向推算父节点和祖先节点是否失衡。 3、被删的节点既有左子树又有右子树 找到被删节点的左子树的最右端的节点，将该结点的的值赋给待删除结点，再用该结点的左孩子替换它本来的位置，然后释放该结点，并利用递归特点，反向推断父节点和祖父节点是否失衡。 7. 一致性Hash 第一：简单介绍 一致性哈希算法是分布式系统中常用的算法。比如，一个分布式的存储系统，要将对象存储到具体的节点上，如果采用普通的hash方法，将数据映射到具体的节点上，如key%N，N是机器节点数。 1、考虑到比如一个服务器down掉，服务器结点N变为N-1，映射公式必须变为key%(N-1) 2、访问量加重，需要添加服务器结点，N变为N+1，映射公式变为hash(object)%(N+1) 当出现1,2的情况意味着我们的映射都将无效，对服务器来说将是一场灾难，尤其是对缓存服务器来说，因为缓存服务器映射的失效，洪水般的访问都将冲向后台服务器。 第二点：hash算法的单调性 Hash 算法的一个衡量指标是单调性，单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲加入到系统中。哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲中去，而不会被映射到旧的缓冲集合中的其他缓冲区。 consistent hash 也是一种hash 算法，简单的说，在移除 / 添加一个结点时，它能够尽可能小的改变已存在的映射关系，尽可能的满足单调性的要求。 第三点：将对象和服务器结点分别映射到环型空间 通常的一致性哈希做法是将 value 映射到一个 32 位的 key 值，也即是 0~2^32-1 次方的数值空间；我们可以将这个空间想象成一个首（ 0 ）尾（ 2^32-1 ）相接的圆环。 我们可以通过hash函数将我们的key映射到环型空间中，同时根据相同的哈希算法把服务器也映射到环型空间中，顺便提一下服务器或者某个计算节点的 hash 计算，一般的方法可以使用机器的 IP 地址或者机器名作为 hash 输入。 第四点：将对象映射到服务器 在这个环形空间中，如果沿着顺时针方向从对象的 key 值出发，直到遇见一个 服务器结点，那么就将该对象存储在这个服务器结点上，因为对象和服务器的hash 值是固定的，因此这个 cache 必然是唯一和确定的。 这时候考察某个服务器down机或者需要添加服务器结点，也就是移除和添加的操作，我们只需要几个对象的映射。 第五点：虚拟结点 Hash 算法的另一个指标是平衡性 (Balance)。平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。 对于上述的做法，可能导致某些对象都映射到某个服务器，使得分布不平衡。为此可以采用“虚拟结点”的做法。 “虚拟结点”（ virtual node ）是实际节点在 hash 空间的复制品，一实际结点对应了若干个“虚拟节点”，这个对应个数也成为“复制个数”，“虚拟节点”在 hash 空间中以 hash 值排列。引入“虚拟结点”会让我们的映射分布更为平衡一些。 引入“虚拟结点”前： Hash(“192.168.1.1”); 引入“虚拟结点”后： Hash(“192.168.1.1#1”); Hash(“192.168.1.1#2”); 8. 如何判断链表是否有环 方法1：双指针法，p每次走两步，q走一步，有环的话会相交 方法2.设两个工作指针p、q，p总是向前走，但q每次都从头开始走，对于每个节点，看p走的步数是否和q一样。比如p从A走到D，用了4步，而q则用了14步。因而步数不等，出现矛盾，存在环。 9. 熟悉哪些算法？ [哈希算法] 一致性哈希 time33哈希 FNV1_32_HASH [排序算法] 快速排序 [搜索算法] DFS BFS [最小生成树算法] Kruskal Prim [最短路径算法] Dijkstra Floyed 七、计算机网络 1.停止等待协议 停止等待协议是最基本的数据链路层协议，它的工作原理是这样的。 在发送端，每发送完一帧就停止发送，等待接收端的确认，如果收到确认就发送下一帧。 在接收端，每收到一个无差错的帧，就把这个帧交付上层并向发送端发送确认。若该帧有差错，就丢弃，其他什么也不做。 其他细节： 停止等待协议为了可靠交付，需要对帧进行编号，由于每次只发送一帧，所以停止等待协议使用1个比特编号，编号0和1 停止等待协议会出现死锁现象（A等待B的确认），解决办法，启动超时计时器，超时计时器有一个重传时间。重传时间一般选择略大于“正常情况下从发完数据帧到收到确认帧所需的平均时间”。 2.滑动窗口协议 再说滑动窗口之前，先说下连续ARQ，连续ARQ又称Go-back-N ARQ，意思是当出现差错必须重传时，要向回走N个帧，然后再开始重传，也就意味着只要有一帧出现差错，即使已经正确的帧也需要重传，白白浪费时间，增大开销。为此，应该对发送出去但未被确认的帧的数目加以限制，这就是滑动窗口协议。滑动窗口指收发两端分别维护一个发送窗口和接收窗口，发送窗口有一个窗口值Wt，窗口值Wt代表在没有收到对方确认的情况下最多可以发送的帧的数目。当发送的帧的序号被接收窗口正确收下后，接收端向前滑动并向发送端发去确认，发送端收到确认后，发送窗口向前滑动。收发两端按规律向前推进。 连续ARQ是发送窗口大于1接收窗口为1的滑动窗口协议. 选择重传ARQ是发送窗口大于1接收窗口大于1的滑动窗口协议。 而停止等待协议相当于收发两端窗口等于1的滑动窗口协议。 滑动窗口指接收和发送两端的窗口按规律不断向前推进，是一种流量控制的策略。 3.Http1.0和Http1.1的区别 1.HTTP/1.0协议使用非持久连接,即在非持久连接下,一个tcp连接只传输一个Web对象。 2.HTTP/1.1默认使用持久连接(然而,HTTP/1.1协议的客户机和服务器可以配置成使用非持久连接)。在持久连接下,不必为每个Web对象的传送建立一个新的连接,一个连接中可以传输多个对象。 4.Post和Get的区别 1.安全性上说：get的方式是把数据在地址栏中明文的形式发送，URL中可见，POST方式对用户是透明的，安全性更高。 2.数据量说：Get传送的数据量较小，一般不能大于2KB，POST传送的数据量更大。 3.适用范围说：查询用Get，数据添加、修改和删除建议Post 5.TCP/IP体系各层功能及协议 TCP/IP体系共有四个层次，分别为网络接口层Host-to-Network Layer, 网际层 Internet Layer， 传输层Transport Layer，应用层Application Layer。 5.1 网络接口层 -> 接收和发送数据报 主要负责将数据发送到网络传输介质上以及从网络上接收TCP/IP数据报，相当于OSI参考模型的物理层和数据链路层。在实际中，先后流行的以太网、令牌环网、ATM、帧中继等都可视为其底层协议。它将发送的信息组装成帧并通过物理层向选定网络发送，或者从网络上接收物理帧，将去除控制信息后的IP数据报交给网络层。 5.2 网际层 -> 数据报封装和路由寻址 网际层主要功能是寻址和对数据报的封装以及路由选择功能。这些功能大部分通过IP协议完成，并通过地址解析协议ARP、逆地址解析协议RARP、因特网控制报文协议ICMP、因特网组管理协议IGMP从旁协助。所以IP协议是网络层的核心。 网际协议IP：IP协议是一个无连接的协议，主要负责将数据报从源结点转发到目的结点。也就是说IP协议通过对数据报中源地址和目的地址进行分析，然后进行路由选择，最后再转发到目的地。需要注意的是：IP协议只负责对数据进行转发，并不对数据进行检查，也就是说，它不负责数据的可靠性，这样设计的主要目的是提高IP协议传送和转发数据的效率。 ARP：该协议负责将IP地址解析转换为计算机的物理地址。 虽然我们使用IP地址进行通信，但IP地址只是主机在抽象的网络层中的地址。最终要传到数据链路层封装成MAC帧才能发送到实际的网络。因此不管使用什么协议最终需要的还是硬件地址。 每个主机拥有一个ARP高速缓存（存放所在局域网内主机和路由器的IP地址到硬件地址的映射表） 举例：A发送B (1)A在自己的ARP高速缓存中查到B的MAC地址，写入MAC帧发往此B (2)没查到，A向本局域网广播ARP请求分组，内容包括自己的地址映射和B的IP地址 (3)B发送ARP响应分组，内容为自己的IP地址到物理地址的映射，同时将A的映射写入自己的ARP高速缓存（单播的方式） 注：ARP Cache映射项目具有一个生存时间。 RARP：将计算机物理地址转换为IP地址 ICMP：该协议主要负责发送和传递包含控制信息的数据报，这些控制信息包括了哪台计算机出现了什么错误，网络路由出现了什么错误等内容。 5.3 传输层 -> 应用进程间端到端的通信 传输层主要负责应用进程间“端到端”的通信，即从某个应用进程传输到另一个应用进程，它与OSI参考模型的传输层功能类似。 传输层在某个时刻可能要同时为多个不同的应用进程服务，因此传输层在每个分组中必须增加用于识别应用进程的标识，即端口。 TCP/IP体系的传输层主要包含两个主要协议，即传输控制协议TCP和用户数据报协议UDP。TCP协议是一种可靠的、面向连接的协议，保证收发两端有可靠的字节流传输，进行了流量控制，协调双方的发送和接收速度，达到正确传输的目的。 UDP是一种不可靠的、无连接的协议，其特点是协议简单、额外开销小、效率较高，不能保证可靠传输。 传输层提供应用进程间的逻辑通信。它使应用进程看见的就好像是在两个运输层实体间一条端到端的逻辑通信信道。 当运输层采用TCP时，尽管下面的网络是不可靠的，但这种逻辑通信信道相当于一条全双工的可靠信道。可以做到报文的无差错、按序、无丢失、无重复。 注：单单面向连接只是可靠的必要条件，不充分。还需要其他措施，如确认重传，按序接收，无丢失无重复。 熟知端口： 20 FTP数据连接 21 FTP控制连接 22 SSH 23 TELNET 25 SMTP 53 DNS 69 TFTP 80 HTTP 161 SNMP UDP重要 UDP的优点： 1.发送之前无需建立连接，减小了开销和发送数据的时延 2.UDP不使用连接，不使用可靠交付，因此主机不需要维护复杂的参数表、连接状态表 3.UDP用户数据报只有8个字节的首部开销，而TCP要20字节。 4.由于没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（IP电话等实时应用要求源主机以恒定的速率发送数据是有利的） Table，使用TCP和UDP的应用 应用 应用层协议 运输层协议 名字转换 DNS UDP 文件传送 TFTP UDP 路由选择协议 RIP UDP IP地址配置 BOOTTP,DHCP UDP 网络管理 SNMP UDP 远程文件服务器 NFS UDP IP电话 专用协议 UDP 流式多媒体通信 专用协议 UDP 电子邮件 SMTP TCP 远程终端接入 TELNET TCP 万维网 HTTP TCP 文件传送 FTP TCP 注：TFTP：Trivial File Transfer Protocol UDP的过程（以TFTP举例）： 1.服务器进程运行着，等待TFTP客户进程的服务请求。客户端TFTP进程启动时，向操作系统申请一个临时端口号，然后操作系统为该进程创建2个队列， 入队列和出队列。只要进程在执行，2个队列一直存在。 2.客户进程将报文发送到出队列中。UDP按报文在队列的先后顺序发送。在传送到IP层之前给报文加上UDP首部，其中目的端口后为69，然后发给IP层。 出队列若溢出，则操作系统通知应用层TFTP客户进程暂停发送。 3.客户端收到来自IP层的报文时，UDP检查报文中目的端口号是否正确，若正确，放入入队列队尾，客户进程按先后顺序一一取走。若不正确，UDP丢弃该报文，并请ICMP发送”端口不可达“差错报文给服务器端。入队列可能会溢出，若溢出，UDP丢弃该报文，不通知对方。 服务器端类似。 UDP首部：源端口 - 目的端口 - 长度 - 检验和，每个字段22字节。 注：IP数据报检验和只检验IP数据报的首部，而UDP的检验和将首部和数据部分一起都检验。 TCP重要 细节： TCP报文段是面向字节的数据流。 TCP首部：20字节固定首部 确认比特ACK，ACK=1 确认号字段才有效；同步比特SYN：SYN=1 ACK=0表示一个连接请求报文段；终止比特FIN，FIN=1时要求释放连接。 窗口：将TCP收发两端记为A和B，A根据TCP缓存空间的大小确定自己的接收窗口大小。并在A发送给B的窗口字段写入该值。作为B的发送窗口的上限。意味着B在未收到A的确认情况下，最多发送的字节数。 选项：最大报文段长度MSS，MSS告诉对方TCP：我的缓存所能接收的报文段的数据字段的最大长度是MSS个字节。若主机未填写，默认为536字节。 TCP的可靠是使用了序号和确认。当TCP发送一个报文时，在自己的重传队列中存放一个副本。若收到确认，删除副本。 TCP使用捎带确认。 TCP报文段的发送时机：1.维持一个变量等于MSS，发送缓存达到MSS就发送 2.发送端应用进程指明要发送，即TCP支持的PUSH操作。3.设定计时器 TCP的拥塞控制：TCP使用慢开始和拥塞避免算法进行拥塞控制 慢开始和拥塞避免 接收端根据自身资源情况控制发送端发送窗口的大小。 每个TCP连接需要维持一下2个状态变量： 接收端窗口rwnd（receiver window）：接收端根据目前接收缓存大小设置的窗口值，是来自接收端的流量控制 拥塞窗口cwnd（congestion window）：是发送端根据自己估计的网络拥塞程度设置的窗口值，是来自发送端的流量控制 发送端的窗口上限值=Min(rwnd, cwnd) 慢开始算法原理：主机刚开始发送数据时，如果立即将较大的发送窗口的全部字节注入网络，由于不清楚网络状况，可能会引起拥塞。通常的做法是将cwnd设置为1个MSS，每收到一个确认，将cwnd+1，由小到大逐步增大cwnd，使分组注入网络的速率更加合理。为了防止拥塞窗口增长引起网络拥塞，还需设置一个状态变量ssthresh，即慢开始门限。 慢开始门限：ssthresh，当cwnd ssthresh，改用拥塞避免算法。 cwnd = ssthresh时，都可以。 拥塞避免算法使发送端的拥塞窗口每经过一个RTT增加一个MSS（而不管在此期间收到多少ACK），这样，拥塞窗口cwnd按线性规律增长，拥塞窗口此时比慢开始增长速率缓慢很多。这一过程称为加法增大，目的在于使拥塞窗口缓慢增长，防止网络过早拥塞。 无论是慢开始还是拥塞避免，只要发送端发现网络出现拥塞（根据是没有按时收到ACK或者收到重复ACK），就将慢开始门限ssthresh设置为拥塞窗口值的一半并将拥塞窗口cwnd置为1，重新执行慢开始算法。这一过程称为乘法减小。目的在于迅速减少主机发送到网络中的分组数，使得发生拥塞的路由器有足够时间把队列中积压的分组处理完毕。 上述TCP确认都是通过捎带确认执行的。 快重传和快恢复 上述的慢开始和拥塞避免算法是早期TCP使用的拥塞控制算法。因为有时TCP连接会在重传时因等待重传计时器的超时时间而空闲。为此在快重传中规定：只要发送端一连收到三个重复的ACK,即可断定分组丢失，不必等待重传计数器，立即重传丢失的报文。 与快重传搭配使用的还有快恢复：当不使用快恢复时，发送端若发现网络拥塞就将拥塞窗口降为1，然后执行慢开始算法，这样的缺点是网络不能很快恢复到正常状态。快恢复是指当发送端收到3个重复的ACK时，执行乘法减小，ssthresh变为拥塞窗口值的一半。但是cwnd不是置为1，而是ssthresh+3xMSS。若收到的重复ACK 为n(n > 3)，则cwnd=ssthresh+n*MSS.这样做的理由是基于发送端已经收到3个重复的ACK，它表明已经有3个分组离开了网络，它们不在消耗网络的资源。 注意的是：在使用快恢复算法时，慢开始算法只在TCP连接建立时使用。 TCP的重传机制 每发送一个报文段，就对这个报文段设置一次计时器。新的重传时间=γ*旧的重传时间。 TCP连接建立和释放的过程 SYN置1和FIN的报文段要消耗一个序号。 客户端连接状态变迁：CLOSED -> 主动打开,发送SYN=1 -> SYN_SENT -> 收到服务器的SYN=1和ACK时,发送三次握手的最后一个ACK -> ESTABLISHED -> 数据传送 -> 主动关闭 -> 发送FIN=1,等待确认ACK的到达 -> FIN_WAIT_1 -> 收到确认ACK后 -> FIN_WAIT_2 -> 收到服务器发送的FIN=1报文，响应，发送四次挥手的的最后一个确认ACK -> 进入TIME_WAIT状态 -> 经过2倍报文寿命，TCP删除连接记录 -> 回到CLOSED状态 客户端状态：CLOSED - SYN_SENT- ESTABLISHED - FIN_WAIT_1 - FIN_WAIT_2 - TIME_WAIT - CLOSED 服务器端连接状态变迁：CLOSED -> 被动打开 -> LISTEN -> 收到SYN=1的报文，发送SYN=1和确认ACK -> 进入SYN_RCVD -> 收到三次握手 的最后一个确认ACK -> ESTABLISHED -> 数据传送 -> 数据传送完毕，收到FIN=1 -> 发送确认ACK并进入CLOSED_WAIT -> 发送FIN=1给客户端 -> LAST_ACK -> 收到客户端四次挥手的最后一个确认ACK -> 删除连接记录 -> 回到CLOSED状态 服务器端：CLOSED - LISTEN - SYN_RCVD - ESTABLISHED - CLOSED_WAIT - LAST_ACK - CLOSED 5.4 应用层 应用层位于TCP/IP体系结构的最高一层，也是直接为应用进程服务的一层，即当不同的应用进程数据交换时，就去调用应用层的不同协议实体，让这些实体去调用传输层的TCP或者UDP来进行网络传输。具体的应用层协议有，SMTP 25、DNS 53、HTTP 80、FTP 20数据端口 21控制端口、TFTP 69、TELNET 23、SNMP 161等 5.5 网络的划分 按网络拓扑结构：总线、星型、环型、树型、网状结构和混合型。 按覆盖范围：局域网、城域网、广域网 按传播方式：广播网络和点对点网络 广播式网络是指网络中的计算机使用一个共享信道进行数据传播，网络中的所有结点都能收到某一结点发出的数据信息。 单播：一对一的发送形式。 组播：采用一对一组的发送形式，将数据发送给网络中的某一组主机。 广播：采用一对所有，将数据发送给网络所有目的结点。 点对点网络中两个结点间的通信方式是点对点的。如果两台计算机之间没有直连的线路，则需要中间结点的接收、存储、转发直至目的结点。 6. TCP的三次握手和四次挥手的过程 以客户端为例 连接建立（三次握手）：首先Client端发送连接请求报文SYN并进入SYN_SENT状态，Server收到后发送ACK+SYN报文，并为这次连接分配资源。Client端接收到Server端的SYN+ACK后发送三次握手的最后一个ACK，并分配资源，连接建立。 连接释放（四次挥手）：假设Client端发起断开连接请求，首先发送FIN=1,等待确认ACK的到达 -> FIN_WAIT_1 -> 收到Server端的确认ACK后时 -> FIN_WAIT_2 ->收到服务器发送的FIN=1报文，响应，发送四次挥手的的最后一个确认ACK ->进入TIME_WAIT状态 -> 经过2倍报文寿命，TCP删除连接记录 -> 回到CLOSED状态 7. 为什么连接建立是三次握手，而连接释放要四次挥手？ 因为当Server端收到Client端发送的SYN连接请求报文后，可以直接发送SYN+ACK报文，其中ACK用来应答，SYN用来同步。但是关闭连接时，当Server端收到FIN报文后，并不会立即关闭socket，所以先回复一个ACK，告诉Client端“你的FIN我收到了”，只有等Server端的所有报文发送完了，Server端才发送FIN报文，因此不能一起发送，故需要四次挥手。 8. 为什么TIME_WAIT状态需要2MSL（最大报文段生存时间）才能返回Closed状态？ 这是因为虽然双方都同意关闭连接了，而且四次挥手的报文也都协调发送完毕。但是我们必须假想网络是不可靠的，无法保证最后发送的ACK报文一定被对方收到，因此处于LAST_ACK状态下的 Server端可能会因未收到ACK而重发FIN，所以TIME_WAIT状态的作用就是用来重发可能丢失的ACK报文。 9. Http报文格式 Http请求报文格式：1.请求行 2.Http头 3.报文主体 请求行由三部分组成，分别是请求方法，请求地址，Http版本 Http头：有三种，分别为请求头（request header），普通头（General Header）和实体头（entity header）。Get方法没有实体头。 报文主体：只在POST方法请求中存在。 Http响应报文：1.状态行 2.Http头 3.返回内容 状态行：第一部分为Http版本，第二部分为响应状态码 第三部分为状态码的描述 其中第三部分为状态码的描述，信息类100-199 响应成功200-299 重定向类300-399 客户端错误400-499 服务器端错误500-599 常见的 100 continue 初始请求已接受，客户端应继续发送请求剩余部分 200 OK 202 Accepted 已接受，处理尚未完成 301 永久重定向 302 临时重定向 400 Bad Request 401 Unauthorized 403 Forbidden 资源不可用 404 Not Found 500 Internal Server Error 服务器错误 502 Bad Gateway 503 Service Unavailable 服务器负载过重 504 Gateway Timeout 未能及时从远程服务器获得应答 Http头：响应头（Response Header），普通头（General Header）和实体头(Entity Header) 返回内容：即Http请求的信息，可以是HTML也可以是图片等等。 10. Http和Https的区别 Https即Secure Hypertext Transfer Protocol，即安全超文本传输协议，它是一个安全通信信道，基于Http开发，用于在客户机和服务器间交换信息。它使用安全套接字层SSL进行信息交换，是Http的安全版。 Https协议需要到CA申请证书，一般免费证书很少，需要交费。 Http是超文本传输协议，信息是明文传输，https则是具有安全性的tls/ssl加密传输协议。 http是80端口，https是443端口 11. 浏览器输入一个URL的过程 浏览器向DNS服务器请求解析该URL中的域名所对应的IP地址 解析出IP地址后，根据IP地址和默认端口80和服务器建立TCP连接 浏览器发出Http请求，该请求报文作为TCP三次握手的第三个报文的数据发送给服务器 服务器做出响应，把对应的请求资源发送给浏览器 释放TCP连接 浏览器解析并显示内容 12. 中间人攻击 中间人获取server发给client的公钥，自己伪造一对公私钥，然后伪造自己让client以为它是server，然后将伪造的公钥发给client，并拦截client发给server的密文，用伪造的私钥即可得到client发出去的内容，最后用真实的公钥对内容加密发给server。 解决办法：数字证书，证书链，可信任的中间人 13. 差错检测 误码率：传输错误的比特与传输总比特数的比率 CRC是检错方法并不能纠错，FCS（Frame Check Sequence）是冗余码。 计算冗余码（余数R）的方法：先补0（n个）再对生成多项式取模。 CRC只能表示以接近1的概率认为它没有差错。但不能做到可靠传输。可靠传输还需要确认和重传机制。 生成多项式P(X)：CRC-16，CRC-CCITT，CRC-32 14. 数据链路层的协议 停止等待协议 - 连续ARQ - 选择重传ARQ - PPP - 以太网协议- 帧中继 - ATM - HDLC 15. 截断二进制指数退避算法 是以太网用于解决当发生碰撞时就停止发送然后重发再碰撞这一问题。 截断二进制指数退避算法：基本退避时间为2τ k=min{重传次数，10} r=random(0~2^k-1) 重传所需时延为r倍的基本退避时间 八、操作系统（OS基础、Linux等） 1. 并发和并行 “并行”是指无论从微观还是宏观，二者都是一起执行的，也就是同一时刻执行 而“并发”在微观上不是同时执行的。是在同一时间间隔交替轮流执行 2. 进程间通信的方式 管道( pipe )：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。 有名管道 (named pipe) ： 有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。 信号量( semophore ) ：信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。 消息队列( message queue ) 消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。 信号 ( sinal ) ： 信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。 共享内存( shared memory ) 共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号量配合使用，来实现进程间的同步和通信。 套接字( socket ) ：套接字也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同机器间的进程通信。 3. LinuxIO模型 1、阻塞IO模型 以socket为例，在进程空间调用recvfrom，其系统调用知道数据包到达且被复制到应用进程的缓冲区或者发生错误才返回，在此期间一直等待，进程从调用recvfrom开始到它返回的整段时间内都是被阻塞的，因此称为阻塞IO 2、非阻塞IO模型 应用进程调用recvfrom，如果缓冲区没有数据直接返回EWOULDBLOCK错误。一般对非阻塞IO进行轮询，以确定是否有数据到来。 3、IO多路复用模型 Linux提供select/poll，通过将一个或多个fd传递给select或poll系统调用，阻塞在select上。select/poll顺序扫描fd是否就绪。 4、信号驱动IO 开启套接字接口信号驱动IO功能，并通过系统调用sigaction执行信号处理函数。当数据准备就绪时，为该进程生成SIGIO信号，通过信号回调通知应用程序调用recvfrom来读取数据，并通知主函数处理数据。 5、异步IO 告知内核启动某个操作，并让内核在整个操作完成后通知我们。它与信号驱动IO的区别在于信号驱动IO由内核通知我们何时可以开始IO操作。而异步IO模型由内核通知我们IO操作已经完成。 九、其他 1. 开源软件有哪些？ Eclipse、Linux及其Linux下的大多数软件、Git等。 Apache下的众多软件：Lucene、Velocity、Maven、高性能Java网络框架MINA、版本控制系统SVN、应用服务器Tomcat、Http服务器Apache、MVC框架Struts、持久层框架iBATIS、Apache SPARK、ActiveMQ 2. 开源协议 MIT：相对宽松。适用：JQuery Apache：相对宽松与MIT类似的协议，考虑有专利的情况。适用：Apache服务器、SVN GPL：GPLV2和GPLV3，如果你在乎作品的传播和别人的修改，希望别人也以相同的协议分享出来。 LGPL：主要用于一些代码库。衍生代码可以以此协议发布（言下之意你可以用其他协议），但与此协议相关的代码必需遵循此协议。 BSD：较为宽松的协议，包含两个变种BSD 2-Clause 和BSD 3-Clause，两者都与MIT协议只存在细微差异。 上面各协议只是针对软件或代码作品，如果你的作品不是代码，比如视频，音乐，图片，文章等，共享于公众之前，也最好声明一下协议以保证自己的权益不被侵犯，CC协议。 致谢和参考文献 深入理解Java虚拟机 最近5年133个Java面试问题列表 联系我 WebSite:http://rannn.cc Mail: xmusaber@163.com 作者 Rannn Tao "},"zother4-EasyJob/Tools/idea.html":{"url":"zother4-EasyJob/Tools/idea.html","title":"Idea","keywords":"","body":"IDEA快捷键 搜索 ctrl + N 搜索类 ctrl + shift + N 搜索文件 ctrl + f 当前文件查找特定内容 ctrl + shift + f 当前项目查找包含特定内容的文件 double shift: search everywhere 编辑 shift + enter 另起一行 ctrl + r当前文件替换特定内容 ctrl + shift + r当前项目替换特定内容 shift + f6 非常非常省心省力的一个快捷键，可以重命名你的类、方法、变量等等，而且这个重命名甚至可以选择替换掉注释中的内容 ctrl + d复制当前行到下一行 ctrl + x剪切当前行 ctrl + c \\ ctrl+v 大家都懂的 ctrl + z撤销 ctrl + shift + z取消撤销 alt + enter 重构代码 ctrl + alt + L 格式化 alt + insert 插入 ctrl + shift + down/up 选中代码块移动 ctrl + alt + t try块 协作 ctrl + k提交代码到SVN ctrl + t更新代码 其他 ctrl + B 跳转定义处 ctrl + alt + B 跳转实现 ctrl + O 覆盖方法 ctrl + I 实现方法 alt + 1 project视图 ctrl + e 最近文件 alt + 7 显示所有方法 alt + f12 系统终端 ctrl + H 查看继承情况，查看类层次 ctrl + f4 关闭当前文件 ctrl + f12 当前类的方法 Ctrl + Tab：切来切去 Ctrl + W ：选你所想 alt + f7：找类或方法的使用 alt + left/right：切换 ctrl + alt + left/right：切换上次编辑 ctrl + alt + t 代码生成（如try catch） 运行调试 F7/F8/F9分别对应Step into，Step over，Continue alt + shift + f10运行程序 ctrl + shift + t 单元测试 shift + f10 运行tomcat shift + f9 启动调试 ctrl + f2 停止 自定义 垂直分屏 ctrl + num1 水平分屏ctrl + num2 联系我 邮箱: xmusaber@163.com "},"zother4-EasyJob/Tools/占用端口.html":{"url":"zother4-EasyJob/Tools/占用端口.html","title":"占用端口","keywords":"","body":"解决占用端口 1、netstat -ano |findstr 端口号 2、tasklist|findstr 进程号 3、taskkill -f -t -im 文件名 "},"zother6-JavaGuide/database/Redis/Redis.html":{"url":"zother6-JavaGuide/database/Redis/Redis.html","title":"Redis","keywords":"","body":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。 redis 简介 为什么要用 redis/为什么要用缓存 为什么要用 redis 而不用 map/guava 做缓存? redis 和 memcached 的区别 redis 常见数据结构以及使用场景分析 1.String 2.Hash 3.List 4.Set 5.Sorted Set redis 设置过期时间 redis 内存淘汰机制(MySQL里有2000w数据，Redis中只存20w的数据，如何保证Redis中的数据都是热点数据?) redis 持久化机制(怎么保证 redis 挂掉之后再重启数据可以进行恢复) redis 事务 缓存雪崩和缓存穿透问题解决方案 如何解决 Redis 的并发竞争 Key 问题 如何保证缓存与数据库双写时的数据一致性? redis 简介 简单来说 redis 就是一个数据库，不过与传统数据库不同的是 redis 的数据是存在内存中的，所以读写速度非常快，因此 redis 被广泛应用于缓存方向。另外，redis 也经常用来做分布式锁。redis 提供了多种数据类型来支持不同的业务场景。除此之外，redis 支持事务 、持久化、LUA脚本、LRU驱动事件、多种集群方案。 为什么要用 redis/为什么要用缓存 主要从“高性能”和“高并发”这两点来看待这个问题。 高性能： 假如用户第一次访问数据库中的某些数据。这个过程会比较慢，因为是从硬盘上读取的。将该用户访问的数据存在缓存中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可！ 高并发： 直接操作缓存能够承受的请求是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。 为什么要用 redis 而不用 map/guava 做缓存? 下面的内容来自 segmentfault 一位网友的提问，地址：https://segmentfault.com/q/1010000009106416 缓存分为本地缓存和分布式缓存。以 Java 为例，使用自带的 map 或者 guava 实现的是本地缓存，最主要的特点是轻量以及快速，生命周期随着 jvm 的销毁而结束，并且在多实例的情况下，每个实例都需要各自保存一份缓存，缓存不具有一致性。 使用 redis 或 memcached 之类的称为分布式缓存，在多实例的情况下，各实例共用一份缓存数据，缓存具有一致性。缺点是需要保持 redis 或 memcached服务的高可用，整个程序架构上较为复杂。 redis 的线程模型 参考地址:https://www.javazhiyin.com/22943.html redis 内部使用文件事件处理器 file event handler，这个文件事件处理器是单线程的，所以 redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 socket，根据 socket 上的事件来选择对应的事件处理器进行处理。 文件事件处理器的结构包含 4 个部分： 多个 socket IO 多路复用程序 文件事件分派器 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器） 多个 socket 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 socket，会将 socket 产生的事件放入队列中排队，事件分派器每次从队列中取出一个事件，把该事件交给对应的事件处理器进行处理。 redis 和 memcached 的区别 对于 redis 和 memcached 我总结了下面四点。现在公司一般都是用 redis 来实现缓存，而且 redis 自身也越来越强大了！ redis支持更丰富的数据类型（支持更复杂的应用场景）：Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。memcache支持简单的数据类型，String。 Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而Memecache把数据全部存在内存之中。 集群模式：memcached没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；但是 redis 目前是原生支持 cluster 模式的. Memcached是多线程，非阻塞IO复用的网络模型；Redis使用单线程的多路 IO 复用模型。 来自网络上的一张图，这里分享给大家！ redis 常见数据结构以及使用场景分析 1.String 常用命令: set,get,decr,incr,mget 等。 String数据结构是简单的key-value类型，value其实不仅可以是String，也可以是数字。 常规key-value缓存应用； 常规计数：微博数，粉丝数等。 2.Hash 常用命令： hget,hset,hgetall 等。 hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象，后续操作的时候，你可以直接仅仅修改这个对象中的某个字段的值。 比如我们可以 hash 数据结构来存储用户信息，商品信息等等。比如下面我就用 hash 类型存放了我本人的一些信息： key=JavaUser293847 value={ “id”: 1, “name”: “SnailClimb”, “age”: 22, “location”: “Wuhan, Hubei” } 3.List 常用命令: lpush,rpush,lpop,rpop,lrange等 list 就是链表，Redis list 的应用场景非常多，也是Redis最重要的数据结构之一，比如微博的关注列表，粉丝列表，消息列表等功能都可以用Redis的 list 结构来实现。 Redis list 的实现为一个双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销。 另外可以通过 lrange 命令，就是从某个元素开始读取多少个元素，可以基于 list 实现分页查询，这个很棒的一个功能，基于 redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西（一页一页的往下走），性能高。 4.Set 常用命令： sadd,spop,smembers,sunion 等 set 对外提供的功能与list类似是一个列表的功能，特殊之处在于 set 是可以自动排重的。 当你需要存储一个列表数据，又不希望出现重复数据时，set是一个很好的选择，并且set提供了判断某个成员是否在一个set集合内的重要接口，这个也是list所不能提供的。可以基于 set 轻易实现交集、并集、差集的操作。 比如：在微博应用中，可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。Redis可以非常方便的实现如共同关注、共同粉丝、共同喜好等功能。这个过程也就是求交集的过程，具体命令如下： sinterstore key1 key2 key3 将交集存在key1内 5.Sorted Set 常用命令： zadd,zrange,zrem,zcard等 和set相比，sorted set增加了一个权重参数score，使得集合中的元素能够按score进行有序排列。 举例： 在直播系统中，实时排行信息包含直播间在线用户列表，各种礼物排行榜，弹幕消息（可以理解为按消息维度的消息排行榜）等信息，适合使用 Redis 中的 Sorted Set 结构进行存储。 redis 设置过期时间 Redis中有个设置时间过期的功能，即对存储在 redis 数据库中的值可以设置一个过期时间。作为一个缓存数据库，这是非常实用的。如我们一般项目中的 token 或者一些登录信息，尤其是短信验证码都是有时间限制的，按照传统的数据库处理方式，一般都是自己判断过期，这样无疑会严重影响项目性能。 我们 set key 的时候，都可以给一个 expire time，就是过期时间，通过过期时间我们可以指定这个 key 可以存活的时间。 如果假设你设置了一批 key 只能存活1个小时，那么接下来1小时后，redis是怎么对这批key进行删除的？ 定期删除+惰性删除。 通过名字大概就能猜出这两个删除方式的意思了。 定期删除：redis默认是每隔 100ms 就随机抽取一些设置了过期时间的key，检查其是否过期，如果过期就删除。注意这里是随机抽取的。为什么要随机呢？你想一想假如 redis 存了几十万个 key ，每隔100ms就遍历所有的设置过期时间的 key 的话，就会给 CPU 带来很大的负载！ 惰性删除 ：定期删除可能会导致很多过期 key 到了时间并没有被删除掉。所以就有了惰性删除。假如你的过期 key，靠定期删除没有被删除掉，还停留在内存里，除非你的系统去查一下那个 key，才会被redis给删除掉。这就是所谓的惰性删除，也是够懒的哈！ 但是仅仅通过设置过期时间还是有问题的。我们想一下：如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期key堆积在内存里，导致redis内存块耗尽了。怎么解决这个问题呢？ redis 内存淘汰机制。 redis 内存淘汰机制(MySQL里有2000w数据，Redis中只存20w的数据，如何保证Redis中的数据都是热点数据?) redis 配置文件 redis.conf 中有相关注释，我这里就不贴了，大家可以自行查阅或者通过这个网址查看： http://download.redis.io/redis-stable/redis.conf redis 提供 6种数据淘汰策略： volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key（这个是最常用的） allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-eviction：禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。这个应该没人使用吧！ 4.0版本后增加以下两种： volatile-lfu：从已设置过期时间的数据集(server.db[i].expires)中挑选最不经常使用的数据淘汰 allkeys-lfu：当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的key 备注： 关于 redis 设置过期时间以及内存淘汰机制，我这里只是简单的总结一下，后面会专门写一篇文章来总结！ redis 持久化机制(怎么保证 redis 挂掉之后再重启数据可以进行恢复) 很多时候我们需要持久化数据也就是将内存中的数据写入到硬盘里面，大部分原因是为了之后重用数据（比如重启机器、机器故障之后恢复数据），或者是为了防止系统故障而将数据备份到一个远程位置。 Redis不同于Memcached的很重一点就是，Redis支持持久化，而且支持两种不同的持久化操作。Redis的一种持久化方式叫快照（snapshotting，RDB），另一种方式是只追加文件（append-only file,AOF）。这两种方法各有千秋，下面我会详细这两种持久化方法是什么，怎么用，如何选择适合自己的持久化方法。 快照（snapshotting）持久化（RDB） Redis可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。Redis创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis主从结构，主要用来提高Redis性能），还可以将快照留在原地以便重启服务器的时候使用。 快照持久化是Redis默认采用的持久化方式，在redis.conf配置文件中默认有此下配置： save 900 1 #在900秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 save 300 10 #在300秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 save 60 10000 #在60秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 AOF（append-only file）持久化 与快照持久化相比，AOF持久化 的实时性更好，因此已成为主流的持久化方案。默认情况下Redis没有开启AOF（append only file）方式的持久化，可以通过appendonly参数开启： appendonly yes 开启AOF持久化后每执行一条会更改Redis中的数据的命令，Redis就会将该命令写入硬盘中的AOF文件。AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的，默认的文件名是appendonly.aof。 在Redis的配置文件中存在三种不同的 AOF 持久化方式，它们分别是： appendfsync always #每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度 appendfsync everysec #每秒钟同步一次，显示地将多个写命令同步到硬盘 appendfsync no #让操作系统决定何时进行同步 为了兼顾数据和写入性能，用户可以考虑 appendfsync everysec选项 ，让Redis每秒同步一次AOF文件，Redis性能几乎没受到任何影响。而且这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操作的时候，Redis还会优雅的放慢自己的速度以便适应硬盘的最大写入速度。 Redis 4.0 对于持久化机制的优化 Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。 如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分是压缩格式不再是 AOF 格式，可读性较差。 补充内容：AOF 重写 AOF重写可以产生一个新的AOF文件，这个新的AOF文件和原有的AOF文件所保存的数据库状态一样，但体积更小。 AOF重写是一个有歧义的名字，该功能是通过读取数据库中的键值对来实现的，程序无须对现有AOF文件进行任何读入、分析或者写入操作。 在执行 BGREWRITEAOF 命令时，Redis 服务器会维护一个 AOF 重写缓冲区，该缓冲区会在子进程创建新AOF文件期间，记录服务器执行的所有写命令。当子进程完成创建新AOF文件的工作之后，服务器会将重写缓冲区中的所有内容追加到新AOF文件的末尾，使得新旧两个AOF文件所保存的数据库状态一致。最后，服务器用新的AOF文件替换旧的AOF文件，以此来完成AOF文件重写操作 更多内容可以查看我的这篇文章： Redis持久化 redis 事务 Redis 通过 MULTI、EXEC、WATCH 等命令来实现事务(transaction)功能。事务提供了一种将多个命令请求打包，然后一次性、按顺序地执行多个命令的机制，并且在事务执行期间，服务器不会中断事务而改去执行其他客户端的命令请求，它会将事务中的所有命令都执行完毕，然后才去处理其他客户端的命令请求。 在传统的关系式数据库中，常常用 ACID 性质来检验事务功能的可靠性和安全性。在 Redis 中，事务总是具有原子性（Atomicity）、一致性（Consistency）和隔离性（Isolation），并且当 Redis 运行在某种特定的持久化模式下时，事务也具有持久性（Durability）。 补充内容： redis同一个事务中如果有一条命令执行失败，其后的命令仍然会被执行，没有回滚。（来自issue:关于Redis事务不是原子性问题 ） 缓存雪崩和缓存穿透问题解决方案 缓存雪崩 什么是缓存雪崩？ 简介：缓存同一时间大面积的失效，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉。 有哪些解决办法？ （中华石杉老师在他的视频中提到过，视频地址在最后一个问题中有提到）： 事前：尽量保证整个 redis 集群的高可用性，发现机器宕机尽快补上。选择合适的内存淘汰策略。 事中：本地ehcache缓存 + hystrix限流&降级，避免MySQL崩掉 事后：利用 redis 持久化机制保存的数据尽快恢复缓存 缓存穿透 什么是缓存穿透？ 缓存穿透说简单点就是大量请求的 key 根本不存在于缓存中，导致请求直接到了数据库上，根本没有经过缓存这一层。举个例子：某个黑客故意制造我们缓存中不存在的 key 发起大量请求，导致大量请求落到数据库。下面用图片展示一下(这两张图片不是我画的，为了省事直接在网上找的，这里说明一下)： 正常缓存处理流程： 缓存穿透情况处理流程： 一般MySQL 默认的最大连接数在 150 左右，这个可以通过 show variables like '%max_connections%';命令来查看。最大连接数一个还只是一个指标，cpu，内存，磁盘，网络等无力条件都是其运行指标，这些指标都会限制其并发能力！所以，一般 3000 个并发请求就能打死大部分数据库了。 有哪些解决办法？ 最基本的就是首先做好参数校验，一些不合法的参数请求直接抛出异常信息返回给客户端。比如查询的数据库 id 不能小于 0、传入的邮箱格式不对的时候直接返回错误消息给客户端等等。 1）缓存无效 key : 如果缓存和数据库都查不到某个 key 的数据就写一个到 redis 中去并设置过期时间，具体命令如下：SET key value EX 10086。这种方式可以解决请求的 key 变化不频繁的情况，如果黑客恶意攻击，每次构建不同的请求key，会导致 redis 中缓存大量无效的 key 。很明显，这种方案并不能从根本上解决此问题。如果非要用这种方式来解决穿透问题的话，尽量将无效的 key 的过期时间设置短一点比如 1 分钟。 另外，这里多说一嘴，一般情况下我们是这样设计 key 的： 表名:列名:主键名:主键值。 如果用 Java 代码展示的话，差不多是下面这样的： public Object getObjectInclNullById(Integer id) { // 从缓存中获取数据 Object cacheValue = cache.get(id); // 缓存为空 if (cacheValue == null) { // 从数据库中获取 Object storageValue = storage.get(key); // 缓存空对象 cache.set(key, storageValue); // 如果存储数据为空，需要设置一个过期时间(300秒) if (storageValue == null) { // 必须设置过期时间，否则有被攻击的风险 cache.expire(key, 60 * 5); } return storageValue; } return cacheValue; } 2）布隆过滤器：布隆过滤器是一个非常神奇的数据结构，通过它我们可以非常方便地判断一个给定数据是否存在与海量数据中。我们需要的就是判断 key 是否合法，有没有感觉布隆过滤器就是我们想要找的那个“人”。具体是这样做的：把所有可能存在的请求的值都存放在布隆过滤器中，当用户请求过来，我会先判断用户发来的请求的值是否存在于布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端，存在的话才会走下面的流程。总结一下就是下面这张图(这张图片不是我画的，为了省事直接在网上找的)： 更多关于布隆过滤器的内容可以看我的这篇原创：《不了解布隆过滤器？一文给你整的明明白白！》 ，强烈推荐，个人感觉网上应该找不到总结的这么明明白白的文章了。 如何解决 Redis 的并发竞争 Key 问题 所谓 Redis 的并发竞争 Key 的问题也就是多个系统同时对一个 key 进行操作，但是最后执行的顺序和我们期望的顺序不同，这样也就导致了结果的不同！ 推荐一种方案：分布式锁（zookeeper 和 redis 都可以实现分布式锁）。（如果不存在 Redis 的并发竞争 Key 问题，不要使用分布式锁，这样会影响性能） 基于zookeeper临时有序节点可以实现的分布式锁。大致思想为：每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。完成业务流程后，删除对应的子节点释放锁。 在实践中，当然是从以可靠性为主。所以首推Zookeeper。 参考： https://www.jianshu.com/p/8bddd381de06 如何保证缓存与数据库双写时的数据一致性? 一般情况下我们都是这样使用缓存的：先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。这种方式很明显会存在缓存和数据库的数据不一致的情况。 你只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？ 一般来说，就是如果你的系统不是严格要求缓存+数据库必须一致性的话，缓存可以稍微的跟数据库偶尔有不一致的情况，最好不要做这个方案，读请求和写请求串行化，串到一个内存队列里去，这样就可以保证一定不会出现不一致的情况 串行化之后，就会导致系统的吞吐量会大幅度的降低，用比正常情况下多几倍的机器去支撑线上的一个请求。 更多内容可以查看：https://github.com/doocs/advanced-java/blob/master/docs/high-concurrency/redis-consistence.md 参考： Java工程师面试突击第1季（可能是史上最好的Java面试突击课程）-中华石杉老师！公众号后台回复关键字“1”即可获取该视频内容。 参考 《Redis开发与运维》 Redis 命令总结：http://redisdoc.com/string/set.html 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \"Java面试突击\" 即可免费领取！ Java工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/database/Redis/Redis持久化.html":{"url":"zother6-JavaGuide/database/Redis/Redis持久化.html","title":"Redis持久化","keywords":"","body":"非常感谢《redis实战》真本书，本文大多内容也参考了书中的内容。非常推荐大家看一下《redis实战》这本书，感觉书中的很多理论性东西还是很不错的。 为什么本文的名字要加上春夏秋冬又一春，哈哈 ，这是一部韩国的电影，我感觉电影不错，所以就用在文章名字上了，没有什么特别的含义，然后下面的有些配图也是电影相关镜头。 很多时候我们需要持久化数据也就是将内存中的数据写入到硬盘里面，大部分原因是为了之后重用数据（比如重启机器、机器故障之后回复数据），或者是为了防止系统故障而将数据备份到一个远程位置。 Redis不同于Memcached的很重一点就是，Redis支持持久化，而且支持两种不同的持久化操作。Redis的一种持久化方式叫快照（snapshotting，RDB）,另一种方式是只追加文件（append-only file,AOF）.这两种方法各有千秋，下面我会详细这两种持久化方法是什么，怎么用，如何选择适合自己的持久化方法。 快照（snapshotting）持久化 Redis可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。Redis创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis主从结构，主要用来提高Redis性能），还可以将快照留在原地以便重启服务器的时候使用。 快照持久化是Redis默认采用的持久化方式，在redis.conf配置文件中默认有此下配置： save 900 1 #在900秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 save 300 10 #在300秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 save 60 10000 #在60秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 根据配置，快照将被写入dbfilename选项指定的文件里面，并存储在dir选项指定的路径上面。如果在新的快照文件创建完毕之前，Redis、系统或者硬件这三者中的任意一个崩溃了，那么Redis将丢失最近一次创建快照写入的所有数据。 举个例子：假设Redis的上一个快照是2：35开始创建的，并且已经创建成功。下午3：06时，Redis又开始创建新的快照，并且在下午3：08快照创建完毕之前，有35个键进行了更新。如果在下午3：06到3：08期间，系统发生了崩溃，导致Redis无法完成新快照的创建工作，那么Redis将丢失下午2：35之后写入的所有数据。另一方面，如果系统恰好在新的快照文件创建完毕之后崩溃，那么Redis将丢失35个键的更新数据。 创建快照的办法有如下几种： BGSAVE命令： 客户端向Redis发送 BGSAVE命令 来创建一个快照。对于支持BGSAVE命令的平台来说（基本上所有平台支持，除了Windows平台），Redis会调用fork来创建一个子进程，然后子进程负责将快照写入硬盘，而父进程则继续处理命令请求。 SAVE命令： 客户端还可以向Redis发送 SAVE命令 来创建一个快照，接到SAVE命令的Redis服务器在快照创建完毕之前不会再响应任何其他命令。SAVE命令不常用，我们通常只会在没有足够内存去执行BGSAVE命令的情况下，又或者即使等待持久化操作执行完毕也无所谓的情况下，才会使用这个命令。 save选项： 如果用户设置了save选项（一般会默认设置），比如 save 60 10000，那么从Redis最近一次创建快照之后开始算起，当“60秒之内有10000次写入”这个条件被满足时，Redis就会自动触发BGSAVE命令。 SHUTDOWN命令： 当Redis通过SHUTDOWN命令接收到关闭服务器的请求时，或者接收到标准TERM信号时，会执行一个SAVE命令，阻塞所有客户端，不再执行客户端发送的任何命令，并在SAVE命令执行完毕之后关闭服务器。 一个Redis服务器连接到另一个Redis服务器： 当一个Redis服务器连接到另一个Redis服务器，并向对方发送SYNC命令来开始一次复制操作的时候，如果主服务器目前没有执行BGSAVE操作，或者主服务器并非刚刚执行完BGSAVE操作，那么主服务器就会执行BGSAVE命令 如果系统真的发生崩溃，用户将丢失最近一次生成快照之后更改的所有数据。因此，快照持久化只适用于即使丢失一部分数据也不会造成一些大问题的应用程序。不能接受这个缺点的话，可以考虑AOF持久化。 AOF（append-only file）持久化 与快照持久化相比，AOF持久化 的实时性更好，因此已成为主流的持久化方案。默认情况下Redis没有开启AOF（append only file）方式的持久化，可以通过appendonly参数开启： appendonly yes 开启AOF持久化后每执行一条会更改Redis中的数据的命令，Redis就会将该命令写入硬盘中的AOF文件。AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的，默认的文件名是appendonly.aof。 在Redis的配置文件中存在三种同步方式，它们分别是： appendfsync always #每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度 appendfsync everysec #每秒钟同步一次，显示地将多个写命令同步到硬盘 appendfsync no #让操作系统决定何时进行同步 appendfsync always 可以实现将数据丢失减到最少，不过这种方式需要对硬盘进行大量的写入而且每次只写入一个命令，十分影响Redis的速度。另外使用固态硬盘的用户谨慎使用appendfsync always选项，因为这会明显降低固态硬盘的使用寿命。 为了兼顾数据和写入性能，用户可以考虑 appendfsync everysec选项 ，让Redis每秒同步一次AOF文件，Redis性能几乎没受到任何影响。而且这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操作的时候，Redis还会优雅的放慢自己的速度以便适应硬盘的最大写入速度。 appendfsync no 选项一般不推荐，这种方案会使Redis丢失不定量的数据而且如果用户的硬盘处理写入操作的速度不够的话，那么当缓冲区被等待写入的数据填满时，Redis的写入操作将被阻塞，这会导致Redis的请求速度变慢。 虽然AOF持久化非常灵活地提供了多种不同的选项来满足不同应用程序对数据安全的不同要求，但AOF持久化也有缺陷——AOF文件的体积太大。 重写/压缩AOF AOF虽然在某个角度可以将数据丢失降低到最小而且对性能影响也很小，但是极端的情况下，体积不断增大的AOF文件很可能会用完硬盘空间。另外，如果AOF体积过大，那么还原操作执行时间就可能会非常长。 为了解决AOF体积过大的问题，用户可以向Redis发送 BGREWRITEAOF命令 ，这个命令会通过移除AOF文件中的冗余命令来重写（rewrite）AOF文件来减小AOF文件的体积。BGREWRITEAOF命令和BGSAVE创建快照原理十分相似，所以AOF文件重写也需要用到子进程，这样会导致性能问题和内存占用问题，和快照持久化一样。更糟糕的是，如果不加以控制的话，AOF文件的体积可能会比快照文件大好几倍。 文件重写流程： 和快照持久化可以通过设置save选项来自动执行BGSAVE一样，AOF持久化也可以通过设置 auto-aof-rewrite-percentage 选项和 auto-aof-rewrite-min-size 选项自动执行BGREWRITEAOF命令。举例：假设用户对Redis设置了如下配置选项并且启用了AOF持久化。那么当AOF文件体积大于64mb，并且AOF的体积比上一次重写之后的体积大了至少一倍（100%）的时候，Redis将执行BGREWRITEAOF命令。 auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb 无论是AOF持久化还是快照持久化，将数据持久化到硬盘上都是非常有必要的，但除了进行持久化外，用户还必须对持久化得到的文件进行备份（最好是备份到不同的地方），这样才能尽量避免数据丢失事故发生。如果条件允许的话，最好能将快照文件和重新重写的AOF文件备份到不同的服务器上面。 随着负载量的上升，或者数据的完整性变得 越来越重要时，用户可能需要使用到复制特性。 Redis 4.0 对于持久化机制的优化 Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。 如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分就是压缩格式不再是 AOF 格式，可读性较差。 参考： 《Redis实战》 深入学习Redis（2）：持久化 "},"zother6-JavaGuide/database/Redis/redis集群以及应用场景.html":{"url":"zother6-JavaGuide/database/Redis/redis集群以及应用场景.html","title":"redis集群以及应用场景","keywords":"","body":"相关阅读： 史上最全Redis高可用技术解决方案大全 Raft协议实战之Redis Sentinel的选举Leader源码解析 目录： Redis 集群以及应用 集群 主从复制 主从链(拓扑结构) 复制模式 问题点 哨兵机制 拓扑图 节点下线 Leader选举 故障转移 读写分离 定时任务 分布式集群(Cluster) 拓扑图 通讯 集中式 Gossip 寻址分片 hash取模 一致性hash hash槽 使用场景 热点数据 会话维持 Session 分布式锁 SETNX 表缓存 消息队列 list 计数器 string 缓存设计 更新策略 更新一致性 缓存粒度 缓存穿透 解决方案 缓存雪崩 出现后应对 请求过程 Redis 集群以及应用 集群 主从复制 主从链(拓扑结构) 复制模式 全量复制：Master 全部同步到 Slave 部分复制：Slave 数据丢失进行备份 问题点 同步故障 复制数据延迟(不一致) 读取过期数据(Slave 不能删除数据) 从节点故障 主节点故障 配置不一致 maxmemory 不一致:丢失数据 优化参数不一致:内存不一致. 避免全量复制 选择小主节点(分片)、低峰期间操作. 如果节点运行 id 不匹配(如主节点重启、运行 id 发送变化)，此时要执行全量复制，应该配合哨兵和集群解决. 主从复制挤压缓冲区不足产生的问题(网络中断，部分复制无法满足)，可增大复制缓冲区( rel_backlog_size 参数). 复制风暴 哨兵机制 拓扑图 节点下线 主观下线 即 Sentinel 节点对 Redis 节点失败的偏见，超出超时时间认为 Master 已经宕机。 Sentinel 集群的每一个 Sentinel 节点会定时对 Redis 集群的所有节点发心跳包检测节点是否正常。如果一个节点在 down-after-milliseconds 时间内没有回复 Sentinel 节点的心跳包，则该 Redis 节点被该 Sentinel 节点主观下线。 客观下线 所有 Sentinel 节点对 Redis 节点失败要达成共识，即超过 quorum 个统一。 当节点被一个 Sentinel 节点记为主观下线时，并不意味着该节点肯定故障了，还需要 Sentinel 集群的其他 Sentinel 节点共同判断为主观下线才行。 该 Sentinel 节点会询问其它 Sentinel 节点，如果 Sentinel 集群中超过 quorum 数量的 Sentinel 节点认为该 Redis 节点主观下线，则该 Redis 客观下线。 Leader选举 选举出一个 Sentinel 作为 Leader：集群中至少有三个 Sentinel 节点，但只有其中一个节点可完成故障转移.通过以下命令可以进行失败判定或领导者选举。 选举流程 每个主观下线的 Sentinel 节点向其他 Sentinel 节点发送命令，要求设置它为领导者. 收到命令的 Sentinel 节点如果没有同意通过其他 Sentinel 节点发送的命令，则同意该请求，否则拒绝。 如果该 Sentinel 节点发现自己的票数已经超过 Sentinel 集合半数且超过 quorum，则它成为领导者。 如果此过程有多个 Sentinel 节点成为领导者，则等待一段时间再重新进行选举。 故障转移 转移流程 Sentinel 选出一个合适的 Slave 作为新的 Master(slaveof no one 命令)。 向其余 Slave 发出通知，让它们成为新 Master 的 Slave( parallel-syncs 参数)。 等待旧 Master 复活，并使之称为新 Master 的 Slave。 向客户端通知 Master 变化。 从 Slave 中选择新 Master 节点的规则(slave 升级成 master 之后) 选择 slave-priority 最高的节点。 选择复制偏移量最大的节点(同步数据最多)。 选择 runId 最小的节点。 Sentinel 集群运行过程中故障转移完成，所有 Sentinel 又会恢复平等。Leader 仅仅是故障转移操作出现的角色。 读写分离 定时任务 每 1s 每个 Sentinel 对其他 Sentinel 和 Redis 执行 ping，进行心跳检测。 每 2s 每个 Sentinel 通过 Master 的 Channel 交换信息(pub - sub)。 每 10s 每个 Sentinel 对 Master 和 Slave 执行 info，目的是发现 Slave 节点、确定主从关系。 分布式集群(Cluster) 拓扑图 通讯 集中式 将集群元数据(节点信息、故障等等)几种存储在某个节点上。 优势 元数据的更新读取具有很强的时效性，元数据修改立即更新 劣势 数据集中存储 Gossip Gossip 协议 寻址分片 hash取模 hash(key)%机器数量 问题 机器宕机，造成数据丢失，数据读取失败 伸缩性 一致性hash 问题 一致性哈希算法在节点太少时，容易因为节点分布不均匀而造成缓存热点的问题。 解决方案 可以通过引入虚拟节点机制解决：即对每一个节点计算多个 hash，每个计算结果位置都放置一个虚拟节点。这样就实现了数据的均匀分布，负载均衡。 hash槽 CRC16(key)%16384 使用场景 热点数据 存取数据优先从 Redis 操作，如果不存在再从文件（例如 MySQL）中操作，从文件操作完后将数据存储到 Redis 中并返回。同时有个定时任务后台定时扫描 Redis 的 key，根据业务规则进行淘汰，防止某些只访问一两次的数据一直存在 Redis 中。 例如使用 Zset 数据结构，存储 Key 的访问次数/最后访问时间作为 Score，最后做排序，来淘汰那些最少访问的 Key。 如果企业级应用，可以参考：[阿里云的 Redis 混合存储版][1] 会话维持 Session 会话维持 Session 场景，即使用 Redis 作为分布式场景下的登录中心存储应用。每次不同的服务在登录的时候，都会去统一的 Redis 去验证 Session 是否正确。但是在微服务场景，一般会考虑 Redis + JWT 做 Oauth2 模块。 其中 Redis 存储 JWT 的相关信息主要是留出口子，方便以后做统一的防刷接口，或者做登录设备限制等。 分布式锁 SETNX 命令格式：SETNX key value：当且仅当 key 不存在，将 key 的值设为 value。若给定的 key 已经存在，则 SETNX 不做任何动作。 超时时间设置：获取锁的同时，启动守护线程，使用 expire 进行定时更新超时时间。如果该业务机器宕机，守护线程也挂掉，这样也会自动过期。如果该业务不是宕机，而是真的需要这么久的操作时间，那么增加超时时间在业务上也是可以接受的，但是肯定有个最大的阈值。 但是为了增加高可用，需要使用多台 Redis，就增加了复杂性，就可以参考 Redlock：Redlock分布式锁 表缓存 Redis 缓存表的场景有黑名单、禁言表等。访问频率较高，即读高。根据业务需求，可以使用后台定时任务定时刷新 Redis 的缓存表数据。 消息队列 list 主要使用了 List 数据结构。List 支持在头部和尾部操作，因此可以实现简单的消息队列。 发消息：在 List 尾部塞入数据。 消费消息：在 List 头部拿出数据。 同时可以使用多个 List，来实现多个队列，根据不同的业务消息，塞入不同的 List，来增加吞吐量。 计数器 string 主要使用了 INCR、DECR、INCRBY、DECRBY 方法。 INCR key：给 key 的 value 值增加一 DECR key：给 key 的 value 值减去一 缓存设计 更新策略 LRU、LFU、FIFO 算法自动清除：一致性最差，维护成本低。 超时自动清除(key expire)：一致性较差，维护成本低。 主动更新：代码层面控制生命周期，一致性最好，维护成本高。 在 Redis 根据在 redis.conf 的参数 maxmemory 来做更新淘汰策略： noeviction: 不删除策略, 达到最大内存限制时, 如果需要更多内存, 直接返回错误信息。大多数写命令都会导致占用更多的内存(有极少数会例外, 如 DEL 命令)。 allkeys-lru: 所有 key 通用; 优先删除最近最少使用(less recently used ,LRU) 的 key。 volatile-lru: 只限于设置了 expire 的部分; 优先删除最近最少使用(less recently used ,LRU) 的 key。 allkeys-random: 所有key通用; 随机删除一部分 key。 volatile-random: 只限于设置了 expire 的部分; 随机删除一部分 key。 volatile-ttl: 只限于设置了 expire 的部分; 优先删除剩余时间(time to live,TTL) 短的key。 更新一致性 读请求：先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。 写请求：先删除缓存，然后再更新数据库(避免大量地写、却又不经常读的数据导致缓存频繁更新)。 缓存粒度 通用性：全量属性更好。 占用空间：部分属性更好。 代码维护成本。 缓存穿透 当大量的请求无命中缓存、直接请求到后端数据库(业务代码的 bug、或恶意攻击)，同时后端数据库也没有查询到相应的记录、无法添加缓存。这种状态会一直维持，流量一直打到存储层上，无法利用缓存、还会给存储层带来巨大压力。 解决方案 请求无法命中缓存、同时数据库记录为空时在缓存添加该 key 的空对象(设置过期时间)，缺点是可能会在缓存中添加大量的空值键(比如遭到恶意攻击或爬虫)，而且缓存层和存储层数据短期内不一致； 使用布隆过滤器在缓存层前拦截非法请求、自动为空值添加黑名单(同时可能要为误判的记录添加白名单).但需要考虑布隆过滤器的维护(离线生成/ 实时生成)。 缓存雪崩 缓存崩溃时请求会直接落到数据库上，很可能由于无法承受大量的并发请求而崩溃，此时如果只重启数据库，或因为缓存重启后没有数据，新的流量进来很快又会把数据库击倒。 出现后应对 事前：Redis 高可用，主从 + 哨兵，Redis Cluster，避免全盘崩溃。 事中：本地 ehcache 缓存 + hystrix 限流 & 降级，避免数据库承受太多压力。 事后：Redis 持久化，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。 请求过程 用户请求先访问本地缓存，无命中后再访问 Redis，如果本地缓存和 Redis 都没有再查数据库，并把数据添加到本地缓存和 Redis； 由于设置了限流，一段时间范围内超出的请求走降级处理(返回默认值，或给出友情提示)。 "},"zother6-JavaGuide/database/Redis/Redlock分布式锁.html":{"url":"zother6-JavaGuide/database/Redis/Redlock分布式锁.html","title":"Redlock分布式锁","keywords":"","body":"这篇文章主要是对 Redis 官方网站刊登的 Distributed locks with Redis 部分内容的总结和翻译。 什么是 RedLock Redis 官方站这篇文章提出了一种权威的基于 Redis 实现分布式锁的方式名叫 Redlock，此种方式比原先的单节点的方法更安全。它可以保证以下特性： 安全特性：互斥访问，即永远只有一个 client 能拿到锁 避免死锁：最终 client 都可能拿到锁，不会出现死锁的情况，即使原本锁住某资源的 client crash 了或者出现了网络分区 容错性：只要大部分 Redis 节点存活就可以正常提供服务 怎么在单节点上实现分布式锁 SET resource_name my_random_value NX PX 30000 主要依靠上述命令，该命令仅当 Key 不存在时（NX保证）set 值，并且设置过期时间 3000ms （PX保证），值 my_random_value 必须是所有 client 和所有锁请求发生期间唯一的，释放锁的逻辑是： if redis.call(\"get\",KEYS[1]) == ARGV[1] then return redis.call(\"del\",KEYS[1]) else return 0 end 上述实现可以避免释放另一个client创建的锁，如果只有 del 命令的话，那么如果 client1 拿到 lock1 之后因为某些操作阻塞了很长时间，此时 Redis 端 lock1 已经过期了并且已经被重新分配给了 client2，那么 client1 此时再去释放这把锁就会造成 client2 原本获取到的锁被 client1 无故释放了，但现在为每个 client 分配一个 unique 的 string 值可以避免这个问题。至于如何去生成这个 unique string，方法很多随意选择一种就行了。 Redlock 算法 算法很易懂，起 5 个 master 节点，分布在不同的机房尽量保证可用性。为了获得锁，client 会进行如下操作： 得到当前的时间，微秒单位 尝试顺序地在 5 个实例上申请锁，当然需要使用相同的 key 和 random value，这里一个 client 需要合理设置与 master 节点沟通的 timeout 大小，避免长时间和一个 fail 了的节点浪费时间 当 client 在大于等于 3 个 master 上成功申请到锁的时候，且它会计算申请锁消耗了多少时间，这部分消耗的时间采用获得锁的当下时间减去第一步获得的时间戳得到，如果锁的持续时长（lock validity time）比流逝的时间多的话，那么锁就真正获取到了。 如果锁申请到了，那么锁真正的 lock validity time 应该是 origin（lock validity time） - 申请锁期间流逝的时间 如果 client 申请锁失败了，那么它就会在少部分申请成功锁的 master 节点上执行释放锁的操作，重置状态 失败重试 如果一个 client 申请锁失败了，那么它需要稍等一会在重试避免多个 client 同时申请锁的情况，最好的情况是一个 client 需要几乎同时向 5 个 master 发起锁申请。另外就是如果 client 申请锁失败了它需要尽快在它曾经申请到锁的 master 上执行 unlock 操作，便于其他 client 获得这把锁，避免这些锁过期造成的时间浪费，当然如果这时候网络分区使得 client 无法联系上这些 master，那么这种浪费就是不得不付出的代价了。 放锁 放锁操作很简单，就是依次释放所有节点上的锁就行了 性能、崩溃恢复和 fsync 如果我们的节点没有持久化机制，client 从 5 个 master 中的 3 个处获得了锁，然后其中一个重启了，这是注意 整个环境中又出现了 3 个 master 可供另一个 client 申请同一把锁！ 违反了互斥性。如果我们开启了 AOF 持久化那么情况会稍微好转一些，因为 Redis 的过期机制是语义层面实现的，所以在 server 挂了的时候时间依旧在流逝，重启之后锁状态不会受到污染。但是考虑断电之后呢，AOF部分命令没来得及刷回磁盘直接丢失了，除非我们配置刷回策略为 fsnyc = always，但这会损伤性能。解决这个问题的方法是，当一个节点重启之后，我们规定在 max TTL 期间它是不可用的，这样它就不会干扰原本已经申请到的锁，等到它 crash 前的那部分锁都过期了，环境不存在历史锁了，那么再把这个节点加进来正常工作。 "},"zother6-JavaGuide/database/Redis/如何做可靠的分布式锁，Redlock真的可行么.html":{"url":"zother6-JavaGuide/database/Redis/如何做可靠的分布式锁，Redlock真的可行么.html","title":"如何做可靠的分布式锁，Redlock真的可行么","keywords":"","body":"本文是对 Martin Kleppmann 的文章 How to do distributed locking 部分内容的翻译和总结，上次写 Redlock 的原因就是看到了 Martin 的这篇文章，写得很好，特此翻译和总结。感兴趣的同学可以翻看原文，相信会收获良多。 开篇作者认为现在 Redis 逐渐被使用到数据管理领域，这个领域需要更强的数据一致性和耐久性，这使得他感到担心，因为这不是 Redis 最初设计的初衷（事实上这也是很多业界程序员的误区，越来越把 Redis 当成数据库在使用），其中基于 Redis 的分布式锁就是令人担心的其一。 Martin 指出首先你要明确你为什么使用分布式锁，为了性能还是正确性？为了帮你区分这二者，在这把锁 fail 了的时候你可以询问自己以下问题： 要性能的： 拥有这把锁使得你不会重复劳动（例如一个 job 做了两次），如果这把锁 fail 了，两个节点同时做了这个 Job，那么这个 Job 增加了你的成本。 要正确性的： 拥有锁可以防止并发操作污染你的系统或者数据，如果这把锁 fail 了两个节点同时操作了一份数据，结果可能是数据不一致、数据丢失、file 冲突等，会导致严重的后果。 上述二者都是需求锁的正确场景，但是你必须清楚自己是因为什么原因需要分布式锁。 如果你只是为了性能，那没必要用 Redlock，它成本高且复杂，你只用一个 Redis 实例也够了，最多加个从防止主挂了。当然，你使用单节点的 Redis 那么断电或者一些情况下，你会丢失锁，但是你的目的只是加速性能且断电这种事情不会经常发生，这并不是什么大问题。并且如果你使用了单节点 Redis，那么很显然你这个应用需要的锁粒度是很模糊粗糙的，也不会是什么重要的服务。 那么是否 Redlock 对于要求正确性的场景就合适呢？Martin 列举了若干场景证明 Redlock 这种算法是不可靠的。 用锁保护资源 这节里 Martin 先将 Redlock 放在了一边而是仅讨论总体上一个分布式锁是怎么工作的。在分布式环境下，锁比 mutex 这类复杂，因为涉及到不同节点、网络通信并且他们随时可能无征兆的 fail 。 Martin 假设了一个场景，一个 client 要修改一个文件，它先申请得到锁，然后修改文件写回，放锁。另一个 client 再申请锁 ... 代码流程如下： // THIS CODE IS BROKEN function writeData(filename, data) { var lock = lockService.acquireLock(filename); if (!lock) { throw 'Failed to acquire lock'; } try { var file = storage.readFile(filename); var updated = updateContents(file, data); storage.writeFile(filename, updated); } finally { lock.release(); } } 可惜即使你的锁服务非常完美，上述代码还是可能跪，下面的流程图会告诉你为什么： 上述图中，得到锁的 client1 在持有锁的期间 pause 了一段时间，例如 GC 停顿。锁有过期时间（一般叫租约，为了防止某个 client 崩溃之后一直占有锁），但是如果 GC 停顿太长超过了锁租约时间，此时锁已经被另一个 client2 所得到，原先的 client1 还没有感知到锁过期，那么奇怪的结果就会发生，曾经 HBase 就发生过这种 Bug。即使你在 client1 写回之前检查一下锁是否过期也无助于解决这个问题，因为 GC 可能在任何时候发生，即使是你非常不便的时候（在最后的检查与写操作期间）。 如果你认为自己的程序不会有长时间的 GC 停顿，还有其他原因会导致你的进程 pause。例如进程可能读取尚未进入内存的数据，所以它得到一个 page fault 并且等待 page 被加载进缓存；还有可能你依赖于网络服务；或者其他进程占用 CPU；或者其他人意外发生 SIGSTOP 等。 ... .... 这里 Martin 又增加了一节列举各种进程 pause 的例子，为了证明上面的代码是不安全的，无论你的锁服务多完美。 使用 Fencing （栅栏）使得锁变安全 修复问题的方法也很简单：你需要在每次写操作时加入一个 fencing token。这个场景下，fencing token 可以是一个递增的数字（lock service 可以做到），每次有 client 申请锁就递增一次： client1 申请锁同时拿到 token33，然后它进入长时间的停顿锁也过期了。client2 得到锁和 token34 写入数据，紧接着 client1 活过来之后尝试写入数据，自身 token33 比 34 小因此写入操作被拒绝。注意这需要存储层来检查 token，但这并不难实现。如果你使用 Zookeeper 作为 lock service 的话那么你可以使用 zxid 作为递增数字。 但是对于 Redlock 你要知道，没什么生成 fencing token 的方式，并且怎么修改 Redlock 算法使其能产生 fencing token 呢？好像并不那么显而易见。因为产生 token 需要单调递增，除非在单节点 Redis 上完成但是这又没有高可靠性，你好像需要引进一致性协议来让 Redlock 产生可靠的 fencing token。 使用时间来解决一致性 Redlock 无法产生 fencing token 早该成为在需求正确性的场景下弃用它的理由，但还有一些值得讨论的地方。 学术界有个说法，算法对时间不做假设：因为进程可能pause一段时间、数据包可能因为网络延迟延后到达、时钟可能根本就是错的。而可靠的算法依旧要在上述假设下做正确的事情。 对于 failure detector 来说，timeout 只能作为猜测某个节点 fail 的依据，因为网络延迟、本地时钟不正确等其他原因的限制。考虑到 Redis 使用 gettimeofday，而不是单调的时钟，会受到系统时间的影响，可能会突然前进或者后退一段时间，这会导致一个 key 更快或更慢地过期。 可见，Redlock 依赖于许多时间假设，它假设所有 Redis 节点都能对同一个 Key 在其过期前持有差不多的时间、跟过期时间相比网络延迟很小、跟过期时间相比进程 pause 很短。 用不可靠的时间打破 Redlock 这节 Martin 举了个因为时间问题，Redlock 不可靠的例子。 client1 从 ABC 三个节点处申请到锁，DE由于网络原因请求没有到达 C节点的时钟往前推了，导致 lock 过期 client2 在CDE处获得了锁，AB由于网络原因请求未到达 此时 client1 和 client2 都获得了锁 在 Redlock 官方文档中也提到了这个情况，不过是C崩溃的时候，Redlock 官方本身也是知道 Redlock 算法不是完全可靠的，官方为了解决这种问题建议使用延时启动，相关内容可以看之前的这篇文章。但是 Martin 这里分析得更加全面，指出延时启动不也是依赖于时钟的正确性的么？ 接下来 Martin 又列举了进程 Pause 时而不是时钟不可靠时会发生的问题： client1 从 ABCDE 处获得了锁 当获得锁的 response 还没到达 client1 时 client1 进入 GC 停顿 停顿期间锁已经过期了 client2 在 ABCDE 处获得了锁 client1 GC 完成收到了获得锁的 response，此时两个 client 又拿到了同一把锁 同时长时间的网络延迟也有可能导致同样的问题。 Redlock 的同步性假设 这些例子说明了，仅有在你假设了一个同步性系统模型的基础上，Redlock 才能正常工作，也就是系统能满足以下属性： 网络延时边界，即假设数据包一定能在某个最大延时之内到达 进程停顿边界，即进程停顿一定在某个最大时间之内 时钟错误边界，即不会从一个坏的 NTP 服务器处取得时间 结论 Martin 认为 Redlock 实在不是一个好的选择，对于需求性能的分布式锁应用它太重了且成本高；对于需求正确性的应用来说它不够安全。因为它对高危的时钟或者说其他上述列举的情况进行了不可靠的假设，如果你的应用只需要高性能的分布式锁不要求多高的正确性，那么单节点 Redis 够了；如果你的应用想要保住正确性，那么不建议 Redlock，建议使用一个合适的一致性协调系统，例如 Zookeeper，且保证存在 fencing token。 "},"zother6-JavaGuide/database/MySQL Index.html":{"url":"zother6-JavaGuide/database/MySQL Index.html","title":"My SQL Index","keywords":"","body":"为什么要使用索引？ 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。 可以大大加快 数据的检索速度（大大减少的检索的数据量）, 这也是创建索引的最主要的原因。 帮助服务器避免排序和临时表。 将随机IO变为顺序IO 可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义。 索引这么多优点，为什么不对表中的每一个列创建一个索引呢？ 当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。 索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。 使用索引的注意事项？ 在经常需要搜索的列上，可以加快搜索的速度； 在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度。 在经常需要排序的列上创 建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间； 对于中到大型表索引都是非常有效的，但是特大型表的话维护开销会很大，不适合建索引 在经常用在连接的列上，这 些列主要是一些外键，可以加快连接的速度； 避免 where 子句中对宇段施加函数，这会造成无法命中索引。 在使用InnoDB时使用与业务无关的自增主键作为主键，即使用逻辑主键，而不要使用业务主键。 将打算加索引的列设置为 NOT NULL ，否则将导致引擎放弃使用索引而进行全表扫描 删除长期未使用的索引，不用的索引的存在会造成不必要的性能损耗 MySQL 5.7 可以通过查询 sys 库的 chema_unused_indexes 视图来查询哪些索引从未被使用 在使用 limit offset 查询缓慢时，可以借助索引来提高性能 Mysql索引主要使用的两种数据结构 哈希索引 对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择BTree索引。 BTree索引 MyISAM和InnoDB实现BTree索引方式的区别 MyISAM B+Tree叶节点的data域存放的是数据记录的地址。在索引检索的时候，首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其 data 域的值，然后以 data 域的值为地址读取相应的数据记录。这被称为“非聚簇索引”。 InnoDB 其数据文件本身就是索引文件。相比MyISAM，索引文件和数据文件是分离的，其表数据文件本身就是按B+Tree组织的一个索引结构，树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。这被称为“聚簇索引（或聚集索引）”，而其余的索引都作为辅助索引，辅助索引的data域存储相应记录主键的值而不是地址，这也是和MyISAM不同的地方。在根据主索引搜索时，直接找到key所在的节点即可取出数据；在根据辅助索引查找时，则需要先取出主键的值，在走一遍主索引。 因此，在设计表的时候，不建议使用过长的字段作为主键，也不建议使用非单调的字段作为主键，这样会造成主索引频繁分裂。 PS：整理自《Java工程师修炼之道》 覆盖索引介绍 什么是覆盖索引 如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称之为“覆盖索引”。我们知道InnoDB存储引擎中，如果不是主键索引，叶子节点存储的是主键+列值。最终还是要“回表”，也就是要通过主键再查找一次。这样就会比较慢覆盖索引就是把要查询出的列和索引是对应的，不做回表操作！ 覆盖索引使用实例 现在我创建了索引(username,age)，我们执行下面的 sql 语句 select username , age from user where username = 'Java' and age = 22 在查询数据的时候：要查询出的列在叶子节点都存在！所以，就不用回表。 选择索引和编写利用这些索引的查询的3个原则 单行访问是很慢的。特别是在机械硬盘存储中(SSD的随机I/O要快很多，不过这一点仍然成立）。如果服务器从存储中读取一个数据块只是为了获取其中一行，那么就浪费了很多工作。最好读取的块中能包含尽可能多所需要的行。使用索引可以创建位置引，用以提升效率。 按顺序访问范围数据是很快的，这有两个原因。第一，顺序1/0不需要多次磁盘寻道，所以比随机I/O要快很多（特别是对机械硬盘）。第二，如果服务器能够按需要顺序读取数据，那么就不再需要额外的排序操作，并且GROUPBY查询也无须再做排序和将行按组进行聚合计算了。 索引覆盖查询是很快的。如果一个索引包含了查询需要的所有列，那么存储引擎就 不需要再回表查找行。这避免了大量的单行访问，而上面的第1点已经写明单行访 问是很慢的。 为什么索引能提高查询速度 以下内容整理自： 地址： https://juejin.im/post/5b55b842f265da0f9e589e79 作者 ：Java3y 先从 MySQL 的基本存储结构说起 MySQL的基本存储结构是页(记录都存在页里边)： 各个数据页可以组成一个双向链表 每个数据页中的记录又可以组成一个单向链表 每个数据页都会为存储在它里边儿的记录生成一个页目录，在通过主键查找某条记录的时候可以在页目录中使用二分法快速定位到对应的槽，然后再遍历该槽对应分组中的记录即可快速找到指定的记录 以其他列(非主键)作为搜索条件：只能从最小记录开始依次遍历单链表中的每条记录。 所以说，如果我们写select * from user where indexname = 'xxx'这样没有进行任何优化的sql语句，默认会这样做： 定位到记录所在的页：需要遍历双向链表，找到所在的页 从所在的页内中查找相应的记录：由于不是根据主键查询，只能遍历所在页的单链表了 很明显，在数据量很大的情况下这样查找会很慢！这样的时间复杂度为O（n）。 使用索引之后 索引做了些什么可以让我们查询加快速度呢？其实就是将无序的数据变成有序(相对)： 要找到id为8的记录简要步骤： 很明显的是：没有用索引我们是需要遍历双向链表来定位对应的页，现在通过 “目录” 就可以很快地定位到对应的页上了！（二分查找，时间复杂度近似为O(logn)） 其实底层结构就是B+树，B+树作为树的一种实现，能够让我们很快地查找出对应的记录。 关于索引其他重要的内容补充 以下内容整理自：《Java工程师修炼之道》 最左前缀原则 MySQL中的索引可以以一定顺序引用多列，这种索引叫作联合索引。如User表的name和city加联合索引就是(name,city)，而最左前缀原则指的是，如果查询的时候查询条件精确匹配索引的左边连续一列或几列，则此列就可以被用到。如下： select * from user where name=xx and city=xx ; ／／可以命中索引 select * from user where name=xx ; // 可以命中索引 select * from user where city=xx ; // 无法命中索引 这里需要注意的是，查询的时候如果两个条件都用上了，但是顺序不同，如 city= xx and name ＝xx，那么现在的查询引擎会自动优化为匹配联合索引的顺序，这样是能够命中索引的。 由于最左前缀原则，在创建联合索引时，索引字段的顺序需要考虑字段值去重之后的个数，较多的放前面。ORDER BY子句也遵循此规则。 注意避免冗余索引 冗余索引指的是索引的功能相同，能够命中 就肯定能命中 ，那么 就是冗余索引如（name,city ）和（name ）这两个索引就是冗余索引，能够命中后者的查询肯定是能够命中前者的 在大多数情况下，都应该尽量扩展已有的索引而不是创建新索引。 MySQL 5.7 版本后，可以通过查询 sys 库的 schema_redundant_indexes 表来查看冗余索引 Mysql如何为表字段添加索引？？？ 1.添加PRIMARY KEY（主键索引） ALTER TABLE `table_name` ADD PRIMARY KEY ( `column` ) 2.添加UNIQUE(唯一索引) ALTER TABLE `table_name` ADD UNIQUE ( `column` ) 3.添加INDEX(普通索引) ALTER TABLE `table_name` ADD INDEX index_name ( `column` ) 4.添加FULLTEXT(全文索引) ALTER TABLE `table_name` ADD FULLTEXT ( `column`) 5.添加多列索引 ALTER TABLE `table_name` ADD INDEX index_name ( `column1`, `column2`, `column3` ) 参考 《Java工程师修炼之道》 《MySQL高性能书籍_第3版》 https://juejin.im/post/5b55b842f265da0f9e589e79 "},"zother6-JavaGuide/database/MySQL.html":{"url":"zother6-JavaGuide/database/MySQL.html","title":"My SQL","keywords":"","body":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。 书籍推荐 文字教程推荐 视频教程推荐 常见问题总结 什么是MySQL? 存储引擎 一些常用命令 MyISAM和InnoDB区别 字符集及校对规则 索引 查询缓存的使用 什么是事务? 事物的四大特性(ACID) 并发事务带来哪些问题? 事务隔离级别有哪些?MySQL的默认隔离级别是? 锁机制与InnoDB锁算法 大表优化 1. 限定数据的范围 2. 读/写分离 3. 垂直分区 4. 水平分区 一条SQL语句在MySQL中如何执行的 MySQL高性能优化规范建议 一条SQL语句执行得很慢的原因有哪些？ 书籍推荐 《SQL基础教程（第2版）》 （入门级） 《高性能MySQL : 第3版》 (进阶) 文字教程推荐 SQL Tutorial （SQL语句学习,英文）、SQL Tutorial（SQL语句学习,中文）、SQL语句在线练习 （非常不错） Github-MySQL入门教程（MySQL tutorial book） （从零开始学习MySQL，主要是面向MySQL数据库管理系统初学者） 官方教程 MySQL 教程（菜鸟教程） 相关资源推荐 中国5级行政区域mysql库 视频教程推荐 基础入门： 与MySQL的零距离接触-慕课网 MySQL开发技巧： MySQL开发技巧（一）　　MySQL开发技巧（二）　　MySQL开发技巧（三） MySQL5.7新特性及相关优化技巧： MySQL5.7版本新特性　　性能优化之MySQL优化 MySQL集群（PXC）入门　　MyCAT入门及应用 常见问题总结 什么是MySQL? MySQL 是一种关系型数据库，在Java企业级开发中非常常用，因为 MySQL 是开源免费的，并且方便扩展。阿里巴巴数据库系统也大量用到了 MySQL，因此它的稳定性是有保障的。MySQL是开放源代码的，因此任何人都可以在 GPL(General Public License) 的许可下下载并根据个性化的需要对其进行修改。MySQL的默认端口号是3306。 存储引擎 一些常用命令 查看MySQL提供的所有存储引擎 mysql> show engines; 从上图我们可以查看出 MySQL 当前默认的存储引擎是InnoDB,并且在5.7版本所有的存储引擎中只有 InnoDB 是事务性存储引擎，也就是说只有 InnoDB 支持事务。 查看MySQL当前默认的存储引擎 我们也可以通过下面的命令查看默认的存储引擎。 mysql> show variables like '%storage_engine%'; 查看表的存储引擎 show table status like \"table_name\" ; MyISAM和InnoDB区别 MyISAM是MySQL的默认数据库引擎（5.5版之前）。虽然性能极佳，而且提供了大量的特性，包括全文索引、压缩、空间函数等，但MyISAM不支持事务和行级锁，而且最大的缺陷就是崩溃后无法安全恢复。不过，5.5版本之后，MySQL引入了InnoDB（事务性数据库引擎），MySQL 5.5版本后默认的存储引擎为InnoDB。 大多数时候我们使用的都是 InnoDB 存储引擎，但是在某些情况下使用 MyISAM 也是合适的比如读密集的情况下。（如果你不介意 MyISAM 崩溃恢复问题的话）。 两者的对比： 是否支持行级锁 : MyISAM 只有表级锁(table-level locking)，而InnoDB 支持行级锁(row-level locking)和表级锁,默认为行级锁。 是否支持事务和崩溃后的安全恢复： MyISAM 强调的是性能，每次查询具有原子性,其执行速度比InnoDB类型更快，但是不提供事务支持。但是InnoDB 提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。 是否支持外键： MyISAM不支持，而InnoDB支持。 是否支持MVCC ：仅 InnoDB 支持。应对高并发事务, MVCC比单纯的加锁更高效;MVCC只在 READ COMMITTED 和 REPEATABLE READ 两个隔离级别下工作;MVCC可以使用 乐观(optimistic)锁 和 悲观(pessimistic)锁来实现;各数据库中MVCC实现并不统一。推荐阅读：MySQL-InnoDB-MVCC多版本并发控制 ...... 《MySQL高性能》上面有一句话这样写到: 不要轻易相信“MyISAM比InnoDB快”之类的经验之谈，这个结论往往不是绝对的。在很多我们已知场景中，InnoDB的速度都可以让MyISAM望尘莫及，尤其是用到了聚簇索引，或者需要访问的数据都可以放入内存的应用。 一般情况下我们选择 InnoDB 都是没有问题的，但是某些情况下你并不在乎可扩展能力和并发能力，也不需要事务支持，也不在乎崩溃后的安全恢复问题的话，选择MyISAM也是一个不错的选择。但是一般情况下，我们都是需要考虑到这些问题的。 字符集及校对规则 字符集指的是一种从二进制编码到某类字符符号的映射。校对规则则是指某种字符集下的排序规则。MySQL中每一种字符集都会对应一系列的校对规则。 MySQL采用的是类似继承的方式指定字符集的默认值，每个数据库以及每张数据表都有自己的默认值，他们逐层继承。比如：某个库中所有表的默认字符集将是该数据库所指定的字符集（这些表在没有指定字符集的情况下，才会采用默认字符集） PS：整理自《Java工程师修炼之道》 详细内容可以参考： MySQL字符集及校对规则的理解 索引 MySQL索引使用的数据结构主要有BTree索引 和 哈希索引 。对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择BTree索引。 MySQL的BTree索引使用的是B树中的B+Tree，但对于主要的两种存储引擎的实现方式是不同的。 MyISAM: B+Tree叶节点的data域存放的是数据记录的地址。在索引检索的时候，首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其 data 域的值，然后以 data 域的值为地址读取相应的数据记录。这被称为“非聚簇索引”。 InnoDB: 其数据文件本身就是索引文件。相比MyISAM，索引文件和数据文件是分离的，其表数据文件本身就是按B+Tree组织的一个索引结构，树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。这被称为“聚簇索引（或聚集索引）”。而其余的索引都作为辅助索引，辅助索引的data域存储相应记录主键的值而不是地址，这也是和MyISAM不同的地方。在根据主索引搜索时，直接找到key所在的节点即可取出数据；在根据辅助索引查找时，则需要先取出主键的值，再走一遍主索引。 因此，在设计表的时候，不建议使用过长的字段作为主键，也不建议使用非单调的字段作为主键，这样会造成主索引频繁分裂。 PS：整理自《Java工程师修炼之道》 更多关于索引的内容可以查看文档首页MySQL目录下关于索引的详细总结。 查询缓存的使用 执行查询语句的时候，会先查询缓存。不过，MySQL 8.0 版本后移除，因为这个功能不太实用 my.cnf加入以下配置，重启MySQL开启查询缓存 query_cache_type=1 query_cache_size=600000 MySQL执行以下命令也可以开启查询缓存 set global query_cache_type=1; set global query_cache_size=600000; 如上，开启查询缓存后在同样的查询条件以及数据情况下，会直接在缓存中返回结果。这里的查询条件包括查询本身、当前要查询的数据库、客户端协议版本号等一些可能影响结果的信息。因此任何两个查询在任何字符上的不同都会导致缓存不命中。此外，如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、MySQL库中的系统表，其查询结果也不会被缓存。 缓存建立之后，MySQL的查询缓存系统会跟踪查询中涉及的每张表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效。 缓存虽然能够提升数据库的查询性能，但是缓存同时也带来了额外的开销，每次查询后都要做一次缓存操作，失效后还要销毁。 因此，开启缓存查询要谨慎，尤其对于写密集的应用来说更是如此。如果开启，要注意合理控制缓存空间大小，一般来说其大小设置为几十MB比较合适。此外，还可以通过sql_cache和sql_no_cache来控制某个查询语句是否需要缓存： select sql_no_cache count(*) from usr; 什么是事务? 事务是逻辑上的一组操作，要么都执行，要么都不执行。 事务最经典也经常被拿出来说例子就是转账了。假如小明要给小红转账1000元，这个转账会涉及到两个关键操作就是：将小明的余额减少1000元，将小红的余额增加1000元。万一在这两个操作之间突然出现错误比如银行系统崩溃，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。 事物的四大特性(ACID) 原子性（Atomicity）： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性（Consistency）： 执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的； 隔离性（Isolation）： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性（Durability）： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 并发事务带来哪些问题? 在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对同一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。 脏读（Dirty read）: 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。 丢失修改（Lost to modify）: 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。 不可重复读（Unrepeatableread）: 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。 幻读（Phantom read）: 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。 不可重复读和幻读区别： 不可重复读的重点是修改比如多次读取一条记录发现其中某些列的值被修改，幻读的重点在于新增或者删除比如多次读取一条记录发现记录增多或减少了。 事务隔离级别有哪些?MySQL的默认隔离级别是? SQL 标准定义了四个隔离级别： READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。 隔离级别 脏读 不可重复读 幻影读 READ-UNCOMMITTED √ √ √ READ-COMMITTED × √ √ REPEATABLE-READ × × √ SERIALIZABLE × × × MySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读）。我们可以通过SELECT @@tx_isolation;命令来查看 mysql> SELECT @@tx_isolation; +-----------------+ | @@tx_isolation | +-----------------+ | REPEATABLE-READ | +-----------------+ 这里需要注意的是：与 SQL 标准不同的地方在于 InnoDB 存储引擎在 REPEATABLE-READ（可重读） 事务隔离级别下使用的是Next-Key Lock 锁算法，因此可以避免幻读的产生，这与其他数据库系统(如 SQL Server) 是不同的。所以说InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读） 已经可以完全保证事务的隔离性要求，即达到了 SQL标准的 SERIALIZABLE(可串行化) 隔离级别。因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是 READ-COMMITTED(读取提交内容) ，但是你要知道的是InnoDB 存储引擎默认使用 REPEAaTABLE-READ（可重读） 并不会有任何性能损失。 InnoDB 存储引擎在 分布式事务 的情况下一般会用到 SERIALIZABLE(可串行化) 隔离级别。 锁机制与InnoDB锁算法 MyISAM和InnoDB存储引擎使用的锁： MyISAM采用表级锁(table-level locking)。 InnoDB支持行级锁(row-level locking)和表级锁,默认为行级锁 表级锁和行级锁对比： 表级锁： MySQL中锁定 粒度最大 的一种锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。其锁定粒度最大，触发锁冲突的概率最高，并发度最低，MyISAM和 InnoDB引擎都支持表级锁。 行级锁： MySQL中锁定 粒度最小 的一种锁，只针对当前操作的行进行加锁。 行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。 详细内容可以参考： MySQL锁机制简单了解一下：https://blog.csdn.net/qq_34337272/article/details/80611486 InnoDB存储引擎的锁的算法有三种： Record lock：单个行记录上的锁 Gap lock：间隙锁，锁定一个范围，不包括记录本身 Next-key lock：record+gap 锁定一个范围，包含记录本身 相关知识点： innodb对于行的查询使用next-key lock Next-locking keying为了解决Phantom Problem幻读问题 当查询的索引含有唯一属性时，将next-key lock降级为record key Gap锁设计的目的是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生 有两种方式显式关闭gap锁：（除了外键约束和唯一性检查外，其余情况仅使用record lock） A. 将事务隔离级别设置为RC B. 将参数innodb_locks_unsafe_for_binlog设置为1 大表优化 当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下： 1. 限定数据的范围 务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内； 2. 读/写分离 经典的数据库拆分方案，主库负责写，从库负责读； 3. 垂直分区 根据数据库里面数据表的相关性进行拆分。 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。 简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表。 如下图所示，这样来说大家应该就更容易理解了。 垂直拆分的优点： 可以使得列数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。 垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂； 4. 水平分区 保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。 水平拆分是指数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。 水平拆分可以支持非常大的数据量。需要注意的一点是：分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以 水平拆分最好分库 。 水平拆分能够 支持非常大的数据量存储，应用端改造也少，但 分片事务难以解决 ，跨节点Join性能较差，逻辑复杂。《Java工程师修炼之道》的作者推荐 尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维的各种复杂度 ，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，尽量选择客户端分片架构，这样可以减少一次和中间件的网络I/O。 下面补充一下数据库分片的两种常见方案： 客户端代理： 分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。 当当网的 Sharding-JDBC 、阿里的TDDL是两种比较常用的实现。 中间件代理： 在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。 我们现在谈的 Mycat 、360的Atlas、网易的DDB等等都是这种架构的实现。 详细内容可以参考： MySQL大表优化方案: https://segmentfault.com/a/1190000006158186 解释一下什么是池化设计思想。什么是数据库连接池?为什么需要数据库连接池? 池化设计应该不是一个新名词。我们常见的如java线程池、jdbc连接池、redis连接池等就是这类设计的代表实现。这种设计会初始预设资源，解决的问题就是抵消每次获取资源的消耗，如创建线程的开销，获取远程连接的开销等。就好比你去食堂打饭，打饭的大妈会先把饭盛好几份放那里，你来了就直接拿着饭盒加菜即可，不用再临时又盛饭又打菜，效率就高了。除了初始化资源，池化设计还包括如下这些特征：池子的初始值、池子的活跃值、池子的最大值等，这些特征可以直接映射到java线程池和数据库连接池的成员属性中。这篇文章对池化设计思想介绍的还不错，直接复制过来，避免重复造轮子了。 数据库连接本质就是一个 socket 的连接。数据库服务端还要维护一些缓存和用户权限信息之类的 所以占用了一些内存。我们可以把数据库连接池是看做是维护的数据库连接的缓存，以便将来需要对数据库的请求时可以重用这些连接。为每个用户打开和维护数据库连接，尤其是对动态数据库驱动的网站应用程序的请求，既昂贵又浪费资源。在连接池中，创建连接后，将其放置在池中，并再次使用它，因此不必建立新的连接。如果使用了所有连接，则会建立一个新连接并将其添加到池中。 连接池还减少了用户必须等待建立与数据库的连接的时间。 分库分表之后,id 主键如何处理？ 因为要是分成多个表之后，每个表都是从 1 开始累加，这样是不对的，我们需要一个全局唯一的 id 来支持。 生成全局 id 有下面这几种方式： UUID：不适合作为主键，因为太长了，并且无序不可读，查询效率低。比较适合用于生成唯一的名字的标示比如文件的名字。 数据库自增 id : 两台数据库分别设置不同步长，生成不重复ID的策略来实现高可用。这种方式生成的 id 有序，但是需要独立部署数据库实例，成本高，还会有性能瓶颈。 利用 redis 生成 id : 性能比较好，灵活方便，不依赖于数据库。但是，引入了新的组件造成系统更加复杂，可用性降低，编码更加复杂，增加了系统成本。 Twitter的snowflake算法 ：Github 地址：https://github.com/twitter-archive/snowflake。 美团的Leaf分布式ID生成系统 ：Leaf 是美团开源的分布式ID生成器，能保证全局唯一性、趋势递增、单调递增、信息安全，里面也提到了几种分布式方案的对比，但也需要依赖关系数据库、Zookeeper等中间件。感觉还不错。美团技术团队的一篇文章：https://tech.meituan.com/2017/04/21/mt-leaf.html 。 ...... 一条SQL语句在MySQL中如何执行的 一条SQL语句在MySQL中如何执行的 MySQL高性能优化规范建议 MySQL高性能优化规范建议 一条SQL语句执行得很慢的原因有哪些？ 腾讯面试：一条SQL语句执行得很慢的原因有哪些？---不看后悔系列 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \"Java面试突击\" 即可免费领取！ Java工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/database/MySQL高性能优化规范建议.html":{"url":"zother6-JavaGuide/database/MySQL高性能优化规范建议.html","title":"MySQL高性能优化规范建议","keywords":"","body":" 作者: 听风，原文地址: https://www.cnblogs.com/huchong/p/10219318.html。JavaGuide 已获得作者授权。 数据库命令规范 数据库基本设计规范 1. 所有表必须使用 Innodb 存储引擎 2. 数据库和表的字符集统一使用 UTF8 3. 所有表和字段都需要添加注释 4. 尽量控制单表数据量的大小,建议控制在 500 万以内。 5. 谨慎使用 MySQL 分区表 6.尽量做到冷热数据分离,减小表的宽度 7. 禁止在表中建立预留字段 8. 禁止在数据库中存储图片,文件等大的二进制数据 9. 禁止在线上做数据库压力测试 10. 禁止从开发环境,测试环境直接连接生成环境数据库 数据库字段设计规范 1. 优先选择符合存储需要的最小的数据类型 2. 避免使用 TEXT,BLOB 数据类型，最常见的 TEXT 类型可以存储 64k 的数据 3. 避免使用 ENUM 类型 4. 尽可能把所有列定义为 NOT NULL 5. 使用 TIMESTAMP(4 个字节) 或 DATETIME 类型 (8 个字节) 存储时间 6. 同财务相关的金额类数据必须使用 decimal 类型 索引设计规范 1. 限制每张表上的索引数量,建议单张表索引不超过 5 个 2. 禁止给表中的每一列都建立单独的索引 3. 每个 Innodb 表必须有个主键 4. 常见索引列建议 5.如何选择索引列的顺序 6. 避免建立冗余索引和重复索引（增加了查询优化器生成执行计划的时间） 7. 对于频繁的查询优先考虑使用覆盖索引 8.索引 SET 规范 数据库 SQL 开发规范 1. 建议使用预编译语句进行数据库操作 2. 避免数据类型的隐式转换 3. 充分利用表上已经存在的索引 4. 数据库设计时，应该要对以后扩展进行考虑 5. 程序连接不同的数据库使用不同的账号，禁止跨库查询 6. 禁止使用 SELECT * 必须使用 SELECT 查询 7. 禁止使用不含字段列表的 INSERT 语句 8. 避免使用子查询，可以把子查询优化为 join 操作 9. 避免使用 JOIN 关联太多的表 10. 减少同数据库的交互次数 11. 对应同一列进行 or 判断时，使用 in 代替 or 12. 禁止使用 order by rand() 进行随机排序 13. WHERE 从句中禁止对列进行函数转换和计算 14. 在明显不会有重复值时使用 UNION ALL 而不是 UNION 15. 拆分复杂的大 SQL 为多个小 SQL 数据库操作行为规范 1. 超 100 万行的批量写 (UPDATE,DELETE,INSERT) 操作,要分批多次进行操作 2. 对于大表使用 pt-online-schema-change 修改表结构 3. 禁止为程序使用的账号赋予 super 权限 4. 对于程序连接数据库账号,遵循权限最小原则 数据库命令规范 所有数据库对象名称必须使用小写字母并用下划线分割 所有数据库对象名称禁止使用 MySQL 保留关键字（如果表名中包含关键字查询时，需要将其用单引号括起来） 数据库对象的命名要能做到见名识意，并且最后不要超过 32 个字符 临时库表必须以 tmp为前缀并以日期为后缀，备份表必须以 bak为前缀并以日期 (时间戳) 为后缀 所有存储相同数据的列名和列类型必须一致（一般作为关联列，如果查询时关联列类型不一致会自动进行数据类型隐式转换，会造成列上的索引失效，导致查询效率降低） 数据库基本设计规范 1. 所有表必须使用 Innodb 存储引擎 没有特殊要求（即 Innodb 无法满足的功能如：列存储，存储空间数据等）的情况下，所有表必须使用 Innodb 存储引擎（MySQL5.5 之前默认使用 Myisam，5.6 以后默认的为 Innodb）。 Innodb 支持事务，支持行级锁，更好的恢复性，高并发下性能更好。 2. 数据库和表的字符集统一使用 UTF8 兼容性更好，统一字符集可以避免由于字符集转换产生的乱码，不同的字符集进行比较前需要进行转换会造成索引失效，如果数据库中有存储 emoji 表情的需要，字符集需要采用 utf8mb4 字符集。 3. 所有表和字段都需要添加注释 使用 comment 从句添加表和列的备注，从一开始就进行数据字典的维护 4. 尽量控制单表数据量的大小,建议控制在 500 万以内。 500 万并不是 MySQL 数据库的限制，过大会造成修改表结构，备份，恢复都会有很大的问题。 可以用历史数据归档（应用于日志数据），分库分表（应用于业务数据）等手段来控制数据量大小 5. 谨慎使用 MySQL 分区表 分区表在物理上表现为多个文件，在逻辑上表现为一个表； 谨慎选择分区键，跨分区查询效率可能更低； 建议采用物理分表的方式管理大数据。 6.尽量做到冷热数据分离,减小表的宽度 MySQL 限制每个表最多存储 4096 列，并且每一行数据的大小不能超过 65535 字节。 减少磁盘 IO,保证热数据的内存缓存命中率（表越宽，把表装载进内存缓冲池时所占用的内存也就越大,也会消耗更多的 IO）； 更有效的利用缓存，避免读入无用的冷数据； 经常一起使用的列放到一个表中（避免更多的关联操作）。 7. 禁止在表中建立预留字段 预留字段的命名很难做到见名识义。 预留字段无法确认存储的数据类型，所以无法选择合适的类型。 对预留字段类型的修改，会对表进行锁定。 8. 禁止在数据库中存储图片,文件等大的二进制数据 通常文件很大，会短时间内造成数据量快速增长，数据库进行数据库读取时，通常会进行大量的随机 IO 操作，文件很大时，IO 操作很耗时。 通常存储于文件服务器，数据库只存储文件地址信息 9. 禁止在线上做数据库压力测试 10. 禁止从开发环境,测试环境直接连接生产环境数据库 数据库字段设计规范 1. 优先选择符合存储需要的最小的数据类型 原因： 列的字段越大，建立索引时所需要的空间也就越大，这样一页中所能存储的索引节点的数量也就越少也越少，在遍历时所需要的 IO 次数也就越多，索引的性能也就越差。 方法： a.将字符串转换成数字类型存储,如:将 IP 地址转换成整形数据 MySQL 提供了两个方法来处理 ip 地址 inet_aton 把 ip 转为无符号整型 (4-8 位) inet_ntoa 把整型的 ip 转为地址 插入数据前，先用 inet_aton 把 ip 地址转为整型，可以节省空间，显示数据时，使用 inet_ntoa 把整型的 ip 地址转为地址显示即可。 b.对于非负型的数据 (如自增 ID,整型 IP) 来说,要优先使用无符号整型来存储 原因： 无符号相对于有符号可以多出一倍的存储空间 SIGNED INT -2147483648~2147483647 UNSIGNED INT 0~4294967295 VARCHAR(N) 中的 N 代表的是字符数，而不是字节数，使用 UTF8 存储 255 个汉字 Varchar(255)=765 个字节。过大的长度会消耗更多的内存。 2. 避免使用 TEXT,BLOB 数据类型，最常见的 TEXT 类型可以存储 64k 的数据 a. 建议把 BLOB 或是 TEXT 列分离到单独的扩展表中 MySQL 内存临时表不支持 TEXT、BLOB 这样的大数据类型，如果查询中包含这样的数据，在排序等操作时，就不能使用内存临时表，必须使用磁盘临时表进行。而且对于这种数据，MySQL 还是要进行二次查询，会使 sql 性能变得很差，但是不是说一定不能使用这样的数据类型。 如果一定要使用，建议把 BLOB 或是 TEXT 列分离到单独的扩展表中，查询时一定不要使用 select * 而只需要取出必要的列，不需要 TEXT 列的数据时不要对该列进行查询。 2、TEXT 或 BLOB 类型只能使用前缀索引 因为MySQL 对索引字段长度是有限制的，所以 TEXT 类型只能使用前缀索引，并且 TEXT 列上是不能有默认值的 3. 避免使用 ENUM 类型 修改 ENUM 值需要使用 ALTER 语句 ENUM 类型的 ORDER BY 操作效率低，需要额外操作 禁止使用数值作为 ENUM 的枚举值 4. 尽可能把所有列定义为 NOT NULL 原因： 索引 NULL 列需要额外的空间来保存，所以要占用更多的空间 进行比较和计算时要对 NULL 值做特别的处理 5. 使用 TIMESTAMP(4 个字节) 或 DATETIME 类型 (8 个字节) 存储时间 TIMESTAMP 存储的时间范围 1970-01-01 00:00:01 ~ 2038-01-19-03:14:07 TIMESTAMP 占用 4 字节和 INT 相同，但比 INT 可读性高 超出 TIMESTAMP 取值范围的使用 DATETIME 类型存储 经常会有人用字符串存储日期型的数据（不正确的做法） 缺点 1：无法用日期函数进行计算和比较 缺点 2：用字符串存储日期要占用更多的空间 6. 同财务相关的金额类数据必须使用 decimal 类型 非精准浮点：float,double 精准浮点：decimal Decimal 类型为精准浮点数，在计算时不会丢失精度 占用空间由定义的宽度决定，每 4 个字节可以存储 9 位数字，并且小数点要占用一个字节 可用于存储比 bigint 更大的整型数据 索引设计规范 1. 限制每张表上的索引数量,建议单张表索引不超过 5 个 索引并不是越多越好！索引可以提高效率同样可以降低效率。 索引可以增加查询效率，但同样也会降低插入和更新的效率，甚至有些情况下会降低查询效率。 因为 MySQL 优化器在选择如何优化查询时，会根据统一信息，对每一个可以用到的索引来进行评估，以生成出一个最好的执行计划，如果同时有很多个索引都可以用于查询，就会增加 MySQL 优化器生成执行计划的时间，同样会降低查询性能。 2. 禁止给表中的每一列都建立单独的索引 5.6 版本之前，一个 sql 只能使用到一个表中的一个索引，5.6 以后，虽然有了合并索引的优化方式，但是还是远远没有使用一个联合索引的查询方式好。 3. 每个 Innodb 表必须有个主键 Innodb 是一种索引组织表：数据的存储的逻辑顺序和索引的顺序是相同的。每个表都可以有多个索引，但是表的存储顺序只能有一种。 Innodb 是按照主键索引的顺序来组织表的 不要使用更新频繁的列作为主键，不适用多列主键（相当于联合索引） 不要使用 UUID,MD5,HASH,字符串列作为主键（无法保证数据的顺序增长） 主键建议使用自增 ID 值 4. 常见索引列建议 出现在 SELECT、UPDATE、DELETE 语句的 WHERE 从句中的列 包含在 ORDER BY、GROUP BY、DISTINCT 中的字段 并不要将符合 1 和 2 中的字段的列都建立一个索引， 通常将 1、2 中的字段建立联合索引效果更好 多表 join 的关联列 5.如何选择索引列的顺序 建立索引的目的是：希望通过索引进行数据查找，减少随机 IO，增加查询性能 ，索引能过滤出越少的数据，则从磁盘中读入的数据也就越少。 区分度最高的放在联合索引的最左侧（区分度=列中不同值的数量/列的总行数） 尽量把字段长度小的列放在联合索引的最左侧（因为字段长度越小，一页能存储的数据量越大，IO 性能也就越好） 使用最频繁的列放到联合索引的左侧（这样可以比较少的建立一些索引） 6. 避免建立冗余索引和重复索引（增加了查询优化器生成执行计划的时间） 重复索引示例：primary key(id)、index(id)、unique index(id) 冗余索引示例：index(a,b,c)、index(a,b)、index(a) 7. 对于频繁的查询优先考虑使用覆盖索引 覆盖索引：就是包含了所有查询字段 (where,select,ordery by,group by 包含的字段) 的索引 覆盖索引的好处： 避免 Innodb 表进行索引的二次查询: Innodb 是以聚集索引的顺序来存储的，对于 Innodb 来说，二级索引在叶子节点中所保存的是行的主键信息，如果是用二级索引查询数据的话，在查找到相应的键值后，还要通过主键进行二次查询才能获取我们真实所需要的数据。而在覆盖索引中，二级索引的键值中可以获取所有的数据，避免了对主键的二次查询 ，减少了 IO 操作，提升了查询效率。 可以把随机 IO 变成顺序 IO 加快查询效率: 由于覆盖索引是按键值的顺序存储的，对于 IO 密集型的范围查找来说，对比随机从磁盘读取每一行的数据 IO 要少的多，因此利用覆盖索引在访问时也可以把磁盘的随机读取的 IO 转变成索引查找的顺序 IO。 8.索引 SET 规范 尽量避免使用外键约束 不建议使用外键约束（foreign key），但一定要在表与表之间的关联键上建立索引 外键可用于保证数据的参照完整性，但建议在业务端实现 外键会影响父表和子表的写操作从而降低性能 数据库 SQL 开发规范 1. 建议使用预编译语句进行数据库操作 预编译语句可以重复使用这些计划，减少 SQL 编译所需要的时间，还可以解决动态 SQL 所带来的 SQL 注入的问题。 只传参数，比传递 SQL 语句更高效。 相同语句可以一次解析，多次使用，提高处理效率。 2. 避免数据类型的隐式转换 隐式转换会导致索引失效如: select name,phone from customer where id = '111'; 3. 充分利用表上已经存在的索引 避免使用双%号的查询条件。如：a like '%123%'，（如果无前置%,只有后置%，是可以用到列上的索引的） 一个 SQL 只能利用到复合索引中的一列进行范围查询。如：有 a,b,c 列的联合索引，在查询条件中有 a 列的范围查询，则在 b,c 列上的索引将不会被用到。 在定义联合索引时，如果 a 列要用到范围查找的话，就要把 a 列放到联合索引的右侧，使用 left join 或 not exists 来优化 not in 操作，因为 not in 也通常会使用索引失效。 4. 数据库设计时，应该要对以后扩展进行考虑 5. 程序连接不同的数据库使用不同的账号，禁止跨库查询 为数据库迁移和分库分表留出余地 降低业务耦合度 避免权限过大而产生的安全风险 6. 禁止使用 SELECT * 必须使用 SELECT 查询 原因： 消耗更多的 CPU 和 IO 以网络带宽资源 无法使用覆盖索引 可减少表结构变更带来的影响 7. 禁止使用不含字段列表的 INSERT 语句 如： insert into values ('a','b','c'); 应使用： insert into t(c1,c2,c3) values ('a','b','c'); 8. 避免使用子查询，可以把子查询优化为 join 操作 通常子查询在 in 子句中，且子查询中为简单 SQL(不包含 union、group by、order by、limit 从句) 时,才可以把子查询转化为关联查询进行优化。 子查询性能差的原因： 子查询的结果集无法使用索引，通常子查询的结果集会被存储到临时表中，不论是内存临时表还是磁盘临时表都不会存在索引，所以查询性能会受到一定的影响。特别是对于返回结果集比较大的子查询，其对查询性能的影响也就越大。 由于子查询会产生大量的临时表也没有索引，所以会消耗过多的 CPU 和 IO 资源，产生大量的慢查询。 9. 避免使用 JOIN 关联太多的表 对于 MySQL 来说，是存在关联缓存的，缓存的大小可以由 join_buffer_size 参数进行设置。 在 MySQL 中，对于同一个 SQL 多关联（join）一个表，就会多分配一个关联缓存，如果在一个 SQL 中关联的表越多，所占用的内存也就越大。 如果程序中大量的使用了多表关联的操作，同时 join_buffer_size 设置的也不合理的情况下，就容易造成服务器内存溢出的情况，就会影响到服务器数据库性能的稳定性。 同时对于关联操作来说，会产生临时表操作，影响查询效率，MySQL 最多允许关联 61 个表，建议不超过 5 个。 10. 减少同数据库的交互次数 数据库更适合处理批量操作，合并多个相同的操作到一起，可以提高处理效率。 11. 对应同一列进行 or 判断时，使用 in 代替 or in 的值不要超过 500 个，in 操作可以更有效的利用索引，or 大多数情况下很少能利用到索引。 12. 禁止使用 order by rand() 进行随机排序 order by rand() 会把表中所有符合条件的数据装载到内存中，然后在内存中对所有数据根据随机生成的值进行排序，并且可能会对每一行都生成一个随机值，如果满足条件的数据集非常大，就会消耗大量的 CPU 和 IO 及内存资源。 推荐在程序中获取一个随机值，然后从数据库中获取数据的方式。 13. WHERE 从句中禁止对列进行函数转换和计算 对列进行函数转换或计算时会导致无法使用索引 不推荐： where date(create_time)='20190101' 推荐： where create_time >= '20190101' and create_time 14. 在明显不会有重复值时使用 UNION ALL 而不是 UNION UNION 会把两个结果集的所有数据放到临时表中后再进行去重操作 UNION ALL 不会再对结果集进行去重操作 15. 拆分复杂的大 SQL 为多个小 SQL 大 SQL 逻辑上比较复杂，需要占用大量 CPU 进行计算的 SQL MySQL 中，一个 SQL 只能使用一个 CPU 进行计算 SQL 拆分后可以通过并行执行来提高处理效率 数据库操作行为规范 1. 超 100 万行的批量写 (UPDATE,DELETE,INSERT) 操作,要分批多次进行操作 大批量操作可能会造成严重的主从延迟 主从环境中,大批量操作可能会造成严重的主从延迟，大批量的写操作一般都需要执行一定长的时间， 而只有当主库上执行完成后，才会在其他从库上执行，所以会造成主库与从库长时间的延迟情况 binlog 日志为 row 格式时会产生大量的日志 大批量写操作会产生大量日志，特别是对于 row 格式二进制数据而言，由于在 row 格式中会记录每一行数据的修改，我们一次修改的数据越多，产生的日志量也就会越多，日志的传输和恢复所需要的时间也就越长，这也是造成主从延迟的一个原因 避免产生大事务操作 大批量修改数据，一定是在一个事务中进行的，这就会造成表中大批量数据进行锁定，从而导致大量的阻塞，阻塞会对 MySQL 的性能产生非常大的影响。 特别是长时间的阻塞会占满所有数据库的可用连接，这会使生产环境中的其他应用无法连接到数据库，因此一定要注意大批量写操作要进行分批 2. 对于大表使用 pt-online-schema-change 修改表结构 避免大表修改产生的主从延迟 避免在对表字段进行修改时进行锁表 对大表数据结构的修改一定要谨慎，会造成严重的锁表操作，尤其是生产环境，是不能容忍的。 pt-online-schema-change 它会首先建立一个与原表结构相同的新表，并且在新表上进行表结构的修改，然后再把原表中的数据复制到新表中，并在原表中增加一些触发器。把原表中新增的数据也复制到新表中，在行所有数据复制完成之后，把新表命名成原表，并把原来的表删除掉。把原来一个 DDL 操作，分解成多个小的批次进行。 3. 禁止为程序使用的账号赋予 super 权限 当达到最大连接数限制时，还运行 1 个有 super 权限的用户连接 super 权限只能留给 DBA 处理问题的账号使用 4. 对于程序连接数据库账号,遵循权限最小原则 程序使用数据库账号只能在一个 DB 下使用，不准跨库 程序使用的账号原则上不准有 drop 权限 "},"zother6-JavaGuide/database/一千行MySQL命令.html":{"url":"zother6-JavaGuide/database/一千行MySQL命令.html","title":"一千行MySQL命令","keywords":"","body":" 原文地址：https://shockerli.net/post/1000-line-mysql-note/ ，JavaGuide 对本文进行了简答排版，新增了目录。 作者：格物 非常不错的总结，强烈建议保存下来，需要的时候看一看。 基本操作 数据库操作 表的操作 数据操作 字符集编码 数据类型(列类型) 列属性(列约束) 建表规范 SELECT UNION 子查询 连接查询(join) TRUNCATE 备份与还原 视图 事务(transaction) 锁表 触发器 SQL编程 存储过程 用户和权限管理 表维护 杂项 基本操作 /* Windows服务 */ -- 启动MySQL net start mysql -- 创建Windows服务 sc create mysql binPath= mysqld_bin_path(注意：等号与值之间有空格) /* 连接与断开服务器 */ mysql -h 地址 -P 端口 -u 用户名 -p 密码 SHOW PROCESSLIST -- 显示哪些线程正在运行 SHOW VARIABLES -- 显示系统变量信息 数据库操作 /* 数据库操作 */ ------------------ -- 查看当前数据库 SELECT DATABASE(); -- 显示当前时间、用户名、数据库版本 SELECT now(), user(), version(); -- 创建库 CREATE DATABASE[ IF NOT EXISTS] 数据库名 数据库选项 数据库选项： CHARACTER SET charset_name COLLATE collation_name -- 查看已有库 SHOW DATABASES[ LIKE 'PATTERN'] -- 查看当前库信息 SHOW CREATE DATABASE 数据库名 -- 修改库的选项信息 ALTER DATABASE 库名 选项信息 -- 删除库 DROP DATABASE[ IF EXISTS] 数据库名 同时删除该数据库相关的目录及其目录内容 表的操作 -- 创建表 CREATE [TEMPORARY] TABLE[ IF NOT EXISTS] [库名.]表名 ( 表的结构定义 )[ 表选项] 每个字段必须有数据类型 最后一个字段后不能有逗号 TEMPORARY 临时表，会话结束时表自动消失 对于字段的定义： 字段名 数据类型 [NOT NULL | NULL] [DEFAULT default_value] [AUTO_INCREMENT] [UNIQUE [KEY] | [PRIMARY] KEY] [COMMENT 'string'] -- 表选项 -- 字符集 CHARSET = charset_name 如果表没有设定，则使用数据库字符集 -- 存储引擎 ENGINE = engine_name 表在管理数据时采用的不同的数据结构，结构不同会导致处理方式、提供的特性操作等不同 常见的引擎：InnoDB MyISAM Memory/Heap BDB Merge Example CSV MaxDB Archive 不同的引擎在保存表的结构和数据时采用不同的方式 MyISAM表文件含义：.frm表定义，.MYD表数据，.MYI表索引 InnoDB表文件含义：.frm表定义，表空间数据和日志文件 SHOW ENGINES -- 显示存储引擎的状态信息 SHOW ENGINE 引擎名 {LOGS|STATUS} -- 显示存储引擎的日志或状态信息 -- 自增起始数 AUTO_INCREMENT = 行数 -- 数据文件目录 DATA DIRECTORY = '目录' -- 索引文件目录 INDEX DIRECTORY = '目录' -- 表注释 COMMENT = 'string' -- 分区选项 PARTITION BY ... (详细见手册) -- 查看所有表 SHOW TABLES[ LIKE 'pattern'] SHOW TABLES FROM 库名 -- 查看表结构 SHOW CREATE TABLE 表名 （信息更详细） DESC 表名 / DESCRIBE 表名 / EXPLAIN 表名 / SHOW COLUMNS FROM 表名 [LIKE 'PATTERN'] SHOW TABLE STATUS [FROM db_name] [LIKE 'pattern'] -- 修改表 -- 修改表本身的选项 ALTER TABLE 表名 表的选项 eg: ALTER TABLE 表名 ENGINE=MYISAM; -- 对表进行重命名 RENAME TABLE 原表名 TO 新表名 RENAME TABLE 原表名 TO 库名.表名 （可将表移动到另一个数据库） -- RENAME可以交换两个表名 -- 修改表的字段机构（13.1.2. ALTER TABLE语法） ALTER TABLE 表名 操作名 -- 操作名 ADD[ COLUMN] 字段定义 -- 增加字段 AFTER 字段名 -- 表示增加在该字段名后面 FIRST -- 表示增加在第一个 ADD PRIMARY KEY(字段名) -- 创建主键 ADD UNIQUE [索引名] (字段名)-- 创建唯一索引 ADD INDEX [索引名] (字段名) -- 创建普通索引 DROP[ COLUMN] 字段名 -- 删除字段 MODIFY[ COLUMN] 字段名 字段属性 -- 支持对字段属性进行修改，不能修改字段名(所有原有属性也需写上) CHANGE[ COLUMN] 原字段名 新字段名 字段属性 -- 支持对字段名修改 DROP PRIMARY KEY -- 删除主键(删除主键前需删除其AUTO_INCREMENT属性) DROP INDEX 索引名 -- 删除索引 DROP FOREIGN KEY 外键 -- 删除外键 -- 删除表 DROP TABLE[ IF EXISTS] 表名 ... -- 清空表数据 TRUNCATE [TABLE] 表名 -- 复制表结构 CREATE TABLE 表名 LIKE 要复制的表名 -- 复制表结构和数据 CREATE TABLE 表名 [AS] SELECT * FROM 要复制的表名 -- 检查表是否有错误 CHECK TABLE tbl_name [, tbl_name] ... [option] ... -- 优化表 OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... -- 修复表 REPAIR [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... [QUICK] [EXTENDED] [USE_FRM] -- 分析表 ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... 数据操作 /* 数据操作 */ ------------------ -- 增 INSERT [INTO] 表名 [(字段列表)] VALUES (值列表)[, (值列表), ...] -- 如果要插入的值列表包含所有字段并且顺序一致，则可以省略字段列表。 -- 可同时插入多条数据记录！ REPLACE 与 INSERT 完全一样，可互换。 INSERT [INTO] 表名 SET 字段名=值[, 字段名=值, ...] -- 查 SELECT 字段列表 FROM 表名[ 其他子句] -- 可来自多个表的多个字段 -- 其他子句可以不使用 -- 字段列表可以用*代替，表示所有字段 -- 删 DELETE FROM 表名[ 删除条件子句] 没有条件子句，则会删除全部 -- 改 UPDATE 表名 SET 字段名=新值[, 字段名=新值] [更新条件] 字符集编码 /* 字符集编码 */ ------------------ -- MySQL、数据库、表、字段均可设置编码 -- 数据编码与客户端编码不需一致 SHOW VARIABLES LIKE 'character_set_%' -- 查看所有字符集编码项 character_set_client 客户端向服务器发送数据时使用的编码 character_set_results 服务器端将结果返回给客户端所使用的编码 character_set_connection 连接层编码 SET 变量名 = 变量值 SET character_set_client = gbk; SET character_set_results = gbk; SET character_set_connection = gbk; SET NAMES GBK; -- 相当于完成以上三个设置 -- 校对集 校对集用以排序 SHOW CHARACTER SET [LIKE 'pattern']/SHOW CHARSET [LIKE 'pattern'] 查看所有字符集 SHOW COLLATION [LIKE 'pattern'] 查看所有校对集 CHARSET 字符集编码 设置字符集编码 COLLATE 校对集编码 设置校对集编码 数据类型(列类型) /* 数据类型（列类型） */ ------------------ 1. 数值类型 -- a. 整型 ---------- 类型 字节 范围（有符号位） tinyint 1字节 -128 ~ 127 无符号位：0 ~ 255 smallint 2字节 -32768 ~ 32767 mediumint 3字节 -8388608 ~ 8388607 int 4字节 bigint 8字节 int(M) M表示总位数 - 默认存在符号位，unsigned 属性修改 - 显示宽度，如果某个数不够定义字段时设置的位数，则前面以0补填，zerofill 属性修改 例：int(5) 插入一个数'123'，补填后为'00123' - 在满足要求的情况下，越小越好。 - 1表示bool值真，0表示bool值假。MySQL没有布尔类型，通过整型0和1表示。常用tinyint(1)表示布尔型。 -- b. 浮点型 ---------- 类型 字节 范围 float(单精度) 4字节 double(双精度) 8字节 浮点型既支持符号位 unsigned 属性，也支持显示宽度 zerofill 属性。 不同于整型，前后均会补填0. 定义浮点型时，需指定总位数和小数位数。 float(M, D) double(M, D) M表示总位数，D表示小数位数。 M和D的大小会决定浮点数的范围。不同于整型的固定范围。 M既表示总位数（不包括小数点和正负号），也表示显示宽度（所有显示符号均包括）。 支持科学计数法表示。 浮点数表示近似值。 -- c. 定点数 ---------- decimal -- 可变长度 decimal(M, D) M也表示总位数，D表示小数位数。 保存一个精确的数值，不会发生数据的改变，不同于浮点数的四舍五入。 将浮点数转换为字符串来保存，每9位数字保存为4个字节。 2. 字符串类型 -- a. char, varchar ---------- char 定长字符串，速度快，但浪费空间 varchar 变长字符串，速度慢，但节省空间 M表示能存储的最大长度，此长度是字符数，非字节数。 不同的编码，所占用的空间不同。 char,最多255个字符，与编码无关。 varchar,最多65535字符，与编码有关。 一条有效记录最大不能超过65535个字节。 utf8 最大为21844个字符，gbk 最大为32766个字符，latin1 最大为65532个字符 varchar 是变长的，需要利用存储空间保存 varchar 的长度，如果数据小于255个字节，则采用一个字节来保存长度，反之需要两个字节来保存。 varchar 的最大有效长度由最大行大小和使用的字符集确定。 最大有效长度是65532字节，因为在varchar存字符串时，第一个字节是空的，不存在任何数据，然后还需两个字节来存放字符串的长度，所以有效长度是65535-1-2=65532字节。 例：若一个表定义为 CREATE TABLE tb(c1 int, c2 char(30), c3 varchar(N)) charset=utf8; 问N的最大值是多少？ 答：(65535-1-2-4-30*3)/3 -- b. blob, text ---------- blob 二进制字符串（字节字符串） tinyblob, blob, mediumblob, longblob text 非二进制字符串（字符字符串） tinytext, text, mediumtext, longtext text 在定义时，不需要定义长度，也不会计算总长度。 text 类型在定义时，不可给default值 -- c. binary, varbinary ---------- 类似于char和varchar，用于保存二进制字符串，也就是保存字节字符串而非字符字符串。 char, varchar, text 对应 binary, varbinary, blob. 3. 日期时间类型 一般用整型保存时间戳，因为PHP可以很方便的将时间戳进行格式化。 datetime 8字节 日期及时间 1000-01-01 00:00:00 到 9999-12-31 23:59:59 date 3字节 日期 1000-01-01 到 9999-12-31 timestamp 4字节 时间戳 19700101000000 到 2038-01-19 03:14:07 time 3字节 时间 -838:59:59 到 838:59:59 year 1字节 年份 1901 - 2155 datetime YYYY-MM-DD hh:mm:ss timestamp YY-MM-DD hh:mm:ss YYYYMMDDhhmmss YYMMDDhhmmss YYYYMMDDhhmmss YYMMDDhhmmss date YYYY-MM-DD YY-MM-DD YYYYMMDD YYMMDD YYYYMMDD YYMMDD time hh:mm:ss hhmmss hhmmss year YYYY YY YYYY YY 4. 枚举和集合 -- 枚举(enum) ---------- enum(val1, val2, val3...) 在已知的值中进行单选。最大数量为65535. 枚举值在保存时，以2个字节的整型(smallint)保存。每个枚举值，按保存的位置顺序，从1开始逐一递增。 表现为字符串类型，存储却是整型。 NULL值的索引是NULL。 空字符串错误值的索引值是0。 -- 集合（set） ---------- set(val1, val2, val3...) create table tab ( gender set('男', '女', '无') ); insert into tab values ('男, 女'); 最多可以有64个不同的成员。以bigint存储，共8个字节。采取位运算的形式。 当创建表时，SET成员值的尾部空格将自动被删除。 列属性(列约束) /* 列属性（列约束） */ ------------------ 1. PRIMARY 主键 - 能唯一标识记录的字段，可以作为主键。 - 一个表只能有一个主键。 - 主键具有唯一性。 - 声明字段时，用 primary key 标识。 也可以在字段列表之后声明 例：create table tab ( id int, stu varchar(10), primary key (id)); - 主键字段的值不能为null。 - 主键可以由多个字段共同组成。此时需要在字段列表后声明的方法。 例：create table tab ( id int, stu varchar(10), age int, primary key (stu, age)); 2. UNIQUE 唯一索引（唯一约束） 使得某字段的值也不能重复。 3. NULL 约束 null不是数据类型，是列的一个属性。 表示当前列是否可以为null，表示什么都没有。 null, 允许为空。默认。 not null, 不允许为空。 insert into tab values (null, 'val'); -- 此时表示将第一个字段的值设为null, 取决于该字段是否允许为null 4. DEFAULT 默认值属性 当前字段的默认值。 insert into tab values (default, 'val'); -- 此时表示强制使用默认值。 create table tab ( add_time timestamp default current_timestamp ); -- 表示将当前时间的时间戳设为默认值。 current_date, current_time 5. AUTO_INCREMENT 自动增长约束 自动增长必须为索引（主键或unique） 只能存在一个字段为自动增长。 默认为1开始自动增长。可以通过表属性 auto_increment = x进行设置，或 alter table tbl auto_increment = x; 6. COMMENT 注释 例：create table tab ( id int ) comment '注释内容'; 7. FOREIGN KEY 外键约束 用于限制主表与从表数据完整性。 alter table t1 add constraint `t1_t2_fk` foreign key (t1_id) references t2(id); -- 将表t1的t1_id外键关联到表t2的id字段。 -- 每个外键都有一个名字，可以通过 constraint 指定 存在外键的表，称之为从表（子表），外键指向的表，称之为主表（父表）。 作用：保持数据一致性，完整性，主要目的是控制存储在外键表（从表）中的数据。 MySQL中，可以对InnoDB引擎使用外键约束： 语法： foreign key (外键字段） references 主表名 (关联字段) [主表记录删除时的动作] [主表记录更新时的动作] 此时需要检测一个从表的外键需要约束为主表的已存在的值。外键在没有关联的情况下，可以设置为null.前提是该外键列，没有not null。 可以不指定主表记录更改或更新时的动作，那么此时主表的操作被拒绝。 如果指定了 on update 或 on delete：在删除或更新时，有如下几个操作可以选择： 1. cascade，级联操作。主表数据被更新（主键值更新），从表也被更新（外键值更新）。主表记录被删除，从表相关记录也被删除。 2. set null，设置为null。主表数据被更新（主键值更新），从表的外键被设置为null。主表记录被删除，从表相关记录外键被设置成null。但注意，要求该外键列，没有not null属性约束。 3. restrict，拒绝父表删除和更新。 注意，外键只被InnoDB存储引擎所支持。其他引擎是不支持的。 建表规范 /* 建表规范 */ ------------------ -- Normal Format, NF - 每个表保存一个实体信息 - 每个具有一个ID字段作为主键 - ID主键 + 原子表 -- 1NF, 第一范式 字段不能再分，就满足第一范式。 -- 2NF, 第二范式 满足第一范式的前提下，不能出现部分依赖。 消除复合主键就可以避免部分依赖。增加单列关键字。 -- 3NF, 第三范式 满足第二范式的前提下，不能出现传递依赖。 某个字段依赖于主键，而有其他字段依赖于该字段。这就是传递依赖。 将一个实体信息的数据放在一个表内实现。 SELECT /* SELECT */ ------------------ SELECT [ALL|DISTINCT] select_expr FROM -> WHERE -> GROUP BY [合计函数] -> HAVING -> ORDER BY -> LIMIT a. select_expr -- 可以用 * 表示所有字段。 select * from tb; -- 可以使用表达式（计算公式、函数调用、字段也是个表达式） select stu, 29+25, now() from tb; -- 可以为每个列使用别名。适用于简化列标识，避免多个列标识符重复。 - 使用 as 关键字，也可省略 as. select stu+10 as add10 from tb; b. FROM 子句 用于标识查询来源。 -- 可以为表起别名。使用as关键字。 SELECT * FROM tb1 AS tt, tb2 AS bb; -- from子句后，可以同时出现多个表。 -- 多个表会横向叠加到一起，而数据会形成一个笛卡尔积。 SELECT * FROM tb1, tb2; -- 向优化符提示如何选择索引 USE INDEX、IGNORE INDEX、FORCE INDEX SELECT * FROM table1 USE INDEX (key1,key2) WHERE key1=1 AND key2=2 AND key3=3; SELECT * FROM table1 IGNORE INDEX (key3) WHERE key1=1 AND key2=2 AND key3=3; c. WHERE 子句 -- 从from获得的数据源中进行筛选。 -- 整型1表示真，0表示假。 -- 表达式由运算符和运算数组成。 -- 运算数：变量（字段）、值、函数返回值 -- 运算符： =, , <>, !=, =, >, !, &&, ||, in (not) null, (not) like, (not) in, (not) between and, is (not), and, or, not, xor is/is not 加上ture/false/unknown，检验某个值的真假 与<>功能相同，可用于null比较 d. GROUP BY 子句, 分组子句 GROUP BY 字段/别名 [排序方式] 分组后会进行排序。升序：ASC，降序：DESC 以下[合计函数]需配合 GROUP BY 使用： count 返回不同的非NULL值数目 count(*)、count(字段) sum 求和 max 求最大值 min 求最小值 avg 求平均值 group_concat 返回带有来自一个组的连接的非NULL值的字符串结果。组内字符串连接。 e. HAVING 子句，条件子句 与 where 功能、用法相同，执行时机不同。 where 在开始时执行检测数据，对原数据进行过滤。 having 对筛选出的结果再次进行过滤。 having 字段必须是查询出来的，where 字段必须是数据表存在的。 where 不可以使用字段的别名，having 可以。因为执行WHERE代码时，可能尚未确定列值。 where 不可以使用合计函数。一般需用合计函数才会用 having SQL标准要求HAVING必须引用GROUP BY子句中的列或用于合计函数中的列。 f. ORDER BY 子句，排序子句 order by 排序字段/别名 排序方式 [,排序字段/别名 排序方式]... 升序：ASC，降序：DESC 支持多个字段的排序。 g. LIMIT 子句，限制结果数量子句 仅对处理好的结果进行数量限制。将处理好的结果的看作是一个集合，按照记录出现的顺序，索引从0开始。 limit 起始位置, 获取条数 省略第一个参数，表示从索引0开始。limit 获取条数 h. DISTINCT, ALL 选项 distinct 去除重复记录 默认为 all, 全部记录 UNION /* UNION */ ------------------ 将多个select查询的结果组合成一个结果集合。 SELECT ... UNION [ALL|DISTINCT] SELECT ... 默认 DISTINCT 方式，即所有返回的行都是唯一的 建议，对每个SELECT查询加上小括号包裹。 ORDER BY 排序时，需加上 LIMIT 进行结合。 需要各select查询的字段数量一样。 每个select查询的字段列表(数量、类型)应一致，因为结果中的字段名以第一条select语句为准。 子查询 /* 子查询 */ ------------------ - 子查询需用括号包裹。 -- from型 from后要求是一个表，必须给子查询结果取个别名。 - 简化每个查询内的条件。 - from型需将结果生成一个临时表格，可用以原表的锁定的释放。 - 子查询返回一个表，表型子查询。 select * from (select * from tb where id>0) as subfrom where id>1; -- where型 - 子查询返回一个值，标量子查询。 - 不需要给子查询取别名。 - where子查询内的表，不能直接用以更新。 select * from tb where money = (select max(money) from tb); -- 列子查询 如果子查询结果返回的是一列。 使用 in 或 not in 完成查询 exists 和 not exists 条件 如果子查询返回数据，则返回1或0。常用于判断条件。 select column1 from t1 where exists (select * from t2); -- 行子查询 查询条件是一个行。 select * from t1 where (id, gender) in (select id, gender from t2); 行构造符：(col1, col2, ...) 或 ROW(col1, col2, ...) 行构造符通常用于与对能返回两个或两个以上列的子查询进行比较。 -- 特殊运算符 != all() 相当于 not in = some() 相当于 in。any 是 some 的别名 != some() 不等同于 not in，不等于其中某一个。 all, some 可以配合其他运算符一起使用。 连接查询(join) /* 连接查询(join) */ ------------------ 将多个表的字段进行连接，可以指定连接条件。 -- 内连接(inner join) - 默认就是内连接，可省略inner。 - 只有数据存在时才能发送连接。即连接结果不能出现空行。 on 表示连接条件。其条件表达式与where类似。也可以省略条件（表示条件永远为真） 也可用where表示连接条件。 还有 using, 但需字段名相同。 using(字段名) -- 交叉连接 cross join 即，没有条件的内连接。 select * from tb1 cross join tb2; -- 外连接(outer join) - 如果数据不存在，也会出现在连接结果中。 -- 左外连接 left join 如果数据不存在，左表记录会出现，而右表为null填充 -- 右外连接 right join 如果数据不存在，右表记录会出现，而左表为null填充 -- 自然连接(natural join) 自动判断连接条件完成连接。 相当于省略了using，会自动查找相同字段名。 natural join natural left join natural right join select info.id, info.name, info.stu_num, extra_info.hobby, extra_info.sex from info, extra_info where info.stu_num = extra_info.stu_id; TRUNCATE /* TRUNCATE */ ------------------ TRUNCATE [TABLE] tbl_name 清空数据 删除重建表 区别： 1，truncate 是删除表再创建，delete 是逐条删除 2，truncate 重置auto_increment的值。而delete不会 3，truncate 不知道删除了几条，而delete知道。 4，当被用于带分区的表时，truncate 会保留分区 备份与还原 /* 备份与还原 */ ------------------ 备份，将数据的结构与表内数据保存起来。 利用 mysqldump 指令完成。 -- 导出 mysqldump [options] db_name [tables] mysqldump [options] ---database DB1 [DB2 DB3...] mysqldump [options] --all--database 1. 导出一张表 　　mysqldump -u用户名 -p密码 库名 表名 > 文件名(D:/a.sql) 2. 导出多张表 　　mysqldump -u用户名 -p密码 库名 表1 表2 表3 > 文件名(D:/a.sql) 3. 导出所有表 　　mysqldump -u用户名 -p密码 库名 > 文件名(D:/a.sql) 4. 导出一个库 　　mysqldump -u用户名 -p密码 --lock-all-tables --database 库名 > 文件名(D:/a.sql) 可以-w携带WHERE条件 -- 导入 1. 在登录mysql的情况下： 　　source 备份文件 2. 在不登录的情况下 　　mysql -u用户名 -p密码 库名 视图 什么是视图： 视图是一个虚拟表，其内容由查询定义。同真实的表一样，视图包含一系列带有名称的列和行数据。但是，视图并不在数据库中以存储的数据值集形式存在。行和列数据来自由定义视图的查询所引用的表，并且在引用视图时动态生成。 视图具有表结构文件，但不存在数据文件。 对其中所引用的基础表来说，视图的作用类似于筛选。定义视图的筛选可以来自当前或其它数据库的一个或多个表，或者其它视图。通过视图进行查询没有任何限制，通过它们进行数据修改时的限制也很少。 视图是存储在数据库中的查询的sql语句，它主要出于两种原因：安全原因，视图可以隐藏一些数据，如：社会保险基金表，可以用视图只显示姓名，地址，而不显示社会保险号和工资数等，另一原因是可使复杂的查询易于理解和使用。 -- 创建视图 CREATE [OR REPLACE] [ALGORITHM = {UNDEFINED | MERGE | TEMPTABLE}] VIEW view_name [(column_list)] AS select_statement - 视图名必须唯一，同时不能与表重名。 - 视图可以使用select语句查询到的列名，也可以自己指定相应的列名。 - 可以指定视图执行的算法，通过ALGORITHM指定。 - column_list如果存在，则数目必须等于SELECT语句检索的列数 -- 查看结构 SHOW CREATE VIEW view_name -- 删除视图 - 删除视图后，数据依然存在。 - 可同时删除多个视图。 DROP VIEW [IF EXISTS] view_name ... -- 修改视图结构 - 一般不修改视图，因为不是所有的更新视图都会映射到表上。 ALTER VIEW view_name [(column_list)] AS select_statement -- 视图作用 1. 简化业务逻辑 2. 对客户端隐藏真实的表结构 -- 视图算法(ALGORITHM) MERGE 合并 将视图的查询语句，与外部查询需要先合并再执行！ TEMPTABLE 临时表 将视图执行完毕后，形成临时表，再做外层查询！ UNDEFINED 未定义(默认)，指的是MySQL自主去选择相应的算法。 事务(transaction) 事务是指逻辑上的一组操作，组成这组操作的各个单元，要不全成功要不全失败。 - 支持连续SQL的集体成功或集体撤销。 - 事务是数据库在数据完整性方面的一个功能。 - 需要利用 InnoDB 或 BDB 存储引擎，对自动提交的特性支持完成。 - InnoDB被称为事务安全型引擎。 -- 事务开启 START TRANSACTION; 或者 BEGIN; 开启事务后，所有被执行的SQL语句均被认作当前事务内的SQL语句。 -- 事务提交 COMMIT; -- 事务回滚 ROLLBACK; 如果部分操作发生问题，映射到事务开启前。 -- 事务的特性 1. 原子性（Atomicity） 事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 2. 一致性（Consistency） 事务前后数据的完整性必须保持一致。 - 事务开始和结束时，外部数据一致 - 在整个事务过程中，操作是连续的 3. 隔离性（Isolation） 多个用户并发访问数据库时，一个用户的事务不能被其它用户的事物所干扰，多个并发事务之间的数据要相互隔离。 4. 持久性（Durability） 一个事务一旦被提交，它对数据库中的数据改变就是永久性的。 -- 事务的实现 1. 要求是事务支持的表类型 2. 执行一组相关的操作前开启事务 3. 整组操作完成后，都成功，则提交；如果存在失败，选择回滚，则会回到事务开始的备份点。 -- 事务的原理 利用InnoDB的自动提交(autocommit)特性完成。 普通的MySQL执行语句后，当前的数据提交操作均可被其他客户端可见。 而事务是暂时关闭“自动提交”机制，需要commit提交持久化数据操作。 -- 注意 1. 数据定义语言（DDL）语句不能被回滚，比如创建或取消数据库的语句，和创建、取消或更改表或存储的子程序的语句。 2. 事务不能被嵌套 -- 保存点 SAVEPOINT 保存点名称 -- 设置一个事务保存点 ROLLBACK TO SAVEPOINT 保存点名称 -- 回滚到保存点 RELEASE SAVEPOINT 保存点名称 -- 删除保存点 -- InnoDB自动提交特性设置 SET autocommit = 0|1; 0表示关闭自动提交，1表示开启自动提交。 - 如果关闭了，那普通操作的结果对其他客户端也不可见，需要commit提交后才能持久化数据操作。 - 也可以关闭自动提交来开启事务。但与START TRANSACTION不同的是， SET autocommit是永久改变服务器的设置，直到下次再次修改该设置。(针对当前连接) 而START TRANSACTION记录开启前的状态，而一旦事务提交或回滚后就需要再次开启事务。(针对当前事务) 锁表 /* 锁表 */ 表锁定只用于防止其它客户端进行不正当地读取和写入 MyISAM 支持表锁，InnoDB 支持行锁 -- 锁定 LOCK TABLES tbl_name [AS alias] -- 解锁 UNLOCK TABLES 触发器 /* 触发器 */ ------------------ 触发程序是与表有关的命名数据库对象，当该表出现特定事件时，将激活该对象 监听：记录的增加、修改、删除。 -- 创建触发器 CREATE TRIGGER trigger_name trigger_time trigger_event ON tbl_name FOR EACH ROW trigger_stmt 参数： trigger_time是触发程序的动作时间。它可以是 before 或 after，以指明触发程序是在激活它的语句之前或之后触发。 trigger_event指明了激活触发程序的语句的类型 INSERT：将新行插入表时激活触发程序 UPDATE：更改某一行时激活触发程序 DELETE：从表中删除某一行时激活触发程序 tbl_name：监听的表，必须是永久性的表，不能将触发程序与TEMPORARY表或视图关联起来。 trigger_stmt：当触发程序激活时执行的语句。执行多个语句，可使用BEGIN...END复合语句结构 -- 删除 DROP TRIGGER [schema_name.]trigger_name 可以使用old和new代替旧的和新的数据 更新操作，更新前是old，更新后是new. 删除操作，只有old. 增加操作，只有new. -- 注意 1. 对于具有相同触发程序动作时间和事件的给定表，不能有两个触发程序。 -- 字符连接函数 concat(str1,str2,...]) concat_ws(separator,str1,str2,...) -- 分支语句 if 条件 then 执行语句 elseif 条件 then 执行语句 else 执行语句 end if; -- 修改最外层语句结束符 delimiter 自定义结束符号 SQL语句 自定义结束符号 delimiter ; -- 修改回原来的分号 -- 语句块包裹 begin 语句块 end -- 特殊的执行 1. 只要添加记录，就会触发程序。 2. Insert into on duplicate key update 语法会触发： 如果没有重复记录，会触发 before insert, after insert; 如果有重复记录并更新，会触发 before insert, before update, after update; 如果有重复记录但是没有发生更新，则触发 before insert, before update 3. Replace 语法 如果有记录，则执行 before insert, before delete, after delete, after insert SQL编程 /* SQL编程 */ ------------------ --// 局部变量 ---------- -- 变量声明 declare var_name[,...] type [default value] 这个语句被用来声明局部变量。要给变量提供一个默认值，请包含一个default子句。值可以被指定为一个表达式，不需要为一个常数。如果没有default子句，初始值为null。 -- 赋值 使用 set 和 select into 语句为变量赋值。 - 注意：在函数内是可以使用全局变量（用户自定义的变量） --// 全局变量 ---------- -- 定义、赋值 set 语句可以定义并为变量赋值。 set @var = value; 也可以使用select into语句为变量初始化并赋值。这样要求select语句只能返回一行，但是可以是多个字段，就意味着同时为多个变量进行赋值，变量的数量需要与查询的列数一致。 还可以把赋值语句看作一个表达式，通过select执行完成。此时为了避免=被当作关系运算符看待，使用:=代替。（set语句可以使用= 和 :=）。 select @var:=20; select @v1:=id, @v2=name from t1 limit 1; select * from tbl_name where @var:=30; select into 可以将表中查询获得的数据赋给变量。 -| select max(height) into @max_height from tb; -- 自定义变量名 为了避免select语句中，用户自定义的变量与系统标识符（通常是字段名）冲突，用户自定义变量在变量名前使用@作为开始符号。 @var=10; - 变量被定义后，在整个会话周期都有效（登录到退出） --// 控制结构 ---------- -- if语句 if search_condition then statement_list [elseif search_condition then statement_list] ... [else statement_list] end if; -- case语句 CASE value WHEN [compare-value] THEN result [WHEN [compare-value] THEN result ...] [ELSE result] END -- while循环 [begin_label:] while search_condition do statement_list end while [end_label]; - 如果需要在循环内提前终止 while循环，则需要使用标签；标签需要成对出现。 -- 退出循环 退出整个循环 leave 退出当前循环 iterate 通过退出的标签决定退出哪个循环 --// 内置函数 ---------- -- 数值函数 abs(x) -- 绝对值 abs(-10.9) = 10 format(x, d) -- 格式化千分位数值 format(1234567.456, 2) = 1,234,567.46 ceil(x) -- 向上取整 ceil(10.1) = 11 floor(x) -- 向下取整 floor (10.1) = 10 round(x) -- 四舍五入去整 mod(m, n) -- m%n m mod n 求余 10%3=1 pi() -- 获得圆周率 pow(m, n) -- m^n sqrt(x) -- 算术平方根 rand() -- 随机数 truncate(x, d) -- 截取d位小数 -- 时间日期函数 now(), current_timestamp(); -- 当前日期时间 current_date(); -- 当前日期 current_time(); -- 当前时间 date('yyyy-mm-dd hh:ii:ss'); -- 获取日期部分 time('yyyy-mm-dd hh:ii:ss'); -- 获取时间部分 date_format('yyyy-mm-dd hh:ii:ss', '%d %y %a %d %m %b %j'); -- 格式化时间 unix_timestamp(); -- 获得unix时间戳 from_unixtime(); -- 从时间戳获得时间 -- 字符串函数 length(string) -- string长度，字节 char_length(string) -- string的字符个数 substring(str, position [,length]) -- 从str的position开始,取length个字符 replace(str ,search_str ,replace_str) -- 在str中用replace_str替换search_str instr(string ,substring) -- 返回substring首次在string中出现的位置 concat(string [,...]) -- 连接字串 charset(str) -- 返回字串字符集 lcase(string) -- 转换成小写 left(string, length) -- 从string2中的左边起取length个字符 load_file(file_name) -- 从文件读取内容 locate(substring, string [,start_position]) -- 同instr,但可指定开始位置 lpad(string, length, pad) -- 重复用pad加在string开头,直到字串长度为length ltrim(string) -- 去除前端空格 repeat(string, count) -- 重复count次 rpad(string, length, pad) --在str后用pad补充,直到长度为length rtrim(string) -- 去除后端空格 strcmp(string1 ,string2) -- 逐字符比较两字串大小 -- 流程函数 case when [condition] then result [when [condition] then result ...] [else result] end 多分支 if(expr1,expr2,expr3) 双分支。 -- 聚合函数 count() sum(); max(); min(); avg(); group_concat() -- 其他常用函数 md5(); default(); --// 存储函数，自定义函数 ---------- -- 新建 CREATE FUNCTION function_name (参数列表) RETURNS 返回值类型 函数体 - 函数名，应该合法的标识符，并且不应该与已有的关键字冲突。 - 一个函数应该属于某个数据库，可以使用db_name.funciton_name的形式执行当前函数所属数据库，否则为当前数据库。 - 参数部分，由\"参数名\"和\"参数类型\"组成。多个参数用逗号隔开。 - 函数体由多条可用的mysql语句，流程控制，变量声明等语句构成。 - 多条语句应该使用 begin...end 语句块包含。 - 一定要有 return 返回值语句。 -- 删除 DROP FUNCTION [IF EXISTS] function_name; -- 查看 SHOW FUNCTION STATUS LIKE 'partten' SHOW CREATE FUNCTION function_name; -- 修改 ALTER FUNCTION function_name 函数选项 --// 存储过程，自定义功能 ---------- -- 定义 存储存储过程 是一段代码（过程），存储在数据库中的sql组成。 一个存储过程通常用于完成一段业务逻辑，例如报名，交班费，订单入库等。 而一个函数通常专注与某个功能，视为其他程序服务的，需要在其他语句中调用函数才可以，而存储过程不能被其他调用，是自己执行 通过call执行。 -- 创建 CREATE PROCEDURE sp_name (参数列表) 过程体 参数列表：不同于函数的参数列表，需要指明参数类型 IN，表示输入型 OUT，表示输出型 INOUT，表示混合型 注意，没有返回值。 存储过程 /* 存储过程 */ ------------------ 存储过程是一段可执行性代码的集合。相比函数，更偏向于业务逻辑。 调用：CALL 过程名 -- 注意 - 没有返回值。 - 只能单独调用，不可夹杂在其他语句中 -- 参数 IN|OUT|INOUT 参数名 数据类型 IN 输入：在调用过程中，将数据输入到过程体内部的参数 OUT 输出：在调用过程中，将过程体处理完的结果返回到客户端 INOUT 输入输出：既可输入，也可输出 -- 语法 CREATE PROCEDURE 过程名 (参数列表) BEGIN 过程体 END 用户和权限管理 /* 用户和权限管理 */ ------------------ -- root密码重置 1. 停止MySQL服务 2. [Linux] /usr/local/mysql/bin/safe_mysqld --skip-grant-tables & [Windows] mysqld --skip-grant-tables 3. use mysql; 4. UPDATE `user` SET PASSWORD=PASSWORD(\"密码\") WHERE `user` = \"root\"; 5. FLUSH PRIVILEGES; 用户信息表：mysql.user -- 刷新权限 FLUSH PRIVILEGES; -- 增加用户 CREATE USER 用户名 IDENTIFIED BY [PASSWORD] 密码(字符串) - 必须拥有mysql数据库的全局CREATE USER权限，或拥有INSERT权限。 - 只能创建用户，不能赋予权限。 - 用户名，注意引号：如 'user_name'@'192.168.1.1' - 密码也需引号，纯数字密码也要加引号 - 要在纯文本中指定密码，需忽略PASSWORD关键词。要把密码指定为由PASSWORD()函数返回的混编值，需包含关键字PASSWORD -- 重命名用户 RENAME USER old_user TO new_user -- 设置密码 SET PASSWORD = PASSWORD('密码') -- 为当前用户设置密码 SET PASSWORD FOR 用户名 = PASSWORD('密码') -- 为指定用户设置密码 -- 删除用户 DROP USER 用户名 -- 分配权限/添加用户 GRANT 权限列表 ON 表名 TO 用户名 [IDENTIFIED BY [PASSWORD] 'password'] - all privileges 表示所有权限 - *.* 表示所有库的所有表 - 库名.表名 表示某库下面的某表 GRANT ALL PRIVILEGES ON `pms`.* TO 'pms'@'%' IDENTIFIED BY 'pms0817'; -- 查看权限 SHOW GRANTS FOR 用户名 -- 查看当前用户权限 SHOW GRANTS; 或 SHOW GRANTS FOR CURRENT_USER; 或 SHOW GRANTS FOR CURRENT_USER(); -- 撤消权限 REVOKE 权限列表 ON 表名 FROM 用户名 REVOKE ALL PRIVILEGES, GRANT OPTION FROM 用户名 -- 撤销所有权限 -- 权限层级 -- 要使用GRANT或REVOKE，您必须拥有GRANT OPTION权限，并且您必须用于您正在授予或撤销的权限。 全局层级：全局权限适用于一个给定服务器中的所有数据库，mysql.user GRANT ALL ON *.*和 REVOKE ALL ON *.*只授予和撤销全局权限。 数据库层级：数据库权限适用于一个给定数据库中的所有目标，mysql.db, mysql.host GRANT ALL ON db_name.*和REVOKE ALL ON db_name.*只授予和撤销数据库权限。 表层级：表权限适用于一个给定表中的所有列，mysql.talbes_priv GRANT ALL ON db_name.tbl_name和REVOKE ALL ON db_name.tbl_name只授予和撤销表权限。 列层级：列权限适用于一个给定表中的单一列，mysql.columns_priv 当使用REVOKE时，您必须指定与被授权列相同的列。 -- 权限列表 ALL [PRIVILEGES] -- 设置除GRANT OPTION之外的所有简单权限 ALTER -- 允许使用ALTER TABLE ALTER ROUTINE -- 更改或取消已存储的子程序 CREATE -- 允许使用CREATE TABLE CREATE ROUTINE -- 创建已存储的子程序 CREATE TEMPORARY TABLES -- 允许使用CREATE TEMPORARY TABLE CREATE USER -- 允许使用CREATE USER, DROP USER, RENAME USER和REVOKE ALL PRIVILEGES。 CREATE VIEW -- 允许使用CREATE VIEW DELETE -- 允许使用DELETE DROP -- 允许使用DROP TABLE EXECUTE -- 允许用户运行已存储的子程序 FILE -- 允许使用SELECT...INTO OUTFILE和LOAD DATA INFILE INDEX -- 允许使用CREATE INDEX和DROP INDEX INSERT -- 允许使用INSERT LOCK TABLES -- 允许对您拥有SELECT权限的表使用LOCK TABLES PROCESS -- 允许使用SHOW FULL PROCESSLIST REFERENCES -- 未被实施 RELOAD -- 允许使用FLUSH REPLICATION CLIENT -- 允许用户询问从属服务器或主服务器的地址 REPLICATION SLAVE -- 用于复制型从属服务器（从主服务器中读取二进制日志事件） SELECT -- 允许使用SELECT SHOW DATABASES -- 显示所有数据库 SHOW VIEW -- 允许使用SHOW CREATE VIEW SHUTDOWN -- 允许使用mysqladmin shutdown SUPER -- 允许使用CHANGE MASTER, KILL, PURGE MASTER LOGS和SET GLOBAL语句，mysqladmin debug命令；允许您连接（一次），即使已达到max_connections。 UPDATE -- 允许使用UPDATE USAGE -- “无权限”的同义词 GRANT OPTION -- 允许授予权限 表维护 /* 表维护 */ -- 分析和存储表的关键字分布 ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE 表名 ... -- 检查一个或多个表是否有错误 CHECK TABLE tbl_name [, tbl_name] ... [option] ... option = {QUICK | FAST | MEDIUM | EXTENDED | CHANGED} -- 整理数据文件的碎片 OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... 杂项 /* 杂项 */ ------------------ 1. 可用反引号（`）为标识符（库名、表名、字段名、索引、别名）包裹，以避免与关键字重名！中文也可以作为标识符！ 2. 每个库目录存在一个保存当前数据库的选项文件db.opt。 3. 注释： 单行注释 # 注释内容 多行注释 /* 注释内容 */ 单行注释 -- 注释内容 (标准SQL注释风格，要求双破折号后加一空格符（空格、TAB、换行等）) 4. 模式通配符： _ 任意单个字符 % 任意多个字符，甚至包括零字符 单引号需要进行转义 \\' 5. CMD命令行内的语句结束符可以为 \";\", \"\\G\", \"\\g\"，仅影响显示结果。其他地方还是用分号结束。delimiter 可修改当前对话的语句结束符。 6. SQL对大小写不敏感 7. 清除已有语句：\\c "},"zother6-JavaGuide/database/一条sql语句在mysql中如何执行的.html":{"url":"zother6-JavaGuide/database/一条sql语句在mysql中如何执行的.html","title":"一条sql语句在mysql中如何执行的","keywords":"","body":"本文来自木木匠投稿。 一 MySQL 基础架构分析 1.1 MySQL 基本架构概览 1.2 Server 层基本组件介绍 1) 连接器 2) 查询缓存(MySQL 8.0 版本后移除) 3) 分析器 4) 优化器 5) 执行器 二 语句分析 2.1 查询语句 2.2 更新语句 三 总结 四 参考 本篇文章会分析下一个 sql 语句在 MySQL 中的执行流程，包括 sql 的查询在 MySQL 内部会怎么流转，sql 语句的更新是怎么完成的。 在分析之前我会先带着你看看 MySQL 的基础架构，知道了 MySQL 由那些组件组成已经这些组件的作用是什么，可以帮助我们理解和解决这些问题。 一 MySQL 基础架构分析 1.1 MySQL 基本架构概览 下图是 MySQL 的一个简要架构图，从下图你可以很清晰的看到用户的 SQL 语句在 MySQL 内部是如何执行的。 先简单介绍一下下图涉及的一些组件的基本作用帮助大家理解这幅图，在 1.2 节中会详细介绍到这些组件的作用。 连接器： 身份认证和权限相关(登录 MySQL 的时候)。 查询缓存: 执行查询语句的时候，会先查询缓存（MySQL 8.0 版本后移除，因为这个功能不太实用）。 分析器: 没有命中缓存的话，SQL 语句就会经过分析器，分析器说白了就是要先看你的 SQL 语句要干嘛，再检查你的 SQL 语句语法是否正确。 优化器： 按照 MySQL 认为最优的方案去执行。 执行器: 执行语句，然后从存储引擎返回数据。 简单来说 MySQL 主要分为 Server 层和存储引擎层： Server 层：主要包括连接器、查询缓存、分析器、优化器、执行器等，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图，函数等，还有一个通用的日志模块 binglog 日志模块。 存储引擎： 主要负责数据的存储和读取，采用可以替换的插件式架构，支持 InnoDB、MyISAM、Memory 等多个存储引擎，其中 InnoDB 引擎有自有的日志模块 redolog 模块。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始就被当做默认存储引擎了。 1.2 Server 层基本组件介绍 1) 连接器 连接器主要和身份认证和权限相关的功能相关，就好比一个级别很高的门卫一样。 主要负责用户登录数据库，进行用户的身份认证，包括校验账户密码，权限等操作，如果用户账户密码已通过，连接器会到权限表中查询该用户的所有权限，之后在这个连接里的权限逻辑判断都是会依赖此时读取到的权限数据，也就是说，后续只要这个连接不断开，即时管理员修改了该用户的权限，该用户也是不受影响的。 2) 查询缓存(MySQL 8.0 版本后移除) 查询缓存主要用来缓存我们所执行的 SELECT 语句以及该语句的结果集。 连接建立后，执行查询语句的时候，会先查询缓存，MySQL 会先校验这个 sql 是否执行过，以 Key-Value 的形式缓存在内存中，Key 是查询预计，Value 是结果集。如果缓存 key 被命中，就会直接返回给客户端，如果没有命中，就会执行后续的操作，完成后也会把结果缓存起来，方便下一次调用。当然在真正执行缓存查询的时候还是会校验用户的权限，是否有该表的查询条件。 MySQL 查询不建议使用缓存，因为查询缓存失效在实际业务场景中可能会非常频繁，假如你对一个表更新的话，这个表上的所有的查询缓存都会被清空。对于不经常更新的数据来说，使用缓存还是可以的。 所以，一般在大多数情况下我们都是不推荐去使用查询缓存的。 MySQL 8.0 版本后删除了缓存的功能，官方也是认为该功能在实际的应用场景比较少，所以干脆直接删掉了。 3) 分析器 MySQL 没有命中缓存，那么就会进入分析器，分析器主要是用来分析 SQL 语句是来干嘛的，分析器也会分为几步： 第一步，词法分析，一条 SQL 语句有多个字符串组成，首先要提取关键字，比如 select，提出查询的表，提出字段名，提出查询条件等等。做完这些操作后，就会进入第二步。 第二步，语法分析，主要就是判断你输入的 sql 是否正确，是否符合 MySQL 的语法。 完成这 2 步之后，MySQL 就准备开始执行了，但是如何执行，怎么执行是最好的结果呢？这个时候就需要优化器上场了。 4) 优化器 优化器的作用就是它认为的最优的执行方案去执行（有时候可能也不是最优，这篇文章涉及对这部分知识的深入讲解），比如多个索引的时候该如何选择索引，多表查询的时候如何选择关联顺序等。 可以说，经过了优化器之后可以说这个语句具体该如何执行就已经定下来。 5) 执行器 当选择了执行方案后，MySQL 就准备开始执行了，首先执行前会校验该用户有没有权限，如果没有权限，就会返回错误信息，如果有权限，就会去调用引擎的接口，返回接口执行的结果。 二 语句分析 2.1 查询语句 说了以上这么多，那么究竟一条 sql 语句是如何执行的呢？其实我们的 sql 可以分为两种，一种是查询，一种是更新（增加，更新，删除）。我们先分析下查询语句，语句如下： select * from tb_student A where A.age='18' and A.name=' 张三 '; 结合上面的说明，我们分析下这个语句的执行流程： 先检查该语句是否有权限，如果没有权限，直接返回错误信息，如果有权限，在 MySQL8.0 版本以前，会先查询缓存，以这条 sql 语句为 key 在内存中查询是否有结果，如果有直接缓存，如果没有，执行下一步。 通过分析器进行词法分析，提取 sql 语句的关键元素，比如提取上面这个语句是查询 select，提取需要查询的表名为 tb_student,需要查询所有的列，查询条件是这个表的 id='1'。然后判断这个 sql 语句是否有语法错误，比如关键词是否正确等等，如果检查没问题就执行下一步。 接下来就是优化器进行确定执行方案，上面的 sql 语句，可以有两种执行方案： a.先查询学生表中姓名为“张三”的学生，然后判断是否年龄是 18。 b.先找出学生中年龄 18 岁的学生，然后再查询姓名为“张三”的学生。 那么优化器根据自己的优化算法进行选择执行效率最好的一个方案（优化器认为，有时候不一定最好）。那么确认了执行计划后就准备开始执行了。 进行权限校验，如果没有权限就会返回错误信息，如果有权限就会调用数据库引擎接口，返回引擎的执行结果。 2.2 更新语句 以上就是一条查询 sql 的执行流程，那么接下来我们看看一条更新语句如何执行的呢？sql 语句如下： update tb_student A set A.age='19' where A.name=' 张三 '; 我们来给张三修改下年龄，在实际数据库肯定不会设置年龄这个字段的，不然要被技术负责人打的。其实条语句也基本上会沿着上一个查询的流程走，只不过执行更新的时候肯定要记录日志啦，这就会引入日志模块了，MySQL 自带的日志模块式 binlog（归档日志） ，所有的存储引擎都可以使用，我们常用的 InnoDB 引擎还自带了一个日志模块 redo log（重做日志），我们就以 InnoDB 模式下来探讨这个语句的执行流程。流程如下： 先查询到张三这一条数据，如果有缓存，也是会用到缓存。 然后拿到查询的语句，把 age 改为 19，然后调用引擎 API 接口，写入这一行数据，InnoDB 引擎把数据保存在内存中，同时记录 redo log，此时 redo log 进入 prepare 状态，然后告诉执行器，执行完成了，随时可以提交。 执行器收到通知后记录 binlog，然后调用引擎接口，提交 redo log 为提交状态。 更新完成。 这里肯定有同学会问，为什么要用两个日志模块，用一个日志模块不行吗? 这是因为最开始 MySQL 并没与 InnoDB 引擎( InnoDB 引擎是其他公司以插件形式插入 MySQL 的) ，MySQL 自带的引擎是 MyISAM，但是我们知道 redo log 是 InnoDB 引擎特有的，其他存储引擎都没有，这就导致会没有 crash-safe 的能力(crash-safe 的能力即使数据库发生异常重启，之前提交的记录都不会丢失)，binlog 日志只能用来归档。 并不是说只用一个日志模块不可以，只是 InnoDB 引擎就是通过 redo log 来支持事务的。那么，又会有同学问，我用两个日志模块，但是不要这么复杂行不行，为什么 redo log 要引入 prepare 预提交状态？这里我们用反证法来说明下为什么要这么做？ 先写 redo log 直接提交，然后写 binlog，假设写完 redo log 后，机器挂了，binlog 日志没有被写入，那么机器重启后，这台机器会通过 redo log 恢复数据，但是这个时候 bingog 并没有记录该数据，后续进行机器备份的时候，就会丢失这一条数据，同时主从同步也会丢失这一条数据。 先写 binlog，然后写 redo log，假设写完了 binlog，机器异常重启了，由于没有 redo log，本机是无法恢复这一条记录的，但是 binlog 又有记录，那么和上面同样的道理，就会产生数据不一致的情况。 如果采用 redo log 两阶段提交的方式就不一样了，写完 binglog 后，然后再提交 redo log 就会防止出现上述的问题，从而保证了数据的一致性。那么问题来了，有没有一个极端的情况呢？假设 redo log 处于预提交状态，binglog 也已经写完了，这个时候发生了异常重启会怎么样呢？ 这个就要依赖于 MySQL 的处理机制了，MySQL 的处理过程如下： 判断 redo log 是否完整，如果判断是完整的，就立即提交。 如果 redo log 只是预提交但不是 commit 状态，这个时候就会去判断 binlog 是否完整，如果完整就提交 redo log, 不完整就回滚事务。 这样就解决了数据一致性的问题。 三 总结 MySQL 主要分为 Server 层和引擎层，Server 层主要包括连接器、查询缓存、分析器、优化器、执行器，同时还有一个日志模块（binlog），这个日志模块所有执行引擎都可以共用,redolog 只有 InnoDB 有。 引擎层是插件式的，目前主要包括，MyISAM,InnoDB,Memory 等。 查询语句的执行流程如下：权限校验（如果命中缓存）---》查询缓存---》分析器---》优化器---》权限校验---》执行器---》引擎 更新语句执行流程如下：分析器----》权限校验----》执行器---》引擎---redo log(prepare 状态---》binlog---》redo log(commit状态) 四 参考 《MySQL 实战45讲》 MySQL 5.6参考手册:https://dev.MySQL.com/doc/refman/5.6/en/ "},"zother6-JavaGuide/database/关于数据库存储时间的一点思考.html":{"url":"zother6-JavaGuide/database/关于数据库存储时间的一点思考.html","title":"关于数据库存储时间的一点思考","keywords":"","body":"我们平时开发中不可避免的就是要存储时间，比如我们要记录操作表中这条记录的时间、记录转账的交易时间、记录出发时间等等。你会发现这个时间这个东西与我们开发的联系还是非常紧密的，用的好与不好会给我们的业务甚至功能带来很大的影响。所以，我们有必要重新出发，好好认识一下这个东西。 这是一篇短小精悍的文章，仔细阅读一定能学到不少东西！ 1.切记不要用字符串存储日期 我记得我在大学的时候就这样干过，而且现在很多对数据库不太了解的新手也会这样干，可见，这种存储日期的方式的优点还是有的，就是简单直白，容易上手。 但是，这是不正确的做法，主要会有下面两个问题： 字符串占用的空间更大！ 字符串存储的日期比较效率比较低（逐个字符进行比对），无法用日期相关的 API 进行计算和比较。 2.Datetime 和 Timestamp 之间抉择 Datetime 和 Timestamp 是 MySQL 提供的两种比较相似的保存时间的数据类型。他们两者究竟该如何选择呢？ 通常我们都会首选 Timestamp。 下面说一下为什么这样做! 2.1 DateTime 类型没有时区信息的 DateTime 类型是没有时区信息的（时区无关） ，DateTime 类型保存的时间都是当前会话所设置的时区对应的时间。这样就会有什么问题呢？当你的时区更换之后，比如你的服务器更换地址或者更换客户端连接时区设置的话，就会导致你从数据库中读出的时间错误。不要小看这个问题，很多系统就是因为这个问题闹出了很多笑话。 Timestamp 和时区有关。Timestamp 类型字段的值会随着服务器时区的变化而变化，自动换算成相应的时间，说简单点就是在不同时区，查询到同一个条记录此字段的值会不一样。 下面实际演示一下！ 建表 SQL 语句： CREATE TABLE `time_zone_test` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `date_time` datetime DEFAULT NULL, `time_stamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; 插入数据： INSERT INTO time_zone_test(date_time,time_stamp) VALUES(NOW(),NOW()); 查看数据： select date_time,time_stamp from time_zone_test; 结果： +---------------------+---------------------+ | date_time | time_stamp | +---------------------+---------------------+ | 2020-01-11 09:53:32 | 2020-01-11 09:53:32 | +---------------------+---------------------+ 现在我们运行 修改当前会话的时区: set time_zone='+8:00'; 再次查看数据： +---------------------+---------------------+ | date_time | time_stamp | +---------------------+---------------------+ | 2020-01-11 09:53:32 | 2020-01-11 17:53:32 | +---------------------+---------------------+ 扩展：一些关于 MySQL 时区设置的一个常用 sql 命令 # 查看当前会话时区 SELECT @@session.time_zone; # 设置当前会话时区 SET time_zone = 'Europe/Helsinki'; SET time_zone = \"+00:00\"; # 数据库全局时区设置 SELECT @@global.time_zone; # 设置全局时区 SET GLOBAL time_zone = '+8:00'; SET GLOBAL time_zone = 'Europe/Helsinki'; 2.2 DateTime 类型耗费空间更大 Timestamp 只需要使用 4 个字节的存储空间，但是 DateTime 需要耗费 8 个字节的存储空间。但是，这样同样造成了一个问题，Timestamp 表示的时间范围更小。 DateTime ：1000-01-01 00:00:00 ~ 9999-12-31 23:59:59 Timestamp： 1970-01-01 00:00:01 ~ 2037-12-31 23:59:59 Timestamp 在不同版本的 MySQL 中有细微差别。 3 再看 MySQL 日期类型存储空间 下图是 MySQL 5.6 版本中日期类型所占的存储空间： 可以看出 5.6.4 之后的 MySQL 多出了一个需要 0 ～ 3 字节的小数位。Datatime 和 Timestamp 会有几种不同的存储空间占用。 为了方便，本文我们还是默认 Timestamp 只需要使用 4 个字节的存储空间，但是 DateTime 需要耗费 8 个字节的存储空间。 4.数值型时间戳是更好的选择吗？ 很多时候，我们也会使用 int 或者 bigint 类型的数值也就是时间戳来表示时间。 这种存储方式的具有 Timestamp 类型的所具有一些优点，并且使用它的进行日期排序以及对比等操作的效率会更高，跨系统也很方便，毕竟只是存放的数值。缺点也很明显，就是数据的可读性太差了，你无法直观的看到具体时间。 时间戳的定义如下： 时间戳的定义是从一个基准时间开始算起，这个基准时间是「1970-1-1 00:00:00 +0:00」，从这个时间开始，用整数表示，以秒计时，随着时间的流逝这个时间整数不断增加。这样一来，我只需要一个数值，就可以完美地表示时间了，而且这个数值是一个绝对数值，即无论的身处地球的任何角落，这个表示时间的时间戳，都是一样的，生成的数值都是一样的，并且没有时区的概念，所以在系统的中时间的传输中，都不需要进行额外的转换了，只有在显示给用户的时候，才转换为字符串格式的本地时间。 数据库中实际操作： mysql> select UNIX_TIMESTAMP('2020-01-11 09:53:32'); +---------------------------------------+ | UNIX_TIMESTAMP('2020-01-11 09:53:32') | +---------------------------------------+ | 1578707612 | +---------------------------------------+ 1 row in set (0.00 sec) mysql> select FROM_UNIXTIME(1578707612); +---------------------------+ | FROM_UNIXTIME(1578707612) | +---------------------------+ | 2020-01-11 09:53:32 | +---------------------------+ 1 row in set (0.01 sec) 5.总结 MySQL 中时间到底怎么存储才好？Datetime?Timestamp? 数值保存的时间戳？ 好像并没有一个银弹，很多程序员会觉得数值型时间戳是真的好，效率又高还各种兼容，但是很多人又觉得它表现的不够直观。这里插一嘴，《高性能 MySQL 》这本神书的作者就是推荐 Timestamp，原因是数值表示时间不够直观。下面是原文： 每种方式都有各自的优势，根据实际场景才是王道。下面再对这三种方式做一个简单的对比，以供大家实际开发中选择正确的存放时间的数据类型： 如果还有什么问题欢迎给我留言！如果文章有什么问题的话，也劳烦指出，Guide 哥感激不尽！ 后面的文章我会介绍： [ ] Java8 对日期的支持以及为啥不能用 SimpleDateFormat。 [ ] SpringBoot 中如何实际使用(JPA 为例) "},"zother6-JavaGuide/database/数据库索引.html":{"url":"zother6-JavaGuide/database/数据库索引.html","title":"数据库索引","keywords":"","body":"什么是索引? 索引是一种用于快速查询和检索数据的数据结构。常见的索引结构有: B树， B+树和Hash。 索引的作用就相当于目录的作用。打个比方: 我们在查字典的时候，如果没有目录，那我们就只能一页一页的去找我们需要查的那个字，速度很慢。如果有目录了，我们只需要先去目录里查找字的位置，然后直接翻到那一页就行了。 为什么要用索引?索引的优缺点分析 索引的优点 可以大大加快 数据的检索速度（大大减少的检索的数据量）, 这也是创建索引的最主要的原因。毕竟大部分系统的读请求总是大于写请求的。 另外，通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。 索引的缺点 创建索引和维护索引需要耗费许多时间：当对表中的数据进行增删改的时候，如果数据有索引，那么索引也需要动态的修改，会降低SQL执行效率。 占用物理存储空间 ：索引需要使用物理文件存储，也会耗费一定空间。 B树和B+树区别 B树的所有节点既存放 键(key) 也存放 数据(data);而B+树只有叶子节点存放 key 和 data，其他内节点只存放key。 B树的叶子节点都是独立的;B+树的叶子节点有一条引用链指向与它相邻的叶子节点。 B树的检索的过程相当于对范围内的每个节点的关键字做二分查找，可能还没有到达叶子节点，检索就结束了。而B+树的检索效率就很稳定了，任何查找都是从根节点到叶子节点的过程，叶子节点的顺序检索很明显。 Hash索引和 B+树索引优劣分析 Hash索引定位快 Hash索引指的就是Hash表，最大的优点就是能够在很短的时间内，根据Hash函数定位到数据所在的位置，这是B+树所不能比的。 Hash冲突问题 知道HashMap或HashTable的同学，相信都知道它们最大的缺点就是Hash冲突了。不过对于数据库来说这还不算最大的缺点。 Hash索引不支持顺序和范围查询(Hash索引不支持顺序和范围查询是它最大的缺点。 试想一种情况: SELECT * FROM tb1 WHERE id B+树是有序的，在这种范围查询中，优势非常大，直接遍历比500小的叶子节点就够了。而Hash索引是根据hash算法来定位的，难不成还要把 1 - 499的数据，每个都进行一次hash计算来定位吗?这就是Hash最大的缺点了。 索引类型 主键索引(Primary Key) 数据表的主键列使用的就是主键索引。 一张数据表有只能有一个主键，并且主键不能为null，不能重复。 在mysql的InnoDB的表中，当没有显示的指定表的主键时，InnoDB会自动先检查表中是否有唯一索引的字段，如果有，则选择该字段为默认的主键，否则InnoDB将会自动创建一个6Byte的自增主键。 二级索引(辅助索引) 二级索引又称为辅助索引，是因为二级索引的叶子节点存储的数据是主键。也就是说，通过二级索引，可以定位主键的位置。 唯一索引，普通索引，前缀索引等索引属于二级索引。 PS:不懂的同学可以暂存疑，慢慢往下看，后面会有答案的，也可以自行搜索。 唯一索引(Unique Key) ：唯一索引也是一种约束。唯一索引的属性列不能出现重复的数据，但是允许数据为NULL，一张表允许创建多个唯一索引。建立唯一索引的目的大部分时候都是为了该属性列的数据的唯一性，而不是为了查询效率。 普通索引(Index) ：普通索引的唯一作用就是为了快速查询数据，一张表允许创建多个普通索引，并允许数据重复和NULL。 前缀索引(Prefix) ：前缀索引只适用于字符串类型的数据。前缀索引是对文本的前几个字符创建索引，相比普通索引建立的数据更小， 因为只取前几个字符。 全文索引(Full Text) ：全文索引主要是为了检索大文本数据中的关键字的信息，是目前搜索引擎数据库使用的一种技术。Mysql5.6之前只有MYISAM引擎支持全文索引，5.6之后InnoDB也支持了全文索引。 二级索引: .png) 聚集索引与非聚集索引 聚集索引 聚集索引即索引结构和数据一起存放的索引。主键索引属于聚集索引。 在 Mysql 中，InnoDB引擎的表的 .ibd文件就包含了该表的索引和数据，对于 InnoDB 引擎表来说，该表的索引(B+树)的每个非叶子节点存储索引，叶子节点存储索引和索引对应的数据。 聚集索引的优点 聚集索引的查询速度非常的快，因为整个B+树本身就是一颗多叉平衡树，叶子节点也都是有序的，定位到索引的节点，就相当于定位到了数据。 聚集索引的缺点 依赖于有序的数据 ：因为B+树是多路平衡树，如果索引的数据不是有序的，那么就需要在插入时排序，如果数据是整型还好，否则类似于字符串或UUID这种又长又难比较的数据，插入或查找的速度肯定比较慢。 更新代价大 ： 如果对索引列的数据被修改时，那么对应的索引也将会被修改， 而且况聚集索引的叶子节点还存放着数据，修改代价肯定是较大的， 所以对于主键索引来说，主键一般都是不可被修改的。 非聚集索引 非聚集索引即索引结构和数据分开存放的索引。 二级索引属于非聚集索引。 MYISAM引擎的表的.MYI文件包含了表的索引， 该表的索引(B+树)的每个叶子非叶子节点存储索引， 叶子节点存储索引和索引对应数据的指针，指向.MYD文件的数据。 非聚集索引的叶子节点并不一定存放数据的指针， 因为二级索引的叶子节点就存放的是主键，根据主键再回表查数据。 非聚集索引的优点 更新代价比聚集索引要小 。非聚集索引的更新代价就没有聚集索引那么大了，非聚集索引的叶子节点是不存放数据的 非聚集索引的缺点 跟聚集索引一样，非聚集索引也依赖于有序的数据 可能会二次查询(回表) :这应该是非聚集索引最大的缺点了。 当查到索引对应的指针或主键后，可能还需要根据指针或主键再到数据文件或表中查询。 这是Mysql的表的文件截图: 聚集索引和非聚集索引: 非聚集索引一定回表查询吗(覆盖索引)? 非聚集索引不一定回表查询。 试想一种情况，用户准备使用SQL查询用户名，而用户名字段正好建立了索引。 SELECT name FROM table WHERE username='guang19'; 那么这个索引的key本身就是name，查到对应的name直接返回就行了，无需回表查询。 即使是MYISAM也是这样，虽然MYISAM的主键索引确实需要回表， 因为它的主键索引的叶子节点存放的是指针。但是如果SQL查的就是主键呢? SELECT id FROM table WHERE id=1; 主键索引本身的key就是主键，查到返回就行了。这种情况就称之为覆盖索引了。 覆盖索引 如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称之为“覆盖索引”。我们知道在InnoDB存储引擎中，如果不是主键索引，叶子节点存储的是主键+列值。最终还是要“回表”，也就是要通过主键再查找一次。这样就会比较慢覆盖索引就是把要查询出的列和索引是对应的，不做回表操作！ 覆盖索引即需要查询的字段正好是索引的字段，那么直接根据该索引，就可以查到数据了， 而无需回表查询。 如主键索引，如果一条SQL需要查询主键，那么正好根据主键索引就可以查到主键。 再如普通索引，如果一条SQL需要查询name，name字段正好有索引， 那么直接根据这个索引就可以查到数据，也无需回表。 覆盖索引: 索引创建原则 单列索引 单列索引即由一列属性组成的索引。 联合索引(多列索引) 联合索引即由多列属性组成索引。 最左前缀原则 假设创建的联合索引由三个字段组成: ALTER TABLE table ADD INDEX index_name (num,name,age) 那么当查询的条件有为:num / (num AND name) / (num AND name AND age)时，索引才生效。所以在创建联合索引时，尽量把查询最频繁的那个字段作为最左(第一个)字段。查询的时候也尽量以这个字段为第一条件。 但可能由于版本原因(我的mysql版本为8.0.x),我创建的联合索引，相当于在联合索引的每个字段上都创建了相同的索引: .png) 无论是否符合最左前缀原则，每个字段的索引都生效: 索引创建注意点 最左前缀原则 虽然我目前的Mysql版本较高，好像不遵守最左前缀原则，索引也会生效。 但是我们仍应遵守最左前缀原则，以免版本更迭带来的麻烦。 选择合适的字段 1.不为NULL的字段 索引字段的数据应该尽量不为NULL，因为对于数据为NULL的字段，数据库较难优化。如果字段频繁被查询，但又避免不了为NULL，建议使用0,1,true,false这样语义较为清晰的短值或短字符作为替代。 2.被频繁查询的字段 我们创建索引的字段应该是查询操作非常频繁的字段。 3.被作为条件查询的字段 被作为WHERE条件查询的字段，应该被考虑建立索引。 4.被经常频繁用于连接的字段 经常用于连接的字段可能是一些外键列，对于外键列并不一定要建立外键，只是说该列涉及到表与表的关系。对于频繁被连接查询的字段，可以考虑建立索引，提高多表连接查询的效率。 不合适创建索引的字段 1.被频繁更新的字段应该慎重建立索引 虽然索引能带来查询上的效率，但是维护索引的成本也是不小的。 如果一个字段不被经常查询，反而被经常修改，那么就更不应该在这种字段上建立索引了。 2.不被经常查询的字段没有必要建立索引 3.尽可能的考虑建立联合索引而不是单列索引 因为索引是需要占用磁盘空间的，可以简单理解为每个索引都对应着一颗B+树。如果一个表的字段过多，索引过多，那么当这个表的数据达到一个体量后，索引占用的空间也是很多的，且修改索引时，耗费的时间也是较多的。如果是联合索引，多个字段在一个索引上，那么将会节约很大磁盘空间，且修改数据的操作效率也会提升。 4.注意避免冗余索引 冗余索引指的是索引的功能相同，能够命中 就肯定能命中 ，那么 就是冗余索引如（name,city ）和（name ）这两个索引就是冗余索引，能够命中后者的查询肯定是能够命中前者的 在大多数情况下，都应该尽量扩展已有的索引而不是创建新索引。 5.考虑在字符串类型的字段上使用前缀索引代替普通索引 前缀索引仅限于字符串类型，较普通索引会占用更小的空间，所以可以考虑使用前缀索引带替普通索引。 使用索引一定能提高查询性能吗? 大多数情况下，索引查询都是比全表扫描要快的。但是如果数据库的数据量不大，那么使用索引也不一定能够带来很大提升。 "},"zother6-JavaGuide/database/数据库连接池.html":{"url":"zother6-JavaGuide/database/数据库连接池.html","title":"数据库连接池","keywords":"","body":" 公众号和Github待发文章：数据库：数据库连接池原理详解与自定义连接池实现 基于JDBC的数据库连接池技术研究与应用 数据库连接池技术详解 数据库连接本质就是一个 socket 的连接。数据库服务端还要维护一些缓存和用户权限信息之类的 所以占用了一些内存 连接池是维护的数据库连接的缓存，以便将来需要对数据库的请求时可以重用这些连接。为每个用户打开和维护数据库连接，尤其是对动态数据库驱动的网站应用程序的请求，既昂贵又浪费资源。在连接池中，创建连接后，将其放置在池中，并再次使用它，因此不必建立新的连接。如果使用了所有连接，则会建立一个新连接并将其添加到池中。连接池还减少了用户必须等待建立与数据库的连接的时间。 操作过数据库的朋友应该都知道数据库连接池这个概念，它几乎每天都在和我们打交道，但是你真的了解 数据库连接池 吗？ 没有数据库连接池之前 我相信你一定听过这样一句话：Java语言中，JDBC（Java DataBase Connection）是应用程序与数据库沟通的桥梁。 "},"zother6-JavaGuide/database/阿里巴巴开发手册数据库部分的一些最佳实践.html":{"url":"zother6-JavaGuide/database/阿里巴巴开发手册数据库部分的一些最佳实践.html","title":"阿里巴巴开发手册数据库部分的一些最佳实践","keywords":"","body":"阿里巴巴Java开发手册数据库部分的一些最佳实践总结 模糊查询 对于模糊查询阿里巴巴开发手册这样说到： 【强制】页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。 说明:索引文件具有 B-Tree 的最左前缀匹配特性，如果左边的值未确定，那么无法使用此索引。 外键和级联 对于外键和级联，阿里巴巴开发手册这样说到： 【强制】不得使用外键与级联，一切外键概念必须在应用层解决。 说明:以学生和成绩的关系为例，学生表中的 student_id 是主键，那么成绩表中的 student_id 则为外键。如果更新学生表中的 student_id，同时触发成绩表中的 student_id 更新，即为级联更新。外键与级联更新适用于单机低并发，不适合分布式、高并发集群;级联更新是强阻塞，存在数据库更新风暴的风 险;外键影响数据库的插入速度 为什么不要用外键呢？大部分人可能会这样回答： 增加了复杂性： a.每次做DELETE 或者UPDATE都必须考虑外键约束，会导致开发的时候很痛苦,测试数据极为不方便;b.外键的主从关系是定的，假如那天需求有变化，数据库中的这个字段根本不需要和其他表有关联的话就会增加很多麻烦。 增加了额外工作： 数据库需要增加维护外键的工作，比如当我们做一些涉及外键字段的增，删，更新操作之后，需要触发相关操作去检查，保证数据的的一致性和正确性，这样会不得不消耗资源；（个人觉得这个不是不用外键的原因，因为即使你不使用外键，你在应用层面也还是要保证的。所以，我觉得这个影响可以忽略不计。） 外键还会因为需要请求对其他表内部加锁而容易出现死锁情况； 对分不分表不友好 ：因为分库分表下外键是无法生效的。 ...... 我个人觉得上面这种回答不是特别的全面，只是说了外键存在的一个常见的问题。实际上，我们知道外键也是有很多好处的，比如： 保证了数据库数据的一致性和完整性； 级联操作方便，减轻了程序代码量； ...... 所以说，不要一股脑的就抛弃了外键这个概念，既然它存在就有它存在的道理，如果系统不涉及分不分表，并发量不是很高的情况还是可以考虑使用外键的。 我个人是不太喜欢外键约束，比较喜欢在应用层去进行相关操作。 关于@Transactional注解 对于@Transactional事务注解，阿里巴巴开发手册这样说到： 【参考】@Transactional事务不要滥用。事务会影响数据库的QPS，另外使用事务的地方需要考虑各方面的回滚方案，包括缓存回滚、搜索引擎回滚、消息补偿、统计修正等。 "},"zother6-JavaGuide/dataStructures-algorithms/data-structure/bloom-filter.html":{"url":"zother6-JavaGuide/dataStructures-algorithms/data-structure/bloom-filter.html","title":"Bloom Filter","keywords":"","body":"海量数据处理以及缓存穿透这两个场景让我认识了 布隆过滤器 ，我查阅了一些资料来了解它，但是很多现成资料并不满足我的需求，所以就决定自己总结一篇关于布隆过滤器的文章。希望通过这篇文章让更多人了解布隆过滤器，并且会实际去使用它！ 下面我们将分为几个方面来介绍布隆过滤器： 什么是布隆过滤器？ 布隆过滤器的原理介绍。 布隆过滤器使用场景。 通过 Java 编程手动实现布隆过滤器。 利用Google开源的Guava中自带的布隆过滤器。 Redis 中的布隆过滤器。 1.什么是布隆过滤器？ 首先，我们需要了解布隆过滤器的概念。 布隆过滤器（Bloom Filter）是一个叫做 Bloom 的老哥于1970年提出的。我们可以把它看作由二进制向量（或者说位数组）和一系列随机映射函数（哈希函数）两部分组成的数据结构。相比于我们平时常用的的 List、Map 、Set 等数据结构，它占用空间更少并且效率更高，但是缺点是其返回的结果是概率性的，而不是非常准确的。理论情况下添加到集合中的元素越多，误报的可能性就越大。并且，存放在布隆过滤器的数据不容易删除。 位数组中的每个元素都只占用 1 bit ，并且每个元素只能是 0 或者 1。这样申请一个 100w 个元素的位数组只占用 1000000Bit / 8 = 125000 Byte = 125000/1024 kb ≈ 122kb 的空间。 总结：一个名叫 Bloom 的人提出了一种来检索元素是否在给定大集合中的数据结构，这种数据结构是高效且性能很好的，但缺点是具有一定的错误识别率和删除难度。并且，理论情况下，添加到集合中的元素越多，误报的可能性就越大。 2.布隆过滤器的原理介绍 当一个元素加入布隆过滤器中的时候，会进行如下操作： 使用布隆过滤器中的哈希函数对元素值进行计算，得到哈希值（有几个哈希函数得到几个哈希值）。 根据得到的哈希值，在位数组中把对应下标的值置为 1。 当我们需要判断一个元素是否存在于布隆过滤器的时候，会进行如下操作： 对给定元素再次进行相同的哈希计算； 得到值之后判断位数组中的每个元素是否都为 1，如果值都为 1，那么说明这个值在布隆过滤器中，如果存在一个值不为 1，说明该元素不在布隆过滤器中。 举个简单的例子： 如图所示，当字符串存储要加入到布隆过滤器中时，该字符串首先由多个哈希函数生成不同的哈希值，然后在对应的位数组的下表的元素设置为 1（当位数组初始化时 ，所有位置均为0）。当第二次存储相同字符串时，因为先前的对应位置已设置为1，所以很容易知道此值已经存在（去重非常方便）。 如果我们需要判断某个字符串是否在布隆过滤器中时，只需要对给定字符串再次进行相同的哈希计算，得到值之后判断位数组中的每个元素是否都为 1，如果值都为 1，那么说明这个值在布隆过滤器中，如果存在一个值不为 1，说明该元素不在布隆过滤器中。 不同的字符串可能哈希出来的位置相同，这种情况我们可以适当增加位数组大小或者调整我们的哈希函数。 综上，我们可以得出：布隆过滤器说某个元素存在，小概率会误判。布隆过滤器说某个元素不在，那么这个元素一定不在。 3.布隆过滤器使用场景 判断给定数据是否存在：比如判断一个数字是否在于包含大量数字的数字集中（数字集很大，5亿以上！）、 防止缓存穿透（判断请求的数据是否有效避免直接绕过缓存请求数据库）等等、邮箱的垃圾邮件过滤、黑名单功能等等。 去重：比如爬给定网址的时候对已经爬取过的 URL 去重。 4.通过 Java 编程手动实现布隆过滤器 我们上面已经说了布隆过滤器的原理，知道了布隆过滤器的原理之后就可以自己手动实现一个了。 如果你想要手动实现一个的话，你需要： 一个合适大小的位数组保存数据 几个不同的哈希函数 添加元素到位数组（布隆过滤器）的方法实现 判断给定元素是否存在于位数组（布隆过滤器）的方法实现。 下面给出一个我觉得写的还算不错的代码（参考网上已有代码改进得到，对于所有类型对象皆适用）： import java.util.BitSet; public class MyBloomFilter { /** * 位数组的大小 */ private static final int DEFAULT_SIZE = 2 >> 16))); } } } 测试： String value1 = \"https://javaguide.cn/\"; String value2 = \"https://github.com/Snailclimb\"; MyBloomFilter filter = new MyBloomFilter(); System.out.println(filter.contains(value1)); System.out.println(filter.contains(value2)); filter.add(value1); filter.add(value2); System.out.println(filter.contains(value1)); System.out.println(filter.contains(value2)); Output: false false true true 测试： Integer value1 = 13423; Integer value2 = 22131; MyBloomFilter filter = new MyBloomFilter(); System.out.println(filter.contains(value1)); System.out.println(filter.contains(value2)); filter.add(value1); filter.add(value2); System.out.println(filter.contains(value1)); System.out.println(filter.contains(value2)); Output: false false true true 5.利用Google开源的 Guava中自带的布隆过滤器 自己实现的目的主要是为了让自己搞懂布隆过滤器的原理，Guava 中布隆过滤器的实现算是比较权威的，所以实际项目中我们不需要手动实现一个布隆过滤器。 首先我们需要在项目中引入 Guava 的依赖： com.google.guava guava 28.0-jre 实际使用如下： 我们创建了一个最多存放 最多 1500个整数的布隆过滤器，并且我们可以容忍误判的概率为百分之（0.01） // 创建布隆过滤器对象 BloomFilter filter = BloomFilter.create( Funnels.integerFunnel(), 1500, 0.01); // 判断指定元素是否存在 System.out.println(filter.mightContain(1)); System.out.println(filter.mightContain(2)); // 将元素添加进布隆过滤器 filter.put(1); filter.put(2); System.out.println(filter.mightContain(1)); System.out.println(filter.mightContain(2)); 在我们的示例中，当mightContain（） 方法返回true时，我们可以99％确定该元素在过滤器中，当过滤器返回false时，我们可以100％确定该元素不存在于过滤器中。 Guava 提供的布隆过滤器的实现还是很不错的（想要详细了解的可以看一下它的源码实现），但是它有一个重大的缺陷就是只能单机使用（另外，容量扩展也不容易），而现在互联网一般都是分布式的场景。为了解决这个问题，我们就需要用到 Redis 中的布隆过滤器了。 6.Redis 中的布隆过滤器 6.1介绍 Redis v4.0 之后有了 Module（模块/插件） 功能，Redis Modules 让 Redis 可以使用外部模块扩展其功能 。布隆过滤器就是其中的 Module。详情可以查看 Redis 官方对 Redis Modules 的介绍 ：https://redis.io/modules。 另外，官网推荐了一个 RedisBloom 作为 Redis 布隆过滤器的 Module,地址：https://github.com/RedisBloom/RedisBloom。其他还有： redis-lua-scaling-bloom-filter （lua 脚本实现）：https://github.com/erikdubbelboer/redis-lua-scaling-bloom-filter pyreBloom（Python中的快速Redis 布隆过滤器） ：https://github.com/seomoz/pyreBloom ...... RedisBloom 提供了多种语言的客户端支持，包括：Python、Java、JavaScript 和 PHP。 6.2使用Docker安装 如果我们需要体验 Redis 中的布隆过滤器非常简单，通过 Docker 就可以了！我们直接在 Google 搜索docker redis bloomfilter 然后在排除广告的第一条搜素结果就找到了我们想要的答案（这是我平常解决问题的一种方式，分享一下），具体地址：https://hub.docker.com/r/redislabs/rebloom/ （介绍的很详细 ）。 具体操作如下： ➜ ~ docker run -p 6379:6379 --name redis-redisbloom redislabs/rebloom:latest ➜ ~ docker exec -it redis-redisbloom bash root@21396d02c252:/data# redis-cli 127.0.0.1:6379> 6.3常用命令一览 注意： key:布隆过滤器的名称，item : 添加的元素。 BF.ADD：将元素添加到布隆过滤器中，如果该过滤器尚不存在，则创建该过滤器。格式：BF.ADD {key} {item}。 BF.MADD : 将一个或多个元素添加到“布隆过滤器”中，并创建一个尚不存在的过滤器。该命令的操作方式BF.ADD与之相同，只不过它允许多个输入并返回多个值。格式：BF.MADD {key} {item} [item ...] 。 BF.EXISTS : 确定元素是否在布隆过滤器中存在。格式：BF.EXISTS {key} {item}。 BF.MEXISTS ： 确定一个或者多个元素是否在布隆过滤器中存在格式：BF.MEXISTS {key} {item} [item ...]。 另外，BF.RESERVE 命令需要单独介绍一下： 这个命令的格式如下： BF.RESERVE {key} {error_rate} {capacity} [EXPANSION expansion]。 下面简单介绍一下每个参数的具体含义： key：布隆过滤器的名称 error_rate :误报的期望概率。这应该是介于0到1之间的十进制值。例如，对于期望的误报率0.1％（1000中为1），error_rate应该设置为0.001。该数字越接近零，则每个项目的内存消耗越大，并且每个操作的CPU使用率越高。 capacity: 过滤器的容量。当实际存储的元素个数超过这个值之后，性能将开始下降。实际的降级将取决于超出限制的程度。随着过滤器元素数量呈指数增长，性能将线性下降。 可选参数： expansion：如果创建了一个新的子过滤器，则其大小将是当前过滤器的大小乘以expansion。默认扩展值为2。这意味着每个后续子过滤器将是前一个子过滤器的两倍。 6.4实际使用 127.0.0.1:6379> BF.ADD myFilter java (integer) 1 127.0.0.1:6379> BF.ADD myFilter javaguide (integer) 1 127.0.0.1:6379> BF.EXISTS myFilter java (integer) 1 127.0.0.1:6379> BF.EXISTS myFilter javaguide (integer) 1 127.0.0.1:6379> BF.EXISTS myFilter github (integer) 0 "},"zother6-JavaGuide/dataStructures-algorithms/Backtracking-NQueens.html":{"url":"zother6-JavaGuide/dataStructures-algorithms/Backtracking-NQueens.html","title":"Backtracking N Queens","keywords":"","body":"N皇后 51. N皇后 题目描述 n 皇后问题研究的是如何将 n 个皇后放置在 n×n 的棋盘上，并且使皇后彼此之间不能相互攻击。 上图为 8 皇后问题的一种解法。 给定一个整数 n，返回所有不同的 n 皇后问题的解决方案。 每一种解法包含一个明确的 n 皇后问题的棋子放置方案，该方案中 'Q' 和 '.' 分别代表了皇后和空位。 示例： 输入: 4 输出: [ [\".Q..\", // 解法 1 \"...Q\", \"Q...\", \"..Q.\"], [\"..Q.\", // 解法 2 \"Q...\", \"...Q\", \".Q..\"] ] 解释: 4 皇后问题存在两个不同的解法。 问题分析 约束条件为每个棋子所在的行、列、对角线都不能有另一个棋子。 使用一维数组表示一种解法，下标（index）表示行，值（value）表示该行的Q（皇后）在哪一列。每行只存储一个元素，然后递归到下一行，这样就不用判断行了，只需要判断列和对角线。 Solution1 当result[row] = column时，即row行的棋子在column列。 对于[0, row-1]的任意一行（i 行），若 row 行的棋子和 i 行的棋子在同一列，则有result[i] == column;若 row 行的棋子和 i 行的棋子在同一对角线，等腰直角三角形两直角边相等，即 row - i == Math.abs(result[i] - column) 布尔类型变量 isValid 的作用是剪枝，减少不必要的递归。 public List> solveNQueens(int n) { // 下标代表行，值代表列。如result[0] = 3 表示第1行的Q在第3列 int[] result = new int[n]; List> resultList = new LinkedList<>(); dfs(resultList, result, 0, n); return resultList; } void dfs(List> resultList, int[] result, int row, int n) { // 递归终止条件 if (row == n) { List list = new LinkedList<>(); for (int x = 0; x = 0; --i) { if (result[i] == column || row - i == Math.abs(result[i] - column)) { isValid = false; break; } } if (isValid) dfs(resultList, result, row + 1, n); } } Solution2 使用LinkedList表示一种解法，下标（index）表示行，值（value）表示该行的Q（皇后）在哪一列。 解法二和解法一的不同在于，相同列以及相同对角线的校验。 将对角线抽象成【一次函数】这个简单的数学模型，根据一次函数的截距是常量这一特性进行校验。 这里，我将右上-左下对角线，简称为“\\”对角线；左上-右下对角线简称为“/”对角线。 “/”对角线斜率为1，对应方程为y = x + b，其中b为截距。对于线上任意一点，均有y - x = b，即row - i = b;定义一个布尔类型数组anti_diag，将b作为下标，当anti_diag[b] = true时，表示相应对角线上已经放置棋子。但row - i有可能为负数，负数不能作为数组下标，row - i 的最小值为-n（当row = 0，i = n时），可以加上n作为数组下标，即将row -i + n 作为数组下标。row - i + n 的最大值为 2n（当row = n，i = 0时），故anti_diag的容量设置为 2n 即可。 “\\”对角线斜率为-1，对应方程为y = -x + b，其中b为截距。对于线上任意一点，均有y + x = b，即row + i = b;同理，定义数组main_diag，将b作为下标，当main_diag[row + i] = true时，表示相应对角线上已经放置棋子。 有了两个校验对角线的数组，再来定义一个用于校验列的数组cols，这个太简单啦，不解释。 解法二时间复杂度为O(n!)，在校验相同列和相同对角线时，引入三个布尔类型数组进行判断。相比解法一，少了一层循环，用空间换时间。 List> resultList = new LinkedList<>(); public List> solveNQueens(int n) { boolean[] cols = new boolean[n]; boolean[] main_diag = new boolean[2 * n]; boolean[] anti_diag = new boolean[2 * n]; LinkedList result = new LinkedList<>(); dfs(result, 0, cols, main_diag, anti_diag, n); return resultList; } void dfs(LinkedList result, int row, boolean[] cols, boolean[] main_diag, boolean[] anti_diag, int n) { if (row == n) { List list = new LinkedList<>(); for (int x = 0; x "},"zother6-JavaGuide/dataStructures-algorithms/公司真题.html":{"url":"zother6-JavaGuide/dataStructures-algorithms/公司真题.html","title":"公司真题","keywords":"","body":"网易 2018 下面三道编程题来自网易2018校招编程题，这三道应该来说是非常简单的编程题了，这些题目大家稍微有点编程和数学基础的话应该没什么问题。看答案之前一定要自己先想一下如果是自己做的话会怎么去做，然后再对照这我的答案看看，和你自己想的有什么区别？那一种方法更好？ 问题 一 获得特定数量硬币问题 小易准备去魔法王国采购魔法神器,购买魔法神器需要使用魔法币,但是小易现在一枚魔法币都没有,但是小易有两台魔法机器可以通过投入x(x可以为0)个魔法币产生更多的魔法币。 魔法机器1:如果投入x个魔法币,魔法机器会将其变为2x+1个魔法币 魔法机器2:如果投入x个魔法币,魔法机器会将其变为2x+2个魔法币 小易采购魔法神器总共需要n个魔法币,所以小易只能通过两台魔法机器产生恰好n个魔法币,小易需要你帮他设计一个投入方案使他最后恰好拥有n个魔法币。 输入描述: 输入包括一行,包括一个正整数n(1 ≤ n ≤ 10^9),表示小易需要的魔法币数量。 输出描述: 输出一个字符串,每个字符表示该次小易选取投入的魔法机器。其中只包含字符'1'和'2'。 输入例子1: 10 输出例子1: 122 二 求“相反数”问题 为了得到一个数的\"相反数\",我们将这个数的数字顺序颠倒,然后再加上原先的数得到\"相反数\"。例如,为了得到1325的\"相反数\",首先我们将该数的数字顺序颠倒,我们得到5231,之后再加上原先的数,我们得到5231+1325=6556.如果颠倒之后的数字有前缀零,前缀零将会被忽略。例如n = 100, 颠倒之后是1. 输入描述: 输入包括一个整数n,(1 ≤ n ≤ 10^5) 输出描述: 输出一个整数,表示n的相反数 输入例子1: 1325 输出例子1: 6556 三 字符串碎片的平均长度 一个由小写字母组成的字符串可以看成一些同一字母的最大碎片组成的。例如,\"aaabbaaac\"是由下面碎片组成的:'aaa','bb','c'。牛牛现在给定一个字符串,请你帮助计算这个字符串的所有碎片的平均长度是多少。 输入描述: 输入包括一个字符串s,字符串s的长度length(1 ≤ length ≤ 50),s只含小写字母('a'-'z') 输出描述: 输出一个整数,表示所有碎片的平均长度,四舍五入保留两位小数。 如样例所示: s = \"aaabbaaac\" 所有碎片的平均长度 = (3 + 2 + 3 + 1) / 4 = 2.25 输入例子1: aaabbaaac 输出例子1: 2.25 答案 一 获得特定数量硬币问题 分析： 作为该试卷的第一题，这道题应该只要思路正确就很简单了。 解题关键：明确魔法机器1只能产生奇数，魔法机器2只能产生偶数即可。我们从后往前一步一步推回去即可。 示例代码 注意：由于用户的输入不确定性，一般是为了程序高可用性使需要将捕获用户输入异常然后友好提示用户输入类型错误并重新输入的。所以下面我给了两个版本，这两个版本都是正确的。这里只是给大家演示如何捕获输入类型异常，后面的题目中我给的代码没有异常处理的部分，参照下面两个示例代码，应该很容易添加。（PS：企业面试中没有明确就不用添加异常处理，当然你有的话也更好） 不带输入异常处理判断的版本： import java.util.Scanner; public class Main2 { // 解题关键：明确魔法机器1只能产生奇数，魔法机器2只能产生偶数即可。我们从后往前一步一步推回去即可。 public static void main(String[] args) { System.out.println(\"请输入要获得的硬币数量：\"); Scanner scanner = new Scanner(System.in); int coincount = scanner.nextInt(); StringBuilder sb = new StringBuilder(); while (coincount >= 1) { // 偶数的情况 if (coincount % 2 == 0) { coincount = (coincount - 2) / 2; sb.append(\"2\"); // 奇数的情况 } else { coincount = (coincount - 1) / 2; sb.append(\"1\"); } } // 输出反转后的字符串 System.out.println(sb.reverse()); } } 带输入异常处理判断的版本（当输入的不是整数的时候会提示重新输入）： import java.util.InputMismatchException; import java.util.Scanner; public class Main { // 解题关键：明确魔法机器1只能产生奇数，魔法机器2只能产生偶数即可。我们从后往前一步一步推回去即可。 public static void main(String[] args) { System.out.println(\"请输入要获得的硬币数量：\"); Scanner scanner = new Scanner(System.in); boolean flag = true; while (flag) { try { int coincount = scanner.nextInt(); StringBuilder sb = new StringBuilder(); while (coincount >= 1) { // 偶数的情况 if (coincount % 2 == 0) { coincount = (coincount - 2) / 2; sb.append(\"2\"); // 奇数的情况 } else { coincount = (coincount - 1) / 2; sb.append(\"1\"); } } // 输出反转后的字符串 System.out.println(sb.reverse()); flag=false;//程序结束 } catch (InputMismatchException e) { System.out.println(\"输入数据类型不匹配，请您重新输入:\"); scanner.nextLine(); continue; } } } } 二 求“相反数”问题 分析： 解决本道题有几种不同的方法，但是最快速的方法就是利用reverse()方法反转字符串然后再将字符串转换成int类型的整数，这个方法是快速解决本题关键。我们先来回顾一下下面两个知识点： 1)String转int； 在 Java 中要将 String 类型转化为 int 类型时,需要使用 Integer 类中的 parseInt() 方法或者 valueOf() 方法进行转换. String str = \"123\"; int a = Integer.parseInt(str); 或 String str = \"123\"; int a = Integer.valueOf(str).intValue()； 2)next()和nextLine()的区别 在Java中输入字符串有两种方法，就是next()和nextLine().两者的区别就是：nextLine()的输入是碰到回车就终止输入，而next()方法是碰到空格，回车，Tab键都会被视为终止符。所以next()不会得到带空格的字符串，而nextLine()可以得到带空格的字符串。 示例代码： import java.util.Scanner; /** * 本题关键：①String转int；②next()和nextLine()的区别 */ public class Main { public static void main(String[] args) { System.out.println(\"请输入一个整数：\"); Scanner scanner = new Scanner(System.in); String s=scanner.next(); //将字符串转换成数字 int number1=Integer.parseInt(s); //将字符串倒序后转换成数字 //因为Integer.parseInt()的参数类型必须是字符串所以必须加上toString() int number2=Integer.parseInt(new StringBuilder(s).reverse().toString()); System.out.println(number1+number2); } } 三 字符串碎片的平均长度 分析： 这道题的意思也就是要求：(字符串的总长度)/(相同字母团构成的字符串的个数)。 这样就很简单了，就变成了字符串的字符之间的比较。如果需要比较字符串的字符的话，我们可以利用charAt(i)方法：取出特定位置的字符与后一个字符比较，或者利用toCharArray()方法将字符串转换成字符数组采用同样的方法做比较。 示例代码 利用charAt(i)方法： import java.util.Scanner; public class Main { public static void main(String[] args) { Scanner sc = new Scanner(System.in); while (sc.hasNext()) { String s = sc.next(); //个数至少为一个 float count = 1; for (int i = 0; i 利用toCharArray()方法： import java.util.Scanner; public class Main2 { public static void main(String[] args) { Scanner sc = new Scanner(System.in); while (sc.hasNext()) { String s = sc.next(); //个数至少为一个 float count = 1; char [] stringArr = s.toCharArray(); for (int i = 0; i "},"zother6-JavaGuide/dataStructures-algorithms/几道常见的子符串算法题.html":{"url":"zother6-JavaGuide/dataStructures-algorithms/几道常见的子符串算法题.html","title":"几道常见的子符串算法题","keywords":"","body":" 说明 1. KMP 算法 2. 替换空格 3. 最长公共前缀 4. 回文串 4.1. 最长回文串 4.2. 验证回文串 4.3. 最长回文子串 4.4. 最长回文子序列 5. 括号匹配深度 6. 把字符串转换成整数 授权转载！ 本文作者：wwwxmu 原文地址:https://www.weiweiblog.cn/13string/ 考虑到篇幅问题，我会分两次更新这个内容。本篇文章只是原文的一部分，我在原文的基础上增加了部分内容以及修改了部分代码和注释。另外，我增加了爱奇艺 2018 秋招 Java：求给定合法括号序列的深度 这道题。所有代码均编译成功，并带有注释，欢迎各位享用！ 1. KMP 算法 谈到字符串问题，不得不提的就是 KMP 算法，它是用来解决字符串查找的问题，可以在一个字符串（S）中查找一个子串（W）出现的位置。KMP 算法把字符匹配的时间复杂度缩小到 O(m+n) ,而空间复杂度也只有O(m)。因为“暴力搜索”的方法会反复回溯主串，导致效率低下，而KMP算法可以利用已经部分匹配这个有效信息，保持主串上的指针不回溯，通过修改子串的指针，让模式串尽量地移动到有效的位置。 具体算法细节请参考： 字符串匹配的KMP算法: http://www.ruanyifeng.com/blog/2013/05/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm.html 从头到尾彻底理解KMP: https://blog.csdn.net/v_july_v/article/details/7041827 如何更好的理解和掌握 KMP 算法?: https://www.zhihu.com/question/21923021 KMP 算法详细解析: https://blog.sengxian.com/algorithms/kmp 图解 KMP 算法: http://blog.jobbole.com/76611/ 汪都能听懂的KMP字符串匹配算法【双语字幕】: https://www.bilibili.com/video/av3246487/?from=search&seid=17173603269940723925 KMP字符串匹配算法1: https://www.bilibili.com/video/av11866460?from=search&seid=12730654434238709250 除此之外，再来了解一下BM算法！ BM算法也是一种精确字符串匹配算法，它采用从右向左比较的方法，同时应用到了两种启发式规则，即坏字符规则 和好后缀规则 ，来决定向右跳跃的距离。基本思路就是从右往左进行字符匹配，遇到不匹配的字符后从坏字符表和好后缀表找一个最大的右移值，将模式串右移继续匹配。 《字符串匹配的KMP算法》:http://www.ruanyifeng.com/blog/2013/05/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm.html 2. 替换空格 剑指offer：请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。 这里我提供了两种方法：①常规方法；②利用 API 解决。 //https://www.weiweiblog.cn/replacespace/ public class Solution { /** * 第一种方法：常规方法。利用String.charAt(i)以及String.valueOf(char).equals(\" \" * )遍历字符串并判断元素是否为空格。是则替换为\"%20\",否则不替换 */ public static String replaceSpace(StringBuffer str) { int length = str.length(); // System.out.println(\"length=\" + length); StringBuffer result = new StringBuffer(); for (int i = 0; i 3. 最长公共前缀 Leetcode: 编写一个函数来查找字符串数组中的最长公共前缀。如果不存在公共前缀，返回空字符串 \"\"。 示例 1: 输入: [\"flower\",\"flow\",\"flight\"] 输出: \"fl\" 示例 2: 输入: [\"dog\",\"racecar\",\"car\"] 输出: \"\" 解释: 输入不存在公共前缀。 思路很简单！先利用Arrays.sort(strs)为数组排序，再将数组第一个元素和最后一个元素的字符从前往后对比即可！ public class Main { public static String replaceSpace(String[] strs) { // 如果检查值不合法及就返回空串 if (!checkStrs(strs)) { return \"\"; } // 数组长度 int len = strs.length; // 用于保存结果 StringBuilder res = new StringBuilder(); // 给字符串数组的元素按照升序排序(包含数字的话，数字会排在前面) Arrays.sort(strs); int m = strs[0].length(); int n = strs[len - 1].length(); int num = Math.min(m, n); for (int i = 0; i 4. 回文串 4.1. 最长回文串 LeetCode: 给定一个包含大写字母和小写字母的字符串，找到通过这些字母构造成的最长的回文串。在构造过程中，请注意区分大小写。比如\"Aa\"不能当做一个回文字符串。注 意:假设字符串的长度不会超过 1010。 回文串：“回文串”是一个正读和反读都一样的字符串，比如“level”或者“noon”等等就是回文串。——百度百科 地址：https://baike.baidu.com/item/%E5%9B%9E%E6%96%87%E4%B8%B2/1274921?fr=aladdin 示例 1: 输入: \"abccccdd\" 输出: 7 解释: 我们可以构造的最长的回文串是\"dccaccd\", 它的长度是 7。 我们上面已经知道了什么是回文串？现在我们考虑一下可以构成回文串的两种情况： 字符出现次数为双数的组合 字符出现次数为偶数的组合+单个字符中出现次数最多且为奇数次的字符 （参见 issue665 ） 统计字符出现的次数即可，双数才能构成回文。因为允许中间一个数单独出现，比如“abcba”，所以如果最后有字母落单，总长度可以加 1。首先将字符串转变为字符数组。然后遍历该数组，判断对应字符是否在hashset中，如果不在就加进去，如果在就让count++，然后移除该字符！这样就能找到出现次数为双数的字符个数。 //https://leetcode-cn.com/problems/longest-palindrome/description/ class Solution { public int longestPalindrome(String s) { if (s.length() == 0) return 0; // 用于存放字符 HashSet hashset = new HashSet(); char[] chars = s.toCharArray(); int count = 0; for (int i = 0; i 4.2. 验证回文串 LeetCode: 给定一个字符串，验证它是否是回文串，只考虑字母和数字字符，可以忽略字母的大小写。 说明：本题中，我们将空字符串定义为有效的回文串。 示例 1: 输入: \"A man, a plan, a canal: Panama\" 输出: true 示例 2: 输入: \"race a car\" 输出: false //https://leetcode-cn.com/problems/valid-palindrome/description/ class Solution { public boolean isPalindrome(String s) { if (s.length() == 0) return true; int l = 0, r = s.length() - 1; while (l 4.3. 最长回文子串 Leetcode: LeetCode: 最长回文子串 给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为1000。 示例 1： 输入: \"babad\" 输出: \"bab\" 注意: \"aba\"也是一个有效答案。 示例 2： 输入: \"cbbd\" 输出: \"bb\" 以某个元素为中心，分别计算偶数长度的回文最大长度和奇数长度的回文最大长度。给大家大致花了个草图，不要嫌弃！ //https://leetcode-cn.com/problems/longest-palindromic-substring/description/ class Solution { private int index, len; public String longestPalindrome(String s) { if (s.length() = 0 && r 4.4. 最长回文子序列 LeetCode: 最长回文子序列 给定一个字符串s，找到其中最长的回文子序列。可以假设s的最大长度为1000。 最长回文子序列和上一题最长回文子串的区别是，子串是字符串中连续的一个序列，而子序列是字符串中保持相对位置的字符序列，例如，\"bbbb\"可以是字符串\"bbbab\"的子序列但不是子串。 给定一个字符串s，找到其中最长的回文子序列。可以假设s的最大长度为1000。 示例 1: 输入: \"bbbab\" 输出: 4 一个可能的最长回文子序列为 \"bbbb\"。 示例 2: 输入: \"cbbd\" 输出: 2 一个可能的最长回文子序列为 \"bb\"。 动态规划： dp[i][j] = dp[i+1][j-1] + 2 if s.charAt(i) == s.charAt(j) otherwise, dp[i][j] = Math.max(dp[i+1][j], dp[i][j-1]) class Solution { public int longestPalindromeSubseq(String s) { int len = s.length(); int [][] dp = new int[len][len]; for(int i = len - 1; i>=0; i--){ dp[i][i] = 1; for(int j = i+1; j 5. 括号匹配深度 爱奇艺 2018 秋招 Java： 一个合法的括号匹配序列有以下定义: 空串\"\"是一个合法的括号匹配序列 如果\"X\"和\"Y\"都是合法的括号匹配序列,\"XY\"也是一个合法的括号匹配序列 如果\"X\"是一个合法的括号匹配序列,那么\"(X)\"也是一个合法的括号匹配序列 每个合法的括号序列都可以由以上规则生成。 例如: \"\",\"()\",\"()()\",\"((()))\"都是合法的括号序列 对于一个合法的括号序列我们又有以下定义它的深度: 空串\"\"的深度是0 如果字符串\"X\"的深度是x,字符串\"Y\"的深度是y,那么字符串\"XY\"的深度为max(x,y) 如果\"X\"的深度是x,那么字符串\"(X)\"的深度是x+1 例如: \"()()()\"的深度是1,\"((()))\"的深度是3。牛牛现在给你一个合法的括号序列,需要你计算出其深度。 输入描述: 输入包括一个合法的括号序列s,s长度length(2 ≤ length ≤ 50),序列中只包含'('和')'。 输出描述: 输出一个正整数,即这个序列的深度。 示例： 输入: (()) 输出: 2 思路草图： 代码如下： import java.util.Scanner; /** * https://www.nowcoder.com/test/8246651/summary * * @author Snailclimb * @date 2018年9月6日 * @Description: TODO 求给定合法括号序列的深度 */ public class Main { public static void main(String[] args) { Scanner sc = new Scanner(System.in); String s = sc.nextLine(); int cnt = 0, max = 0, i; for (i = 0; i 6. 把字符串转换成整数 剑指offer: 将一个字符串转换成一个整数(实现Integer.valueOf(string)的功能，但是string不符合数字要求时返回0)，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。 //https://www.weiweiblog.cn/strtoint/ public class Main { public static int StrToInt(String str) { if (str.length() == 0) return 0; char[] chars = str.toCharArray(); // 判断是否存在符号位 int flag = 0; if (chars[0] == '+') flag = 1; else if (chars[0] == '-') flag = 2; int start = flag > 0 ? 1 : 0; int res = 0;// 保存结果 for (int i = start; i "},"zother6-JavaGuide/dataStructures-algorithms/几道常见的链表算法题.html":{"url":"zother6-JavaGuide/dataStructures-algorithms/几道常见的链表算法题.html","title":"几道常见的链表算法题","keywords":"","body":" 1. 两数相加 题目描述 问题分析 Solution 2. 翻转链表 题目描述 问题分析 Solution 3. 链表中倒数第k个节点 题目描述 问题分析 Solution 4. 删除链表的倒数第N个节点 问题分析 Solution 5. 合并两个排序的链表 题目描述 问题分析 Solution 1. 两数相加 题目描述 Leetcode:给定两个非空链表来表示两个非负整数。位数按照逆序方式存储，它们的每个节点只存储单个数字。将两数相加返回一个新的链表。 你可以假设除了数字 0 之外，这两个数字都不会以零开头。 示例： 输入：(2 -> 4 -> 3) + (5 -> 6 -> 4) 输出：7 -> 0 -> 8 原因：342 + 465 = 807 问题分析 Leetcode官方详细解答地址： https://leetcode-cn.com/problems/add-two-numbers/solution/ 要对头结点进行操作时，考虑创建哑节点dummy，使用dummy->next表示真正的头节点。这样可以避免处理头节点为空的边界问题。 我们使用变量来跟踪进位，并从包含最低有效位的表头开始模拟逐 位相加的过程。 Solution 我们首先从最低有效位也就是列表 l1和 l2 的表头开始相加。注意需要考虑到进位的情况！ /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ //https://leetcode-cn.com/problems/add-two-numbers/description/ class Solution { public ListNode addTwoNumbers(ListNode l1, ListNode l2) { ListNode dummyHead = new ListNode(0); ListNode p = l1, q = l2, curr = dummyHead; //carry 表示进位数 int carry = 0; while (p != null || q != null) { int x = (p != null) ? p.val : 0; int y = (q != null) ? q.val : 0; int sum = carry + x + y; //进位数 carry = sum / 10; //新节点的数值为sum % 10 curr.next = new ListNode(sum % 10); curr = curr.next; if (p != null) p = p.next; if (q != null) q = q.next; } if (carry > 0) { curr.next = new ListNode(carry); } return dummyHead.next; } } 2. 翻转链表 题目描述 剑指 offer:输入一个链表，反转链表后，输出链表的所有元素。 问题分析 这道算法题，说直白点就是：如何让后一个节点指向前一个节点！在下面的代码中定义了一个 next 节点，该节点主要是保存要反转到头的那个节点，防止链表 “断裂”。 Solution public class ListNode { int val; ListNode next = null; ListNode(int val) { this.val = val; } } /** * * @author Snailclimb * @date 2018年9月19日 * @Description: TODO */ public class Solution { public ListNode ReverseList(ListNode head) { ListNode next = null; ListNode pre = null; while (head != null) { // 保存要反转到头的那个节点 next = head.next; // 要反转的那个节点指向已经反转的上一个节点(备注:第一次反转的时候会指向null) head.next = pre; // 上一个已经反转到头部的节点 pre = head; // 一直向链表尾走 head = next; } return pre; } } 测试方法： public static void main(String[] args) { ListNode a = new ListNode(1); ListNode b = new ListNode(2); ListNode c = new ListNode(3); ListNode d = new ListNode(4); ListNode e = new ListNode(5); a.next = b; b.next = c; c.next = d; d.next = e; new Solution().ReverseList(a); while (e != null) { System.out.println(e.val); e = e.next; } } 输出： 5 4 3 2 1 3. 链表中倒数第k个节点 题目描述 剑指offer: 输入一个链表，输出该链表中倒数第k个结点。 问题分析 链表中倒数第k个节点也就是正数第(L-K+1)个节点，知道了只一点，这一题基本就没问题！ 首先两个节点/指针，一个节点 node1 先开始跑，指针 node1 跑到 k-1 个节点后，另一个节点 node2 开始跑，当 node1 跑到最后时，node2 所指的节点就是倒数第k个节点也就是正数第(L-K+1)个节点。 Solution /* public class ListNode { int val; ListNode next = null; ListNode(int val) { this.val = val; } }*/ // 时间复杂度O(n),一次遍历即可 // https://www.nowcoder.com/practice/529d3ae5a407492994ad2a246518148a?tpId=13&tqId=11167&tPage=1&rp=1&ru=/ta/coding-interviews&qru=/ta/coding-interviews/question-ranking public class Solution { public ListNode FindKthToTail(ListNode head, int k) { // 如果链表为空或者k小于等于0 if (head == null || k 4. 删除链表的倒数第N个节点 Leetcode:给定一个链表，删除链表的倒数第 n 个节点，并且返回链表的头结点。 示例： 给定一个链表: 1->2->3->4->5, 和 n = 2. 当删除了倒数第二个节点后，链表变为 1->2->3->5. 说明： 给定的 n 保证是有效的。 进阶： 你能尝试使用一趟扫描实现吗？ 该题在 leetcode 上有详细解答，具体可参考 Leetcode. 问题分析 我们注意到这个问题可以容易地简化成另一个问题：删除从列表开头数起的第 (L - n + 1)个结点，其中 L是列表的长度。只要我们找到列表的长度 L，这个问题就很容易解决。 Solution 两次遍历法 首先我们将添加一个 哑结点 作为辅助，该结点位于列表头部。哑结点用来简化某些极端情况，例如列表中只含有一个结点，或需要删除列表的头部。在第一次遍历中，我们找出列表的长度 L。然后设置一个指向哑结点的指针，并移动它遍历列表，直至它到达第 (L - n) 个结点那里。我们把第 (L - n)个结点的 next 指针重新链接至第 (L - n + 2)个结点，完成这个算法。 /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ // https://leetcode-cn.com/problems/remove-nth-node-from-end-of-list/description/ public class Solution { public ListNode removeNthFromEnd(ListNode head, int n) { // 哑结点，哑结点用来简化某些极端情况，例如列表中只含有一个结点，或需要删除列表的头部 ListNode dummy = new ListNode(0); // 哑结点指向头结点 dummy.next = head; // 保存链表长度 int length = 0; ListNode len = head; while (len != null) { length++; len = len.next; } length = length - n; ListNode target = dummy; // 找到 L-n 位置的节点 while (length > 0) { target = target.next; length--; } // 把第 (L - n)个结点的 next 指针重新链接至第 (L - n + 2)个结点 target.next = target.next.next; return dummy.next; } } 复杂度分析： 时间复杂度 O(L) ：该算法对列表进行了两次遍历，首先计算了列表的长度 LL 其次找到第 (L - n)(L−n) 个结点。 操作执行了 2L-n2L−n 步，时间复杂度为 O(L)O(L)。 空间复杂度 O(1) ：我们只用了常量级的额外空间。 进阶——一次遍历法： **链表中倒数第N个节点也就是正数第(L-N+1)个节点。 其实这种方法就和我们上面第四题找“链表中倒数第k个节点”所用的思想是一样的。基本思路就是： 定义两个节点 node1、node2;node1 节点先跑，node1节点 跑到第 n+1 个节点的时候,node2 节点开始跑.当node1 节点跑到最后一个节点时，node2 节点所在的位置就是第 （L-n ） 个节点（L代表总链表长度，也就是倒数第 n+1 个节点） /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ public class Solution { public ListNode removeNthFromEnd(ListNode head, int n) { ListNode dummy = new ListNode(0); dummy.next = head; // 声明两个指向头结点的节点 ListNode node1 = dummy, node2 = dummy; // node1 节点先跑，node1节点 跑到第 n 个节点的时候,node2 节点开始跑 // 当node1 节点跑到最后一个节点时，node2 节点所在的位置就是第 （L-n ） 个节点，也就是倒数第 n+1（L代表总链表长度） while (node1 != null) { node1 = node1.next; if (n 5. 合并两个排序的链表 题目描述 剑指offer:输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。 问题分析 我们可以这样分析: 假设我们有两个链表 A,B； A的头节点A1的值与B的头结点B1的值比较，假设A1小，则A1为头节点； A2再和B1比较，假设B1小,则，A1指向B1； A2再和B2比较 就这样循环往复就行了，应该还算好理解。 考虑通过递归的方式实现！ Solution 递归版本： /* public class ListNode { int val; ListNode next = null; ListNode(int val) { this.val = val; } }*/ //https://www.nowcoder.com/practice/d8b6b4358f774294a89de2a6ac4d9337?tpId=13&tqId=11169&tPage=1&rp=1&ru=/ta/coding-interviews&qru=/ta/coding-interviews/question-ranking public class Solution { public ListNode Merge(ListNode list1,ListNode list2) { if(list1 == null){ return list2; } if(list2 == null){ return list1; } if(list1.val "},"zother6-JavaGuide/dataStructures-algorithms/剑指offer部分编程题.html":{"url":"zother6-JavaGuide/dataStructures-algorithms/剑指offer部分编程题.html","title":"剑指offer部分编程题","keywords":"","body":"一 斐波那契数列 题目描述： 大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项。 n 问题分析： 可以肯定的是这一题通过递归的方式是肯定能做出来，但是这样会有一个很大的问题，那就是递归大量的重复计算会导致内存溢出。另外可以使用迭代法，用fn1和fn2保存计算过程中的结果，并复用起来。下面我会把两个方法示例代码都给出来并给出两个方法的运行时间对比。 示例代码： 采用迭代法： int Fibonacci(int number) { if (number 采用递归： public int Fibonacci(int n) { if (n 二 跳台阶问题 题目描述： 一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 问题分析： 正常分析法： a.如果两种跳法，1阶或者2阶，那么假定第一次跳的是一阶，那么剩下的是n-1个台阶，跳法是f(n-1); b.假定第一次跳的是2阶，那么剩下的是n-2个台阶，跳法是f(n-2) c.由a，b假设可以得出总跳法为: f(n) = f(n-1) + f(n-2) d.然后通过实际的情况可以得出：只有一阶的时候 f(1) = 1 ,只有两阶的时候可以有 f(2) = 2 找规律分析法： f(1) = 1, f(2) = 2, f(3) = 3, f(4) = 5， 可以总结出f(n) = f(n-1) + f(n-2)的规律。 但是为什么会出现这样的规律呢？假设现在6个台阶，我们可以从第5跳一步到6，这样的话有多少种方案跳到5就有多少种方案跳到6，另外我们也可以从4跳两步跳到6，跳到4有多少种方案的话，就有多少种方案跳到6，其他的不能从3跳到6什么的啦，所以最后就是f(6) = f(5) + f(4)；这样子也很好理解变态跳台阶的问题了。 所以这道题其实就是斐波那契数列的问题。 代码只需要在上一题的代码稍做修改即可。和上一题唯一不同的就是这一题的初始元素变为 1 2 3 5 8.....而上一题为1 1 2 3 5 .......。另外这一题也可以用递归做，但是递归效率太低，所以我这里只给出了迭代方式的代码。 示例代码： int jumpFloor(int number) { if (number 三 变态跳台阶问题 题目描述： 一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 问题分析： 假设n>=2，第一步有n种跳法：跳1级、跳2级、到跳n级 跳1级，剩下n-1级，则剩下跳法是f(n-1) 跳2级，剩下n-2级，则剩下跳法是f(n-2) ...... 跳n-1级，剩下1级，则剩下跳法是f(1) 跳n级，剩下0级，则剩下跳法是f(0) 所以在n>=2的情况下： f(n)=f(n-1)+f(n-2)+...+f(1) 因为f(n-1)=f(n-2)+f(n-3)+...+f(1) 所以f(n)=2f(n-1) 又f(1)=1,所以可得*f(n)=2^(number-1) 示例代码： int JumpFloorII(int number) { return 1 补充： java中有三种移位运算符： “左移运算符，等同于乘2的n次方 “>>”: 右移运算符，等同于除2的n次方 “>>>” 无符号右移运算符，不管移动前最高位是0还是1，右移后左侧产生的空位部分都以0来填充。与>>类似。 例： int a = 16; int b = a 2的2次方，也就是16 4 int c = a >> 2;//右移2，等同于16 / 2的2次方，也就是16 / 4 四 二维数组查找 题目描述： 在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 问题解析： 这一道题还是比较简单的，我们需要考虑的是如何做，效率最快。这里有一种很好理解的思路： 矩阵是有序的，从左下角来看，向上数字递减，向右数字递增， 因此从左下角开始查找，当要查找数字比左下角数字大时。右移 要查找数字比左下角数字小时，上移。这样找的速度最快。 示例代码： public boolean Find(int target, int [][] array) { //基本思路从左下角开始找，这样速度最快 int row = array.length-1;//行 int column = 0;//列 //当行数大于0，当前列数小于总列数时循环条件成立 while((row >= 0)&& (column target){ row--; }else if(array[row][column] 五 替换空格 题目描述： 请实现一个函数，将一个字符串中的空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。 问题分析： 这道题不难，我们可以通过循环判断字符串的字符是否为空格，是的话就利用append()方法添加追加“%20”，否则还是追加原字符。 或者最简单的方法就是利用： replaceAll(String regex,String replacement)方法了，一行代码就可以解决。 示例代码： 常规做法： public String replaceSpace(StringBuffer str) { StringBuffer out=new StringBuffer(); for (int i = 0; i 一行代码解决： public String replaceSpace(StringBuffer str) { //return str.toString().replaceAll(\" \", \"%20\"); //public String replaceAll(String regex,String replacement) //用给定的替换替换与给定的regular expression匹配的此字符串的每个子字符串。 //\\ 转义字符. 如果你要使用 \"\\\" 本身, 则应该使用 \"\\\\\". String类型中的空格用“\\s”表示，所以我这里猜测\"\\\\s\"就是代表空格的意思 return str.toString().replaceAll(\"\\\\s\", \"%20\"); } 六 数值的整数次方 题目描述： 给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。 问题解析： 这道题算是比较麻烦和难一点的一个了。我这里采用的是二分幂思想，当然也可以采用快速幂。 更具剑指offer书中细节，该题的解题思路如下： 1.当底数为0且指数（a^n/2）； 当n为奇数，a^n = a^[(n-1)/2] a^[(n-1)/2] * a。时间复杂度O(logn) 时间复杂度：O(logn) 示例代码： public class Solution { boolean invalidInput=false; public double Power(double base, int exponent) { //如果底数等于0并且指数小于0 //由于base为double型，不能直接用==判断 if(equal(base,0.0)&&exponent-0.000001&&num1-num2>1相等于e/2，这里就是求a^n =（a^n/2）*（a^n/2） double result=getPower(b,e>>1); result*=result; //如果指数n为奇数，则要再乘一次底数base if((e&1)==1) result*=b; return result; } } 当然这一题也可以采用笨方法：累乘。不过这种方法的时间复杂度为O（n），这样没有前一种方法效率高。 // 使用累乘 public double powerAnother(double base, int exponent) { double result = 1.0; for (int i = 0; i = 0) return result; else return 1 / result; } 七 调整数组顺序使奇数位于偶数前面 题目描述： 输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。 问题解析： 这道题有挺多种解法的，给大家介绍一种我觉得挺好理解的方法： 我们首先统计奇数的个数假设为n,然后新建一个等长数组，然后通过循环判断原数组中的元素为偶数还是奇数。如果是则从数组下标0的元素开始，把该奇数添加到新数组；如果是偶数则从数组下标为n的元素开始把该偶数添加到新数组中。 示例代码： 时间复杂度为O（n），空间复杂度为O（n）的算法 public class Solution { public void reOrderArray(int [] array) { //如果数组长度等于0或者等于1，什么都不做直接返回 if(array.length==0||array.length==1) return; //oddCount：保存奇数个数 //oddBegin：奇数从数组头部开始添加 int oddCount=0,oddBegin=0; //新建一个数组 int[] newArray=new int[array.length]; //计算出（数组中的奇数个数）开始添加元素 for(int i=0;i 八 链表中倒数第k个节点 题目描述： 输入一个链表，输出该链表中倒数第k个结点 问题分析： 一句话概括： 两个指针一个指针p1先开始跑，指针p1跑到k-1个节点后，另一个节点p2开始跑，当p1跑到最后时，p2所指的指针就是倒数第k个节点。 思想的简单理解： 前提假设：链表的结点个数(长度)为n。 规律一：要找到倒数第k个结点，需要向前走多少步呢？比如倒数第一个结点，需要走n步，那倒数第二个结点呢？很明显是向前走了n-1步，所以可以找到规律是找到倒数第k个结点，需要向前走n-k+1步。 算法开始： 设两个都指向head的指针p1和p2，当p1走了k-1步的时候，停下来。p2之前一直不动。 p1的下一步是走第k步，这个时候，p2开始一起动了。至于为什么p2这个时候动呢？看下面的分析。 当p1走到链表的尾部时，即p1走了n步。由于我们知道p2是在p1走了k-1步才开始动的，也就是说p1和p2永远差k-1步。所以当p1走了n步时，p2走的应该是在n-(k-1)步。即p2走了n-k+1步，此时巧妙的是p2正好指向的是规律一的倒数第k个结点处。 这样是不是很好理解了呢？ 考察内容： 链表+代码的鲁棒性 示例代码： /* //链表类 public class ListNode { int val; ListNode next = null; ListNode(int val) { this.val = val; } }*/ //时间复杂度O(n),一次遍历即可 public class Solution { public ListNode FindKthToTail(ListNode head,int k) { ListNode pre=null,p=null; //两个指针都指向头结点 p=head; pre=head; //记录k值 int a=k; //记录节点的个数 int count=0; //p指针先跑，并且记录节点数，当p指针跑了k-1个节点后，pre指针开始跑， //当p指针跑到最后时，pre所指指针就是倒数第k个节点 while(p!=null){ p=p.next; count++; if(k 九 反转链表 题目描述： 输入一个链表，反转链表后，输出链表的所有元素。 问题分析： 链表的很常规的一道题，这一道题思路不算难，但自己实现起来真的可能会感觉无从下手，我是参考了别人的代码。 思路就是我们根据链表的特点，前一个节点指向下一个节点的特点，把后面的节点移到前面来。 就比如下图：我们把1节点和2节点互换位置，然后再将3节点指向2节点，4节点指向3节点，这样以来下面的链表就被反转了。 考察内容： 链表+代码的鲁棒性 示例代码： /* public class ListNode { int val; ListNode next = null; ListNode(int val) { this.val = val; } }*/ public class Solution { public ListNode ReverseList(ListNode head) { ListNode next = null; ListNode pre = null; while (head != null) { //保存要反转到头来的那个节点 next = head.next; //要反转的那个节点指向已经反转的上一个节点 head.next = pre; //上一个已经反转到头部的节点 pre = head; //一直向链表尾走 head = next; } return pre; } } 十 合并两个排序的链表 题目描述： 输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。 问题分析： 我们可以这样分析: 假设我们有两个链表 A,B； A的头节点A1的值与B的头结点B1的值比较，假设A1小，则A1为头节点； A2再和B1比较，假设B1小,则，A1指向B1； A2再和B2比较。。。。。。。 就这样循环往复就行了，应该还算好理解。 考察内容： 链表+代码的鲁棒性 示例代码： 非递归版本： /* public class ListNode { int val; ListNode next = null; ListNode(int val) { this.val = val; } }*/ public class Solution { public ListNode Merge(ListNode list1,ListNode list2) { //list1为空，直接返回list2 if(list1 == null){ return list2; } //list2为空，直接返回list1 if(list2 == null){ return list1; } ListNode mergeHead = null; ListNode current = null; //当list1和list2不为空时 while(list1!=null && list2!=null){ //取较小值作头结点 if(list1.val 递归版本： public ListNode Merge(ListNode list1,ListNode list2) { if(list1 == null){ return list2; } if(list2 == null){ return list1; } if(list1.val 十一 用两个栈实现队列 题目描述： 用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。 问题分析： 先来回顾一下栈和队列的基本特点： 栈：后进先出（LIFO） 队列： 先进先出 很明显我们需要根据JDK给我们提供的栈的一些基本方法来实现。先来看一下Stack类的一些基本方法： 既然题目给了我们两个栈，我们可以这样考虑当push的时候将元素push进stack1，pop的时候我们先把stack1的元素pop到stack2，然后再对stack2执行pop操作，这样就可以保证是先进先出的。（负[pop]负[pop]得正[先进先出]） 考察内容： 队列+栈 示例代码： //左程云的《程序员代码面试指南》的答案 import java.util.Stack; public class Solution { Stack stack1 = new Stack(); Stack stack2 = new Stack(); //当执行push操作时，将元素添加到stack1 public void push(int node) { stack1.push(node); } public int pop() { //如果两个队列都为空则抛出异常,说明用户没有push进任何元素 if(stack1.empty()&&stack2.empty()){ throw new RuntimeException(\"Queue is empty!\"); } //如果stack2不为空直接对stack2执行pop操作， if(stack2.empty()){ while(!stack1.empty()){ //将stack1的元素按后进先出push进stack2里面 stack2.push(stack1.pop()); } } return stack2.pop(); } } 十二 栈的压入,弹出序列 题目描述： 输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4，5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的） 题目分析： 这道题想了半天没有思路，参考了Alias的答案，他的思路写的也很详细应该很容易看懂。 作者：Alias https://www.nowcoder.com/questionTerminal/d77d11405cc7470d82554cb392585106 来源：牛客网 【思路】借用一个辅助的栈，遍历压栈顺序，先讲第一个放入栈中，这里是1，然后判断栈顶元素是不是出栈顺序的第一个元素，这里是4，很显然1≠4，所以我们继续压栈，直到相等以后开始出栈，出栈一个元素，则将出栈顺序向后移动一位，直到不相等，这样循环等压栈顺序遍历完成，如果辅助栈还不为空，说明弹出序列不是该栈的弹出顺序。 举例： 入栈1,2,3,4,5 出栈4,5,3,2,1 首先1入辅助栈，此时栈顶1≠4，继续入栈2 此时栈顶2≠4，继续入栈3 此时栈顶3≠4，继续入栈4 此时栈顶4＝4，出栈4，弹出序列向后一位，此时为5，,辅助栈里面是1,2,3 此时栈顶3≠5，继续入栈5 此时栈顶5=5，出栈5,弹出序列向后一位，此时为3，,辅助栈里面是1,2,3 …. 依次执行，最后辅助栈为空。如果不为空说明弹出序列不是该栈的弹出顺序。 考察内容： 栈 示例代码： import java.util.ArrayList; import java.util.Stack; //这道题没想出来，参考了Alias同学的答案：https://www.nowcoder.com/questionTerminal/d77d11405cc7470d82554cb392585106 public class Solution { public boolean IsPopOrder(int [] pushA,int [] popA) { if(pushA.length == 0 || popA.length == 0) return false; Stack s = new Stack(); //用于标识弹出序列的位置 int popIndex = 0; for(int i = 0; i "},"zother6-JavaGuide/dataStructures-algorithms/数据结构.html":{"url":"zother6-JavaGuide/dataStructures-algorithms/数据结构.html","title":"数据结构","keywords":"","body":"下面只是简单地总结，给了一些参考文章，后面会对这部分内容进行重构。 Queue 什么是队列 队列的种类 Java 集合框架中的队列 Queue 推荐文章 Set 什么是 Set 补充：有序集合与无序集合说明 HashSet 和 TreeSet 底层数据结构 推荐文章 List 什么是List List的常见实现类 ArrayList 和 LinkedList 源码学习 推荐阅读 Map 树 Queue 什么是队列 队列是数据结构中比较重要的一种类型，它支持 FIFO，尾部添加、头部删除（先进队列的元素先出队列），跟我们生活中的排队类似。 队列的种类 单队列（单队列就是常见的队列, 每次添加元素时，都是添加到队尾，存在“假溢出”的问题也就是明明有位置却不能添加的情况） 循环队列（避免了“假溢出”的问题） Java 集合框架中的队列 Queue Java 集合中的 Queue 继承自 Collection 接口 ，Deque, LinkedList, PriorityQueue, BlockingQueue 等类都实现了它。 Queue 用来存放 等待处理元素 的集合，这种场景一般用于缓冲、并发访问。 除了继承 Collection 接口的一些方法，Queue 还添加了额外的 添加、删除、查询操作。 推荐文章 Java 集合深入理解（9）：Queue 队列 Set 什么是 Set Set 继承于 Collection 接口，是一个不允许出现重复元素，并且无序的集合，主要 HashSet 和 TreeSet 两大实现类。 在判断重复元素的时候，HashSet 集合会调用 hashCode()和 equal()方法来实现；TreeSet 集合会调用compareTo方法来实现。 补充：有序集合与无序集合说明 有序集合：集合里的元素可以根据 key 或 index 访问 (List、Map) 无序集合：集合里的元素只能遍历。（Set） HashSet 和 TreeSet 底层数据结构 HashSet 是哈希表结构，主要利用 HashMap 的 key 来存储元素，计算插入元素的 hashCode 来获取元素在集合中的位置； TreeSet 是红黑树结构，每一个元素都是树中的一个节点，插入的元素都会进行排序； 推荐文章 Java集合--Set(基础) List 什么是List 在 List 中，用户可以精确控制列表中每个元素的插入位置，另外用户可以通过整数索引（列表中的位置）访问元素，并搜索列表中的元素。 与 Set 不同，List 通常允许重复的元素。 另外 List 是有序集合而 Set 是无序集合。 List的常见实现类 ArrayList 是一个数组队列，相当于动态数组。它由数组实现，随机访问效率高，随机插入、随机删除效率低。 LinkedList 是一个双向链表。它也可以被当作堆栈、队列或双端队列进行操作。LinkedList随机访问效率低，但随机插入、随机删除效率高。 Vector 是矢量队列，和ArrayList一样，它也是一个动态数组，由数组实现。但是ArrayList是非线程安全的，而Vector是线程安全的。 Stack 是栈，它继承于Vector。它的特性是：先进后出(FILO, First In Last Out)。相关阅读：java数据结构与算法之栈（Stack）设计与实现 ArrayList 和 LinkedList 源码学习 ArrayList 源码学习 LinkedList 源码学习 推荐阅读 java 数据结构与算法之顺序表与链表深入分析 Map 集合框架源码学习之 HashMap(JDK1.8) ConcurrentHashMap 实现原理及源码分析 树 1 二叉树 二叉树（百度百科） (1)完全二叉树——若设二叉树的高度为h，除第 h 层外，其它各层 (1～h-1) 的结点数都达到最大个数，第h层有叶子结点，并且叶子结点都是从左到右依次排布，这就是完全二叉树。 (2)满二叉树——除了叶结点外每一个结点都有左右子叶且叶子结点都处在最底层的二叉树。 (3)平衡二叉树——平衡二叉树又被称为AVL树（区别于AVL算法），它是一棵二叉排序树，且具有以下性质：它是一棵空树或它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树。 2 完全二叉树 完全二叉树（百度百科） 完全二叉树：叶节点只能出现在最下层和次下层，并且最下面一层的结点都集中在该层最左边的若干位置的二叉树。 3 满二叉树 满二叉树（百度百科，国内外的定义不同） 国内教程定义：一个二叉树，如果每一个层的结点数都达到最大值，则这个二叉树就是满二叉树。也就是说，如果一个二叉树的层数为K，且结点总数是(2^k) -1 ，则它就是满二叉树。 堆 数据结构之堆的定义 堆是具有以下性质的完全二叉树：每个结点的值都大于或等于其左右孩子结点的值，称为大顶堆；或者每个结点的值都小于或等于其左右孩子结点的值，称为小顶堆。 4 二叉查找树（BST） 浅谈算法和数据结构: 七 二叉查找树 二叉查找树的特点： 若任意节点的左子树不空，则左子树上所有结点的 值均小于它的根结点的值； 若任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 任意节点的左、右子树也分别为二叉查找树； 没有键值相等的节点（no duplicate nodes）。 5 平衡二叉树（Self-balancing binary search tree） 平衡二叉树（百度百科，平衡二叉树的常用实现方法有红黑树、AVL、替罪羊树、Treap、伸展树等） 6 红黑树 红黑树特点: 每个节点非红即黑； 根节点总是黑色的； 每个叶子节点都是黑色的空节点（NIL节点）； 如果节点是红色的，则它的子节点必须是黑色的（反之不一定）； 从根节点到叶节点或空子节点的每条路径，必须包含相同数目的黑色节点（即相同的黑色高度）。 红黑树的应用： TreeMap、TreeSet以及JDK1.8的HashMap底层都用到了红黑树。 为什么要用红黑树？ 简单来说红黑树就是为了解决二叉查找树的缺陷，因为二叉查找树在某些情况下会退化成一个线性结构。详细了解可以查看 漫画：什么是红黑树？（也介绍到了二叉查找树，非常推荐） 推荐文章： 漫画：什么是红黑树？（也介绍到了二叉查找树，非常推荐） 寻找红黑树的操作手册（文章排版以及思路真的不错） 红黑树深入剖析及Java实现（美团点评技术团队） 7 B-，B+，B*树 二叉树学习笔记之B树、B+树、B*树 《B-树，B+树，B*树详解》 《B-树，B+树与B*树的优缺点比较》 B-树（或B树）是一种平衡的多路查找（又称排序）树，在文件系统中有所应用。主要用作文件的索引。其中的B就表示平衡(Balance) B+ 树的叶子节点链表结构相比于 B- 树便于扫库，和范围检索。 B+树支持range-query（区间查询）非常方便，而B树不支持。这是数据库选用B+树的最主要原因。 B*树 是B+树的变体，B*树分配新结点的概率比B+树要低，空间使用率更高； 8 LSM 树 [HBase] LSM树 VS B+树 B+树最大的性能问题是会产生大量的随机IO 为了克服B+树的弱点，HBase引入了LSM树的概念，即Log-Structured Merge-Trees。 LSM树由来、设计思想以及应用到HBase的索引 图 BFS及DFS 《使用BFS及DFS遍历树和图的思路及实现》 "},"zother6-JavaGuide/dataStructures-algorithms/算法学习资源推荐.html":{"url":"zother6-JavaGuide/dataStructures-algorithms/算法学习资源推荐.html","title":"算法学习资源推荐","keywords":"","body":"先占个坑，说一下我觉得算法这部分学习比较好的规划： 未入门（对算法和基本数据结构不了解）之前建议先找一本入门书籍看； 如果时间比较多可以看一下我推荐的经典部分的书籍，《算法》这本书是首要要看的，其他推荐的神书看自己时间和心情就好，不要太纠结。 如果要准备面试，时间比较紧的话，就不需要再去看《算法》这本书了，时间来不及，当然你也可以选取其特定的章节查看。我也推荐了几本不错的专门为算法面试准备的书籍比如《剑指offer》和《程序员代码面试指南》。除了这两本书籍的话，我在下面推荐了 Leetcode 和牛客网这两个常用的刷题网站以及一些比较好的题目资源。 书籍推荐 以下提到的部分书籍的 PDF 高清阅读版本在我的公众号“JavaGuide”后台回复“书籍”即可获取。 先来看三本入门书籍，这三本入门书籍中的任何一本拿来作为入门学习都非常好。我个人比较倾向于 《我的第一本算法书》 这本书籍，虽然它相比于其他两本书集它的豆瓣评分略低一点。我觉得它的配图以及讲解是这三本书中最优秀，唯一比较明显的问题就是没有代码示例。但是，我觉得这不影响它是一本好的算法书籍。因为本身下面这三本入门书籍的目的就不是通过代码来让你的算法有多厉害，只是作为一本很好的入门书籍让你进入算法学习的大门。 入门 我的第一本算法书 （豆瓣评分 7.1，0.2K+人评价） 一本不那么“专业”的算法书籍。和下面两本推荐的算法书籍都是比较通俗易懂，“不那么深入”的算法书籍。我个人非常推荐，配图和讲解都非常不错！ 《算法图解》（豆瓣评分 8.4，1.5K+人评价） 入门类型的书籍，读起来比较浅显易懂，非常适合没有算法基础或者说算法没学好的小伙伴用来入门。示例丰富，图文并茂，以让人容易理解的方式阐释了算法.读起来比较快，内容不枯燥！ 啊哈!算法 （豆瓣评分 7.7，0.5K+人评价） 和《算法图解》类似的算法趣味入门书籍。 经典 《算法 第四版》（豆瓣评分 9.3，0.4K+人评价） 我在大二的时候被我们的一个老师强烈安利过！自己也在当时购买了一本放在宿舍，到离开大学的时候自己大概看了一半多一点。因为内容实在太多了！另外，这本书还提供了详细的Java代码，非常适合学习 Java 的朋友来看，可以说是 Java 程序员的必备书籍之一了。 再来介绍一下这本书籍吧！这本书籍算的上是算法领域经典的参考书，全面介绍了关于算法和数据结构的必备知识，并特别针对排序、搜索、图处理和字符串处理进行了论述。 下面这些书籍都是经典中的经典，但是阅读起来难度也比较大，不做太多阐述，神书就完事了！推荐先看 《算法》，然后再选下面的书籍进行进一步阅读。不需要都看，找一本好好看或者找某本书的某一个章节知识点好好看。 编程珠玑（豆瓣评分 9.1，2K+人评价） 经典名著，被无数读者强烈推荐的书籍，几乎是顶级程序员必看的书籍之一了。这本书的作者也非常厉害，Java之父 James Gosling 就是他的学生。 很多人都说这本书不是教你具体的算法，而是教你一种编程的思考方式。这种思考方式不仅仅在编程领域适用，在其他同样适用。 《算法设计手册》（豆瓣评分9.1 ， 45人评价） 被 Teach Yourself Computer Science 强烈推荐的一本算法书籍。 《算法导论》 （豆瓣评分 9.2，0.4K+人评价） 《计算机程序设计艺术（第1卷）》（豆瓣评分 9.4，0.4K+人评价） 面试 《剑指Offer》（豆瓣评分 8.3，0.7K+人评价） 这本面试宝典上面涵盖了很多经典的算法面试题，如果你要准备大厂面试的话一定不要错过这本书。 《剑指Offer》 对应的算法编程题部分的开源项目解析：CodingInterviews 程序员代码面试指南：IT名企算法与数据结构题目最优解（第2版） （豆瓣评分 8.7，0.2K+人评价） 题目相比于《剑指 offer》 来说要难很多，题目涵盖面相比于《剑指 offer》也更加全面。全书一共有将近300道真实出现过的经典代码面试题。 编程之美（豆瓣评分 8.4，3K+人评价） 这本书收集了约60道算法和程序设计题目，这些题目大部分在近年的笔试、面试中出现过，或者是被微软员工热烈讨论过。作者试图从书中各种有趣的问题出发，引导读者发现问题，分析问题，解决问题，寻找更优的解法。 网站推荐 我比较推荐大家可以刷一下 Leetcode ，我自己平时没事也会刷一下，我觉得刷 Leetcode 不仅是为了能让你更从容地面对面试中的手撕算法问题，更可以提高你的编程思维能力、解决问题的能力以及你对某门编程语言 API 的熟练度。当然牛客网也有一些算法题，我下面也整理了一些。 LeetCode 如何高效地使用 LeetCode 《程序员代码面试指南》 《剑指offer》 牛客网 在线编程： 《剑指offer》 《程序员代码面试指南》 2019 校招真题 大一大二编程入门训练 ....... 大厂编程面试真题 "},"zother6-JavaGuide/essential-content-for-interview/BATJrealInterviewExperience/2019alipay-pinduoduo-toutiao.html":{"url":"zother6-JavaGuide/essential-content-for-interview/BATJrealInterviewExperience/2019alipay-pinduoduo-toutiao.html","title":"2019 Alipay Pinduoduo Toutiao","keywords":"","body":"作者： rhwayfun,原文地址：https://mp.weixin.qq.com/s/msYty4vjjC0PvrwasRH5Bw ,JavaGuide 已经获得作者授权并对原文进行了重新排版。 写在2019年后的蚂蚁、头条、拼多多的面试总结 准备过程 蚂蚁金服 一面 二面 三面 四面 五面 小结 拼多多 面试前 一面 二面 三面 小结 字节跳动 面试前 一面 二面 小结 总结 2019年蚂蚁金服、头条、拼多多的面试总结 文章有点长，请耐心看完，绝对有收获！不想听我BB直接进入面试分享： 准备过程 蚂蚁金服面试分享 拼多多面试分享 字节跳动面试分享 总结 说起来开始进行面试是年前倒数第二周，上午9点，我还在去公司的公交上，突然收到蚂蚁的面试电话，其实算不上真正的面试。面试官只是和我聊了下他们在做的事情（主要是做双十一这里大促的稳定性保障，偏中间件吧），说的很详细，然后和我沟通了下是否有兴趣，我表示有兴趣，后面就收到正式面试的通知，最后没选择去蚂蚁表示抱歉。 当时我自己也准备出去看看机会，顺便看看自己的实力。当时我其实挺纠结的，一方面现在部门也正需要我，还是可以有一番作为的，另一方面觉得近一年来进步缓慢，没有以前飞速进步的成就感了，而且业务和技术偏于稳定，加上自己也属于那种比较懒散的人，骨子里还是希望能够突破现状，持续在技术上有所精进。 在开始正式的总结之前，还是希望各位同仁能否听我继续发泄一会，抱拳！ 我翻开自己2018年初立的flag，觉得甚是惭愧。其中就有一条是保持一周写一篇博客，奈何中间因为各种原因没能坚持下去。细细想来，主要是自己没能真正静下来心认真投入到技术的研究和学习，那么为什么会这样？说白了还是因为没有确定目标或者目标不明确，没有目标或者目标不明确都可能导致行动的失败。 那么问题来了，目标是啥？就我而言，短期目标是深入研究某一项技术，比如最近在研究mysql，那么深入研究一定要动手实践并且有所产出，这就够了么？还需要我们能够举一反三，结合实际开发场景想一想日常开发要注意什么，这中间有没有什么坑？可以看出，要进步真的不是一件简单的事，这种反人类的行为需要我们克服自我的弱点，逐渐形成习惯。真正牛逼的人，从不觉得认真学习是一件多么难的事，因为这已经形成了他的习惯，就喝早上起床刷牙洗脸那么自然简单。 扯了那么多，开始进入正题，先后进行了蚂蚁、拼多多和字节跳动的面试。 准备过程 先说说我自己的情况，我2016先在蚂蚁实习了将近三个月，然后去了我现在的老东家，2.5年工作经验，可以说毕业后就一直老老实实在老东家打怪升级，虽说有蚂蚁的实习经历，但是因为时间太短，还是有点虚的。所以面试官看到我简历第一个问题绝对是这样的。 “哇，你在蚂蚁待过，不错啊”，面试官笑嘻嘻地问到。“是的，还好”，我说。“为啥才三个月？”，面试官脸色一沉问到。“哗啦啦解释一通。。。”，我解释道。“哦，原来如此，那我们开始面试吧”，面试官一本正经说到。 尼玛，早知道不写蚂蚁的实习经历了，后面仔细一想，当初写上蚂蚁不就给简历加点料嘛。 言归正传，准备过程其实很早开始了（当然这不是说我工作时老想着跳槽，因为我明白现在的老东家并不是终点，我还需要不断提升），具体可追溯到从蚂蚁离职的时候，当时出来也面了很多公司，没啥大公司，面了大概5家公司，都拿到offer了。 工作之余常常会去额外研究自己感兴趣的技术以及工作用到的技术，力求把原理搞明白，并且会自己实践一把。此外，买了N多书，基本有时间就会去看，补补基础，什么操作系统、数据结构与算法、MySQL、JDK之类的源码，基本都好好温习了（文末会列一下自己看过的书和一些好的资料）。我深知基础就像“木桶效应”的短板，决定了能装多少水。 此外，在正式决定看机会之前，我给自己列了一个提纲，主要包括Java要掌握的核心要点，有不懂的就查资料搞懂。我给自己定位还是Java工程师，所以Java体系是一定要做到心中有数的，很多东西没有常年的积累面试的时候很容易露馅，学习要对得起自己，不要骗人。 剩下的就是找平台和内推了，除了蚂蚁，头条和拼多多都是找人内推的，感谢蚂蚁面试官对我的欣赏，以后说不定会去蚂蚁咯😄。 平台：脉脉、GitHub、v2 蚂蚁金服 一面 二面 三面 四面 五面 小结 一面 一面就做了一道算法题，要求两小时内完成，给了长度为N的有重复元素的数组，要求输出第10大的数。典型的TopK问题，快排算法搞定。 算法题要注意的是合法性校验、边界条件以及异常的处理。另外，如果要写测试用例，一定要保证测试覆盖场景尽可能全。加上平时刷刷算法题，这种考核应该没问题的。 二面 自我介绍下呗 开源项目贡献过代码么？（Dubbo提过一个打印accesslog的bug算么） 目前在部门做什么，业务简单介绍下，内部有哪些系统，作用和交互过程说下 Dubbo踩过哪些坑，分别是怎么解决的？（说了异常处理时业务异常捕获的问题，自定义了一个异常拦截器） 开始进入正题，说下你对线程安全的理解（多线程访问同一个对象，如果不需要考虑额外的同步，调用对象的行为就可以获得正确的结果就是线程安全） 事务有哪些特性？（ACID） 怎么理解原子性？（同一个事务下，多个操作要么成功要么失败，不存在部分成功或者部分失败的情况） 乐观锁和悲观锁的区别？（悲观锁假定会发生冲突，访问的时候都要先获得锁，保证同一个时刻只有线程获得锁，读读也会阻塞；乐观锁假设不会发生冲突，只有在提交操作的时候检查是否有冲突）这两种锁在Java和MySQL分别是怎么实现的？（Java乐观锁通过CAS实现，悲观锁通过synchronize实现。mysql乐观锁通过MVCC，也就是版本实现，悲观锁可以通过select... for update加上排它锁） HashMap为什么不是线程安全的？（多线程操作无并发控制，顺便说了在扩容的时候多线程访问时会造成死锁，会形成一个环，不过扩容时多线程操作形成环的问题再JDK1.8已经解决，但多线程下使用HashMap还会有一些其他问题比如数据丢失，所以多线程下不应该使用HashMap，而应该使用ConcurrentHashMap）怎么让HashMap变得线程安全？(Collections的synchronize方法包装一个线程安全的Map，或者直接用ConcurrentHashMap)两者的区别是什么？（前者直接在put和get方法加了synchronize同步，后者采用了分段锁以及CAS支持更高的并发） jdk1.8对ConcurrentHashMap做了哪些优化？（插入的时候如果数组元素使用了红黑树，取消了分段锁设计，synchronize替代了Lock锁）为什么这样优化？（避免冲突严重时链表多长，提高查询效率，时间复杂度从O(N)提高到O(logN)） redis主从机制了解么？怎么实现的？ 有过GC调优的经历么？（有点虚，答得不是很好） 有什么想问的么？ 三面 简单自我介绍下 监控系统怎么做的，分为哪些模块，模块之间怎么交互的？用的什么数据库？（MySQL）使用什么存储引擎，为什么使用InnnoDB？(支持事务、聚簇索引、MVCC) 订单表有做拆分么，怎么拆的？(垂直拆分和水平拆分) 水平拆分后查询过程描述下 如果落到某个分片的数据很大怎么办？(按照某种规则，比如哈希取模、range，将单张表拆分为多张表) 哈希取模会有什么问题么？(有的，数据分布不均，扩容缩容相对复杂 ) 分库分表后怎么解决读写压力？(一主多从、多主多从) 拆分后主键怎么保证惟一？(UUID、Snowflake算法) Snowflake生成的ID是全局递增唯一么？(不是，只是全局唯一，单机递增) 怎么实现全局递增的唯一ID？(讲了TDDL的一次取一批ID，然后再本地慢慢分配的做法) Mysql的索引结构说下(说了B+树，B+树可以对叶子结点顺序查找，因为叶子结点存放了数据结点且有序) 主键索引和普通索引的区别(主键索引的叶子结点存放了整行记录，普通索引的叶子结点存放了主键ID，查询的时候需要做一次回表查询)一定要回表查询么？(不一定，当查询的字段刚好是索引的字段或者索引的一部分，就可以不用回表，这也是索引覆盖的原理) 你们系统目前的瓶颈在哪里？ 你打算怎么优化？简要说下你的优化思路 有什么想问我么？ 四面 介绍下自己 为什么要做逆向？ 怎么理解微服务？ 服务治理怎么实现的？(说了限流、压测、监控等模块的实现) 这个不是中间件做的事么，为什么你们部门做？(当时没有单独的中间件团队，微服务刚搞不久，需要进行监控和性能优化) 说说Spring的生命周期吧 说说GC的过程(说了young gc和full gc的触发条件和回收过程以及对象创建的过程) CMS GC有什么问题？(并发清除算法，浮动垃圾，短暂停顿) 怎么避免产生浮动垃圾？(记得有个VM参数设置可以让扫描新生代之前进行一次young gc，但是因为gc是虚拟机自动调度的，所以不保证一定执行。但是还有参数可以让虚拟机强制执行一次young gc) 强制young gc会有什么问题？(STW停顿时间变长) 知道G1么？(了解一点 ) 回收过程是怎么样的？(young gc、并发阶段、混合阶段、full gc，说了Remember Set) 你提到的Remember Set底层是怎么实现的？ 有什么想问的么？ 五面 五面是HRBP面的，和我提前预约了时间，主要聊了之前在蚂蚁的实习经历、部门在做的事情、职业发展、福利待遇等。阿里面试官确实是具有一票否决权的，很看重你的价值观是否match，一般都比较喜欢皮实的候选人。HR面一定要诚实，不要说谎，只要你说谎HR都会去证实，直接cut了。 之前蚂蚁实习三个月怎么不留下来？ 实习的时候主管是谁？ 实习做了哪些事情？（尼玛这种也问？） 你对技术怎么看？平时使用什么技术栈？（阿里HR真的是既当爹又当妈，😂） 最近有在研究什么东西么 你对SRE怎么看 对待遇有什么预期么 最后HR还对我说目前稳定性保障部挺缺人的，希望我尽快回复。 小结 蚂蚁面试比较重视基础，所以Java那些基本功一定要扎实。蚂蚁的工作环境还是挺赞的，因为我面的是稳定性保障部门，还有许多单独的小组，什么三年1班，很有青春的感觉。面试官基本水平都比较高，基本都P7以上，除了基础还问了不少架构设计方面的问题，收获还是挺大的。 拼多多 面试前 一面 二面 三面 小结 面试前 面完蚂蚁后，早就听闻拼多多这个独角兽，决定也去面一把。首先我在脉脉找了一个拼多多的HR，加了微信聊了下，发了简历便开始我的拼多多面试之旅。这里要非常感谢拼多多HR小姐姐，从面试内推到offer确认一直都在帮我，人真的很nice。 一面 为啥蚂蚁只待了三个月？没转正？(转正了，解释了一通。。。) Java中的HashMap、TreeMap解释下？(TreeMap红黑树，有序，HashMap无序，数组+链表) TreeMap查询写入的时间复杂度多少？(O(logN)) HashMap多线程有什么问题？(线程安全，死锁)怎么解决？( jdk1.8用了synchronize + CAS，扩容的时候通过CAS检查是否有修改，是则重试)重试会有什么问题么？(CAS（Compare And Swap）是比较和交换，不会导致线程阻塞，但是因为重试是通过自旋实现的，所以仍然会占用CPU时间，还有ABA的问题)怎么解决？(超时，限定自旋的次数，ABA可以通过原理变量AtomicStampedReference解决，原理利用版本号进行比较)超过重试次数如果仍然失败怎么办？(synchronize互斥锁) CAS和synchronize有什么区别？都用synchronize不行么？(CAS是乐观锁，不需要阻塞，硬件级别实现的原子性；synchronize会阻塞，JVM级别实现的原子性。使用场景不同，线程冲突严重时CAS会造成CPU压力过大，导致吞吐量下降，synchronize的原理是先自旋然后阻塞，线程冲突严重仍然有较高的吞吐量，因为线程都被阻塞了，不会占用CPU ) 如果要保证线程安全怎么办？(ConcurrentHashMap) ConcurrentHashMap怎么实现线程安全的？(分段锁) get需要加锁么，为什么？(不用，volatile关键字) volatile的作用是什么？(保证内存可见性) 底层怎么实现的？(说了主内存和工作内存，读写内存屏障，happen-before，并在纸上画了线程交互图) 在多核CPU下，可见性怎么保证？(思考了一会，总线嗅探技术) 聊项目，系统之间是怎么交互的？ 系统并发多少，怎么优化？ 给我一张纸，画了一个九方格，都填了数字，给一个MN矩阵，从1开始逆时针打印这MN个数，要求时间复杂度尽可能低（内心OS：之前貌似碰到过这题，最优解是怎么实现来着）思考中。。。 可以先说下你的思路(想起来了，说了什么时候要变换方向的条件，向右、向下、向左、向上，依此循环) 有什么想问我的？ 二面 自我介绍下 手上还有其他offer么？(拿了蚂蚁的offer) 部门组织结构是怎样的？(这轮不是技术面么，不过还是老老实实说了) 系统有哪些模块，每个模块用了哪些技术，数据怎么流转的？（面试官有点秃顶，一看级别就很高）给了我一张纸，我在上面简单画了下系统之间的流转情况 链路追踪的信息是怎么传递的？(RpcContext的attachment，说了Span的结构:parentSpanId + curSpanId) SpanId怎么保证唯一性？(UUID，说了下内部的定制改动) RpcContext是在什么维度传递的？(线程) Dubbo的远程调用怎么实现的？(讲了读取配置、拼装url、创建Invoker、服务导出、服务注册以及消费者通过动态代理、filter、获取Invoker列表、负载均衡等过程（哗啦啦讲了10多分钟），我可以喝口水么) Spring的单例是怎么实现的？(单例注册表) 为什么要单独实现一个服务治理框架？(说了下内部刚搞微服务不久，主要对服务进行一些监控和性能优化) 谁主导的？内部还在使用么？ 逆向有想过怎么做成通用么？ 有什么想问的么？ 三面 二面老大面完后就直接HR面了，主要问了些职业发展、是否有其他offer、以及入职意向等问题，顺便说了下公司的福利待遇等，都比较常规啦。不过要说的是手上有其他offer或者大厂经历会有一定加分。 小结 拼多多的面试流程就简单许多，毕竟是一个成立三年多的公司。面试难度中规中矩，只要基础扎实应该不是问题。但不得不说工作强度很大，开始面试前HR就提前和我确认能否接受这样强度的工作，想来的老铁还是要做好准备 字节跳动 面试前 一面 二面 小结 面试前 头条的面试是三家里最专业的，每次面试前有专门的HR和你约时间，确定OK后再进行面试。每次都是通过视频面试，因为都是之前都是电话面或现场面，所以视频面试还是有点不自然。也有人觉得视频面试体验很赞，当然萝卜青菜各有所爱。最坑的二面的时候对方面试官的网络老是掉线，最后很冤枉的挂了（当然有一些点答得不好也是原因之一）。所以还是有点遗憾的。 一面 先自我介绍下 聊项目，逆向系统是什么意思 聊项目，逆向系统用了哪些技术 线程池的线程数怎么确定？ 如果是IO操作为主怎么确定？ 如果计算型操作又怎么确定？ Redis熟悉么，了解哪些数据结构?(说了zset) zset底层怎么实现的?(跳表) 跳表的查询过程是怎么样的，查询和插入的时间复杂度?(说了先从第一层查找，不满足就下沉到第二层找，因为每一层都是有序的，写入和插入的时间复杂度都是O(logN)) 红黑树了解么，时间复杂度?(说了是N叉平衡树，O(logN)) 既然两个数据结构时间复杂度都是O(logN)，zset为什么不用红黑树(跳表实现简单，踩坑成本低，红黑树每次插入都要通过旋转以维持平衡，实现复杂) 点了点头，说下Dubbo的原理?(说了服务注册与发布以及消费者调用的过程)踩过什么坑没有？（说了dubbo异常处理的和打印accesslog的问题） CAS了解么？（说了CAS的实现）还了解其他同步机制么？（说了synchronize以及两者的区别，一个乐观锁，一个悲观锁） 那我们做一道题吧，数组A，2*n个元素，n个奇数、n个偶数，设计一个算法，使得数组奇数下标位置放置的都是奇数，偶数下标位置放置的都是偶数 先说下你的思路（从0下标开始遍历，如果是奇数下标判断该元素是否奇数，是则跳过，否则从该位置寻找下一个奇数） 下一个奇数？怎么找？（有点懵逼，思考中。。） 有思路么？（仍然是先遍历一次数组，并对下标进行判断，如果下标属性和该位置元素不匹配从当前下标的下一个遍历数组元素，然后替换） 你这样时间复杂度有点高，如果要求O(N)要怎么做（思考一会，答道“定义两个指针，分别从下标0和1开始遍历，遇见奇数位是是偶数和偶数位是奇数就停下，交换内容”） 时间差不多了，先到这吧。你有什么想问我的？ 二面 面试官和蔼很多，你先介绍下自己吧 你对服务治理怎么理解的？ 项目中的限流怎么实现的？（Guava ratelimiter，令牌桶算法） 具体怎么实现的？（要点是固定速率且令牌数有限） 如果突然很多线程同时请求令牌，有什么问题？（导致很多请求积压，线程阻塞） 怎么解决呢？（可以把积压的请求放到消息队列，然后异步处理） 如果不用消息队列怎么解决？（说了RateLimiter预消费的策略） 分布式追踪的上下文是怎么存储和传递的？（ThreadLocal + spanId，当前节点的spanId作为下个节点的父spanId） Dubbo的RpcContext是怎么传递的？（ThreadLocal）主线程的ThreadLocal怎么传递到线程池？（说了先在主线程通过ThreadLocal的get方法拿到上下文信息，在线程池创建新的ThreadLocal并把之前获取的上下文信息设置到ThreadLocal中。这里要注意的线程池创建的ThreadLocal要在finally中手动remove，不然会有内存泄漏的问题） 你说的内存泄漏具体是怎么产生的？（说了ThreadLocal的结构，主要分两种场景：主线程仍然对ThreadLocal有引用和主线程不存在对ThreadLocal的引用。第一种场景因为主线程仍然在运行，所以还是有对ThreadLocal的引用，那么ThreadLocal变量的引用和value是不会被回收的。第二种场景虽然主线程不存在对ThreadLocal的引用，且该引用是弱引用，所以会在gc的时候被回收，但是对用的value不是弱引用，不会被内存回收，仍然会造成内存泄漏） 线程池的线程是不是必须手动remove才可以回收value？（是的，因为线程池的核心线程是一直存在的，如果不清理，那么核心线程的threadLocals变量会一直持有ThreadLocal变量） 那你说的内存泄漏是指主线程还是线程池？（主线程 ） 可是主线程不是都退出了，引用的对象不应该会主动回收么？（面试官和内存泄漏杠上了），沉默了一会。。。 那你说下SpringMVC不同用户登录的信息怎么保证线程安全的？（刚才解释的有点懵逼，一下没反应过来，居然回答成锁了。大脑有点晕了，此时已经一个小时过去了，感觉情况不妙。。。） 这个直接用ThreadLocal不就可以么，你见过SpringMVC有锁实现的代码么？（有点晕菜。。。） 我们聊聊mysql吧，说下索引结构（说了B+树） 为什么使用B+树？（ 说了查询效率高，O(logN)，可以充分利用磁盘预读的特性，多叉树，深度小，叶子结点有序且存储数据） 什么是索引覆盖？（忘记了。。。 ） Java为什么要设计双亲委派模型？ 什么时候需要自定义类加载器？ 我们做一道题吧，手写一个对象池 有什么想问我的么？（感觉我很多点都没答好，是不是挂了（结果真的是） ） 小结 头条的面试确实很专业，每次面试官会提前给你发一个视频链接，然后准点开始面试，而且考察的点都比较全。 面试官都有一个特点，会抓住一个值得深入的点或者你没说清楚的点深入下去直到你把这个点讲清楚，不然面试官会觉得你并没有真正理解。二面面试官给了我一点建议，研究技术的时候一定要去研究产生的背景，弄明白在什么场景解决什么特定的问题，其实很多技术内部都是相通的。很诚恳，还是很感谢这位面试官大大。 总结 从年前开始面试到头条面完大概一个多月的时间，真的有点身心俱疲的感觉。最后拿到了拼多多、蚂蚁的offer，还是蛮幸运的。头条的面试对我帮助很大，再次感谢面试官对我的诚恳建议，以及拼多多的HR对我的啰嗦的问题详细解答。 这里要说的是面试前要做好两件事：简历和自我介绍，简历要好好回顾下自己做的一些项目，然后挑几个亮点项目。自我介绍基本每轮面试都有，所以最好提前自己练习下，想好要讲哪些东西，分别怎么讲。此外，简历提到的技术一定是自己深入研究过的，没有深入研究也最好找点资料预热下，不打无准备的仗。 这些年看过的书： 《Effective Java》、《现代操作系统》、《TCP/IP详解：卷一》、《代码整洁之道》、《重构》、《Java程序性能优化》、《Spring实战》、《Zookeeper》、《高性能MySQL》、《亿级网站架构核心技术》、《可伸缩服务架构》、《Java编程思想》 说实话这些书很多只看了一部分，我通常会带着问题看书，不然看着看着就睡着了，简直是催眠良药😅。 最后，附一张自己面试前准备的脑图： 链接:https://pan.baidu.com/s/1o2l1tuRakBEP0InKEh4Hzw 密码:300d 全文完。 "},"zother6-JavaGuide/essential-content-for-interview/BATJrealInterviewExperience/2020-zijietiaodong.html":{"url":"zother6-JavaGuide/essential-content-for-interview/BATJrealInterviewExperience/2020-zijietiaodong.html","title":"2020 Zijietiaodong","keywords":"","body":" 本文来自读者 Boyn 投稿！恭喜这位粉丝拿到了含金量极高的字节跳动实习 offer!赞！ 基本条件 本人是底层 211 本科,现在大三,无科研经历,但是有一些项目经历,在国内监控行业某头部企业做过一段时间的实习。想着投一下字节,可以积累一下面试经验和为春招做准备.投了简历之后,过了一段时间,HR 就打电话跟我约时间,在年后进行远程面。 说明一下，我投的是北京 office。 一面 面试官很和蔼,由于疫情的原因,大家都在家里面进行远程面试 开头没有自我介绍,直接开始问项目了,问了比如 常用的 Web 组件有哪些(回答了自己经常用到的 SpringBoot,Redis,Mysql 等等,字节这边基本没有用 Java 的后台,所以感觉面试官不大会问 Spring,Java 这些东西,反倒是对数据库和中间件比较感兴趣) Kafka 相关,如何保证不会重复消费,Kafka 消费组结构等等(这个只是凭着感觉和面试官说了,因为 Kafka 自己确实准备得不充分,但是心态稳住了) Mysql 索引,B+树(必考嗷同学们) 还有一些项目中的细节,这些因人而异,就不放上来了,提示一点就是要在项目中介绍一些亮眼的地方,比如用了什么牛逼的数据结构,架构上有什么特点,并发量大小还有怎么去 hold 住并发量 后面就是算法题了,一共做了两道 判断平衡二叉树(这道题总体来说并不难,但是面试官在中间穿插了垃圾回收的知识,这就很难受了,具体的就是大家要判断一下对象在什么时候会回收,可达性分析什么时候对这个对象来说是不可达的,还有在递归函数中内存如何变化,这个是让我们来对这个函数进行执行过程的建模,只看栈帧大小变化的话,应该有是两个峰值,中间会有抖动的情况) 二分查找法的变种题,给定target和一个升序的数组,寻找下一个比数组大的数.这道题也不难,靠大家对二分查找法的熟悉程度,当然,这边还有一个优化的点,可以看看我的博客找找灵感 完成了之后,面试官让我等一会有二面,大概 10 分钟左右吧,休息了一会就继续了 二面 二面一上来就是先让我自我介绍,当然还是同样的套路,同样的香脆 然后问了我一些关于 Redis 的问题,比如 zset 的实现(跳表,这个高频) ,键的过期策略,持久化等等,这些在大多数 Redis 的介绍中都可以找到,就不细说了 还有一些数据结构的问题,比如说问了哈希表是什么,给面试官详细说了一下java.util.HashMap是怎么实现(当然里面就穿插着红黑树了,多看看红黑树是有什么特点之类的)的,包括说为什么要用链地址法来避免冲突,探测法有哪些,链地址法和探测法的优劣对比 后面还跟我讨论了很久的项目,所以说大家的项目一定要做好,要有亮点的地方,在这里跟面试官讨论了很多项目优化的地方,还有什么不足,还有什么地方可以新增功能等等,同样不细说了 一边讨论的时候劈里啪啦敲了很多,应该是对个人的面试评价一类的 后面就是字节的传统艺能手撕算法了,一共做了三道 一二道是连在一起的.给定一个规则S_0 = {1} S_1={1,2,1} S_2 = {1,2,1,3,1,2,1} S_n = {S_n-1 , n + 1, S_n-1}.第一个问题是他们的个数有什么关系(1 3 7 15... 2 的 n 次方-1,用位运算解决).第二个问题是给定数组个数下标 n 和索引 k,让我们求出 S_n(k)所指的数,假如S_2(2) = 1,我在做的时候没有什么好的思路,如果有的话大家可以分享一下 第三道是下一个排列：https://leetcode-cn.com/problems/next-permutation 的题型,不过做了一些修改,数组大小10000,不能用暴力法,还有数字是在 1-9 之间会有重复 hr 面 一些偏职业规划的话题了,实习时间,项目经历,实习经历这些。 总结 基础很重要!这次准备到的 Redis,Mysql,JVM 原理等等都有问到了,(网络这一块没问,但是也是要好好准备的,对于后台来说,网络知识不仅仅是面试,还是以后工作的知识基础).当然自己也有准备不足的地方,比如 Kafka 等中间件,只会用不会原理是万万不行的.并且这些基础知识不能只靠背,面试官还会融合在项目里面进行串问 问到了不会的不要慌,因为面试官是在试探你的技术深度,有可能会针对某一个问题,问到你不会为止,所以你出现不会的问题是很正常的,心态把控住就行. 无论是做题,还是回答问题的时候,牢记你不是在考试,而是在交流,和面试官有互动和沟通是很重要的,你说的一些疏漏的地方,如果你及时跟面试官反馈,还是可以补救一下的 最重要的一点字节的面试就是算法一定要牢固,每一轮都会有手撕算法的,这个不用想,LeetCode+剑指 Offer 走起来就对了,心态很重要,算法题不一定都是你会的,要有一定的心理准备,遇到难题可以先冷静分析一波.而且写出Bug free的代码也是很重要的,我前面的几题算法因为在牛客网上进行面试,所以要运行出来. 最后祝大家在春招取得好的 Offer,奥力给! "},"zother6-JavaGuide/essential-content-for-interview/BATJrealInterviewExperience/5面阿里,终获offer.html":{"url":"zother6-JavaGuide/essential-content-for-interview/BATJrealInterviewExperience/5面阿里,终获offer.html","title":"5面阿里,终获offer","keywords":"","body":" 作者：ppxyn。本文来自读者投稿，同时也欢迎各位投稿，对于不错的原创文章我根据你的选择给予现金(50-200)、付费专栏或者任选书籍进行奖励！所以，快提 pr 或者邮件的方式（邮件地址在主页）给我投稿吧！ 当然，我觉得奖励是次要的，最重要的是你可以从自己整理知识点的过程中学习到很多知识。 目录 前言 一面(技术面) 二面(技术面) 三面(技术面) 四面(半个技术面) 五面(HR面) 总结 前言 在接触 Java 之前我接触的比较多的是硬件方面，用的比较多的语言就是C和C++。到了大三我才正式选择 Java 方向，到目前为止使用Java到现在大概有一年多的时间，所以Java算不上很好。刚开始投递的时候，实习刚辞职，也没准备笔试面试，很多东西都忘记了。所以，刚开始我并没有直接就投递阿里，毕竟心里还是有一点点小害怕的。于是，我就先投递了几个不算大的公司来练手，就是想着刷刷经验而已或者说是练练手（ps：还是挺对不起那些公司的）。面了一个月其他公司后，我找了我实验室的学长内推我，后面就有了这5次面试。 下面简单的说一下我的这5次面试：4次技术面+1次HR面，希望我的经历能对你有所帮助。 一面(技术面) 自我介绍（主要讲自己会的技术细节，项目经验，经历那些就一语带过，后面面试官会问你的）。 聊聊项目（就是一个很普通的分布式商城，自己做了一些改进），让我画了整个项目的架构图，然后针对项目抛了一系列的提高性能的问题，还问了我做项目的过程中遇到了那些问题，如何解决的，差不读就这些吧。 可能是我前面说了我会数据库优化，然后面试官就开始问索引、事务隔离级别、悲观锁和乐观锁、索引、ACID、MVVC这些问题。 浏览器输入URL发生了什么? TCP和UDP区别? TCP如何保证传输可靠性? 讲下跳表怎么实现的?哈夫曼编码是怎么回事？非递归且不用额外空间（不用栈），如何遍历二叉树 后面又问了很多JVM方面的问题，比如Java内存模型、常见的垃圾回收器、双亲委派模型这些 你有什么问题要问吗？ 二面(技术面) 自我介绍（主要讲自己会的技术细节，项目经验，经历那些就一语带过，后面面试官会问你的）。 操作系统的内存管理机制 进程和线程的区别 说下你对线程安全的理解 volatile 有什么作用 ，sychronized和lock有什么区别 ReentrantLock实现原理 用过CountDownLatch么？什么场景下用的？ AQS底层原理。 造成死锁的原因有哪些，如何预防？ 加锁会带来哪些性能问题。如何解决？ HashMap、ConcurrentHashMap源码。HashMap是线程安全的吗？Hashtable呢？ConcurrentHashMap有了解吗？ 是否可以实习？ 你有什么问题要问吗？ 三面(技术面) 有没有参加过 ACM 或者他竞赛，有没有拿过什么奖？（ 我说我没参加过ACM，本科参加过数学建模竞赛，名次并不好，没拿过什么奖。面试官好像有点失望，然后我又赶紧补充说我和老师一起做过一个项目，目前已经投入使用。面试官还比较感兴趣，后面又和他聊了一下这个项目。） 研究生期间，做过什么项目，发过论文吗？有什么成果吗？ 你觉得你有什么优点和缺点？你觉得你相比于那些比你更优秀的人欠缺什么？ 有读过什么源码吗？（我说我读过 Java 集合框架和 Netty 的，面试官说 Java 集合前几面一定问的差不多，就不问了，然后就问我 Netty的，我当时很慌啊！） 介绍一下自己对 Netty 的认识，为什么要用。说说业务中，Netty 的使用场景。什么是TCP 粘包/拆包,解决办法。Netty线程模型。Dubbo 在使用 Netty 作为网络通讯时候是如何避免粘包与半包问题？讲讲Netty的零拷贝？巴拉巴拉问了好多，我记得有好几个我都没回答上来，心里想着凉凉了啊。 用到了那些开源技术、在开源领域做过贡献吗？ 常见的排序算法及其复杂度，现场写了快排。 红黑树，B树的一些问题。 讲讲算法及数据结构在实习项目中的用处。 自己的未来规划（就简单描述了一下自己未来的设想啊，说的还挺诚恳，面试官好像还挺满意的） 你有什么问题要问吗？ 四面(半个技术面) 三面面完当天，晚上9点接到面试电话，感觉像是部门或者项目主管。 这个和之前的面试不大相同，感觉面试官主要考察的是你解决问题的能力、学习能力和团队协作能力。 让我讲一个自己觉得最不错的项目。然后就巴拉巴拉的聊，我记得主要是问了项目是如何进行协作的、遇到问题是如何解决的、与他人发生冲突是如何解决的这些。感觉聊了挺久。 出现 OOM 后你会怎么排查问题？ 自己平时是如何学习新技术的？除了 Java 还回去了解其他技术吗? 上一段实习经历的收获。 NginX如何做负载均衡、常见的负载均衡算法有哪些、一致性哈希的一致性是什么意思、一致性哈希是如何做哈希的 你有什么问题问我吗？ 还有一些其他的，想不起来了，感觉这一面不是偏向技术来问。 五面(HR面) 自我介绍（主要讲能突出自己的经历，会的编程技术一语带过）。 你觉得你有什么优点和缺点？如何克服这些缺点？ 说一件大学里你自己比较有成就感的一件事情，为此付出了那些努力。 你前面跟其他面试官讲过一些你做的项目吧？可以给我讲讲吗？你要考虑到我不是一个做技术的人，怎么让我也听得懂。项目中有什么问题，你怎么解决的？你最大的收获是什么？ 你目前有面试过其他公司吗？如果让你选，这些公司和阿里，你选哪个？（送分题，回答不好可能送命） 你期望的工作地点是哪里？ 你有什么问题吗？ 总结 可以看出面试官问我的很多问题都是比较常见的问题，所以记得一定要提前准备，还要深入准备，不要回答的太皮毛。很多时候一个问题可能会牵扯出很多问题，遇到不会的问题不要慌，冷静分析，如果你真的回答不上来，也不要担心自己是不是就要挂了，很可能这个问题本身就比较难。 表达能力和沟通能力太重要了，一定要提前练一下，我自身就是一个不太会说话的人，所以，面试前我对于自我介绍、项目介绍和一些常见问题都在脑子里练了好久，确保面试的时候能够很清晰和简洁的说出来。 等待面试的过程和面试的过程真的好熬人，那段时间我压力也比较大，好在我私下找到学长聊了很多，心情也好了很多。 面试之后及时总结，面的好的话，不要得意，尽快准备下一场面试吧！ 我觉得我还算是比较幸运的，最后也祝大家都能获得心仪的Offer。 "},"zother6-JavaGuide/essential-content-for-interview/BATJrealInterviewExperience/bingo-interview.html":{"url":"zother6-JavaGuide/essential-content-for-interview/BATJrealInterviewExperience/bingo-interview.html","title":"Bingo Interview","keywords":"","body":" 本文是鄙人薛某这位老哥的投稿，虽然面试最后挂了，但是老哥本身还是挺优秀的，而且通过这次面试学到了很多东西，我想这就足够了！加油！不要畏惧面试失败，好好修炼自己，多准备一下，后面一定会找到让自己满意的工作。 背景 前段时间家里出了点事，辞职回家待了一段时间，处理完老家的事情后就回到广州这边继续找工作，大概是国庆前几天我去面试了一家叫做Bigo(YY的子公司)，面试的职位是面向3-5年的Java开发，最终自己倒在了第三轮的技术面上。虽然有些遗憾和泄气，但想着还是写篇博客来记录一下自己的面试过程好了，也算是对广大程序员同胞们的分享，希望对你们以后的学习和面试能有所帮助。 个人情况 先说下LZ的个人情况。 17年毕业，二本，目前位于广州，是一个非常普通的Java开发程序员，算起来有两年多的开发经验。 其实这个阶段有点尴尬，高不成低不就，比初级程序员稍微好点，但也达不到高级的程度。加上现如今IT行业接近饱和，很多岗位都是要求至少3-5年以上开发经验，所以对于两年左右开发经验的需求其实是比较小的，这点在LZ找工作的过程中深有体会。最可悲的是，今年的大环境不好，很多公司不断的在裁员，更别说招人了，残酷的形势对于求职者来说更是雪上加霜，相信很多求职的同学也有所体会。所以，不到万不得已的情况下，建议不要裸辞！ Bigo面试 面试岗位：Java后台开发 经验要求：3-5年 由于是国庆前去面试Bigo的，到现在也有一个多月的时间了，虽然仍有印象，但也有不少面试题忘了，所以我只能尽量按照自己的回忆来描述面试的过程，不明白之处还请见谅！ 一面(微信电话面) bigo的第一面是微信电话面试，本来是想直接电话面，但面试官说需要手写算法题，就改成微信电话面。 自我介绍 先了解一下Java基础吧，什么是内存泄漏和内存溢出？（溢出是指创建太多对象导致内存空间不足，泄漏是无用对象没有回收） JVM怎么判断对象是无用对象？（根搜索算法，从GC Root出发，对象没有引用，就判定为无用对象） 根搜索算法中的根节点可以是哪些对象？（类对象，虚拟机栈的对象，常量引用的对象） 重载和重写的区别？（重载发生在同个类，方法名相同，参数列表不同；重写是父子类之间的行为，方法名好参数列表都相同，方法体内的程序不同） 重写有什么限制没有？ Java有哪些同步工具？（synchronized和Lock） 这两者有什么区别？ ArrayList和LinkedList的区别？（ArrayList基于数组，搜索快，增删元素慢，LinkedList基于链表，增删快，搜索因为要遍历元素所以效率低） 这两种集合哪个比较占内存？（看情况的，ArrayList如果有扩容并且元素没占满数组的话，浪费的内存空间也是比较多的，但一般情况下，LinkedList占用的内存会相对多点，因为每个元素都包含了指向前后节点的指针） 说一下HashMap的底层结构（数组 + 链表，链表过长变成红黑树） HashMap为什么线程不安全，1.7版本之前HashMap有什么问题（扩容时多线程操作可能会导致链表成环的出现，然后调用get方法会死循环） 了解ConcurrentHashMap吗？说一下它为什么能线程安全（用了分段锁） 哪些方法需要锁住整个集合的？（读取size的时候） 看你简历写着你了解RPC啊，那你说下RPC的整个过程？（从客户端发起请求，到socket传输，然后服务端处理消息，以及怎么序列化之类的都大概讲了一下） 服务端获取客户端要调用的接口信息后，怎么找到对应的实现类的？(反射 + 注解吧，这里也不是很懂) dubbo的负载均衡有几种算法?（随机，轮询，最少活跃请求数，一致性hash） 你说的最少活跃数算法是怎么回事？（服务提供者有一个计数器，记录当前同时请求个数，值越小说明该服务器负载越小，路由器会优先选择该服务器） 服务端怎么知道客户端要调用的算法的？(socket传递消息过来的时候会把算法策略传递给服务端) 你用过redis做分布式锁是吧，你们是自己写的工具类吗？（不是，我们用redission做分布式锁） 线程拿到key后是怎么保证不死锁的呢？（给这个key加上一个过期时间） 如果这个过期时间到了，但是业务程序还没处理完，该怎么办？（额......可以在业务逻辑上保证幂等性吧） 那如果多个业务都用到分布式锁的话，每个业务都要保证幂等性了，有没有更好的方法？（额......思考了下暂时没有头绪，面试官就说那先跳过吧。事后我了解到redission本身是有个看门狗的监控线程的，如果检测到key被持有的话就会再次重置过期时间） 你那边有纸和笔吧，写一道算法，用两个栈模拟一个队列的入队和出队。（因为之前复习的时候对这道题有印象，写的时候也比较快，大概是用了五分钟，然后就拍成图片发给了面试官，对方看完后表示没问题就结束了面试。） 第一面问的不算难，问题也都是偏基础之类的，虽然答得不算完美，但过程还是比较顺利的。几天之后，Bigo的hr就邀请我去他们公司参加现场面试。 二面 到Bigo公司后，一位hr小姐姐招待我到了一个会议室，等了大概半个小时，一位中年男子走了进来，非常的客气，说不好意思让我等那么久了，并且介绍了自己是技术经理，然后就开始了我们的交谈。 依照惯例，让我简单做下自我介绍，这个过程他也在边看我的简历。 说下你最熟悉的项目吧。（我就拿我上家公司最近做的一个电商项目开始介绍，从简单的项目描述，到项目的主要功能，以及我主要负责的功能模块，吧啦吧啦..............） 你对这个项目这么熟悉，那你根据你的理解画一下你的项目架构图，还有说下你具体参与了哪部分。（这个题目还是比较麻烦的，毕竟我当时离职的时间也挺长了，对这个项目的架构也是有些模糊。当然，最后还是硬着头皮还是画了个大概，从前端开始访问，然后通过nginx网关层，最后到具体的服务等等，并且把自己参与的服务模块也标示了出来） 你的项目用到了Spring Cloud GateWay，既然你已经有nginx做网关了，为什么还要用gateWay呢？（nginx是做负载均衡，还有针对客户端的访问做网关用的，gateWay是接入业务层做的网关，而且还整合了熔断器Hystrix） 熔断器Hystrix最主要的作用是什么？（防止服务调用失败导致的服务雪崩，能降级） 你的项目用到了redis，你们的redis是怎么部署的？（额。。。。好像是哨兵模式部署的吧。） 说一下你对哨兵模式的理解？（我对哨兵模式了解的不多，就大概说了下Sentinel监控之类的，还有类似ping命令的心跳机制，以及怎么判断一个master是下线之类。。。。。） 那你们为什么要用哨兵模式呢？怎么不用集群的方式部署呢？一开始get不到他的点，就说哨兵本身就是多实例部署的，他解释了一下，说的是redis-cluster的部署方案。（额......redis的环境搭建有专门的运维人员部署的，应该是优先考虑高可用吧..........开始有点心慌了，因为我也不知道为什么） 哦，那你是觉得集群没有办法实现高可用吗？（不....不是啊，只是觉得哨兵模式可能比较保证主从复制安全性吧........我也不知道自己在说什么） 集群也是能保证高可用的，你知道它又是怎么保证主从一致性的吗？（好吧，这里真的不知道了，只能跳过） 你肯定有微信吧，如果让你来设计微信朋友圈的话，你会怎么设计它的属性成员呢？（嗯......需要有用户表，朋友圈的表，好友表之类的吧） 嗯，好，你也知道微信用户有接近10亿之多，那肯定要涉及到分库分表，如果是你的话，怎么设计分库分表呢？（这个问题考察的点比较大，我答的其实一般，而且这个过程面试官还不断的进行连环炮发问，导致这个话题说了有将近20分钟，限于篇幅，这里就不再详述了） 这边差不多了，最后你写一道算法吧，有一组未排序的整形数组，你设计一个算法，对数组的元素两两配对，然后输出最大的绝对值差和最小的绝对值差的\"对数\"。（听到这道题，我第一想法就是用HashMap来保存，key是两个元素的绝对值差，value是配对的数量，如果有相同的就加1，没有就赋值为1，然后最后对map做排序，输出最大和最小的value值，写完后面试官说结果虽然是正确的，但是不够效率，因为遍历的时间复杂度成了O(n^2)，然后提醒了我往排序这方面想。我灵机一动，可以先对数组做排序，然后首元素与第二个元素做绝对值差，记为num，然后首元素循环和后面的元素做计算，直到绝对值差不等于num位置，这样效率比起O(n^2)快多了。） 面试完后，技术官就问我有什么要问他的，我就针对这个岗位的职责和项目所用的技术栈做了询问，然后就让我先等下，等他去通知三面的技术官。说实话，二面给我的感觉是最舒服的，因为面试官很亲切，面试的过程一直积极的引导我，而且在职业规划方面给了我很多的建议，让我受益匪浅，虽然面试时间有一个半小时，但却丝毫不觉得长，整个面试过程聊得挺舒服的，不过因为时间比较久了，很多问题我也记不清了。 三面 二面结束后半个小时，三面的技术面试官就开始进来了，从他的额头发量分布情况就能猜想是个大牛，人狠话不多，坐下后也没让我做自我介绍，直接开问，整个过程我答的也不好，而且面试官的问题表述有些不太清晰，经常需要跟他重复确认清楚。 对事务了解吗？说一下事务的隔离级别有哪些（我以比较了解的Spring来说，把Spring的四种事务隔离级别都叙述了一遍） 你做过电商，那应该知道下单的时候需要减库存对吧，假设现在有两个服务A和B，分别操作订单和库存表，A保存订单后，调用B减库存的时候失败了，这个时候A也要回滚，这个事务要怎么设计？（B服务的减库存方法不抛异常，由调用方也就是A服务来抛异常） 了解过读写分离吗？（额。。。大概了解一点，就是写的时候进主库，读的时候读从库） 你说读的时候读从库，现在假设有一张表User做了读写分离，然后有个线程在一个事务范围内对User表先做了写的处理，然后又做了读的处理，这时候数据还没同步到从库，怎么保证读的时候能读到最新的数据呢？（听完顿时有点懵圈，一时间答不上来，后来面试官说想办法保证一个事务中读写都是同一个库才行） 你的项目里用到了rabbitmq，那你说下mq的消费端是怎么处理的？（就是消费端接收到消息之后，会先把消息存到数据库中，然后再从数据库中定时跑消息） 也就是说你的mq是先保存到数据库中，然后业务逻辑就是从mq中读取消息然后再处理的是吧？（是的） 那你的消息是唯一的吗？（是的，用了唯一约束） 你怎么保证消息一定能被消费？或者说怎么保证一定能存到数据库中？（这里开始慌了，因为mq接入那一块我只是看过部分逻辑，但没有亲自参与，凭着自己对mq的了解就答道，应该是靠rabbitmq的ack确认机制） 好，那你整理一下你的消费端的整个处理逻辑流程，然后说说你的ack是在哪里返回的（听到这里我的心凉了一截，mq接入这部分我确实没有参与，硬着头皮按照自己的理解画了一下流程，但其实漏洞百出） 按照你这样画的话，如果数据库突然宕机，你的消息该怎么确认已经接收？（额.....那发送消息的时候就存放消息可以吧.........回答的时候心里千万只草泥马路过........行了吧，没玩没了了。） 那如果发送端的服务是多台部署呢？你保存消息的时候数据库就一直报唯一性的错误？（好吧，你赢了。。。最后硬是憋出了一句，您说的是，这样设计确实不好。。。。） 算了，跳过吧，现在你来设计一个map，然后有两个线程对这个map进行操作，主线程高速增加和删除map的元素，然后有个异步线程定时去删除map中主线程5秒内没有删除的数据，你会怎么设计？ （这道题我答得并不好，做了下简单的思考就说可以把map的key加上时间戳的标志，遍历的时候发现小于当前时间戳5秒前的元素就进行删除，面试官对这样的回答明显不太满意，说这样遍历会影响效率，ps：对这道题，大佬们如果有什么高见可以在评论区说下！） ......还有其他问题，但我只记住了这么多，就这样吧。 面完最后一道题后，面试官就表示这次面试过程结束了，让我回去等消息。听到这里，我知道基本上算是宣告结果了。回想起来，自己这一轮面试确实表现的很一般，加上时间拖得很长，从当天的2点半一直面试到6点多，精神上也尽显疲态。果然，几天之后，hr微信通知了我，说我第三轮技术面试没有通过，这一次面试以失败告终。 总结 以上就是面试的大概过程，不得不说，大厂的面试还是非常有技术水平的，这个过程中我学到了很多，这里分享下个人的一些心得： 1、基础！基础！基础！重要的事情说三遍，无论是什么阶段的程序员，基础都是最重要的。每个公司的面试一定会涉及到基础知识的提问，如果你的基础不扎实，往往第一面就可能被淘汰。 2、简历需要适当的包装。老实说，我的简历肯定是经过包装的，这也是我的工作年限不够，但却能获取Bigo面试机会的重要原因，所以适当的包装一下简历很有必要，不过切记一点，就是不能脱离现实，比如明明只有两年经验，却硬是写到三年。小厂还可能蒙混过关，但大厂基本很难，因为很多公司会在入职前做背景调查。 3、要对简历上的技术点很熟悉。简历包装可以，但一定要对简历上的技术点很熟悉，比如只是简单写过rabbitmq的demo的话，就不要写“熟悉”等字眼，因为很多的面试官会针对一个技能点问的很深入，像连环炮一样的深耕你对这个技能点的理解程度。 4、简历上的项目要非常熟悉。一般我们写简历都是需要对自己的项目做一定程序的包装和美化，项目写得好能给简历加很多分。但一定要对项目非常的熟悉，不熟悉的模块最好不要写上去。笔者这次就吃了大亏，我的简历上有个电商项目就写到了用rabbitmq处理下单，虽然稍微了解过那部分下单的处理逻辑，但由于没有亲自参与就没有做深入的了解，面试时在这一块内容上被Bigo三面的面试官逼得最后哑口无言。 5、提升自己的架构思维。对于初中级程序员来说，日常的工作就是基本的增删改查，把功能实现就完事了，这种思维不能说不好，只是想更上一层楼的话，业务时间需要提升下自己的架构思维能力，比如说如果让你接手一个项目的话，你会怎么考虑设计这个项目，从整体架构，到引入一些组件，再到设计具体的业务服务，这些都是设计一个项目必须要考虑的环节，对于提升我们的架构思维是一种很好的锻炼，这也是很多大厂面试高级程序员时的重要考察部分。 6、不要裸辞。这也是我最朴实的建议了，大环境不好，且行且珍惜吧，唉~~~~ 总的来说，这次面试Bigo还是收获颇丰的，虽然有点遗憾，但也没什么后悔的，毕竟自己面试之前也是准备的很充分了，有些题目答得不好说明我还有很多技术盲区，不懂就是不懂，再这么吹也吹不出来。这也算是给我提了个醒，你还嫩着呢，好好修炼内功吧，毕竟菜可是原罪啊。 "},"zother6-JavaGuide/essential-content-for-interview/PreparingForInterview/interviewPrepare.html":{"url":"zother6-JavaGuide/essential-content-for-interview/PreparingForInterview/interviewPrepare.html","title":"Interview Prepare","keywords":"","body":"不论是笔试还是面试都是有章可循的，但是，一定要不要想着如何去应付面试，糊弄面试官，这样做终究是欺骗自己。这篇文章的目的也主要想让大家知道自己应该从哪些方向去准备面试，有哪些可以提高的方向。 网上已经有很多面经了，但是我认为网上的各种面经仅仅只能作为参考，你的实际面试与之还是有一些区别的。另外如果要在网上看别人的面经的话，建议即要看别人成功的案例也要适当看看别人失败的案例。看面经没问题，不论是你要找工作还是平时学习，这都是一种比较好地检验自己水平的一种方式。但是，一定不要过分寄希望于各种面经，试着去提高自己的综合能力。 “ 80% 的 offer 掌握在 20% 的人手 ” 中这句话也不是不无道理的。决定你面试能否成功的因素中实力固然占有很大一部分比例，但是如果你的心态或者说运气不好的话，依然无法拿到满意的 offer。 运气暂且不谈，就拿心态来说，千万不要因为面试失败而气馁或者说怀疑自己的能力，面试失败之后多总结一下失败的原因，后面你就会发现自己会越来越强大。 另外，笔主只是在这里分享一下自己对于 “ 如何备战大厂面试 ” 的一个看法，以下大部分理论／言辞都经过过反复推敲验证，如果有不对的地方或者和你想法不同的地方，请您敬请雅正、不舍赐教。 1 如何获取大厂面试机会？ 2 面试前的准备 2.1 准备自己的自我介绍 2.2 搞清楚技术面可能会问哪些方向的问题 2.2 休闲着装即可 2.3 随身带上自己的成绩单和简历 2.4 如果需要笔试就提前刷一些笔试题 2.5 花时间一些逻辑题 2.6 准备好自己的项目介绍 2.7 提前准备技术面试 2.7 面试之前做好定向复习 3 面试之后复盘 4 如何学习?学会各种框架有必要吗? 4.1 我该如何学习？ 4.2 学会各种框架有必要吗？ 1 如何获取大厂面试机会？ 在讲如何获取大厂面试机会之前，先来给大家科普/对比一下两个校招非常常见的概念——春招和秋招。 招聘人数 ：秋招多于春招 ； 招聘时间 ： 秋招一般7月左右开始，大概一直持续到10月底。但是大厂（如BAT）都会早开始早结束，所以一定要把握好时间。春招最佳时间为3月，次佳时间为4月，进入5月基本就不会再有春招了（金三银四）。 应聘难度 ：秋招略大于春招； 招聘公司： 秋招数量多，而春招数量较少，一般为秋招的补充。 综上，一般来说，秋招的含金量明显是高于春招的。 下面我就说一下我自己知道的一些方法，不过应该也涵盖了大部分获取面试机会的方法。 关注大厂官网，随时投递简历（走流程的网申）； 线下参加宣讲会，直接投递简历； 找到师兄师姐/认识的人，帮忙内推（能够让你避开网申简历筛选，笔试筛选，还是挺不错的，不过也还是需要你的简历够棒）； 博客发文被看中/Github优秀开源项目作者，大厂内部人员邀请你面试； 求职类网站投递简历（不是太推荐，适合海投）； 除了这些方法，我也遇到过这样的经历：有些大公司的一些部门可能暂时没招够人，然后如果你的亲戚或者朋友刚好在这个公司，而你正好又在寻求offer，那么面试机会基本上是有了，而且这种面试的难度好像一般还普遍比其他正规面试低很多。 2 面试前的准备 2.1 准备自己的自我介绍 自我介绍一般是你和面试官的第一次面对面正式交流，换位思考一下，假如你是面试官的话，你想听到被你面试的人如何介绍自己呢？一定不是客套地说说自己喜欢编程、平时花了很多时间来学习、自己的兴趣爱好是打球吧？ 我觉得一个好的自我介绍应该包含这几点要素： 用简单的话说清楚自己主要的技术栈于擅长的领域； 把重点放在自己在行的地方以及自己的优势之处； 重点突出自己的能力比如自己的定位的bug的能力特别厉害； 从社招和校招两个角度来举例子吧！我下面的两个例子仅供参考，自我介绍并不需要死记硬背，记住要说的要点，面试的时候根据公司的情况临场发挥也是没问题的。另外，网上一般建议的是准备好两份自我介绍：一份对hr说的，主要讲能突出自己的经历，会的编程技术一语带过；另一份对技术面试官说的，主要讲自己会的技术细节和项目经验。 社招： 面试官，您好！我叫独秀儿。我目前有1年半的工作经验，熟练使用Spring、MyBatis等框架、了解 Java 底层原理比如JVM调优并且有着丰富的分布式开发经验。离开上一家公司是因为我想在技术上得到更多的锻炼。在上一个公司我参与了一个分布式电子交易系统的开发，负责搭建了整个项目的基础架构并且通过分库分表解决了原始数据库以及一些相关表过于庞大的问题，目前这个网站最高支持 10 万人同时访问。工作之余，我利用自己的业余时间写了一个简单的 RPC 框架，这个框架用到了Netty进行网络通信， 目前我已经将这个项目开源，在 Github 上收获了 2k的 Star! 说到业余爱好的话，我比较喜欢通过博客整理分享自己所学知识，现在已经是多个博客平台的认证作者。 生活中我是一个比较积极乐观的人，一般会通过运动打球的方式来放松。我一直都非常想加入贵公司，我觉得贵公司的文化和技术氛围我都非常喜欢，期待能与你共事！ 校招： 面试官，您好！我叫秀儿。大学时间我主要利用课外时间学习了 Java 以及 Spring、MyBatis等框架 。在校期间参与过一个考试系统的开发，这个系统的主要用了 Spring、MyBatis 和 shiro 这三种框架。我在其中主要担任后端开发，主要负责了权限管理功能模块的搭建。另外，我在大学的时候参加过一次软件编程大赛，我和我的团队做的在线订餐系统成功获得了第二名的成绩。我还利用自己的业余时间写了一个简单的 RPC 框架，这个框架用到了Netty进行网络通信， 目前我已经将这个项目开源，在 Github 上收获了 2k的 Star! 说到业余爱好的话，我比较喜欢通过博客整理分享自己所学知识，现在已经是多个博客平台的认证作者。 生活中我是一个比较积极乐观的人，一般会通过运动打球的方式来放松。我一直都非常想加入贵公司，我觉得贵公司的文化和技术氛围我都非常喜欢，期待能与你共事！ 2.2 搞清楚技术面可能会问哪些方向的问题 你准备面试的话首先要搞清技术面可能会被问哪些方向的问题吧！ 我直接用思维导图的形式展示出来吧！这样更加直观形象一点，细化到某个知识点的话这张图没有介绍到，留个悬念，下篇文章会详细介绍。 上面思维导图大概涵盖了技术面试可能会设计的技术，但是你不需要把上面的每一个知识点都搞得很熟悉，要分清主次，对于自己不熟悉的技术不要写在简历上，对于自己简单了解的技术不要说自己熟练掌握！ 2.2 休闲着装即可 穿西装、打领带、小皮鞋？NO！NO！NO！这是互联网公司面试又不是去走红毯，所以你只需要穿的简单大方就好，不需要太正式。 2.3 随身带上自己的成绩单和简历 有的公司在面试前都会让你交一份成绩单和简历当做面试中的参考。 2.4 如果需要笔试就提前刷一些笔试题 平时空闲时间多的可以刷一下笔试题目（牛客网上有很多）。但是不要只刷面试题，不动手code，程序员不是为了考试而存在的。 2.5 花时间一些逻辑题 面试中发现有些公司都有逻辑题测试环节，并且都把逻辑笔试成绩作为很重要的一个参考。 2.6 准备好自己的项目介绍 如果有项目的话，技术面试第一步，面试官一般都是让你自己介绍一下你的项目。你可以从下面几个方向来考虑： 对项目整体设计的一个感受（面试官可能会让你画系统的架构图） 在这个项目中你负责了什么、做了什么、担任了什么角色 从这个项目中你学会了那些东西，使用到了那些技术，学会了那些新技术的使用 另外项目描述中，最好可以体现自己的综合素质，比如你是如何协调项目组成员协同开发的或者在遇到某一个棘手的问题的时候你是如何解决的又或者说你在这个项目用了什么技术实现了什么功能比如：用redis做缓存提高访问速度和并发量、使用消息队列削峰和降流等等。 2.7 提前准备技术面试 搞清楚自己面试中可能涉及哪些知识点、哪些知识点是重点。面试中哪些问题会被经常问到、自己该如何回答。(强烈不推荐背题，第一：通过背这种方式你能记住多少？能记住多久？第二：背题的方式的学习很难坚持下去！) 2.7 面试之前做好定向复习 所谓定向复习就是专门针对你要面试的公司来复习。比如你在面试之前可以在网上找找有没有你要面试的公司的面经。 举个栗子：在我面试 ThoughtWorks 的前几天我就在网上找了一些关于 ThoughtWorks 的技术面的一些文章。然后知道了 ThoughtWorks 的技术面会让我们在之前做的作业的基础上增加一个或两个功能，所以我提前一天就把我之前做的程序重新重构了一下。然后在技术面的时候，简单的改了几行代码之后写个测试就完事了。如果没有提前准备，我觉得 20 分钟我很大几率会完不成这项任务。 3 面试之后复盘 如果失败，不要灰心；如果通过，切勿狂喜。面试和工作实际上是两回事，可能很多面试未通过的人，工作能力比你强的多，反之亦然。我个人觉得面试也像是一场全新的征程，失败和胜利都是平常之事。所以，劝各位不要因为面试失败而灰心、丧失斗志。也不要因为面试通过而沾沾自喜，等待你的将是更美好的未来，继续加油！ 4 如何学习?学会各种框架有必要吗? 4.1 我该如何学习？ 最最最关键也是对自己最最最重要的就是学习！看看别人分享的面经，看看我写的这篇文章估计你只需要10分钟不到。但这些东西终究是空洞的理论，最主要的还是自己平时的学习！ 如何去学呢？我觉得学习每个知识点可以考虑这样去入手： 官网（大概率是英文，不推荐初学者看）。 书籍（知识更加系统完全，推荐）。 视频（比较容易理解，推荐，特别是初学的时候。慕课网和哔哩哔哩上面有挺多学习视频可以看，只直接在上面搜索关键词就可以了）。 网上博客（解决某一知识点的问题的时候可以看看）。 这里给各位一个建议，看视频的过程中最好跟着一起练，要做笔记！！！ 最好可以边看视频边找一本书籍看，看视频没弄懂的知识点一定要尽快解决，如何解决？ 首先百度/Google，通过搜索引擎解决不了的话就找身边的朋友或者认识的一些人。 4.2 学会各种框架有必要吗？ 一定要学会分配自己时间，要学的东西很多，真的很多，搞清楚哪些东西是重点，哪些东西仅仅了解就够了。一定不要把精力都花在了学各种框架上，算法、数据结构还有计算机网络真的很重要！ 另外，学习的过程中有一个可以参考的文档很重要，非常有助于自己的学习。我当初弄 JavaGuide： https://github.com/Snailclimb/JavaGuide 的很大一部分目的就是因为这个。客观来说，相比于博客，JavaGuide 里面的内容因为更多人的参与变得更加准确和完善。 如果大家觉得这篇文章不错的话，欢迎给我来个三连（评论+转发+在看）！我会在下一篇文章中介绍如何从技术面时的角度准备面试？ "},"zother6-JavaGuide/essential-content-for-interview/PreparingForInterview/JavaInterviewLibrary.html":{"url":"zother6-JavaGuide/essential-content-for-interview/PreparingForInterview/JavaInterviewLibrary.html","title":"Java Interview Library","keywords":"","body":"昨天我整理了公众号历史所有和面试相关的我觉得还不错的文章：整理了一些有助于你拿Offer的文章 。今天分享一下最近逛Github看到了一些我觉得对于Java面试以及学习有帮助的仓库，这些仓库涉及Java核心知识点整理、Java常见面试题、算法、基础知识点比如网络和操作系统等等。 知识点相关 1.JavaGuide Github地址： https://github.com/Snailclimb/JavaGuide star: 64.0k 介绍: 【Java学习+面试指南】 一份涵盖大部分Java程序员所需要掌握的核心知识。 2.CS-Notes Github 地址：https://github.com/CyC2018/CS-Notes Star: 68.3k 介绍: 技术面试必备基础知识、Leetcode 题解、后端面试、Java 面试、春招、秋招、操作系统、计算机网络、系统设计。 3. advanced-java Github地址：https://github.com/doocs/advanced-java star: 23.4k 介绍: 互联网 Java 工程师进阶知识完全扫盲：涵盖高并发、分布式、高可用、微服务等领域知识，后端同学必看，前端同学也可学习。 4.JCSprout Github地址：https://github.com/crossoverJie/JCSprout star: 21.2k 介绍: Java Core Sprout：处于萌芽阶段的 Java 核心知识库。 5.toBeTopJavaer Github地址：https://github.com/hollischuang/toBeTopJavaer star: 4.0 k 介绍: Java工程师成神之路。 6.architect-awesome Github地址：https://github.com/xingshaocheng/architect-awesome star: 34.4 k 介绍:后端架构师技术图谱。 7.technology-talk Github地址： https://github.com/aalansehaiyang/technology-talk star: 6.1k 介绍: 汇总java生态圈常用技术框架、开源中间件，系统架构、项目管理、经典架构案例、数据库、常用三方库、线上运维等知识。 8.fullstack-tutorial Github地址： https://github.com/frank-lam/fullstack-tutorial star: 4.0k 介绍: fullstack tutorial 2019，后台技术栈/架构师之路/全栈开发社区，春招/秋招/校招/面试。 9.3y Github地址：https://github.com/ZhongFuCheng3y/3y star: 1.9 k 介绍: Java 知识整合。 10.java-bible Github地址：https://github.com/biezhi/java-bible star: 2.3k 介绍: 这里记录了一些技术摘要，部分文章来自网络，本项目的目的力求分享精品技术干货，以Java为主。 11.interviews Github地址: https://github.com/kdn251/interviews/blob/master/README-zh-cn.md star: 35.3k 介绍: 软件工程技术面试个人指南（国外的一个项目，虽然有翻译版，但是不太推荐，因为很多内容并不适用于国内）。 算法相关 1.LeetCodeAnimation Github 地址： https://github.com/MisterBooo/LeetCodeAnimation Star: 33.4k 介绍: Demonstrate all the questions on LeetCode in the form of animation.（用动画的形式呈现解LeetCode题目的思路）。 2.awesome-java-leetcode Github地址：https://github.com/Blankj/awesome-java-leetcode star: 6.1k 介绍: LeetCode 上 Facebook 的面试题目。 3.leetcode Github地址：https://github.com/azl397985856/leetcode star: 12.0k 介绍: LeetCode Solutions: A Record of My Problem Solving Journey.( leetcode题解，记录自己的leetcode解题之路。) "},"zother6-JavaGuide/essential-content-for-interview/PreparingForInterview/JavaProgrammerNeedKnow.html":{"url":"zother6-JavaGuide/essential-content-for-interview/PreparingForInterview/JavaProgrammerNeedKnow.html","title":"Java Programmer Need Know","keywords":"","body":"身边的朋友或者公众号的粉丝很多人都向我询问过：“我是双非/三本/专科学校的，我有机会进入大厂吗？”、“非计算机专业的学生能学好吗？”、“如何学习 Java？”、“Java 学习该学哪些东西？”、“我该如何准备 Java 面试？”......这些方面的问题。我会根据自己的一点经验对大部分人关心的这些问题进行答疑解惑。现在又刚好赶上考研结束，这篇文章也算是给考研结束准备往 Java 后端方向发展的朋友们指明一条学习之路。道理懂了如果没有实际行动，那这篇文章对你或许没有任何意义。 Question1:我是双非/三本/专科学校的，我有机会进入大厂吗？ Question2:非计算机专业的学生能学好 Java 后台吗？我能进大厂吗？ Question3: 我没有实习经历的话找工作是不是特别艰难？ Question4: 我该如何准备面试呢？面试的注意事项有哪些呢？ Question5: 我该自学还是报培训班呢？ Question6: 没有项目经历/博客/Github 开源项目怎么办？ Question7: 大厂青睐什么样的人？ Question1:我是双非/三本/专科学校的，我有机会进入大厂吗？ 我自己也是非 985 非 211 学校的，结合自己的经历以及一些朋友的经历，我觉得让我回答这个问题再好不过。 首先，我觉得学校歧视很正常，真的太正常了，如果要抱怨的话，你只能抱怨自己没有进入名校。但是，千万不要动不动说自己学校差，动不动拿自己学校当做自己进不了大厂的借口，学历只是筛选简历的很多标准中的一个而已，如果你够优秀，简历够丰富，你也一样可以和名校同学一起同台竞争。 企业 HR 肯定是更喜欢高学历的人，毕竟 985、211 优秀人才比例肯定比普通学校高很多，HR 团队肯定会优先在这些学校里选。这就好比相亲，你是愿意在很多优秀的人中选一个优秀的，还是愿意在很多普通的人中选一个优秀的呢？ 双非本科甚至是二本、三本甚至是专科的同学也有很多进入大厂的，不过比率相比于名校的低很多而已。从大厂招聘的结果上看，高学历人才的数量占据大头，那些成功进入 BAT、美团，京东，网易等大厂的双非本科甚至是二本、三本甚至是专科的同学往往是因为具备丰富的项目经历或者在某个含金量比较高的竞赛比如 ACM 中取得了不错的成绩。一部分学历不突出但能力出众的面试者能够进入大厂并不是说明学历不重要，而是学历的软肋能够通过其他的优势来弥补。 所以，如果你的学校不够好而你自己又想去大厂的话，建议你可以从这几点来做：① 尽量在面试前最好有一个可以拿的出手的项目；② 有实习条件的话，尽早出去实习，实习经历也会是你的简历的一个亮点（有能力在大厂实习最佳！）；③ 参加一些含金量比较高的比赛，拿不拿得到名次没关系，重在锻炼。 Question2:非计算机专业的学生能学好 Java 后台吗？我能进大厂吗？ 当然可以！现在非科班的程序员很多，很大一部分原因是互联网行业的工资比较高。我们学校外面的培训班里面 90%都是非科班，我觉得他们很多人学的都还不错。另外，我的一个朋友本科是机械专业，大一开始自学安卓，技术贼溜，在我看来他比大部分本科是计算机的同学学的还要好。参考 Question1 的回答，即使你是非科班程序员，如果你想进入大厂的话，你也可以通过自己的其他优势来弥补。 我觉得我们不应该因为自己的专业给自己划界限或者贴标签，说实话，很多科班的同学可能并不如你，你以为科班的同学就会认真听讲吗？还不是几乎全靠自己课下自学！不过如果你是非科班的话，你想要学好，那么注定就要舍弃自己本专业的一些学习时间，这是无可厚非的。 建议非科班的同学，首先要打好计算机基础知识基础：① 计算机网络、② 操作系统、③ 数据机构与算法，我个人觉得这 3 个对你最重要。这些东西就像是内功，对你以后的长远发展非常有用。当然，如果你想要进大厂的话，这些知识也是一定会被问到的。另外，“一定学好数据结构与算法！一定学好数据结构与算法！一定学好数据结构与算法！”，重要的东西说 3 遍。 Question3: 我没有实习经历的话找工作是不是特别艰难？ 没有实习经历没关系，只要你有拿得出手的项目或者大赛经历的话，你依然有可能拿到大厂的 offer 。笔主当时找工作的时候就没有实习经历以及大赛获奖经历，单纯就是凭借自己的项目经验撑起了整个面试。 如果你既没有实习经历，又没有拿得出手的项目或者大赛经历的话，我觉得在简历关，除非你有其他特别的亮点，不然，你应该就会被刷。 Question4: 我该如何准备面试呢？面试的注意事项有哪些呢？ 下面是我总结的一些准备面试的 Tips 以及面试必备的注意事项： 准备一份自己的自我介绍，面试的时候根据面试对象适当进行修改（突出重点，突出自己的优势在哪里，切忌流水账）； 注意随身带上自己的成绩单和简历复印件； （有的公司在面试前都会让你交一份成绩单和简历当做面试中的参考。） 如果需要笔试就提前刷一些笔试题，大部分在线笔试的类型是选择题+编程题，有的还会有简答题。（平时空闲时间多的可以刷一下笔试题目（牛客网上有很多），但是不要只刷面试题，不动手 code，程序员不是为了考试而存在的。）另外，注意抓重点，因为题目太多了，但是有很多题目几乎次次遇到，像这样的题目一定要搞定。 提前准备技术面试。 搞清楚自己面试中可能涉及哪些知识点、哪些知识点是重点。面试中哪些问题会被经常问到、自己该如何回答。(强烈不推荐背题，第一：通过背这种方式你能记住多少？能记住多久？第二：背题的方式的学习很难坚持下去！) 面试之前做好定向复习。 也就是专门针对你要面试的公司来复习。比如你在面试之前可以在网上找找有没有你要面试的公司的面经。 准备好自己的项目介绍。 如果有项目的话，技术面试第一步，面试官一般都是让你自己介绍一下你的项目。你可以从下面几个方向来考虑：① 对项目整体设计的一个感受（面试官可能会让你画系统的架构图）；② 在这个项目中你负责了什么、做了什么、担任了什么角色；③ 从这个项目中你学会了那些东西，使用到了那些技术，学会了那些新技术的使用；④ 项目描述中，最好可以体现自己的综合素质，比如你是如何协调项目组成员协同开发的或者在遇到某一个棘手的问题的时候你是如何解决的又或者说你在这个项目用了什么技术实现了什么功能比如：用 redis 做缓存提高访问速度和并发量、使用消息队列削峰和降流等等。 面试之后记得复盘。 面试遭遇失败是很正常的事情，所以善于总结自己的失败原因才是最重要的。如果失败，不要灰心；如果通过，切勿狂喜。 一些还算不错的 Java 面试/学习相关的仓库，相信对大家准备面试一定有帮助：盘点一下 Github 上开源的 Java 面试/学习相关的仓库，看完弄懂薪资至少增加 10k Question5: 我该自学还是报培训班呢？ 我本人更加赞同自学（你要知道去了公司可没人手把手教你了，而且几乎所有的公司都对培训班出生的有偏见。为什么有偏见，你学个东西还要去培训班，说明什么，同等水平下，你的自学能力以及自律能力一定是比不上自学的人的）。但是如果，你连每天在寝室坚持学上 8 个小时以上都坚持不了，或者总是容易半途而废的话，我还是推荐你去培训班。观望身边同学去培训班的，大多是非计算机专业或者是没有自律能力以及自学能力非常差的人。 另外，如果自律能力不行，你也可以通过结伴学习、参加老师的项目等方式来督促自己学习。 总结：去不去培训班主要还是看自己，如果自己能坚持自学就自学，坚持不下来就去培训班。 Question6: 没有项目经历/博客/Github 开源项目怎么办？ 从现在开始做！ 网上有很多非常不错的项目视频，你就跟着一步一步做，不光要做，还要改进，改善。另外，如果你的老师有相关 Java 后台项目的话，你也可以主动申请参与进来。 如果有自己的博客，也算是简历上的一个亮点。建议可以在掘金、Segmentfault、CSDN 等技术交流社区写博客，当然，你也可以自己搭建一个博客（采用 Hexo+Githu Pages 搭建非常简单）。写一些什么？学习笔记、实战内容、读书笔记等等都可以。 多用 Github，用好 Github，上传自己不错的项目，写好 readme 文档，在其他技术社区做好宣传。相信你也会收获一个不错的开源项目！ Question7: 大厂青睐什么样的人？ 先从已经有两年左右开发经验的工程师角度来看： 我们来看一下阿里官网支付宝 Java 高级开发工程师的招聘要求，从下面的招聘信息可以看出，除去 Java 基础/集合/多线程这些，这些能力格外重要： 底层知识比如 jvm ：不只是懂理论更会实操； 面向对象编程能力 ：我理解这个不仅包括“面向对象编程”，还有 SOLID 软件设计原则，相关阅读：《写了这么多年代码，你真的了解 SOLID 吗？》（我司大佬的一篇文章） 框架能力 ：不只是使用那么简单，更要搞懂原理和机制！搞懂原理和机制的基础是要学会看源码。 分布式系统开发能力 ：缓存、消息队列等等都要掌握，关键是还要能使用这些技术解决实际问题而不是纸上谈兵。 不错的 sense :喜欢和尝试新技术、追求编写优雅的代码等等。 再从应届生的角度来看： 我们还是看阿里巴巴的官网相关应届生 Java 工程师招聘岗位的相关要求。 结合阿里、腾讯等大厂招聘官网对于 Java 后端方向/后端方向的应届实习生的要求下面几点也提升你的个人竞争力： 参加过竞赛（ 含金量超高的是 ACM ）； 对数据结构与算法非常熟练； 参与过实际项目（比如学校网站） 熟悉 Python、Shell、Perl 其中一门脚本语言； 熟悉如何优化 Java 代码、有写出质量更高的代码的意识； 熟悉 SOA 分布式相关的知识尤其是理论知识； 熟悉自己所用框架的底层知识比如 Spring； 有高并发开发经验； 有大数据开发经验等等。 从来到大学之后，我的好多阅历非常深的老师经常就会告诫我们：“ 一定要有一门自己的特长，不管是技术还好还是其他能力 ” 。我觉得这句话真的非常有道理！ 刚刚也提到了要有一门特长，所以在这里再强调一点：公司不需要你什么都会，但是在某一方面你一定要有过于常人的优点。换言之就是我们不需要去掌握每一门技术（你也没精力去掌握这么多技术），而是需要去深入研究某一门技术，对于其他技术我们可以简单了解一下。 "},"zother6-JavaGuide/essential-content-for-interview/PreparingForInterview/应届生面试最爱问的几道Java基础问题.html":{"url":"zother6-JavaGuide/essential-content-for-interview/PreparingForInterview/应届生面试最爱问的几道Java基础问题.html","title":"应届生面试最爱问的几道Java基础问题","keywords":"","body":" 一 为什么 Java 中只有值传递？ 二 ==与 equals(重要) 三 hashCode 与 equals（重要） 3.1 hashCode（）介绍 3.2 为什么要有 hashCode 3.3 hashCode（）与 equals（）的相关规定 3.4 为什么两个对象有相同的 hashcode 值，它们也不一定是相等的？ 四 String 和 StringBuffer、StringBuilder 的区别是什么？String 为什么是不可变的？ - [String 为什么是不可变的吗？](#string-为什么是不可变的吗) - [String 真的是不可变的吗？](#string-真的是不可变的吗) 五 什么是反射机制？反射机制的应用场景有哪些？ 5.1 反射机制介绍 5.2 静态编译和动态编译 5.3 反射机制优缺点 5.4 反射的应用场景 六 什么是 JDK?什么是 JRE？什么是 JVM？三者之间的联系与区别 6.1 JVM 6.2 JDK 和 JRE 七 什么是字节码？采用字节码的最大好处是什么？ 八 接口和抽象类的区别是什么? 九 重载和重写的区别 重载 重写 十. Java 面向对象编程三大特性: 封装 继承 多态 封装 继承 多态 十一. 什么是线程和进程? 11.1 何为进程? 11.2 何为线程? 十二. 请简要描述线程与进程的关系,区别及优缺点？ 12.1 图解进程和线程的关系 12.2 程序计数器为什么是私有的? 12.3 虚拟机栈和本地方法栈为什么是私有的? 12.4 一句话简单了解堆和方法区 十三. 说说并发与并行的区别? 十四. 什么是上下文切换? 十五. 什么是线程死锁?如何避免死锁? 15.1. 认识线程死锁 15.2 如何避免线程死锁? 十六. 说说 sleep() 方法和 wait() 方法区别和共同点? 十七. 为什么我们调用 start() 方法时会执行 run() 方法，为什么我们不能直接调用 run() 方法？ 参考 一 为什么 Java 中只有值传递？ 首先回顾一下在程序设计语言中有关将参数传递给方法（或函数）的一些专业术语。按值调用(call by value)表示方法接收的是调用者提供的值，而按引用调用（call by reference)表示方法接收的是调用者提供的变量地址。一个方法可以修改传递引用所对应的变量值，而不能修改传递值调用所对应的变量值。 它用来描述各种程序设计语言（不只是 Java)中方法参数传递方式。 Java 程序设计语言总是采用按值调用。也就是说，方法得到的是所有参数值的一个拷贝，也就是说，方法不能修改传递给它的任何参数变量的内容。 下面通过 3 个例子来给大家说明 example 1 public static void main(String[] args) { int num1 = 10; int num2 = 20; swap(num1, num2); System.out.println(\"num1 = \" + num1); System.out.println(\"num2 = \" + num2); } public static void swap(int a, int b) { int temp = a; a = b; b = temp; System.out.println(\"a = \" + a); System.out.println(\"b = \" + b); } 结果： a = 20 b = 10 num1 = 10 num2 = 20 解析： 在 swap 方法中，a、b 的值进行交换，并不会影响到 num1、num2。因为，a、b 中的值，只是从 num1、num2 的复制过来的。也就是说，a、b 相当于 num1、num2 的副本，副本的内容无论怎么修改，都不会影响到原件本身。 通过上面例子，我们已经知道了一个方法不能修改一个基本数据类型的参数，而对象引用作为参数就不一样，请看 example2. example 2 public static void main(String[] args) { int[] arr = { 1, 2, 3, 4, 5 }; System.out.println(arr[0]); change(arr); System.out.println(arr[0]); } public static void change(int[] array) { // 将数组的第一个元素变为0 array[0] = 0; } 结果： 1 0 解析： array 被初始化 arr 的拷贝也就是一个对象的引用，也就是说 array 和 arr 指向的是同一个数组对象。 因此，外部对引用对象的改变会反映到所对应的对象上。 通过 example2 我们已经看到，实现一个改变对象参数状态的方法并不是一件难事。理由很简单，方法得到的是对象引用的拷贝，对象引用及其他的拷贝同时引用同一个对象。 很多程序设计语言（特别是，C++和 Pascal)提供了两种参数传递的方式：值调用和引用调用。有些程序员（甚至本书的作者）认为 Java 程序设计语言对对象采用的是引用调用，实际上，这种理解是不对的。由于这种误解具有一定的普遍性，所以下面给出一个反例来详细地阐述一下这个问题。 example 3 public class Test { public static void main(String[] args) { // TODO Auto-generated method stub Student s1 = new Student(\"小张\"); Student s2 = new Student(\"小李\"); Test.swap(s1, s2); System.out.println(\"s1:\" + s1.getName()); System.out.println(\"s2:\" + s2.getName()); } public static void swap(Student x, Student y) { Student temp = x; x = y; y = temp; System.out.println(\"x:\" + x.getName()); System.out.println(\"y:\" + y.getName()); } } 结果： x:小李 y:小张 s1:小张 s2:小李 解析： 交换之前： 交换之后： 通过上面两张图可以很清晰的看出： 方法并没有改变存储在变量 s1 和 s2 中的对象引用。swap 方法的参数 x 和 y 被初始化为两个对象引用的拷贝，这个方法交换的是这两个拷贝 总结 Java 程序设计语言对对象采用的不是引用调用，实际上，对象引用是按 值传递的。 下面再总结一下 Java 中方法参数的使用情况： 一个方法不能修改一个基本数据类型的参数（即数值型或布尔型）。 一个方法可以改变一个对象参数的状态。 一个方法不能让对象参数引用一个新的对象。 参考： 《Java 核心技术卷 Ⅰ》基础知识第十版第四章 4.5 小节 二 ==与 equals(重要) == : 它的作用是判断两个对象的地址是不是相等。即，判断两个对象是不是同一个对象。(基本数据类型==比较的是值，引用数据类型==比较的是内存地址) equals() : 它的作用也是判断两个对象是否相等。但它一般有两种使用情况： 情况 1：类没有覆盖 equals()方法。则通过 equals()比较该类的两个对象时，等价于通过“==”比较这两个对象。 情况 2：类覆盖了 equals()方法。一般，我们都覆盖 equals()方法来两个对象的内容相等；若它们的内容相等，则返回 true(即，认为这两个对象相等)。 举个例子： public class test1 { public static void main(String[] args) { String a = new String(\"ab\"); // a 为一个引用 String b = new String(\"ab\"); // b为另一个引用,对象的内容一样 String aa = \"ab\"; // 放在常量池中 String bb = \"ab\"; // 从常量池中查找 if (aa == bb) // true System.out.println(\"aa==bb\"); if (a == b) // false，非同一对象 System.out.println(\"a==b\"); if (a.equals(b)) // true System.out.println(\"aEQb\"); if (42 == 42.0) { // true System.out.println(\"true\"); } } } 说明： String 中的 equals 方法是被重写过的，因为 object 的 equals 方法是比较的对象的内存地址，而 String 的 equals 方法比较的是对象的值。 当创建 String 类型的对象时，虚拟机会在常量池中查找有没有已经存在的值和要创建的值相同的对象，如果有就把它赋给当前引用。如果没有就在常量池中重新创建一个 String 对象。 三 hashCode 与 equals（重要） 面试官可能会问你：“你重写过 hashcode 和 equals 么，为什么重写 equals 时必须重写 hashCode 方法？” 3.1 hashCode（）介绍 hashCode() 的作用是获取哈希码，也称为散列码；它实际上是返回一个 int 整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode() 定义在 JDK 的 Object.java 中，这就意味着 Java 中的任何类都包含有 hashCode() 函数。另外需要注意的是： Object 的 hashcode 方法是本地方法，也就是用 c 语言或 c++ 实现的，该方法通常用来将对象的 内存地址 转换为整数之后返回。 /** * Returns a hash code value for the object. This method is * supported for the benefit of hash tables such as those provided by * {@link java.util.HashMap}. * * As much as is reasonably practical, the hashCode method defined by * class {@code Object} does return distinct integers for distinct * objects. (This is typically implemented by converting the internal * address of the object into an integer, but this implementation * technique is not required by the * Java&trade; programming language.) * * @return a hash code value for this object. * @see java.lang.Object#equals(java.lang.Object) * @see java.lang.System#identityHashCode */ public native int hashCode(); 散列表存储的是键值对(key-value)，它的特点是：能根据“键”快速的检索出对应的“值”。这其中就利用到了散列码！（可以快速找到所需要的对象） 3.2 为什么要有 hashCode 我们以“HashSet 如何检查重复”为例子来说明为什么要有 hashCode： 当你把对象加入 HashSet 时，HashSet 会先计算对象的 hashcode 值来判断对象加入的位置，同时也会与其他已经加入的对象的 hashcode 值作比较，如果没有相符的 hashcode，HashSet 会假设对象没有重复出现。但是如果发现有相同 hashcode 值的对象，这时会调用 equals（）方法来检查 hashcode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让其加入操作成功。如果不同的话，就会重新散列到其他位置。（摘自我的 Java 启蒙书《Head fist java》第二版）。这样我们就大大减少了 equals 的次数，相应就大大提高了执行速度。 3.3 hashCode（）与 equals（）的相关规定 如果两个对象相等，则 hashcode 一定也是相同的 两个对象相等,对两个对象分别调用 equals 方法都返回 true 两个对象有相同的 hashcode 值，它们也不一定是相等的 因此，equals 方法被覆盖过，则 hashCode 方法也必须被覆盖 hashCode()的默认行为是对堆上的对象产生独特值。如果没有重写 hashCode()，则该 class 的两个对象无论如何都不会相等（即使这两个对象指向相同的数据） 3.4 为什么两个对象有相同的 hashcode 值，它们也不一定是相等的？ 在这里解释一位小伙伴的问题。以下内容摘自《Head Fisrt Java》。 因为 hashCode() 所使用的杂凑算法也许刚好会让多个对象传回相同的杂凑值。越糟糕的杂凑算法越容易碰撞，但这也与数据值域分布的特性有关（所谓碰撞也就是指的是不同的对象得到相同的 hashCode）。 我们刚刚也提到了 HashSet,如果 HashSet 在对比的时候，同样的 hashcode 有多个对象，它会使用 equals() 来判断是否真的相同。也就是说 hashcode 只是用来缩小查找成本。 四 String 和 StringBuffer、StringBuilder 的区别是什么？String 为什么是不可变的？ 可变性 简单的来说：String 类中使用 final 关键字修饰字符数组来保存字符串，private final char value[]，所以 String 对象是不可变的。而 StringBuilder 与 StringBuffer 都继承自 AbstractStringBuilder 类，在 AbstractStringBuilder 中也是使用字符数组保存字符串char[]value 但是没有用 final 关键字修饰，所以这两种对象都是可变的。 StringBuilder 与 StringBuffer 的构造方法都是调用父类构造方法也就是 AbstractStringBuilder 实现的，大家可以自行查阅源码。 AbstractStringBuilder.java abstract class AbstractStringBuilder implements Appendable, CharSequence { char[] value; int count; AbstractStringBuilder() { } AbstractStringBuilder(int capacity) { value = new char[capacity]; } 线程安全性 String 中的对象是不可变的，也就可以理解为常量，线程安全。AbstractStringBuilder 是 StringBuilder 与 StringBuffer 的公共父类，定义了一些字符串的基本操作，如 expandCapacity、append、insert、indexOf 等公共方法。StringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。StringBuilder 并没有对方法进行加同步锁，所以是非线程安全的。 性能 每次对 String 类型进行改变的时候，都会生成一个新的 String 对象，然后将指针指向新的 String 对象。StringBuffer 每次都会对 StringBuffer 对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用 StringBuilder 相比使用 StringBuffer 仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险。 对于三者使用的总结： 操作少量的数据: 适用 String 单线程操作字符串缓冲区下操作大量数据: 适用 StringBuilder 多线程操作字符串缓冲区下操作大量数据: 适用 StringBuffer String 为什么是不可变的吗？ 简单来说就是 String 类利用了 final 修饰的 char 类型数组存储字符，源码如下图所以： /** The value is used for character storage. */ private final char value[]; String 真的是不可变的吗？ 我觉得如果别人问这个问题的话，回答不可变就可以了。 下面只是给大家看两个有代表性的例子： 1) String 不可变但不代表引用不可以变 String str = \"Hello\"; str = str + \" World\"; System.out.println(\"str=\" + str); 结果： str=Hello World 解析： 实际上，原来 String 的内容是不变的，只是 str 由原来指向\"Hello\"的内存地址转为指向\"Hello World\"的内存地址而已，也就是说多开辟了一块内存区域给\"Hello World\"字符串。 2) 通过反射是可以修改所谓的“不可变”对象 // 创建字符串\"Hello World\"， 并赋给引用s String s = \"Hello World\"; System.out.println(\"s = \" + s); // Hello World // 获取String类中的value字段 Field valueFieldOfString = String.class.getDeclaredField(\"value\"); // 改变value属性的访问权限 valueFieldOfString.setAccessible(true); // 获取s对象上的value属性的值 char[] value = (char[]) valueFieldOfString.get(s); // 改变value所引用的数组中的第5个字符 value[5] = '_'; System.out.println(\"s = \" + s); // Hello_World 结果： s = Hello World s = Hello_World 解析： 用反射可以访问私有成员， 然后反射出 String 对象中的 value 属性， 进而改变通过获得的 value 引用改变数组的结构。但是一般我们不会这么做，这里只是简单提一下有这个东西。 五 什么是反射机制？反射机制的应用场景有哪些？ 5.1 反射机制介绍 JAVA 反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为 java 语言的反射机制。 5.2 静态编译和动态编译 静态编译：在编译时确定类型，绑定对象 动态编译：运行时确定类型，绑定对象 5.3 反射机制优缺点 优点： 运行期类型的判断，动态加载类，提高代码灵活度。 缺点： 性能瓶颈：反射相当于一系列解释操作，通知 JVM 要做的事情，性能比直接的 java 代码要慢很多。 5.4 反射的应用场景 反射是框架设计的灵魂。 在我们平时的项目开发过程中，基本上很少会直接使用到反射机制，但这不能说明反射机制没有用，实际上有很多设计、开发都与反射机制有关，例如模块化的开发，通过反射去调用对应的字节码；动态代理设计模式也采用了反射机制，还有我们日常使用的 Spring／Hibernate 等框架也大量使用到了反射机制。 举例：① 我们在使用 JDBC 连接数据库时使用 Class.forName()通过反射加载数据库的驱动程序；②Spring 框架也用到很多反射机制，最经典的就是 xml 的配置模式。Spring 通过 XML 配置模式装载 Bean 的过程：1) 将程序内所有 XML 或 Properties 配置文件加载入内存中; 2)Java 类里面解析 xml 或 properties 里面的内容，得到对应实体类的字节码字符串以及相关的属性信息; 3)使用反射机制，根据这个字符串获得某个类的 Class 实例; 4)动态配置实例的属性 推荐阅读： Reflection：Java 反射机制的应用场景 Java 基础之—反射（非常重要） 六 什么是 JDK?什么是 JRE？什么是 JVM？三者之间的联系与区别 6.1 JVM Java 虚拟机（JVM）是运行 Java 字节码的虚拟机。JVM 有针对不同系统的特定实现（Windows，Linux，macOS），目的是使用相同的字节码，它们都会给出相同的结果。 什么是字节码?采用字节码的好处是什么? 在 Java 中，JVM 可以理解的代码就叫做字节码（即扩展名为 .class 的文件），它不面向任何特定的处理器，只面向虚拟机。Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。所以 Java 程序运行时比较高效，而且，由于字节码并不针对一种特定的机器，因此，Java 程序无须重新编译便可在多种不同操作系统的计算机上运行。 Java 程序从源代码到运行一般有下面 3 步： 我们需要格外注意的是 .class->机器码 这一步。在这一步 JVM 类加载器首先加载字节码文件，然后通过解释器逐行解释执行，这种方式的执行速度会相对比较慢。而且，有些方法和代码块是经常需要被调用的(也就是所谓的热点代码)，所以后面引进了 JIT 编译器，而 JIT 属于运行时编译。当 JIT 编译器完成第一次编译后，其会将字节码对应的机器码保存下来，下次可以直接使用。而我们知道，机器码的运行效率肯定是高于 Java 解释器的。这也解释了我们为什么经常会说 Java 是编译与解释共存的语言。 HotSpot 采用了惰性评估(Lazy Evaluation)的做法，根据二八定律，消耗大部分系统资源的只有那一小部分的代码（热点代码），而这也就是 JIT 所需要编译的部分。JVM 会根据代码每次被执行的情况收集信息并相应地做出一些优化，因此执行的次数越多，它的速度就越快。JDK 9 引入了一种新的编译模式 AOT(Ahead of Time Compilation)，它是直接将字节码编译成机器码，这样就避免了 JIT 预热等各方面的开销。JDK 支持分层编译和 AOT 协作使用。但是 ，AOT 编译器的编译质量是肯定比不上 JIT 编译器的。 总结： Java 虚拟机（JVM）是运行 Java 字节码的虚拟机。JVM 有针对不同系统的特定实现（Windows，Linux，macOS），目的是使用相同的字节码，它们都会给出相同的结果。字节码和不同系统的 JVM 实现是 Java 语言“一次编译，随处可以运行”的关键所在。 6.2 JDK 和 JRE JDK 是 Java Development Kit，它是功能齐全的 Java SDK。它拥有 JRE 所拥有的一切，还有编译器（javac）和工具（如 javadoc 和 jdb）。它能够创建和编译程序。 JRE 是 Java 运行时环境。它是运行已编译 Java 程序所需的所有内容的集合，包括 Java 虚拟机（JVM），Java 类库，java 命令和其他的一些基础构件。但是，它不能用于创建新程序。 如果你只是为了运行一下 Java 程序的话，那么你只需要安装 JRE 就可以了。如果你需要进行一些 Java 编程方面的工作，那么你就需要安装 JDK 了。但是，这不是绝对的。有时，即使您不打算在计算机上进行任何 Java 开发，仍然需要安装 JDK。例如，如果要使用 JSP 部署 Web 应用程序，那么从技术上讲，您只是在应用程序服务器中运行 Java 程序。那你为什么需要 JDK 呢？因为应用程序服务器会将 JSP 转换为 Java servlet，并且需要使用 JDK 来编译 servlet。 七 什么是字节码？采用字节码的最大好处是什么？ 先看下 java 中的编译器和解释器： Java 中引入了虚拟机的概念，即在机器和编译程序之间加入了一层抽象的虚拟的机器。这台虚拟的机器在任何平台上都提供给编译程序一个的共同的接口。编译程序只需要面向虚拟机，生成虚拟机能够理解的代码，然后由解释器来将虚拟机代码转换为特定系统的机器码执行。在 Java 中，这种供虚拟机理解的代码叫做字节码（即扩展名为.class的文件），它不面向任何特定的处理器，只面向虚拟机。每一种平台的解释器是不同的，但是实现的虚拟机是相同的。Java 源程序经过编译器编译后变成字节码，字节码由虚拟机解释执行，虚拟机将每一条要执行的字节码送给解释器，解释器将其翻译成特定机器上的机器码，然后在特定的机器上运行。这也就是解释了 Java 的编译与解释并存的特点。 Java 源代码---->编译器---->jvm 可执行的 Java 字节码(即虚拟指令)---->jvm---->jvm 中解释器----->机器可执行的二进制机器码---->程序运行。 采用字节码的好处： Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。所以 Java 程序运行时比较高效，而且，由于字节码并不专对一种特定的机器，因此，Java 程序无须重新编译便可在多种不同的计算机上运行。 八 接口和抽象类的区别是什么? 接口的方法默认是 public，所有方法在接口中不能有实现，抽象类可以有非抽象的方法 接口中的实例变量默认是 final 类型的，而抽象类中则不一定 一个类可以实现多个接口，但最多只能实现一个抽象类 一个类实现接口的话要实现接口的所有方法，而抽象类不一定 接口不能用 new 实例化，但可以声明，但是必须引用一个实现该接口的对象 从设计层面来说，抽象是对类的抽象，是一种模板设计，接口是行为的抽象，是一种行为的规范。 注意：Java8 后接口可以有默认实现( default )。 九 重载和重写的区别 重载 发生在同一个类中，方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同。 下面是《Java 核心技术》对重载这个概念的介绍： 重写 重写是子类对父类的允许访问的方法的实现过程进行重新编写,发生在子类中，方法名、参数列表必须相同，返回值范围小于等于父类，抛出的异常范围小于等于父类，访问修饰符范围大于等于父类。另外，如果父类方法访问修饰符为 private 则子类就不能重写该方法。也就是说方法提供的行为改变，而方法的外貌并没有改变。 十. Java 面向对象编程三大特性: 封装 继承 多态 封装 封装把一个对象的属性私有化，同时提供一些可以被外界访问的属性的方法，如果属性不想被外界访问，我们大可不必提供方法给外界访问。但是如果一个类没有提供给外界访问的方法，那么这个类也没有什么意义了。 继承 继承是使用已存在的类的定义作为基础建立新类的技术，新类的定义可以增加新的数据或新的功能，也可以用父类的功能，但不能选择性地继承父类。通过使用继承我们能够非常方便地复用以前的代码。 关于继承如下 3 点请记住： 子类拥有父类对象所有的属性和方法（包括私有属性和私有方法），但是父类中的私有属性和方法子类是无法访问，只是拥有。 子类可以拥有自己属性和方法，即子类可以对父类进行扩展。 子类可以用自己的方式实现父类的方法。（以后介绍）。 多态 所谓多态就是指程序中定义的引用变量所指向的具体类型和通过该引用变量发出的方法调用在编程时并不确定，而是在程序运行期间才确定，即一个引用变量到底会指向哪个类的实例对象，该引用变量发出的方法调用到底是哪个类中实现的方法，必须在由程序运行期间才能决定。 在 Java 中有两种形式可以实现多态：继承（多个子类对同一方法的重写）和接口（实现接口并覆盖接口中同一方法）。 十一. 什么是线程和进程? 11.1 何为进程? 进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。 在 Java 中，当我们启动 main 函数时其实就是启动了一个 JVM 的进程，而 main 函数所在的线程就是这个进程中的一个线程，也称主线程。 如下图所示，在 windows 中通过查看任务管理器的方式，我们就可以清楚看到 window 当前运行的进程（.exe 文件的运行）。 11.2 何为线程? 线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享进程的堆和方法区资源，但每个线程有自己的程序计数器、虚拟机栈和本地方法栈，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。 Java 程序天生就是多线程程序，我们可以通过 JMX 来看一下一个普通的 Java 程序有哪些线程，代码如下。 public class MultiThread { public static void main(String[] args) { // 获取 Java 线程管理 MXBean ThreadMXBean threadMXBean = ManagementFactory.getThreadMXBean(); // 不需要获取同步的 monitor 和 synchronizer 信息，仅获取线程和线程堆栈信息 ThreadInfo[] threadInfos = threadMXBean.dumpAllThreads(false, false); // 遍历线程信息，仅打印线程 ID 和线程名称信息 for (ThreadInfo threadInfo : threadInfos) { System.out.println(\"[\" + threadInfo.getThreadId() + \"] \" + threadInfo.getThreadName()); } } } 上述程序输出如下（输出内容可能不同，不用太纠结下面每个线程的作用，只用知道 main 线程执行 main 方法即可）： [5] Attach Listener //添加事件 [4] Signal Dispatcher // 分发处理给 JVM 信号的线程 [3] Finalizer //调用对象 finalize 方法的线程 [2] Reference Handler //清除 reference 线程 [1] main //main 线程,程序入口 从上面的输出内容可以看出：一个 Java 程序的运行是 main 线程和多个其他线程同时运行。 十二. 请简要描述线程与进程的关系,区别及优缺点？ 从 JVM 角度说进程和线程之间的关系 12.1 图解进程和线程的关系 下图是 Java 内存区域，通过下图我们从 JVM 的角度来说一下线程和进程之间的关系。如果你对 Java 内存区域 (运行时数据区) 这部分知识不太了解的话可以阅读一下这篇文章：《可能是把 Java 内存区域讲的最清楚的一篇文章》 从上图可以看出：一个进程中可以有多个线程，多个线程共享进程的堆和方法区 (JDK1.8 之后的元空间)资源，但是每个线程有自己的程序计数器、虚拟机栈 和 本地方法栈。 总结： 线程 是 进程 划分成的更小的运行单位。线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。线程执行开销小，但不利于资源的管理和保护；而进程正相反 下面是该知识点的扩展内容！ 下面来思考这样一个问题：为什么程序计数器、虚拟机栈和本地方法栈是线程私有的呢？为什么堆和方法区是线程共享的呢？ 12.2 程序计数器为什么是私有的? 程序计数器主要有下面两个作用： 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 需要注意的是，如果执行的是 native 方法，那么程序计数器记录的是 undefined 地址，只有执行的是 Java 代码时程序计数器记录的才是下一条指令的地址。 所以，程序计数器私有主要是为了线程切换后能恢复到正确的执行位置。 12.3 虚拟机栈和本地方法栈为什么是私有的? 虚拟机栈： 每个 Java 方法在执行的同时会创建一个栈帧用于存储局部变量表、操作数栈、常量池引用等信息。从方法调用直至执行完成的过程，就对应着一个栈帧在 Java 虚拟机栈中入栈和出栈的过程。 本地方法栈： 和虚拟机栈所发挥的作用非常相似，区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。 所以，为了保证线程中的局部变量不被别的线程访问到，虚拟机栈和本地方法栈是线程私有的。 12.4 一句话简单了解堆和方法区 堆和方法区是所有线程共享的资源，其中堆是进程中最大的一块内存，主要用于存放新创建的对象 (所有对象都在这里分配内存)，方法区主要用于存放已被加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 十三. 说说并发与并行的区别? 并发： 同一时间段，多个任务都在执行 (单位时间内不一定同时执行)； 并行： 单位时间内，多个任务同时执行。 十四. 什么是上下文切换? 多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。 概括来说就是：当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换。 上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。 Linux 相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。 十五. 什么是线程死锁?如何避免死锁? 15.1. 认识线程死锁 多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放。由于线程被无限期地阻塞，因此程序不可能正常终止。 如下图所示，线程 A 持有资源 2，线程 B 持有资源 1，他们同时都想申请对方的资源，所以这两个线程就会互相等待而进入死锁状态。 下面通过一个例子来说明线程死锁,代码模拟了上图的死锁的情况 (代码来源于《并发编程之美》)： public class DeadLockDemo { private static Object resource1 = new Object();//资源 1 private static Object resource2 = new Object();//资源 2 public static void main(String[] args) { new Thread(() -> { synchronized (resource1) { System.out.println(Thread.currentThread() + \"get resource1\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \"waiting get resource2\"); synchronized (resource2) { System.out.println(Thread.currentThread() + \"get resource2\"); } } }, \"线程 1\").start(); new Thread(() -> { synchronized (resource2) { System.out.println(Thread.currentThread() + \"get resource2\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \"waiting get resource1\"); synchronized (resource1) { System.out.println(Thread.currentThread() + \"get resource1\"); } } }, \"线程 2\").start(); } } Output Thread[线程 1,5,main]get resource1 Thread[线程 2,5,main]get resource2 Thread[线程 1,5,main]waiting get resource2 Thread[线程 2,5,main]waiting get resource1 线程 A 通过 synchronized (resource1) 获得 resource1 的监视器锁，然后通过Thread.sleep(1000);让线程 A 休眠 1s 为的是让线程 B 得到执行然后获取到 resource2 的监视器锁。线程 A 和线程 B 休眠结束了都开始企图请求获取对方的资源，然后这两个线程就会陷入互相等待的状态，这也就产生了死锁。上面的例子符合产生死锁的四个必要条件。 学过操作系统的朋友都知道产生死锁必须具备以下四个条件： 互斥条件：该资源任意一个时刻只由一个线程占用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件:线程已获得的资源在末使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。 15.2 如何避免线程死锁? 我们只要破坏产生死锁的四个条件中的其中一个就可以了。 破坏互斥条件 这个条件我们没有办法破坏，因为我们用锁本来就是想让他们互斥的（临界资源需要互斥访问）。 破坏请求与保持条件 一次性申请所有的资源。 破坏不剥夺条件 占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。 破坏循环等待条件 靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。 我们对线程 2 的代码修改成下面这样就不会产生死锁了。 new Thread(() -> { synchronized (resource1) { System.out.println(Thread.currentThread() + \"get resource1\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \"waiting get resource2\"); synchronized (resource2) { System.out.println(Thread.currentThread() + \"get resource2\"); } } }, \"线程 2\").start(); Output Thread[线程 1,5,main]get resource1 Thread[线程 1,5,main]waiting get resource2 Thread[线程 1,5,main]get resource2 Thread[线程 2,5,main]get resource1 Thread[线程 2,5,main]waiting get resource2 Thread[线程 2,5,main]get resource2 Process finished with exit code 0 我们分析一下上面的代码为什么避免了死锁的发生? 线程 1 首先获得到 resource1 的监视器锁,这时候线程 2 就获取不到了。然后线程 1 再去获取 resource2 的监视器锁，可以获取到。然后线程 1 释放了对 resource1、resource2 的监视器锁的占用，线程 2 获取到就可以执行了。这样就破坏了破坏循环等待条件，因此避免了死锁。 十六. 说说 sleep() 方法和 wait() 方法区别和共同点? 两者最主要的区别在于：sleep 方法没有释放锁，而 wait 方法释放了锁 。 两者都可以暂停线程的执行。 Wait 通常被用于线程间交互/通信，sleep 通常被用于暂停执行。 wait() 方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的 notify() 或者 notifyAll() 方法。sleep() 方法执行完成后，线程会自动苏醒。或者可以使用 wait(long timeout)超时后线程会自动苏醒。 十七. 为什么我们调用 start() 方法时会执行 run() 方法，为什么我们不能直接调用 run() 方法？ 这是另一个非常经典的 java 多线程面试问题，而且在面试中会经常被问到。很简单，但是很多人都会答不上来！ new 一个 Thread，线程进入了新建状态;调用 start() 方法，会启动一个线程并使线程进入了就绪状态，当分配到时间片后就可以开始运行了。 start() 会执行线程的相应准备工作，然后自动执行 run() 方法的内容，这是真正的多线程工作。 而直接执行 run() 方法，会把 run 方法当成一个 main 线程下的普通方法去执行，并不会在某个线程中执行它，所以这并不是多线程工作。 总结： 调用 start 方法方可启动线程并使线程进入就绪状态，而 run 方法只是 thread 的一个普通方法调用，还是在主线程里执行。 参考 https://blog.csdn.net/zhzhao999/article/details/53449504 https://www.cnblogs.com/skywang12345/p/3324958.html https://www.cnblogs.com/Eason-S/p/5524837.html "},"zother6-JavaGuide/essential-content-for-interview/PreparingForInterview/程序员的简历之道.html":{"url":"zother6-JavaGuide/essential-content-for-interview/PreparingForInterview/程序员的简历之道.html","title":"程序员的简历之道","keywords":"","body":" 程序员简历就该这样写 为什么说简历很重要？ 先从面试前来说 再从面试中来说 下面这几点你必须知道 必须了解的两大法则 STAR法则（Situation Task Action Result） FAB 法则（Feature Advantage Benefit） 项目经历怎么写？ 专业技能该怎么写？ 排版注意事项 其他的一些小tips 推荐的工具/网站 程序员简历就该这样写 本篇文章除了教大家用Markdown如何写一份程序员专属的简历，后面还会给大家推荐一些不错的用来写Markdown简历的软件或者网站，以及如何优雅的将Markdown格式转变为PDF格式或者其他格式。 推荐大家使用Markdown语法写简历，然后再将Markdown格式转换为PDF格式后进行简历投递。 如果你对Markdown语法不太了解的话，可以花半个小时简单看一下Markdown语法说明: http://www.markdown.cn 。 为什么说简历很重要？ 一份好的简历可以在整个申请面试以及面试过程中起到非常好的作用。 在不夸大自己能力的情况下，写出一份好的简历也是一项很棒的能力。为什么说简历很重要呢? 先从面试前来说 假如你是网申，你的简历必然会经过HR的筛选，一张简历HR可能也就花费10秒钟看一下，然后HR就会决定你这一关是Fail还是Pass。 假如你是内推，如果你的简历没有什么优势的话，就算是内推你的人再用心，也无能为力。 另外，就算你通过了筛选，后面的面试中，面试官也会根据你的简历来判断你究竟是否值得他花费很多时间去面试。 所以，简历就像是我们的一个门面一样，它在很大程度上决定了你能否进入到下一轮的面试中。 再从面试中来说 我发现大家比较喜欢看面经 ，这点无可厚非，但是大部分面经都没告诉你很多问题都是在特定条件下才问的。举个简单的例子：一般情况下你的简历上注明你会的东西才会被问到（Java、数据结构、网络、算法这些基础是每个人必问的），比如写了你会 redis,那面试官就很大概率会问你 redis 的一些问题。比如：redis的常见数据类型及应用场景、redis是单线程为什么还这么快、 redis 和 memcached 的区别、redis 内存淘汰机制等等。 所以，首先，你要明确的一点是：你不会的东西就不要写在简历上。另外，你要考虑你该如何才能让你的亮点在简历中凸显出来，比如：你在某某项目做了什么事情解决了什么问题（只要有项目就一定有要解决的问题）、你的某一个项目里使用了什么技术后整体性能和并发量提升了很多等等。 面试和工作是两回事，聪明的人会把面试官往自己擅长的领域领，其他人则被面试官牵着鼻子走。虽说面试和工作是两回事，但是你要想要获得自己满意的 offer ，你自身的实力必须要强。 下面这几点你必须知道 大部分公司的HR都说我们不看重学历（骗你的！），但是如果你的学校不出众的话，很难在一堆简历中脱颖而出，除非你的简历上有特别的亮点，比如：某某大厂的实习经历、获得了某某大赛的奖等等。 大部分应届生找工作的硬伤是没有工作经验或实习经历，所以如果你是应届生就不要错过秋招和春招。一旦错过，你后面就极大可能会面临社招，这个时候没有工作经验的你可能就会面临各种碰壁，导致找不到一个好的工作 写在简历上的东西一定要慎重，这是面试官大量提问的地方； 将自己的项目经历完美的展示出来非常重要。 必须了解的两大法则 STAR法则（Situation Task Action Result） Situation： 事情是在什么情况下发生； Task:： 你是如何明确你的任务的； Action： 针对这样的情况分析，你采用了什么行动方式； Result： 结果怎样，在这样的情况下你学习到了什么。 简而言之，STAR法则，就是一种讲述自己故事的方式，或者说，是一个清晰、条理的作文模板。不管是什么，合理熟练运用此法则，可以轻松的对面试官描述事物的逻辑方式，表现出自己分析阐述问题的清晰性、条理性和逻辑性。 FAB 法则（Feature Advantage Benefit） Feature： 是什么； Advantage： 比别人好在哪些地方； Benefit： 如果雇佣你，招聘方会得到什么好处。 简单来说，这个法则主要是让你的面试官知道你的优势、招了你之后对公司有什么帮助。 项目经历怎么写？ 简历上有一两个项目经历很正常，但是真正能把项目经历很好的展示给面试官的非常少。对于项目经历大家可以考虑从如下几点来写： 对项目整体设计的一个感受 在这个项目中你负责了什么、做了什么、担任了什么角色 从这个项目中你学会了那些东西，使用到了那些技术，学会了那些新技术的使用 另外项目描述中，最好可以体现自己的综合素质，比如你是如何协调项目组成员协同开发的或者在遇到某一个棘手的问题的时候你是如何解决的又或者说你在这个项目用了什么技术实现了什么功能比如:用redis做缓存提高访问速度和并发量、使用消息队列削峰和降流等等。 专业技能该怎么写？ 先问一下你自己会什么，然后看看你意向的公司需要什么。一般HR可能并不太懂技术，所以他在筛选简历的时候可能就盯着你专业技能的关键词来看。对于公司有要求而你不会的技能，你可以花几天时间学习一下，然后在简历上可以写上自己了解这个技能。比如你可以这样写(下面这部分内容摘自我的简历，大家可以根据自己的情况做一些修改和完善)： 计算机网络、数据结构、算法、操作系统等课内基础知识：掌握 Java 基础知识：掌握 JVM 虚拟机（Java内存区域、虚拟机垃圾算法、虚拟垃圾收集器、JVM内存管理）：掌握 高并发、高可用、高性能系统开发：掌握 Struts2、Spring、Hibernate、Ajax、Mybatis、JQuery ：掌握 SSH 整合、SSM 整合、 SOA 架构：掌握 Dubbo： 掌握 Zookeeper: 掌握 常见消息队列: 掌握 Linux：掌握 MySQL常见优化手段：掌握 Spring Boot +Spring Cloud +Docker:了解 Hadoop 生态相关技术中的 HDFS、Storm、MapReduce、Hive、Hbase ：了解 Python 基础、一些常见第三方库比如OpenCV、wxpy、wordcloud、matplotlib：熟悉 排版注意事项 尽量简洁，不要太花里胡哨； 一些技术名词不要弄错了大小写比如MySQL不要写成mysql，Java不要写成java。这个在我看来还是比较忌讳的，所以一定要注意这个细节； 中文和数字英文之间加上空格的话看起来会舒服一点； 其他的一些小tips 尽量避免主观表述，少一点语义模糊的形容词，尽量要简洁明了，逻辑结构清晰。 如果自己有博客或者个人技术栈点的话，写上去会为你加分很多。 如果自己的Github比较活跃的话，写上去也会为你加分很多。 注意简历真实性，一定不要写自己不会的东西，或者带有欺骗性的内容 项目经历建议以时间倒序排序，另外项目经历不在于多，而在于有亮点。 如果内容过多的话，不需要非把内容压缩到一页，保持排版干净整洁就可以了。 简历最后最好能加上：“感谢您花时间阅读我的简历，期待能有机会和您共事。”这句话，显的你会很有礼貌。 推荐的工具/网站 冷熊简历(MarkDown在线简历工具，可在线预览、编辑和生成PDF):http://cv.ftqq.com/ Typora+Java程序员简历模板 Guide哥自己写的Markdown模板：https://github.com/Snailclimb/typora-markdown-resume "},"zother6-JavaGuide/essential-content-for-interview/PreparingForInterview/美团面试常见问题总结.html":{"url":"zother6-JavaGuide/essential-content-for-interview/PreparingForInterview/美团面试常见问题总结.html","title":"美团面试常见问题总结","keywords":"","body":" 一 基础篇 1. System.out.println(3|9)输出什么? 2. 说一下转发(Forward)和重定向(Redirect)的区别 3. 在浏览器中输入 url 地址到显示主页的过程,整个过程会使用哪些协议 4. TCP 三次握手和四次挥手 - [为什么要三次握手](#为什么要三次握手) - [为什么要传回 SYN](#为什么要传回-syn) - [传了 SYN,为啥还要传 ACK](#传了-syn为啥还要传-ack) - [为什么要四次挥手](#为什么要四次挥手) 5. IP 地址与 MAC 地址的区别 6. HTTP 请求,响应报文格式 7. 为什么要使用索引?索引这么多优点,为什么不对表中的每一个列创建一个索引呢?索引是如何提高查询速度的?说一下使用索引的注意事项?Mysql 索引主要使用的两种数据结构?什么是覆盖索引? 8. 进程与线程的区别是什么?进程间的几种通信方式说一下?线程间的几种通信方式知道不? 9. 为什么要用单例模式?手写几种线程安全的单例模式? 10. 简单介绍一下 bean;知道 Spring 的 bean 的作用域与生命周期吗? 11. Spring 中的事务传播行为了解吗?TransactionDefinition 接口中哪五个表示隔离级别的常量? - [事务传播行为](#事务传播行为) - [隔离级别](#隔离级别) 12. SpringMVC 原理了解吗? 13. Spring AOP IOC 实现原理 二 进阶篇 1 消息队列 MQ 的套路 1.1 介绍一下消息队列 MQ 的应用场景/使用消息队列的好处 1)通过异步处理提高系统性能 2)降低系统耦合性 1.2 那么使用消息队列会带来什么问题?考虑过这些问题吗? 1.3 介绍一下你知道哪几种消息队列,该如何选择呢? 1.4 关于消息队列其他一些常见的问题展望 2 谈谈 InnoDB 和 MyIsam 两者的区别 2.1 两者的对比 2.2 关于两者的总结 3 聊聊 Java 中的集合吧! 3.1 Arraylist 与 LinkedList 有什么不同?(注意加上从数据结构分析的内容) 3.2 HashMap 的底层实现 1)JDK1.8 之前 2)JDK1.8 之后 3.3 既然谈到了红黑树,你给我手绘一个出来吧,然后简单讲一下自己对于红黑树的理解 3.4 红黑树这么优秀,为何不直接使用红黑树得了? 3.5 HashMap 和 Hashtable 的区别/HashSet 和 HashMap 区别 三 终结篇 1. Object 类有哪些方法? 1.1 Object 类的常见方法总结 1.2 hashCode 与 equals 1.2.1 hashCode()介绍 1.2.2 为什么要有 hashCode 1.2.3 hashCode()与 equals()的相关规定 1.2.4 为什么两个对象有相同的 hashcode 值,它们也不一定是相等的? 1.3 ==与 equals 2 ConcurrentHashMap 相关问题 2.1 ConcurrentHashMap 和 Hashtable 的区别 2.2 ConcurrentHashMap 线程安全的具体实现方式/底层具体实现 JDK1.7(上面有示意图) JDK1.8(上面有示意图) 3 谈谈 synchronized 和 ReentrantLock 的区别 4 线程池了解吗? 4.1 为什么要用线程池? 4.2 Java 提供了哪几种线程池?他们各自的使用场景是什么? Java 主要提供了下面 4 种线程池 各种线程池的适用场景介绍 4.3 创建的线程池的方式 5 Nginx 5.1 简单介绍一下 Nginx 反向代理 负载均衡 动静分离 5.2 为什么要用 Nginx? 5.3 Nginx 的四个主要组成部分了解吗? 这些问题是 2018 年去美团面试的同学被问到的一些常见的问题，希望对你有帮助！ 一 基础篇 1. System.out.println(3|9)输出什么? 正确答案：11。 考察知识点：&和&&；|和|| &和&&： 共同点：两者都可做逻辑运算符。它们都表示运算符的两边都是 true 时，结果为 true； 不同点: &也是位运算符。& 表示在运算时两边都会计算，然后再判断；&&表示先运算符号左边的东西，然后判断是否为 true，是 true 就继续运算右边的然后判断并输出，是 false 就停下来直接输出不会再运行后面的东西。 |和||： 共同点：两者都可做逻辑运算符。它们都表示运算符的两边任意一边为 true，结果为 true，两边都不是 true，结果就为 false； 不同点：|也是位运算符。| 表示两边都会运算，然后再判断结果；|| 表示先运算符号左边的东西，然后判断是否为 true，是 true 就停下来直接输出不会再运行后面的东西，是 false 就继续运算右边的然后判断并输出。 回到本题： 3 | 9=0011（二进制） | 1001（二进制）=1011（二进制）=11（十进制） 2. 说一下转发(Forward)和重定向(Redirect)的区别 转发是服务器行为，重定向是客户端行为。 转发（Forword） 通过 RequestDispatcher 对象的forward（HttpServletRequest request,HttpServletResponse response）方法实现的。RequestDispatcher 可以通过HttpServletRequest 的 getRequestDispatcher()方法获得。例如下面的代码就是跳转到 login_success.jsp 页面。 request.getRequestDispatcher(\"login_success.jsp\").forward(request, response); 重定向（Redirect） 是利用服务器返回的状态码来实现的。客户端浏览器请求服务器的时候，服务器会返回一个状态码。服务器通过 HttpServletRequestResponse 的 setStatus(int status)方法设置状态码。如果服务器返回 301 或者 302，则浏览器会到新的网址重新请求该资源。 从地址栏显示来说：forward 是服务器请求资源，服务器直接访问目标地址的 URL，把那个 URL 的响应内容读取过来，然后把这些内容再发给浏览器。浏览器根本不知道服务器发送的内容从哪里来的，所以它的地址栏还是原来的地址。redirect 是服务端根据逻辑，发送一个状态码，告诉浏览器重新去请求那个地址。所以地址栏显示的是新的 URL。 从数据共享来说：forward：转发页面和转发到的页面可以共享 request 里面的数据。redirect：不能共享数据。 从运用地方来说：forward：一般用于用户登陆的时候，根据角色转发到相应的模块。redirect：一般用于用户注销登陆时返回主页面和跳转到其它的网站等。 从效率来说：forward：高。redirect：低。 3. 在浏览器中输入 url 地址到显示主页的过程,整个过程会使用哪些协议 图片来源：《图解 HTTP》： 总体来说分为以下几个过程: DNS 解析 TCP 连接 发送 HTTP 请求 服务器处理请求并返回 HTTP 报文 浏览器解析渲染页面 连接结束 具体可以参考下面这篇文章： https://segmentfault.com/a/1190000006879700 修正 issue-568：上图中 IP 数据包在路由器之间使用的协议为 OPSF 协议错误，应该为 OSPF 协议 。 IP 数据包在路由器之间传播大致分为 IGP 和 BGP 协议，而 IGP 目前主流为 OSPF 协议，思科，华为和 H3C 等主流厂商都有各自实现并使用；BGP 协议为不同 AS（自治系统号）间路由传输，也分为 I-BGP 和 E-BGP，详细资料请查看《TCP/IP 卷一》 4. TCP 三次握手和四次挥手 为了准确无误地把数据送达目标处，TCP 协议采用了三次握手策略。 漫画图解： 图片来源：《图解 HTTP》 简单示意图： 客户端–发送带有 SYN 标志的数据包–一次握手–服务端 服务端–发送带有 SYN/ACK 标志的数据包–二次握手–客户端 客户端–发送带有带有 ACK 标志的数据包–三次握手–服务端 为什么要三次握手 三次握手的目的是建立可靠的通信信道，说到通讯，简单来说就是数据的发送与接收，而三次握手最主要的目的就是双方确认自己与对方的发送与接收是正常的。 第一次握手：Client 什么都不能确认；Server 确认了对方发送正常，自己接收正常。 第二次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己接收正常，对方发送正常 第三次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己发送、接收正常，对方发送、接收正常 所以三次握手就能确认双发收发功能都正常，缺一不可。 为什么要传回 SYN 接收端传回发送端所发送的 SYN 是为了告诉发送端，我接收到的信息确实就是你所发送的信号了。 SYN 是 TCP/IP 建立连接时使用的握手信号。在客户机和服务器之间建立正常的 TCP 网络连接时，客户机首先发出一个 SYN 消息，服务器使用 SYN-ACK 应答表示接收到了这个消息，最后客户机再以 ACK(Acknowledgement[汉译：确认字符 ,在数据通信传输中，接收站发给发送站的一种传输控制字符。它表示确认发来的数据已经接受无误。 ]）消息响应。这样在客户机和服务器之间才能建立起可靠的 TCP 连接，数据才可以在客户机和服务器之间传递。 传了 SYN,为啥还要传 ACK 双方通信无误必须是两者互相发送信息都无误。传了 SYN，证明发送方（主动关闭方）到接收方（被动关闭方）的通道没有问题，但是接收方到发送方的通道还需要 ACK 信号来进行验证。 断开一个 TCP 连接则需要“四次挥手”： 客户端-发送一个 FIN，用来关闭客户端到服务器的数据传送 服务器-收到这个 FIN，它发回一 个 ACK，确认序号为收到的序号加 1 。和 SYN 一样，一个 FIN 将占用一个序号 服务器-关闭与客户端的连接，发送一个 FIN 给客户端 客户端-发回 ACK 报文确认，并将确认序号设置为收到序号加 1 为什么要四次挥手 任何一方都可以在数据传送结束后发出连接释放的通知，待对方确认后进入半关闭状态。当另一方也没有数据再发送的时候，则发出连接释放通知，对方确认后就完全关闭了 TCP 连接。 举个例子：A 和 B 打电话，通话即将结束后，A 说“我没啥要说的了”，B 回答“我知道了”，但是 B 可能还会有要说的话，A 不能要求 B 跟着自己的节奏结束通话，于是 B 可能又巴拉巴拉说了一通，最后 B 说“我说完了”，A 回答“知道了”，这样通话才算结束。 上面讲的比较概括，推荐一篇讲的比较细致的文章：https://blog.csdn.net/qzcsu/article/details/72861891 5. IP 地址与 MAC 地址的区别 参考：https://blog.csdn.net/guoweimelon/article/details/50858597 IP 地址是指互联网协议地址（Internet Protocol Address）IP Address 的缩写。IP 地址是 IP 协议提供的一种统一的地址格式，它为互联网上的每一个网络和每一台主机分配一个逻辑地址，以此来屏蔽物理地址的差异。 MAC 地址又称为物理地址、硬件地址，用来定义网络设备的位置。网卡的物理地址通常是由网卡生产厂家写入网卡的，具有全球唯一性。MAC 地址用于在网络中唯一标示一个网卡，一台电脑会有一或多个网卡，每个网卡都需要有一个唯一的 MAC 地址。 6. HTTP 请求,响应报文格式 HTTP 请求报文主要由请求行、请求头部、请求正文 3 部分组成 HTTP 响应报文主要由状态行、响应头部、响应正文 3 部分组成 详细内容可以参考：https://blog.csdn.net/a19881029/article/details/14002273 7. 为什么要使用索引?索引这么多优点,为什么不对表中的每一个列创建一个索引呢?索引是如何提高查询速度的?说一下使用索引的注意事项?Mysql 索引主要使用的两种数据结构?什么是覆盖索引? 为什么要使用索引？ 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。 可以大大加快 数据的检索速度（大大减少的检索的数据量）, 这也是创建索引的最主要的原因。 帮助服务器避免排序和临时表 将随机 IO 变为顺序 IO 可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义。 索引这么多优点，为什么不对表中的每一个列创建一个索引呢？ 当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。 索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。 索引是如何提高查询速度的？ 将无序的数据变成相对有序的数据（就像查目录一样） 说一下使用索引的注意事项 避免 where 子句中对字段施加函数，这会造成无法命中索引。 在使用 InnoDB 时使用与业务无关的自增主键作为主键，即使用逻辑主键，而不要使用业务主键。 将打算加索引的列建议设置为 NOT NULL ，因为 NULL 比空字符串需要更多的存储空间（不仅仅是索引列，普通的列如果业务允许都建议设置为 NOT NULL） 删除长期未使用的索引，不用的索引的存在会造成不必要的性能损耗 MySQL 5.7 可以通过查询 sys 库的 schema_unused_indexes 视图来查询哪些索引从未被使用 在使用 limit offset 查询缓慢时，可以借助索引来提高性能 Mysql 索引主要使用的哪两种数据结构？ 哈希索引：对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择 BTree 索引。 BTree 索引：Mysql 的 BTree 索引使用的是 B 树中的 B+Tree。但对于主要的两种存储引擎（MyISAM 和 InnoDB）的实现方式是不同的。 更多关于索引的内容可以查看我的这篇文章：【思维导图-索引篇】搞定数据库索引就是这么简单 什么是覆盖索引? 如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称 之为“覆盖索引”。我们知道在 InnoDB 存储引擎中，如果不是主键索引，叶子节点存储的是主键+列值。最终还是要“回表”，也就是要通过主键再查找一次,这样就会比较慢。覆盖索引就是把要查询出的列和索引是对应的，不做回表操作！ 8. 进程与线程的区别是什么?进程间的几种通信方式说一下?线程间的几种通信方式知道不? 进程与线程的区别是什么？ 线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享同一块内存空间和一组系统资源，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。另外，也正是因为共享资源，所以线程中执行时一般都要进行同步和互斥。总的来说，进程和线程的主要差别在于它们是不同的操作系统资源管理方式。 进程间的几种通信方式说一下？ 管道（pipe）：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有血缘关系的进程间使用。进程的血缘关系通常指父子进程关系。管道分为 pipe（无名管道）和 fifo（命名管道）两种，有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间通信。 信号量（semophore）：信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它通常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。 消息队列（message queue）：消息队列是由消息组成的链表，存放在内核中 并由消息队列标识符标识。消息队列克服了信号传递信息少，管道只能承载无格式字节流以及缓冲区大小受限等缺点。消息队列与管道通信相比，其优势是对每个消息指定特定的消息类型，接收的时候不需要按照队列次序，而是可以根据自定义条件接收特定类型的消息。 信号（signal）：信号是一种比较复杂的通信方式，用于通知接收进程某一事件已经发生。 共享内存（shared memory）：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问，共享内存是最快的 IPC 方式，它是针对其他进程间的通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号量配合使用，来实现进程间的同步和通信。 套接字（socket）：socket，即套接字是一种通信机制，凭借这种机制，客户/服务器（即要进行通信的进程）系统的开发工作既可以在本地单机上进行，也可以跨网络进行。也就是说它可以让不在同一台计算机但通过网络连接计算机上的进程进行通信。也因为这样，套接字明确地将客户端和服务器区分开来。 线程间的几种通信方式知道不？ 1、锁机制 互斥锁：提供了以排它方式阻止数据结构被并发修改的方法。 读写锁：允许多个线程同时读共享数据，而对写操作互斥。 条件变量：可以以原子的方式阻塞进程，直到某个特定条件为真为止。对条件测试是在互斥锁的保护下进行的。条件变量始终与互斥锁一起使用。 2、信号量机制：包括无名线程信号量与有名线程信号量 3、信号机制：类似于进程间的信号处理。 线程间通信的主要目的是用于线程同步，所以线程没有象进程通信中用于数据交换的通信机制。 9. 为什么要用单例模式?手写几种线程安全的单例模式? 简单来说使用单例模式可以带来下面几个好处: 对于频繁使用的对象，可以省略创建对象所花费的时间，这对于那些重量级对象而言，是非常可观的一笔系统开销； 由于 new 操作的次数减少，因而对系统内存的使用频率也会降低，这将减轻 GC 压力，缩短 GC 停顿时间。 懒汉式(双重检查加锁版本) public class Singleton { //volatile保证，当uniqueInstance变量被初始化成Singleton实例时，多个线程可以正确处理uniqueInstance变量 private volatile static Singleton uniqueInstance; private Singleton() { } public static Singleton getInstance() { //检查实例，如果不存在，就进入同步代码块 if (uniqueInstance == null) { //只有第一次才彻底执行这里的代码 synchronized(Singleton.class) { //进入同步代码块后，再检查一次，如果仍是null，才创建实例 if (uniqueInstance == null) { uniqueInstance = new Singleton(); } } } return uniqueInstance; } } 静态内部类方式 静态内部实现的单例是懒加载的且线程安全。 只有通过显式调用 getInstance 方法时，才会显式装载 SingletonHolder 类，从而实例化 instance（只有第一次使用这个单例的实例的时候才加载，同时不会有线程安全问题）。 public class Singleton { private static class SingletonHolder { private static final Singleton INSTANCE = new Singleton(); } private Singleton (){} public static final Singleton getInstance() { return SingletonHolder.INSTANCE; } } 10. 简单介绍一下 bean;知道 Spring 的 bean 的作用域与生命周期吗? 在 Spring 中，那些组成应用程序的主体及由 Spring IOC 容器所管理的对象，被称之为 bean。简单地讲，bean 就是由 IOC 容器初始化、装配及管理的对象，除此之外，bean 就与应用程序中的其他对象没有什么区别了。而 bean 的定义以及 bean 相互间的依赖关系将通过配置元数据来描述。 Spring 中的 bean 默认都是单例的，这些单例 Bean 在多线程程序下如何保证线程安全呢？ 例如对于 Web 应用来说，Web 容器对于每个用户请求都创建一个单独的 Sevlet 线程来处理请求，引入 Spring 框架之后，每个 Action 都是单例的，那么对于 Spring 托管的单例 Service Bean，如何保证其安全呢？ Spring 的单例是基于 BeanFactory 也就是 Spring 容器的，单例 Bean 在此容器内只有一个，Java 的单例是基于 JVM，每个 JVM 内只有一个实例。 Spring 的 bean 的生命周期以及更多内容可以查看：一文轻松搞懂 Spring 中 bean 的作用域与生命周期 11. Spring 中的事务传播行为了解吗?TransactionDefinition 接口中哪五个表示隔离级别的常量? 事务传播行为 事务传播行为（为了解决业务层方法之间互相调用的事务问题）： 当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。在 TransactionDefinition 定义中包括了如下几个表示传播行为的常量： 支持当前事务的情况： TransactionDefinition.PROPAGATION_REQUIRED： 如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。 TransactionDefinition.PROPAGATION_SUPPORTS： 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_MANDATORY： 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性） 不支持当前事务的情况： TransactionDefinition.PROPAGATION_REQUIRES_NEW： 创建一个新的事务，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED： 以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER： 以非事务方式运行，如果当前存在事务，则抛出异常。 其他情况： TransactionDefinition.PROPAGATION_NESTED： 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于 TransactionDefinition.PROPAGATION_REQUIRED。 隔离级别 TransactionDefinition 接口中定义了五个表示隔离级别的常量： TransactionDefinition.ISOLATION_DEFAULT: 使用后端数据库默认的隔离级别，Mysql 默认采用的 REPEATABLE_READ 隔离级别 Oracle 默认采用的 READ_COMMITTED 隔离级别. TransactionDefinition.ISOLATION_READ_UNCOMMITTED: 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 TransactionDefinition.ISOLATION_READ_COMMITTED: 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 TransactionDefinition.ISOLATION_REPEATABLE_READ: 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 TransactionDefinition.ISOLATION_SERIALIZABLE: 最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 12. SpringMVC 原理了解吗? 客户端发送请求-> 前端控制器 DispatcherServlet 接受客户端请求 -> 找到处理器映射 HandlerMapping 解析请求对应的 Handler-> HandlerAdapter 会根据 Handler 来调用真正的处理器处理请求，并处理相应的业务逻辑 -> 处理器返回一个模型视图 ModelAndView -> 视图解析器进行解析 -> 返回一个视图对象->前端控制器 DispatcherServlet 渲染数据（Model）->将得到视图对象返回给用户 关于 SpringMVC 原理更多内容可以查看我的这篇文章：SpringMVC 工作原理详解 13. Spring AOP IOC 实现原理 过了秋招挺长一段时间了，说实话我自己也忘了如何简要概括 Spring AOP IOC 实现原理，就在网上找了一个较为简洁的答案，下面分享给各位。 IOC： 控制反转也叫依赖注入。IOC 利用 java 反射机制，AOP 利用代理模式。IOC 概念看似很抽象，但是很容易理解。说简单点就是将对象交给容器管理，你只需要在 spring 配置文件中配置对应的 bean 以及设置相关的属性，让 spring 容器来生成类的实例对象以及管理对象。在 spring 容器启动的时候，spring 会把你在配置文件中配置的 bean 都初始化好，然后在你需要调用的时候，就把它已经初始化好的那些 bean 分配给你需要调用这些 bean 的类。 AOP： 面向切面编程。（Aspect-Oriented Programming） 。AOP 可以说是对 OOP 的补充和完善。OOP 引入封装、继承和多态性等概念来建立一种对象层次结构，用以模拟公共行为的一个集合。实现 AOP 的技术，主要分为两大类：一是采用动态代理技术，利用截取消息的方式，对该消息进行装饰，以取代原有对象行为的执行；二是采用静态织入的方式，引入特定的语法创建“方面”，从而使得编译器可以在编译期间织入有关“方面”的代码，属于静态代理。 二 进阶篇 1 消息队列 MQ 的套路 消息队列/消息中间件应该是 Java 程序员必备的一个技能了，如果你之前没接触过消息队列的话，建议先去百度一下某某消息队列入门，然后花 2 个小时就差不多可以学会任何一种消息队列的使用了。如果说仅仅学会使用是万万不够的，在实际生产环境还要考虑消息丢失等等情况。关于消息队列面试相关的问题，推荐大家也可以看一下视频《Java 工程师面试突击第 1 季-中华石杉老师》，如果大家没有资源的话，可以在我的公众号“Java 面试通关手册”后台回复关键字“1”即可！ 1.1 介绍一下消息队列 MQ 的应用场景/使用消息队列的好处 面试官一般会先问你这个问题，预热一下，看你知道消息队列不，一般在第一面的时候面试官可能只会问消息队列 MQ 的应用场景/使用消息队列的好处、使用消息队列会带来什么问题、消息队列的技术选型这几个问题，不会太深究下去，在后面的第二轮/第三轮技术面试中可能会深入问一下。 《大型网站技术架构》第四章和第七章均有提到消息队列对应用性能及扩展性的提升。 1)通过异步处理提高系统性能 如上图，在不使用消息队列服务器的时候，用户的请求数据直接写入数据库，在高并发的情况下数据库压力剧增，使得响应速度变慢。但是在使用消息队列之后，用户的请求数据发送给消息队列之后立即 返回，再由消息队列的消费者进程从消息队列中获取数据，异步写入数据库。由于消息队列服务器处理速度快于数据库（消息队列也比数据库有更好的伸缩性），因此响应速度得到大幅改善。 通过以上分析我们可以得出消息队列具有很好的削峰作用的功能——即通过异步处理，将短时间高并发产生的事务消息存储在消息队列中，从而削平高峰期的并发事务。 举例：在电子商务一些秒杀、促销活动中，合理使用消息队列可以有效抵御促销活动刚开始大量订单涌入对系统的冲击。如下图所示： 因为用户请求数据写入消息队列之后就立即返回给用户了，但是请求数据在后续的业务校验、写数据库等操作中可能失败。因此使用消息队列进行异步处理之后，需要适当修改业务流程进行配合，比如用户在提交订单之后，订单数据写入消息队列，不能立即返回用户订单提交成功，需要在消息队列的订单消费者进程真正处理完该订单之后，甚至出库后，再通过电子邮件或短信通知用户订单成功，以免交易纠纷。这就类似我们平时手机订火车票和电影票。 2)降低系统耦合性 我们知道模块分布式部署以后聚合方式通常有两种：1.分布式消息队列和 2.分布式服务。 先来简单说一下分布式服务： 目前使用比较多的用来构建SOA（Service Oriented Architecture 面向服务体系结构）的分布式服务框架是阿里巴巴开源的Dubbo。如果想深入了解 Dubbo 的可以看我写的关于 Dubbo 的这一篇文章：《高性能优秀的服务框架-dubbo 介绍》：https://juejin.im/post/5acadeb1f265da2375072f9c 再来谈我们的分布式消息队列： 我们知道如果模块之间不存在直接调用，那么新增模块或者修改模块就对其他模块影响较小，这样系统的可扩展性无疑更好一些。 我们最常见的事件驱动架构类似生产者消费者模式，在大型网站中通常用利用消息队列实现事件驱动结构。如下图所示： 消息队列使利用发布-订阅模式工作，消息发送者（生产者）发布消息，一个或多个消息接受者（消费者）订阅消息。 从上图可以看到消息发送者（生产者）和消息接受者（消费者）之间没有直接耦合，消息发送者将消息发送至分布式消息队列即结束对消息的处理，消息接受者从分布式消息队列获取该消息后进行后续处理，并不需要知道该消息从何而来。对新增业务，只要对该类消息感兴趣，即可订阅该消息，对原有系统和业务没有任何影响，从而实现网站业务的可扩展性设计。 消息接受者对消息进行过滤、处理、包装后，构造成一个新的消息类型，将消息继续发送出去，等待其他消息接受者订阅该消息。因此基于事件（消息对象）驱动的业务架构可以是一系列流程。 另外为了避免消息队列服务器宕机造成消息丢失，会将成功发送到消息队列的消息存储在消息生产者服务器上，等消息真正被消费者服务器处理后才删除消息。在消息队列服务器宕机后，生产者服务器会选择分布式消息队列服务器集群中的其他服务器发布消息。 备注： 不要认为消息队列只能利用发布-订阅模式工作，只不过在解耦这个特定业务环境下是使用发布-订阅模式的，比如在我们的 ActiveMQ 消息队列中还有点对点工作模式，具体的会在后面的文章给大家详细介绍，这一篇文章主要还是让大家对消息队列有一个更透彻的了解。 这个问题一般会在上一个问题问完之后，紧接着被问到。“使用消息队列会带来什么问题？”这个问题要引起重视，一般我们都会考虑使用消息队列会带来的好处而忽略它带来的问题！ 1.2 那么使用消息队列会带来什么问题?考虑过这些问题吗? 系统可用性降低： 系统可用性在某种程度上降低，为什么这样说呢？在加入 MQ 之前，你不用考虑消息丢失或者说 MQ 挂掉等等的情况，但是，引入 MQ 之后你就需要去考虑了！ 系统复杂性提高： 加入 MQ 之后，你需要保证消息没有被重复消费、处理消息丢失的情况、保证消息传递的顺序性等等问题！ 一致性问题： 我上面讲了消息队列可以实现异步，消息队列带来的异步确实可以提高系统响应速度。但是，万一消息的真正消费者并没有正确消费消息怎么办？这样就会导致数据不一致的情况了! 了解下面这个问题是为了我们更好的进行技术选型！该部分摘自：《Java 工程师面试突击第 1 季-中华石杉老师》，如果大家没有资源的话，可以在我的公众号“Java 面试通关手册”后台回复关键字“1”即可！ 1.3 介绍一下你知道哪几种消息队列,该如何选择呢? 特性 ActiveMQ RabbitMQ RocketMQ Kafka 单机吞吐量 万级，吞吐量比 RocketMQ 和 Kafka 要低了一个数量级 万级，吞吐量比 RocketMQ 和 Kafka 要低了一个数量级 10 万级，RocketMQ 也是可以支撑高吞吐的一种 MQ 10 万级别，这是 kafka 最大的优点，就是吞吐量高。一般配合大数据类的系统来进行实时数据计算、日志采集等场景 topic 数量对吞吐量的影响 topic 可以达到几百，几千个的级别，吞吐量会有较小幅度的下降这是 RocketMQ 的一大优势，在同等机器下，可以支撑大量的 topic topic 从几十个到几百个的时候，吞吐量会大幅度下降。所以在同等机器下，kafka 尽量保证 topic 数量不要过多。如果要支撑大规模 topic，需要增加更多的机器资源 可用性 高，基于主从架构实现高可用性 高，基于主从架构实现高可用性 非常高，分布式架构 非常高，kafka 是分布式的，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 消息可靠性 有较低的概率丢失数据 经过参数优化配置，可以做到 0 丢失 经过参数优化配置，消息可以做到 0 丢失 时效性 ms 级 微秒级，这是 rabbitmq 的一大特点，延迟是最低的 ms 级 延迟在 ms 级以内 功能支持 MQ 领域的功能极其完备 基于 erlang 开发，所以并发能力很强，性能极其好，延时很低 MQ 功能较为完善，还是分布式的，扩展性好 功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用，是事实上的标准 优劣势总结 非常成熟，功能强大，在业内大量的公司以及项目中都有应用。偶尔会有较低概率丢失消息，而且现在社区以及国内应用都越来越少，官方社区现在对 ActiveMQ 5.x 维护越来越少，几个月才发布一个版本而且确实主要是基于解耦和异步来用的，较少在大规模吞吐的场景中使用 erlang 语言开发，性能极其好，延时很低；吞吐量到万级，MQ 功能比较完备而且开源提供的管理界面非常棒，用起来很好用。社区相对比较活跃，几乎每个月都发布几个版本分在国内一些互联网公司近几年用 rabbitmq 也比较多一些但是问题也是显而易见的，RabbitMQ 确实吞吐量会低一些，这是因为他做的实现机制比较重。而且 erlang 开发，国内有几个公司有实力做 erlang 源码级别的研究和定制？如果说你没这个实力的话，确实偶尔会有一些问题，你很难去看懂源码，你公司对这个东西的掌控很弱，基本职能依赖于开源社区的快速维护和修复 bug。而且 rabbitmq 集群动态扩展会很麻烦，不过这个我觉得还好。其实主要是 erlang 语言本身带来的问题。很难读源码，很难定制和掌控。 接口简单易用，而且毕竟在阿里大规模应用过，有阿里品牌保障。日处理消息上百亿之多，可以做到大规模吞吐，性能也非常好，分布式扩展也很方便，社区维护还可以，可靠性和可用性都是 ok 的，还可以支撑大规模的 topic 数量，支持复杂 MQ 业务场景。而且一个很大的优势在于，阿里出品都是 java 系的，我们可以自己阅读源码，定制自己公司的 MQ，可以掌控。社区活跃度相对较为一般，不过也还可以，文档相对来说简单一些，然后接口这块不是按照标准 JMS 规范走的有些系统要迁移需要修改大量代码。还有就是阿里出台的技术，你得做好这个技术万一被抛弃，社区黄掉的风险，那如果你们公司有技术实力我觉得用 RocketMQ 挺好的 kafka 的特点其实很明显，就是仅仅提供较少的核心功能，但是提供超高的吞吐量，ms 级的延迟，极高的可用性以及可靠性，而且分布式可以任意扩展。同时 kafka 最好是支撑较少的 topic 数量即可，保证其超高吞吐量。而且 kafka 唯一的一点劣势是有可能消息重复消费，那么对数据准确性会造成极其轻微的影响，在大数据领域中以及日志采集中，这点轻微影响可以忽略这个特性天然适合大数据实时计算以及日志收集。 这部分内容，我这里不给出答案，大家可以自行根据自己学习的消息队列查阅相关内容，我可能会在后面的文章中介绍到这部分内容。另外，下面这些问题在视频《Java 工程师面试突击第 1 季-中华石杉老师》中都有提到，如果大家没有资源的话，可以在我的公众号“Java 面试通关手册”后台回复关键字“1”即可！ 1.4 关于消息队列其他一些常见的问题展望 引入消息队列之后如何保证高可用性？ 如何保证消息不被重复消费呢？ 如何保证消息的可靠性传输（如何处理消息丢失的问题）？ 我该怎么保证从消息队列里拿到的数据按顺序执行？ 如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？有几百万消息持续积压几小时，说说怎么解决？ 如果让你来开发一个消息队列中间件，你会怎么设计架构？ 2 谈谈 InnoDB 和 MyIsam 两者的区别 2.1 两者的对比 count 运算上的区别： 因为 MyISAM 缓存有表 meta-data（行数等），因此在做 COUNT(*)时对于一个结构很好的查询是不需要消耗多少资源的。而对于 InnoDB 来说，则没有这种缓存 是否支持事务和崩溃后的安全恢复： MyISAM 强调的是性能，每次查询具有原子性，其执行速度比 InnoDB 类型更快，但是不提供事务支持。但是 InnoDB 提供事务支持，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。 是否支持外键： MyISAM 不支持，而 InnoDB 支持。 2.2 关于两者的总结 MyISAM 更适合读密集的表，而 InnoDB 更适合写密集的表。 在数据库做主从分离的情况下，经常选择 MyISAM 作为主库的存储引擎。 一般来说，如果需要事务支持，并且有较高的并发读取频率(MyISAM 的表锁的粒度太大，所以当该表写并发量较高时，要等待的查询就会很多了)，InnoDB 是不错的选择。如果你的数据量很大（MyISAM 支持压缩特性可以减少磁盘的空间占用），而且不需要支持事务时，MyISAM 是最好的选择。 3 聊聊 Java 中的集合吧! 3.1 Arraylist 与 LinkedList 有什么不同?(注意加上从数据结构分析的内容) 1. 是否保证线程安全： ArrayList 和 LinkedList 都是不同步的，也就是不保证线程安全； 2. 底层数据结构： Arraylist 底层使用的是 Object 数组；LinkedList 底层使用的是双向链表数据结构（注意双向链表和双向循环链表的区别：）； 3. 插入和删除是否受元素位置的影响： ① ArrayList 采用数组存储，所以插入和删除元素的时间复杂度受元素位置的影响。 比如：执行add(E e)方法的时候， ArrayList 会默认在将指定的元素追加到此列表的末尾，这种情况时间复杂度就是 O(1)。但是如果要在指定位置 i 插入和删除元素的话（add(int index, E element)）时间复杂度就为 O(n-i)。因为在进行上述操作的时候集合中第 i 和第 i 个元素之后的(n-i)个元素都要执行向后位/向前移一位的操作。 ② LinkedList 采用链表存储，所以插入，删除元素时间复杂度不受元素位置的影响，都是近似 O(1) 而数组为近似 O(n) 。 4. 是否支持快速随机访问： LinkedList 不支持高效的随机元素访问，而 ArrayList 支持。快速随机访问就是通过元素的序号快速获取元素对象（对应于get(int index)方法）。 5. 内存空间占用： ArrayList 的空 间浪费主要体现在在 list 列表的结尾会预留一定的容量空间，而 LinkedList 的空间花费则体现在它的每一个元素都需要消耗比 ArrayList 更多的空间（因为要存放直接后继和直接前驱以及数据）。 补充内容:RandomAccess 接口 public interface RandomAccess { } 查看源码我们发现实际上 RandomAccess 接口中什么都没有定义。所以，在我看来 RandomAccess 接口不过是一个标识罢了。标识什么？ 标识实现这个接口的类具有随机访问功能。 在 binarySearch() 方法中，它要判断传入的 list 是否 RamdomAccess 的实例，如果是，调用 indexedBinarySearch() 方法，如果不是，那么调用 iteratorBinarySearch() 方法 public static int binarySearch(List> list, T key) { if (list instanceof RandomAccess || list.size() ArraysList 实现了 RandomAccess 接口， 而 LinkedList 没有实现。为什么呢？我觉得还是和底层数据结构有关！ArraysList 底层是数组，而 LinkedList 底层是链表。数组天然支持随机访问，时间复杂度为 O(1) ，所以称为快速随机访问。链表需要遍历到特定位置才能访问特定位置的元素，时间复杂度为 O(n) ，所以不支持快速随机访问。，ArraysList 实现了 RandomAccess 接口，就表明了他具有快速随机访问功能。 RandomAccess 接口只是标识，并不是说 ArraysList 实现 RandomAccess 接口才具有快速随机访问功能的！ 下面再总结一下 list 的遍历方式选择： 实现了 RandomAccess 接口的 list，优先选择普通 for 循环 ，其次 foreach, 未实现 RandomAccess 接口的 ist， 优先选择 iterator 遍历（foreach 遍历底层也是通过 iterator 实现的），大 size 的数据，千万不要使用普通 for 循环 Java 中的集合这类问题几乎是面试必问的，问到这类问题的时候，HashMap 又是几乎必问的问题，所以大家一定要引起重视！ 3.2 HashMap 的底层实现 1)JDK1.8 之前 JDK1.8 之前 HashMap 底层是 数组和链表 结合在一起使用也就是 链表散列。HashMap 通过 key 的 hashCode 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) & hash 判断当前元素存放的位置（这里的 n 指的时数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。 所谓扰动函数指的就是 HashMap 的 hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法 换句话说使用扰动函数之后可以减少碰撞。 JDK 1.8 HashMap 的 hash 方法源码: JDK 1.8 的 hash 方法 相比于 JDK 1.7 hash 方法更加简化，但是原理不变。 static final int hash(Object key) { int h; // key.hashCode()：返回散列值也就是hashcode // ^ ：按位异或 // >>>:无符号右移，忽略符号位，空位都以0补齐 return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16); } 对比一下 JDK1.7 的 HashMap 的 hash 方法源码. static int hash(int h) { // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h >>> 20) ^ (h >>> 12); return h ^ (h >>> 7) ^ (h >>> 4); } 相比于 JDK1.8 的 hash 方法 ，JDK 1.7 的 hash 方法的性能会稍差一点点，因为毕竟扰动了 4 次。 所谓 “拉链法” 就是：将链表和数组相结合。也就是说创建一个链表数组，数组中每一格就是一个链表。若遇到哈希冲突，则将冲突的值加到链表中即可。 2)JDK1.8 之后 相比于之前的版本， JDK1.8 之后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）时，将链表转化为红黑树，以减少搜索时间。 TreeMap、TreeSet 以及 JDK1.8 之后的 HashMap 底层都用到了红黑树。红黑树就是为了解决二叉查找树的缺陷，因为二叉查找树在某些情况下会退化成一个线性结构。 问完 HashMap 的底层原理之后，面试官可能就会紧接着问你 HashMap 底层数据结构相关的问题！ 3.3 既然谈到了红黑树,你给我手绘一个出来吧,然后简单讲一下自己对于红黑树的理解 红黑树特点: 每个节点非红即黑； 根节点总是黑色的； 每个叶子节点都是黑色的空节点（NIL 节点）； 如果节点是红色的，则它的子节点必须是黑色的（反之不一定）； 从根节点到叶节点或空子节点的每条路径，必须包含相同数目的黑色节点（即相同的黑色高度） 红黑树的应用： TreeMap、TreeSet 以及 JDK1.8 之后的 HashMap 底层都用到了红黑树。 为什么要用红黑树 简单来说红黑树就是为了解决二叉查找树的缺陷，因为二叉查找树在某些情况下会退化成一个线性结构。 3.4 红黑树这么优秀,为何不直接使用红黑树得了? 说一下自己对于这个问题的看法：我们知道红黑树属于（自）平衡二叉树，但是为了保持“平衡”是需要付出代价的，红黑树在插入新数据后可能需要通过左旋，右旋、变色这些操作来保持平衡，这费事啊。你说说我们引入红黑树就是为了查找数据快，如果链表长度很短的话，根本不需要引入红黑树的，你引入之后还要付出代价维持它的平衡。但是链表过长就不一样了。至于为什么选 8 这个值呢？通过概率统计所得，这个值是综合查询成本和新增元素成本得出的最好的一个值。 3.5 HashMap 和 Hashtable 的区别/HashSet 和 HashMap 区别 HashMap 和 Hashtable 的区别 线程是否安全： HashMap 是非线程安全的，Hashtable 是线程安全的；Hashtable 内部的方法基本都经过 synchronized 修饰。（如果你要保证线程安全的话就使用 ConcurrentHashMap 吧！）； 效率： 因为线程安全的问题，HashMap 要比 Hashtable 效率高一点。另外，Hashtable 基本被淘汰，不要在代码中使用它； 对 Null key 和 Null value 的支持： HashMap 中，null 可以作为键，这样的键只有一个，可以有一个或多个键所对应的值为 null。但是在 Hashtable 中 put 进的键值只要有一个 null，直接抛出 NullPointerException。 初始容量大小和每次扩充容量大小的不同 ： ① 创建时如果不指定容量初始值，Hashtable 默认的初始大小为 11，之后每次扩充，容量变为原来的 2n+1。HashMap 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。② 创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为 2 的幂次方大小（HashMap 中的tableSizeFor()方法保证，下面给出了源代码）。也就是说 HashMap 总是使用 2 的幂作为哈希表的大小,后面会介绍到为什么是 2 的幂次方。 底层数据结构： JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）时，将链表转化为红黑树，以减少搜索时间。Hashtable 没有这样的机制。 HashSet 和 HashMap 区别 如果你看过 HashSet 源码的话就应该知道：HashSet 底层就是基于 HashMap 实现的。（HashSet 的源码非常非常少，因为除了 clone() 方法、writeObject()方法、readObject()方法是 HashSet 自己不得不实现之外，其他方法都是直接调用 HashMap 中的方法。） 三 终结篇 1. Object 类有哪些方法? 这个问题，面试中经常出现。我觉得不论是出于应付面试还是说更好地掌握 Java 这门编程语言，大家都要掌握！ 1.1 Object 类的常见方法总结 Object 类是一个特殊的类，是所有类的父类。它主要提供了以下 11 个方法： public final native Class getClass()//native方法，用于返回当前运行时对象的Class对象，使用了final关键字修饰，故不允许子类重写。 public native int hashCode() //native方法，用于返回对象的哈希码，主要使用在哈希表中，比如JDK中的HashMap。 public boolean equals(Object obj)//用于比较2个对象的内存地址是否相等，String类对该方法进行了重写用户比较字符串的值是否相等。 protected native Object clone() throws CloneNotSupportedException//naitive方法，用于创建并返回当前对象的一份拷贝。一般情况下，对于任何对象 x，表达式 x.clone() != x 为true，x.clone().getClass() == x.getClass() 为true。Object本身没有实现Cloneable接口，所以不重写clone方法并且进行调用的话会发生CloneNotSupportedException异常。 public String toString()//返回类的名字@实例的哈希码的16进制的字符串。建议Object所有的子类都重写这个方法。 public final native void notify()//native方法，并且不能重写。唤醒一个在此对象监视器上等待的线程(监视器相当于就是锁的概念)。如果有多个线程在等待只会任意唤醒一个。 public final native void notifyAll()//native方法，并且不能重写。跟notify一样，唯一的区别就是会唤醒在此对象监视器上等待的所有线程，而不是一个线程。 public final native void wait(long timeout) throws InterruptedException//native方法，并且不能重写。暂停线程的执行。注意：sleep方法没有释放锁，而wait方法释放了锁 。timeout是等待时间。 public final void wait(long timeout, int nanos) throws InterruptedException//多了nanos参数，这个参数表示额外时间（以毫微秒为单位，范围是 0-999999）。 所以超时的时间还需要加上nanos毫秒。 public final void wait() throws InterruptedException//跟之前的2个wait方法一样，只不过该方法一直等待，没有超时时间这个概念 protected void finalize() throws Throwable { }//实例被垃圾回收器回收的时候触发的操作 问完上面这个问题之后，面试官很可能紧接着就会问你“hashCode 与 equals”相关的问题。 1.2 hashCode 与 equals 面试官可能会问你：“你重写过 hashcode 和 equals 么，为什么重写 equals 时必须重写 hashCode 方法？” 1.2.1 hashCode()介绍 hashCode() 的作用是获取哈希码，也称为散列码；它实际上是返回一个 int 整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode() 定义在 JDK 的 Object.java 中，这就意味着 Java 中的任何类都包含有 hashCode() 函数。另外需要注意的是： Object 的 hashcode 方法是本地方法，也就是用 c 语言或 c++ 实现的，该方法通常用来将对象的 内存地址 转换为整数之后返回。 public native int hashCode(); 散列表存储的是键值对(key-value)，它的特点是：能根据“键”快速的检索出对应的“值”。这其中就利用到了散列码！（可以快速找到所需要的对象） 1.2.2 为什么要有 hashCode 我们以“HashSet 如何检查重复”为例子来说明为什么要有 hashCode： 当你把对象加入 HashSet 时，HashSet 会先计算对象的 hashcode 值来判断对象加入的位置，同时也会与其他已经加入的对象的 hashcode 值作比较，如果没有相符的 hashcode，HashSet 会假设对象没有重复出现。但是如果发现有相同 hashcode 值的对象，这时会调用 equals（）方法来检查 hashcode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让其加入操作成功。如果不同的话，就会重新散列到其他位置。（摘自我的 Java 启蒙书《Head fist java》第二版）。这样我们就大大减少了 equals 的次数，相应就大大提高了执行速度。 1.2.3 hashCode()与 equals()的相关规定 如果两个对象相等，则 hashcode 一定也是相同的 两个对象相等，对两个对象分别调用 equals 方法都返回 true 两个对象有相同的 hashcode 值，它们也不一定是相等的 因此，equals 方法被覆盖过，则 hashCode 方法也必须被覆盖 hashCode()的默认行为是对堆上的对象产生独特值。如果没有重写 hashCode()，则该 class 的两个对象无论如何都不会相等（即使这两个对象指向相同的数据） 1.2.4 为什么两个对象有相同的 hashcode 值,它们也不一定是相等的? 在这里解释一位小伙伴的问题。以下内容摘自《Head Fisrt Java》。 因为 hashCode() 所使用的杂凑算法也许刚好会让多个对象传回相同的杂凑值。越糟糕的杂凑算法越容易碰撞，但这也与数据值域分布的特性有关（所谓碰撞也就是指的是不同的对象得到相同的 hashCode）。 我们刚刚也提到了 HashSet,如果 HashSet 在对比的时候，同样的 hashcode 有多个对象，它会使用 equals() 来判断是否真的相同。也就是说 hashcode 只是用来缩小查找成本。 ==与 equals 的对比也是比较常问的基础问题之一！ 1.3 ==与 equals == : 它的作用是判断两个对象的地址是不是相等。即，判断两个对象是不是同一个对象。(基本数据类型==比较的是值，引用数据类型==比较的是内存地址) equals() : 它的作用也是判断两个对象是否相等。但它一般有两种使用情况： 情况 1：类没有覆盖 equals()方法。则通过 equals()比较该类的两个对象时，等价于通过“==”比较这两个对象。 情况 2：类覆盖了 equals()方法。一般，我们都覆盖 equals()方法来两个对象的内容相等；若它们的内容相等，则返回 true(即，认为这两个对象相等)。 举个例子： public class test1 { public static void main(String[] args) { String a = new String(\"ab\"); // a 为一个引用 String b = new String(\"ab\"); // b为另一个引用,对象的内容一样 String aa = \"ab\"; // 放在常量池中 String bb = \"ab\"; // 从常量池中查找 if (aa == bb) // true System.out.println(\"aa==bb\"); if (a == b) // false，非同一对象 System.out.println(\"a==b\"); if (a.equals(b)) // true System.out.println(\"aEQb\"); if (42 == 42.0) { // true System.out.println(\"true\"); } } } 说明： String 中的 equals()方法是被重写过的，因为 Object 的 equals()方法是比较的对象的内存地址，而 String 的 equals()方法比较的是对象的值。 当创建 String 类型的对象时，虚拟机会在常量池中查找有没有已经存在的值和要创建的值相同的对象，如果有就把它赋给当前引用。如果没有就在常量池中重新创建一个 String 对象。 在【备战春招/秋招系列 5】美团面经总结进阶篇 （附详解答案） 这篇文章中，我们已经提到了一下关于 HashMap 在面试中常见的问题：HashMap 的底层实现、简单讲一下自己对于红黑树的理解、红黑树这么优秀，为何不直接使用红黑树得了、HashMap 和 Hashtable 的区别/HashSet 和 HashMap 区别。HashMap 和 ConcurrentHashMap 这俩兄弟在一般只要面试中问到集合相关的问题就一定会被问到，所以各位务必引起重视！ 2 ConcurrentHashMap 相关问题 2.1 ConcurrentHashMap 和 Hashtable 的区别 ConcurrentHashMap 和 Hashtable 的区别主要体现在实现线程安全的方式上不同。 底层数据结构： JDK1.7 的 ConcurrentHashMap 底层采用 分段的数组+链表 实现，JDK1.8 采用的数据结构跟 HashMap1.8 的结构一样，数组+链表/红黑二叉树。Hashtable 和 JDK1.8 之前的 HashMap 的底层数据结构类似都是采用 数组+链表 的形式，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的； 实现线程安全的方式（重要）： ① 在 JDK1.7 的时候，ConcurrentHashMap（分段锁） 对整个桶数组进行了分割分段(Segment)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。（默认分配 16 个 Segment，比 Hashtable 效率提高 16 倍。） 到了 JDK1.8 的时候已经摒弃了 Segment 的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并发控制使用 synchronized 和 CAS 来操作。（JDK1.6 以后 对 synchronized 锁做了很多优化） 整个看起来就像是优化过且线程安全的 HashMap，虽然在 JDK1.8 中还能看到 Segment 的数据结构，但是已经简化了属性，只是为了兼容旧版本；② Hashtable(同一把锁)：使用 synchronized 来保证线程安全，效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用 put 添加元素，另一个线程不能使用 put 添加元素，也不能使用 get，竞争会越来越激烈效率越低。 两者的对比图： 图片来源：http://www.cnblogs.com/chengxiao/p/6842045.html Hashtable： JDK1.7 的 ConcurrentHashMap： JDK1.8 的 ConcurrentHashMap（TreeBin: 红黑二叉树节点 Node: 链表节点）： 2.2 ConcurrentHashMap 线程安全的具体实现方式/底层具体实现 JDK1.7(上面有示意图) 首先将数据分为一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据时，其他段的数据也能被其他线程访问。 ConcurrentHashMap 是由 Segment 数组结构和 HashEntry 数组结构组成。 Segment 实现了 ReentrantLock，所以 Segment 是一种可重入锁，扮演锁的角色。HashEntry 用于存储键值对数据。 static class Segment extends ReentrantLock implements Serializable { } 一个 ConcurrentHashMap 里包含一个 Segment 数组。Segment 的结构和 HashMap 类似，是一种数组和链表结构，一个 Segment 包含一个 HashEntry 数组，每个 HashEntry 是一个链表结构的元素，每个 Segment 守护着一个 HashEntry 数组里的元素，当对 HashEntry 数组的数据进行修改时，必须首先获得对应的 Segment 的锁。 JDK1.8(上面有示意图) ConcurrentHashMap 取消了 Segment 分段锁，采用 CAS 和 synchronized 来保证并发安全。数据结构跟 HashMap1.8 的结构类似，数组+链表/红黑二叉树。 synchronized 只锁定当前链表或红黑二叉树的首节点，这样只要 hash 不冲突，就不会产生并发，效率又提升 N 倍。 3 谈谈 synchronized 和 ReentrantLock 的区别 ① 两者都是可重入锁 两者都是可重入锁。“可重入锁”概念是：自己可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果不可锁重入的话，就会造成死锁。同一个线程每次获取锁，锁的计数器都自增 1，所以要等到锁的计数器下降为 0 时才能释放锁。 ② synchronized 依赖于 JVM 而 ReentrantLock 依赖于 API synchronized 是依赖于 JVM 实现的，前面我们也讲到了 虚拟机团队在 JDK1.6 为 synchronized 关键字进行了很多优化，但是这些优化都是在虚拟机层面实现的，并没有直接暴露给我们。ReentrantLock 是 JDK 层面实现的（也就是 API 层面，需要 lock() 和 unlock() 方法配合 try/finally 语句块来完成），所以我们可以通过查看它的源代码，来看它是如何实现的。 ③ ReentrantLock 比 synchronized 增加了一些高级功能 相比 synchronized，ReentrantLock 增加了一些高级功能。主要来说主要有三点：① 等待可中断；② 可实现公平锁；③ 可实现选择性通知（锁可以绑定多个条件） ReentrantLock 提供了一种能够中断等待锁的线程的机制，通过 lock.lockInterruptibly() 来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。 ReentrantLock 可以指定是公平锁还是非公平锁。而 synchronized 只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。 ReentrantLock 默认情况是非公平的，可以通过 ReentrantLock 类的ReentrantLock(boolean fair)构造方法来制定是否是公平的。 synchronized 关键字与 wait()和 notify/notifyAll()方法相结合可以实现等待/通知机制，ReentrantLock 类当然也可以实现，但是需要借助于 Condition 接口与 newCondition() 方法。Condition 是 JDK1.5 之后才有的，它具有很好的灵活性，比如可以实现多路通知功能也就是在一个 Lock 对象中可以创建多个 Condition 实例（即对象监视器），线程对象可以注册在指定的 Condition 中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 在使用 notify/notifyAll()方法进行通知时，被通知的线程是由 JVM 选择的，用 ReentrantLock 类结合 Condition 实例可以实现“选择性通知” ，这个功能非常重要，而且是 Condition 接口默认提供的。而 synchronized 关键字就相当于整个 Lock 对象中只有一个 Condition 实例，所有的线程都注册在它一个身上。如果执行 notifyAll()方法的话就会通知所有处于等待状态的线程这样会造成很大的效率问题，而 Condition 实例的 signalAll()方法 只会唤醒注册在该 Condition 实例中的所有等待线程。 如果你想使用上述功能，那么选择 ReentrantLock 是一个不错的选择。 ④ 两者的性能已经相差无几 在 JDK1.6 之前，synchronized 的性能是比 ReentrantLock 差很多。具体表示为：synchronized 关键字吞吐量随线程数的增加，下降得非常严重。而 ReentrantLock 基本保持一个比较稳定的水平。我觉得这也侧面反映了， synchronized 关键字还有非常大的优化余地。后续的技术发展也证明了这一点，我们上面也讲了在 JDK1.6 之后 JVM 团队对 synchronized 关键字做了很多优化。JDK1.6 之后，synchronized 和 ReentrantLock 的性能基本是持平了。所以网上那些说因为性能才选择 ReentrantLock 的文章都是错的！JDK1.6 之后，性能已经不是选择 synchronized 和 ReentrantLock 的影响因素了！而且虚拟机在未来的性能改进中会更偏向于原生的 synchronized，所以还是提倡在 synchronized 能满足你的需求的情况下，优先考虑使用 synchronized 关键字来进行同步！优化后的 synchronized 和 ReentrantLock 一样，在很多地方都是用到了 CAS 操作。 4 线程池了解吗? 4.1 为什么要用线程池? 线程池提供了一种限制和管理资源（包括执行一个任务）。 每个线程池还维护一些基本统计信息，例如已完成任务的数量。 这里借用《Java 并发编程的艺术》提到的来说一下使用线程池的好处： 降低资源消耗。 通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。 当任务到达时，任务可以不需要的等到线程创建就能立即执行。 提高线程的可管理性。 线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 4.2 Java 提供了哪几种线程池?他们各自的使用场景是什么? Java 主要提供了下面 4 种线程池 FixedThreadPool： 该方法返回一个固定线程数量的线程池。该线程池中的线程数量始终不变。当有一个新的任务提交时，线程池中若有空闲线程，则立即执行。若没有，则新的任务会被暂存在一个任务队列中，待有线程空闲时，便处理在任务队列中的任务。 SingleThreadExecutor： 方法返回一个只有一个线程的线程池。若多余一个任务被提交到该线程池，任务会被保存在一个任务队列中，待线程空闲，按先入先出的顺序执行队列中的任务。 CachedThreadPool： 该方法返回一个可根据实际情况调整线程数量的线程池。线程池的线程数量不确定，但若有空闲线程可以复用，则会优先使用可复用的线程。若所有线程均在工作，又有新的任务提交，则会创建新的线程处理任务。所有线程在当前任务执行完毕后，将返回线程池进行复用。 ScheduledThreadPoolExecutor： 主要用来在给定的延迟后运行任务，或者定期执行任务。ScheduledThreadPoolExecutor 又分为：ScheduledThreadPoolExecutor（包含多个线程）和 SingleThreadScheduledExecutor （只包含一个线程）两种。 各种线程池的适用场景介绍 FixedThreadPool： 适用于为了满足资源管理需求，而需要限制当前线程数量的应用场景。它适用于负载比较重的服务器； SingleThreadExecutor： 适用于需要保证顺序地执行各个任务并且在任意时间点，不会有多个线程是活动的应用场景； CachedThreadPool： 适用于执行很多的短期异步任务的小程序，或者是负载较轻的服务器； ScheduledThreadPoolExecutor： 适用于需要多个后台执行周期任务，同时为了满足资源管理需求而需要限制后台线程的数量的应用场景； SingleThreadScheduledExecutor： 适用于需要单个后台线程执行周期任务，同时保证顺序地执行各个任务的应用场景。 4.3 创建的线程池的方式 （1） 使用 Executors 创建 我们上面刚刚提到了 Java 提供的几种线程池，通过 Executors 工具类我们可以很轻松的创建我们上面说的几种线程池。但是实际上我们一般都不是直接使用 Java 提供好的线程池，另外在《阿里巴巴 Java 开发手册》中强制线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 构造函数 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。 Executors 返回线程池对象的弊端如下： FixedThreadPool 和 SingleThreadExecutor ： 允许请求的队列长度为 Integer.MAX_VALUE,可能堆积大量的请求，从而导致OOM。 CachedThreadPool 和 ScheduledThreadPool ： 允许创建的线程数量为 Integer.MAX_VALUE ，可能会创建大量线程，从而导致OOM。 （2） ThreadPoolExecutor 的构造函数创建 我们可以自己直接调用 ThreadPoolExecutor 的构造函数来自己创建线程池。在创建的同时，给 BlockQueue 指定容量就可以了。示例如下： private static ExecutorService executor = new ThreadPoolExecutor(13, 13, 60L, TimeUnit.SECONDS, new ArrayBlockingQueue(13)); 这种情况下，一旦提交的线程数超过当前可用线程数时，就会抛出 java.util.concurrent.RejectedExecutionException，这是因为当前线程池使用的队列是有边界队列，队列已经满了便无法继续处理新的请求。但是异常（Exception）总比发生错误（Error）要好。 （3） 使用开源类库 Hollis 大佬之前在他的文章中也提到了：“除了自己定义 ThreadPoolExecutor 外。还有其他方法。这个时候第一时间就应该想到开源类库，如 apache 和 guava 等。”他推荐使用 guava 提供的 ThreadFactoryBuilder 来创建线程池。下面是参考他的代码示例： public class ExecutorsDemo { private static ThreadFactory namedThreadFactory = new ThreadFactoryBuilder() .setNameFormat(\"demo-pool-%d\").build(); private static ExecutorService pool = new ThreadPoolExecutor(5, 200, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy()); public static void main(String[] args) { for (int i = 0; i 通过上述方式创建线程时，不仅可以避免 OOM 的问题，还可以自定义线程名称，更加方便的出错的时候溯源。 5 Nginx 5.1 简单介绍一下 Nginx Nginx 是一款轻量级的 Web 服务器/反向代理服务器及电子邮件（IMAP/POP3）代理服务器。 Nginx 主要提供反向代理、负载均衡、动静分离(静态资源服务)等服务。下面我简单地介绍一下这些名词。 反向代理 谈到反向代理，就不得不提一下正向代理。无论是正向代理，还是反向代理，说到底，就是代理模式的衍生版本罢了 正向代理：某些情况下，代理我们用户去访问服务器，需要用户手动的设置代理服务器的 ip 和端口号。正向代理比较常见的一个例子就是 VPN 了。 反向代理： 是用来代理服务器的，代理我们要访问的目标服务器。代理服务器接受请求，然后将请求转发给内部网络的服务器，并将从服务器上得到的结果返回给客户端，此时代理服务器对外就表现为一个服务器。 通过下面两幅图，大家应该更好理解（图源：http://blog.720ui.com/2016/nginx_action_05_proxy/）： 所以，简单的理解，就是正向代理是为客户端做代理，代替客户端去访问服务器，而反向代理是为服务器做代理，代替服务器接受客户端请求。 负载均衡 在高并发情况下需要使用，其原理就是将并发请求分摊到多个服务器执行，减轻每台服务器的压力，多台服务器(集群)共同完成工作任务，从而提高了数据的吞吐量。 Nginx 支持的 weight 轮询（默认）、ip_hash、fair、url_hash 这四种负载均衡调度算法，感兴趣的可以自行查阅。 负载均衡相比于反向代理更侧重的是将请求分担到多台服务器上去，所以谈论负载均衡只有在提供某服务的服务器大于两台时才有意义。 动静分离 动静分离是让动态网站里的动态网页根据一定规则把不变的资源和经常变的资源区分开来，动静资源做好了拆分以后，我们就可以根据静态资源的特点将其做缓存操作，这就是网站静态化处理的核心思路。 5.2 为什么要用 Nginx? 这部分内容参考极客时间—Nginx 核心知识 100 讲的内容。 如果面试官问你这个问题，就一定想看你知道 Nginx 服务器的一些优点吗。 Nginx 有以下 5 个优点： 高并发、高性能（这是其他 web 服务器不具有的） 可扩展性好（模块化设计，第三方插件生态圈丰富） 高可靠性（可以在服务器行持续不间断的运行数年） 热部署（这个功能对于 Nginx 来说特别重要，热部署指可以在不停止 Nginx 服务的情况下升级 Nginx） BSD 许可证（意味着我们可以将源代码下载下来进行修改然后使用自己的版本） 5.3 Nginx 的四个主要组成部分了解吗? 这部分内容参考极客时间—Nginx 核心知识 100 讲的内容。 Nginx 二进制可执行文件：由各模块源码编译出一个文件 nginx.conf 配置文件：控制 Nginx 行为 acess.log 访问日志： 记录每一条 HTTP 请求信息 error.log 错误日志：定位问题 "},"zother6-JavaGuide/essential-content-for-interview/PreparingForInterview/面试官-你有什么问题要问我.html":{"url":"zother6-JavaGuide/essential-content-for-interview/PreparingForInterview/面试官-你有什么问题要问我.html","title":"面试官-你有什么问题要问我","keywords":"","body":"我还记得当时我去参加面试的时候，几乎每一场面试，特别是HR面和高管面的时候，面试官总是会在结尾问我:“问了你这么多问题了，你有什么问题问我吗？”。这个时候很多人内心就会陷入短暂的纠结中：我该问吗？不问的话面试官会不会对我影响不好？问什么问题？问这个问题会不会让面试官对我的影响不好啊？ 这个问题对最终面试结果的影响到底大不大? 就技术面试而言，回答这个问题的时候，只要你不是触碰到你所面试的公司的雷区，那么我觉得这对你能不能拿到最终offer来说影响确实是不大的。我说这些并不代表你就可以直接对面试官说：“我没问题了。”，笔主当时面试的时候确实也说过挺多次“没问题要问了。”，最终也没有导致笔主被pass掉（可能是前面表现比较好，哈哈，自恋一下）。我现在回想起来，觉得自己当时做法其实挺不对的。面试本身就是一个双向选择的过程，你对这个问题的回答也会侧面反映出你对这次面试的上心程度，你的问题是否有价值，也影响了你最终的选择与公司是否选择你。 面试官在技术面试中主要考察的还是你这样个人到底有没有胜任这个工作的能力以及你是否适合公司未来的发展需要，很多公司还需要你认同它的文化，我觉得你只要不是太笨，应该不会栽在这里。除非你和另外一个人在能力上相同，但是只能在你们两个人中选一个，那么这个问题才对你能不能拿到offer至关重要。有准备总比没准备好，给面试官留一个好的影响总归是没错的。 但是，就非技术面试来说，我觉得好好回答这个问题对你最终的结果还是比较重要的。 总的来说不管是技术面试还是非技术面试，如果你想赢得公司的青睐和尊重，我觉得我们都应该重视这个问题。 真诚一点,不要问太 Low 的问题 回答这个问题很重要的一点就是你没有必要放低自己的姿态问一些很虚或者故意讨好面试官的问题，也不要把自己从面经上学到的东西照搬下来使用。面试官也不是傻子，特别是那种特别有经验的面试官，你是真心诚意的问问题，还是从别处照搬问题来讨好面试官，人家可能一听就听出来了。总的来说，还是要真诚。除此之外，不要问太 Low 的问题，会显得你整个人格局比较小或者说你根本没有准备（侧面反映你对这家公司不上心，既然你不上心，为什么要要你呢）。举例几个比较 Low 的问题，大家看看自己有没有问过其中的问题： 贵公司的主要业务是什么？（面试之前自己不知道提前网上查一下吗？） 贵公司的男女比例如何？（考虑脱单？记住你是来工作的！） 贵公司一年搞几次外出旅游？（你是来工作的，这些娱乐活动先别放在心上！） ...... 有哪些有价值的问题值得问? 针对这个问题。笔主专门找了几个专门做HR工作的小哥哥小姐姐们询问并且查阅了挺多前辈们的回答，然后结合自己的实际经历，我概括了下面几个比较适合问的问题。 面对HR或者其他Level比较低的面试官时 能不能谈谈你作为一个公司老员工对公司的感受? (这个问题比较容易回答，不会让面试官陷入无话可说的尴尬境地。另外，从面试官的回答中你可以加深对这个公司的了解，让你更加清楚这个公司到底是不是你想的那样或者说你是否能适应这个公司的文化。除此之外，这样的问题在某种程度上还可以拉进你与面试官的距离。) 能不能问一下，你当时因为什么原因选择加入这家公司的呢或者说这家公司有哪些地方吸引你？有什么地方你觉得还不太好或者可以继续完善吗？ （类似第一个问题，都是问面试官个人对于公司的看法。） 我觉得我这次表现的不是太好，你有什么建议或者评价给我吗？(这个是我常问的。我觉得说自己表现不好只是这个语境需要这样来说，这样可以显的你比较谦虚好学上进。) 接下来我会有一段空档期，有什么值得注意或者建议学习的吗？ （体现出你对工作比较上心，自助学习意识比较强。） 这个岗位为什么还在招人？ (岗位真实性和价值咨询) 大概什么时候能给我回复呢？ (终面的时候，如果面试官没有说的话，可以问一下) ...... 面对部门领导 部门的主要人员分配以及对应的主要工作能简单介绍一下吗？ 未来如果我要加入这个团队，你对我的期望是什么？ （部门领导一般情况下是你的直属上级了，你以后和他打交道的机会应该是最多的。你问这个问题，会让他感觉你是一个对他的部门比较上心，比较有团体意识，并且愿意倾听的候选人。） 公司对新入职的员工的培养机制是什么样的呢？ （正规的公司一般都有培养机制，提前问一下是对你自己的负责也会显的你比较上心） 以您来看，这个岗位未来在公司内部的发展如何？ (在我看来，问这个问题也是对你自己的负责吧，谁不想发展前景更好的岗位呢？) 团队现在面临的最大挑战是什么？ (这样的问题不会暴露你对公司的不了解，并且也能让你对未来工作的挑战或困难有一个提前的预期。) 面对Level比较高的(比如总裁,老板) 贵公司的发展目标和方向是什么？ （看下公司的发展是否满足自己的期望） 与同行业的竞争者相比，贵公司的核心竞争优势在什么地方？ （充分了解自己的优势和劣势） 公司现在面临的最大挑战是什么？ 来个补充,顺便送个祝福给大家 薪酬待遇和相关福利问题一般在终面的时候（最好不要在前面几面的时候就问到这个问题），面试官会提出来或者在面试完之后以邮件的形式告知你。一般来说，如果面试官很愿意为你回答问题，对你的问题也比较上心的话，那他肯定是觉得你就是他们要招的人。 大家在面试的时候，可以根据自己对于公司或者岗位的了解程度，对上面提到的问题进行适当修饰或者修改。上面提到的一些问题只是给没有经验的朋友一个参考，如果你还有其他比较好的问题的话，那当然也更好啦！ 金三银四。过了二月就到了面试高峰期或者说是黄金期。几份惊喜几份愁，愿各位能始终不忘初心！每个人都有每个人的难处。引用一句《阿甘正传》里面的台词：“生活就像一盒巧克力，你永远不知道下一块是什么味道“。 "},"zother6-JavaGuide/essential-content-for-interview/real-interview-experience-analysis/alibaba-1.html":{"url":"zother6-JavaGuide/essential-content-for-interview/real-interview-experience-analysis/alibaba-1.html","title":"Alibaba 1","keywords":"","body":"本文的内容都是根据读者投稿的真实面试经历改编而来，首次尝试这种风格的文章，花了几天晚上才总算写完，希望对你有帮助。 本文主要涵盖下面的内容： 分布式商城系统：架构图讲解； 消息队列相关：削峰和解耦； Redis 相关：缓存穿透问题的解决； 一些基础问题： 网络相关：1.浏览器输入 URL 发生了什么? 2.TCP 和 UDP 区别? 3.TCP 如何保证传输可靠性? Java 基础：1. 既然有了字节流,为什么还要有字符流? 2.深拷贝 和 浅拷贝有啥区别呢？ 下面是正文！ 面试开始，坐在我前面的就是这次我的面试官吗？这发量看着根本不像程序员啊？我心里正嘀咕着，只听见面试官说：“小伙，下午好，我今天就是你的面试官，咱们开始面试吧！”。 第一面开始 面试官： 我也不用多说了，你先自我介绍一下吧，简历上有的就不要再说了哈。 我： 内心 os:\"果然如我所料，就知道会让我先自我介绍一下，还好我看了 JavaGuide ，学到了一些套路。套路总结起来就是：最好准备好两份自我介绍，一份对 hr 说的，主要讲能突出自己的经历，会的编程技术一语带过；另一份对技术面试官说的，主要讲自己会的技术细节，项目经验，经历那些就一语带过。 所以，我按照这个套路准备了一个还算通用的模板，毕竟我懒嘛！不想多准备一个自我介绍，整个通用的多好！ 面试官，您好！我叫小李子。大学时间我主要利用课外时间学习 Java 相关的知识。在校期间参与过一个某某系统的开发，主要负责数据库设计和后端系统开发.，期间解决了什么问题，巴拉巴拉。另外，我自己在学习过程中也参照网上的教程写过一个电商系统的网站，写这个电商网站主要是为了能让自己接触到分布式系统的开发。在学习之余，我比较喜欢通过博客整理分享自己所学知识。我现在已经是某社区的认证作者，写过一系列关于 线程池使用以及源码分析的文章深受好评。另外，我获得过省级编程比赛二等奖,我将这个获奖项目开源到 Github 还收获了 2k 的 Star 呢？ 面试官： 你刚刚说参考网上的教程做了一个电商系统？你能画画这个电商系统的架构图吗？ 我： 内心 os: \"这可难不倒我！早知道写在简历上的项目要重视了，提前都把这个系统的架构图画了好多遍了呢！\" 做过分布式电商系统的一定很熟悉上面的架构图（目前比较流行的是微服务架构，但是如果你有分布式开发经验也是非常加分的！）。 面试官： 简单介绍一下你做的这个系统吧！ 我： 我一本正经的对着我刚刚画的商城架构图开始了满嘴造火箭的讲起来： 本系统主要分为展示层、服务层和持久层这三层。表现层顾名思义主要就是为了用来展示，比如我们的后台管理系统的页面、商城首页的页面、搜索系统的页面等等，这一层都只是作为展示，并没有提供任何服务。 展示层和服务层一般是部署在不同的机器上来提高并发量和扩展性，那么展示层和服务层怎样才能交互呢？在本系统中我们使用 Dubbo 来进行服务治理。Dubbo 是一款高性能、轻量级的开源 Java RPC 框架。Dubbo 在本系统的主要作用就是提供远程 RPC 调用。在本系统中服务层的信息通过 Dubbo 注册给 ZooKeeper，表现层通过 Dubbo 去 ZooKeeper 中获取服务的相关信息。Zookeeper 的作用仅仅是存放提供服务的服务器的地址和一些服务的相关信息，实现 RPC 远程调用功能的还是 Dubbo。如果需要引用到某个服务的时候，我们只需要在配置文件中配置相关信息就可以在代码中直接使用了，就像调用本地方法一样。假如说某个服务的使用量增加时，我们只用为这单个服务增加服务器，而不需要为整个系统添加服务。 另外，本系统的数据库使用的是常用的 MySQL，并且用到了数据库中间件 MyCat。另外，本系统还用到 redis 内存数据库来作为缓存来提高系统的反应速度。假如用户第一次访问数据库中的某些数据，这个过程会比较慢，因为是从硬盘上读取的。将该用户访问的数据存在数缓存中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。 系统还用到了 Elasticsearch 来提供搜索功能。使用 Elasticsearch 我们可以非常方便的为我们的商城系统添加必备的搜索功能，并且使用 Elasticsearch 还能提供其它非常实用的功能，并且很容易扩展。 面试官： 我看你的系统里面还用到了消息队列，能说说为什么要用它吗？ 我： 使用消息队列主要是为了： 减少响应所需时间和削峰。 降低系统耦合性（解耦/提升系统可扩展性）。 面试官： 你这说的太简单了！能不能稍微详细一点，最好能画图给我解释一下。 我： 内心 os:\"都 2019 年了，大部分面试者都能对消息队列的为系统带来的这两个好处倒背如流了，如果你想走的更远就要别别人懂的更深一点！\" 当我们不使用消息队列的时候，所有的用户的请求会直接落到服务器，然后通过数据库或者缓存响应。假如在高并发的场景下，如果没有缓存或者数据库承受不了这么大的压力的话，就会造成响应速度缓慢，甚至造成数据库宕机。但是，在使用消息队列之后，用户的请求数据发送给了消息队列之后就可以立即返回，再由消息队列的消费者进程从消息队列中获取数据，异步写入数据库，不过要确保消息不被重复消费还要考虑到消息丢失问题。由于消息队列服务器处理速度快于数据库，因此响应速度得到大幅改善。 文字 is too 空洞，直接上图吧！下图展示了使用消息前后系统处理用户请求的对比（ps:我自己都被我画的这个图美到了，如果你也觉得这张图好看的话麻烦来个素质三连！）。 通过以上分析我们可以得出消息队列具有很好的削峰作用的功能——即通过异步处理，将短时间高并发产生的事务消息存储在消息队列中，从而削平高峰期的并发事务。 举例：在电子商务一些秒杀、促销活动中，合理使用消息队列可以有效抵御促销活动刚开始大量订单涌入对系统的冲击。如下图所示： 使用消息队列还可以降低系统耦合性。我们知道如果模块之间不存在直接调用，那么新增模块或者修改模块就对其他模块影响较小，这样系统的可扩展性无疑更好一些。还是直接上图吧： 生产者（客户端）发送消息到消息队列中去，接受者（服务端）处理消息，需要消费的系统直接去消息队列取消息进行消费即可而不需要和其他系统有耦合， 这显然也提高了系统的扩展性。 面试官： 你觉得它有什么缺点吗？或者说怎么考虑用不用消息队列？ 我： 内心 os: \"面试官真鸡贼！这不是勾引我上钩么？还好我准备充分。\" 我觉得可以从下面几个方面来说： 系统可用性降低： 系统可用性在某种程度上降低，为什么这样说呢？在加入 MQ 之前，你不用考虑消息丢失或者说 MQ 挂掉等等的情况，但是，引入 MQ 之后你就需要去考虑了！ 系统复杂性提高： 加入 MQ 之后，你需要保证消息没有被重复消费、处理消息丢失的情况、保证消息传递的顺序性等等问题！ 一致性问题： 我上面讲了消息队列可以实现异步，消息队列带来的异步确实可以提高系统响应速度。但是，万一消息的真正消费者并没有正确消费消息怎么办？这样就会导致数据不一致的情况了! 面试官：做项目的过程中遇到了什么问题吗？解决了吗？如果解决的话是如何解决的呢？ 我 ： 内心 os: \"做的过程中好像也没有遇到什么问题啊！怎么办？怎么办？突然想到可以说我在使用 Redis 过程中遇到的问题，毕竟我对 Redis 还算熟悉嘛，把面试官往这个方向吸引，准没错。\" 我在使用 Redis 对常用数据进行缓冲的过程中出现了缓存穿透问题。然后，我通过谷歌搜索相关的解决方案来解决的。 面试官： 你还知道缓存穿透啊？不错啊！来说说什么是缓存穿透以及你最后的解决办法。 我： 我先来谈谈什么是缓存穿透吧！ 缓存穿透说简单点就是大量请求的 key 根本不存在于缓存中，导致请求直接到了数据库上，根本没有经过缓存这一层。举个例子：某个黑客故意制造我们缓存中不存在的 key 发起大量请求，导致大量请求落到数据库。 总结一下就是： 缓存层不命中。 存储层不命中，不将空结果写回缓存。 返回空结果给客户端。 一般 MySQL 默认的最大连接数在 150 左右，这个可以通过 show variables like '%max_connections%';命令来查看。最大连接数一个还只是一个指标，cpu，内存，磁盘，网络等物理条件都是其运行指标，这些指标都会限制其并发能力！所以，一般 3000 的并发请求就能打死大部分数据库了。 面试官： 小伙子不错啊！还准备问你：“为什么 3000 的并发能把支持最大连接数 4000 数据库压死？”想不到你自己就提前回答了！不错！ 我： 别夸了！别夸了！我再来说说我知道的一些解决办法以及我最后采用的方案吧！您帮忙看看有没有问题。 最基本的就是首先做好参数校验，一些不合法的参数请求直接抛出异常信息返回给客户端。比如查询的数据库 id 不能小于 0、传入的邮箱格式不对的时候直接返回错误消息给客户端等等。 参数校验通过的情况还是会出现缓存穿透，我们还可以通过以下几个方案来解决这个问题： 1）缓存无效 key : 如果缓存和数据库都查不到某个 key 的数据就写一个到 redis 中去并设置过期时间，具体命令如下：SET key value EX 10086。这种方式可以解决请求的 key 变化不频繁的情况，如何黑客恶意攻击，每次构建的不同的请求 key，会导致 redis 中缓存大量无效的 key 。很明显，这种方案并不能从根本上解决此问题。如果非要用这种方式来解决穿透问题的话，尽量将无效的 key 的过期时间设置短一点比如 1 分钟。 另外，这里多说一嘴，一般情况下我们是这样设计 key 的： 表名:列名:主键名:主键值。 2）布隆过滤器： 布隆过滤器是一个非常神奇的数据结构，通过它我们可以非常方便地判断一个给定数据是否存在于海量数据中。我们需要的就是判断 key 是否合法，有没有感觉布隆过滤器就是我们想要找的那个“人”。 面试官： 不错不错！你还知道布隆过滤器啊！来给我谈一谈。 我： 内心 os：“如果你准备过海量数据处理的面试题，你一定对：“如何确定一个数字是否在于包含大量数字的数字集中（数字集很大，5 亿以上！）?”这个题目很了解了！解决这道题目就要用到布隆过滤器。” 布隆过滤器在针对海量数据去重或者验证数据合法性的时候非常有用。布隆过滤器的本质实际上是 “位(bit)数组”，也就是说每一个存入布隆过滤器的数据都只占一位。相比于我们平时常用的的 List、Map 、Set 等数据结构，它占用空间更少并且效率更高，但是缺点是其返回的结果是概率性的，而不是非常准确的。 当一个元素加入布隆过滤器中的时候，会进行如下操作： 使用布隆过滤器中的哈希函数对元素值进行计算，得到哈希值（有几个哈希函数得到几个哈希值）。 根据得到的哈希值，在位数组中把对应下标的值置为 1。 当我们需要判断一个元素是否存在于布隆过滤器的时候，会进行如下操作： 对给定元素再次进行相同的哈希计算； 得到值之后判断位数组中的每个元素是否都为 1，如果值都为 1，那么说明这个值在布隆过滤器中，如果存在一个值不为 1，说明该元素不在布隆过滤器中。 举个简单的例子： 如图所示，当字符串存储要加入到布隆过滤器中时，该字符串首先由多个哈希函数生成不同的哈希值，然后在对应的位数组的下表的元素设置为 1（当位数组初始化时 ，所有位置均为 0）。当第二次存储相同字符串时，因为先前的对应位置已设置为 1，所以很容易知道此值已经存在（去重非常方便）。 如果我们需要判断某个字符串是否在布隆过滤器中时，只需要对给定字符串再次进行相同的哈希计算，得到值之后判断位数组中的每个元素是否都为 1，如果值都为 1，那么说明这个值在布隆过滤器中，如果存在一个值不为 1，说明该元素不在布隆过滤器中。 不同的字符串可能哈希出来的位置相同，这种情况我们可以适当增加位数组大小或者调整我们的哈希函数。 综上，我们可以得出：布隆过滤器说某个元素存在，小概率会误判。布隆过滤器说某个元素不在，那么这个元素一定不在。 面试官： 看来你对布隆过滤器了解的还挺不错的嘛！那你快说说你最后是怎么利用它来解决缓存穿透的。 我： 知道了布隆过滤器的原理就之后就很容易做了。我是利用 Redis 布隆过滤器来做的。我把所有可能存在的请求的值都存放在布隆过滤器中，当用户请求过来，我会先判断用户发来的请求的值是否存在于布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端，存在的话才会走下面的流程。总结一下就是下面这张图(这张图片不是我画的，为了省事直接在网上找的)： 更多关于布隆过滤器的内容可以看我的这篇原创：《不了解布隆过滤器？一文给你整的明明白白！》 ，强烈推荐，个人感觉网上应该找不到总结的这么明明白白的文章了。 面试官： 好了好了。项目就暂时问到这里吧！下面有一些比较基础的问题我简单地问一下你。内心 os： 难不成这家伙满口高并发，连最基础的东西都不会吧！ 我： 好的好的！没问题！ 面试官： 浏览器输入 URL 发生了什么? 我： 内心 os：“很常问的一个问题，建议拿小本本记好了！另外，百度好像最喜欢问这个问题，去百度面试可要提前备好这道题的功课哦！相似问题：打开一个网页，整个过程会使用哪些协议？”。 图解（图片来源：《图解 HTTP》）： 总体来说分为以下几个过程: DNS 解析 TCP 连接 发送 HTTP 请求 服务器处理请求并返回 HTTP 报文 浏览器解析渲染页面 连接结束 具体可以参考下面这篇文章： https://segmentfault.com/a/1190000006879700 面试官： TCP 和 UDP 区别? 我： UDP 在传送数据之前不需要先建立连接，远地主机在收到 UDP 报文后，不需要给出任何确认。虽然 UDP 不提供可靠交付，但在某些情况下 UDP 确是一种最有效的工作方式（一般用于即时通信），比如： QQ 语音、 QQ 视频 、直播等等 TCP 提供面向连接的服务。在传送数据之前必须先建立连接，数据传送结束后要释放连接。 TCP 不提供广播或多播服务。由于 TCP 要提供可靠的，面向连接的传输服务（TCP 的可靠体现在 TCP 在传递数据之前，会有三次握手来建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制，在数据传完后，还会断开连接用来节约系统资源），这一难以避免增加了许多开销，如确认，流量控制，计时器以及连接管理等。这不仅使协议数据单元的首部增大很多，还要占用许多处理机资源。TCP 一般用于文件传输、发送和接收邮件、远程登录等场景。 面试官： TCP 如何保证传输可靠性? 我： 应用数据被分割成 TCP 认为最适合发送的数据块。 TCP 给发送的每一个包进行编号，接收方对数据包进行排序，把有序数据传送给应用层。 校验和： TCP 将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP 将丢弃这个报文段和不确认收到此报文段。 TCP 的接收端会丢弃重复的数据。 流量控制： TCP 连接的每一方都有固定大小的缓冲空间，TCP 的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP 使用的流量控制协议是可变大小的滑动窗口协议。 （TCP 利用滑动窗口实现流量控制） 拥塞控制： 当网络拥塞时，减少数据的发送。 ARQ 协议： 也是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组。 超时重传： 当 TCP 发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。 面试官： 我再来问你一些 Java 基础的问题吧！小伙子。 我： 好的。（内心 os:“你尽管来！”） 面试官： 既然有了字节流,为什么还要有字符流? 我：内心 os ：“问题本质想问：不管是文件读写还是网络发送接收，信息的最小存储单元都是字节，那为什么 I/O 流操作要分为字节流操作和字符流操作呢？” 字符流是由 Java 虚拟机将字节转换得到的，问题就出在这个过程还算是非常耗时，并且，如果我们不知道编码类型就很容易出现乱码问题。所以， I/O 流就干脆提供了一个直接操作字符的接口，方便我们平时对字符进行流操作。如果音频文件、图片等媒体文件用字节流比较好，如果涉及到字符的话使用字符流比较好。 面试官：深拷贝 和 浅拷贝有啥区别呢？ 我： 浅拷贝：对基本数据类型进行值传递，对引用数据类型进行引用传递般的拷贝，此为浅拷贝。 深拷贝：对基本数据类型进行值传递，对引用数据类型，创建一个新的对象，并复制其内容，此为深拷贝。 面试官： 好的！面试结束。小伙子可以的！回家等通知吧！ 我： 好的好的！辛苦您了！ "},"zother6-JavaGuide/essential-content-for-interview/手把手教你用Markdown写一份高质量的简历.html":{"url":"zother6-JavaGuide/essential-content-for-interview/手把手教你用Markdown写一份高质量的简历.html","title":"手把手教你用Markdown写一份高质量的简历","keywords":"","body":"Markdown 简历模板样式一览 可以看到我把联系方式放在第一位，因为公司一般会与你联系，所以把联系方式放在第一位也是为了方便联系考虑。 为什么要用 Markdown 写简历？ Markdown 语法简单，易于上手。使用正确的 Markdown 语言写出来的简历不论是在排版还是格式上都比较干净，易于阅读。另外，使用 Markdown 写简历也会给面试官一种你比较专业的感觉。 除了这些，我觉得使用 Markdown 写简历可以很方便将其与PDF、HTML、PNG格式之间转换。后面我会介绍到转换方法，只需要一条命令你就可以实现 Markdown 到 PDF、HTML 与 PNG之间的无缝切换。 下面的一些内容我在之前的一篇文章中已经提到过，这里再说一遍，最后会分享如何实现Markdown 到 PDF、HTML、PNG格式之间转换。 为什么说简历很重要？ 假如你是网申，你的简历必然会经过HR的筛选，一张简历HR可能也就花费10秒钟看一下，然后HR就会决定你这一关是Fail还是Pass。 假如你是内推，如果你的简历没有什么优势的话，就算是内推你的人再用心，也无能为力。 另外，就算你通过了筛选，后面的面试中，面试官也会根据你的简历来判断你究竟是否值得他花费很多时间去面试。 写简历的两大法则 目前写简历的方式有两种普遍被认可，一种是 STAR， 一种是 FAB。 STAR法则（Situation Task Action Result）： Situation： 事情是在什么情况下发生； Task:： 你是如何明确你的任务的； Action： 针对这样的情况分析，你采用了什么行动方式； Result： 结果怎样，在这样的情况下你学习到了什么。 FAB 法则（Feature Advantage Benefit）： Feature： 是什么； Advantage： 比别人好在哪些地方； Benefit： 如果雇佣你，招聘方会得到什么好处。 项目经历怎么写？ 简历上有一两个项目经历很正常，但是真正能把项目经历很好的展示给面试官的非常少。对于项目经历大家可以考虑从如下几点来写： 对项目整体设计的一个感受 在这个项目中你负责了什么、做了什么、担任了什么角色 从这个项目中你学会了那些东西，使用到了那些技术，学会了那些新技术的使用 另外项目描述中，最好可以体现自己的综合素质，比如你是如何协调项目组成员协同开发的或者在遇到某一个棘手的问题的时候你是如何解决的。 专业技能该怎么写？ 先问一下你自己会什么，然后看看你意向的公司需要什么。一般HR可能并不太懂技术，所以他在筛选简历的时候可能就盯着你专业技能的关键词来看。对于公司有要求而你不会的技能，你可以花几天时间学习一下，然后在简历上可以写上自己了解这个技能。比如你可以这样写： Dubbo：精通 Spring：精通 Docker：掌握 SOA分布式开发 ：掌握 Spring Cloud:了解 简历模板分享 开源程序员简历模板： https://github.com/geekcompany/ResumeSample（包括PHP程序员简历模板、iOS程序员简历模板、Android程序员简历模板、Web前端程序员简历模板、Java程序员简历模板、C/C++程序员简历模板、NodeJS程序员简历模板、架构师简历模板以及通用程序员简历模板） 上述简历模板的改进版本： https://github.com/Snailclimb/Java-Guide/blob/master/面试必备/简历模板.md 其他的一些小tips 尽量避免主观表述，少一点语义模糊的形容词，尽量要简洁明了，逻辑结构清晰。 注意排版（不需要花花绿绿的），尽量使用Markdown语法。 如果自己有博客或者个人技术栈点的话，写上去会为你加分很多。 如果自己的Github比较活跃的话，写上去也会为你加分很多。 注意简历真实性，一定不要写自己不会的东西，或者带有欺骗性的内容 项目经历建议以时间倒序排序，另外项目经历不在于多，而在于有亮点。 如果内容过多的话，不需要非把内容压缩到一页，保持排版干净整洁就可以了。 简历最后最好能加上：“感谢您花时间阅读我的简历，期待能有机会和您共事。”这句话，显的你会很有礼貌。 我们刚刚讲了很多关于如何写简历的内容并且分享了一份 Markdown 格式的简历文档。下面我们来看看如何实现 Markdown 到 HTML格式、PNG格式之间转换。 Markdown 到 HTML格式、PNG格式之间转换 网上很难找到一个比较方便并且效果好的转换方法，最后我是通过 Visual Studio Code 的 Markdown PDF 插件完美解决了这个问题！ 安装 Markdown PDF 插件 ① 打开Visual Studio Code ，按快捷键 F1，选择安装扩展选项 ② 搜索 “Markdown PDF” 插件并安装 ，然后重启 使用方法 随便打开一份 Markdown 文件 点击F1，然后输入export即可！ "},"zother6-JavaGuide/essential-content-for-interview/简历模板.html":{"url":"zother6-JavaGuide/essential-content-for-interview/简历模板.html","title":"简历模板","keywords":"","body":"联系方式 手机： Email： 微信： 个人信息 姓名/性别/出生日期 本科/xxx计算机系xxx专业/英语六级 技术博客：http://snailclimb.top/ 荣誉奖励：获得了什么奖（获奖时间） Github：https://github.com/Snailclimb Github Resume: http://resume.github.io/?Snailclimb 期望职位：Java 研发程序员/大数据工程师(Java后台开发为首选) 期望城市：xxx城市 项目经历 xxx项目 项目描述 介绍该项目是做什么的、使用到了什么技术以及你对项目整体设计的一个感受 责任描述 主要可以从下面三点来写： 在这个项目中你负责了什么、做了什么、担任了什么角色 从这个项目中你学会了那些东西，使用到了那些技术，学会了那些新技术的使用 另外项目描述中，最好可以体现自己的综合素质，比如你是如何协调项目组成员协同开发的或者在遇到某一个棘手的问题的时候你是如何解决的。 开源项目和技术文章 开源项目 Java-Guide ：一份涵盖大部分Java程序员所需要掌握的核心知识。Star:3.9K; Fork:0.9k。 技术文章推荐 可能是把Java内存区域讲的最清楚的一篇文章 搞定JVM垃圾回收就是这么简单 前端&后端程序员必备的Linux基础知识 可能是把Docker的概念讲的最清楚的一篇文章 校园经历（可选） 2016-2017 担任学校社团-致深社副会长，主要负责团队每周活动的组建以及每周例会的主持。 2017-2018 担任学校传媒组织：“长江大学在线信息传媒”的副站长以及安卓组成员。主要负责每周例会主持、活动策划以及学校校园通APP的研发工作。 技能清单 以下均为我熟练使用的技能 Web开发：PHP/Hack/Node Web框架：ThinkPHP/Yaf/Yii/Lavarel/LazyPHP 前端框架：Bootstrap/AngularJS/EmberJS/HTML5/Cocos2dJS/ionic 前端工具：Bower/Gulp/SaSS/LeSS/PhoneGap 数据库相关：MySQL/PgSQL/PDO/SQLite 版本管理、文档和自动化部署工具：Svn/Git/PHPDoc/Phing/Composer 单元测试：PHPUnit/SimpleTest/Qunit 云和开放平台：SAE/BAE/AWS/微博开放平台/微信应用开发 自我评价（可选） 自我发挥。切记不要过度自夸！！！ 感谢您花时间阅读我的简历，期待能有机会和您共事。 "},"zother6-JavaGuide/essential-content-for-interview/面试必备之乐观锁与悲观锁.html":{"url":"zother6-JavaGuide/essential-content-for-interview/面试必备之乐观锁与悲观锁.html","title":"面试必备之乐观锁与悲观锁","keywords":"","body":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。 何谓悲观锁与乐观锁 悲观锁 乐观锁 两种锁的使用场景 乐观锁常见的两种实现方式 1. 版本号机制 2. CAS算法 乐观锁的缺点 1 ABA 问题 2 循环时间长开销大 3 只能保证一个共享变量的原子操作 CAS与synchronized的使用情景 何谓悲观锁与乐观锁 乐观锁对应于生活中乐观的人总是想着事情往好的方向发展，悲观锁对应于生活中悲观的人总是想着事情往坏的方向发展。这两种人各有优缺点，不能不以场景而定说一种人好于另外一种人。 悲观锁 总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java中synchronized和ReentrantLock等独占锁就是悲观锁思想的实现。 乐观锁 总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，其实都是提供的乐观锁。在Java中java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。 两种锁的使用场景 从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。 乐观锁常见的两种实现方式 乐观锁一般会使用版本号机制或CAS算法实现。 1. 版本号机制 一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。 举一个简单的例子： 假设数据库中帐户信息表中有一个 version 字段，当前值为 1 ；而当前帐户余额字段（ balance ）为 $100 。 操作员 A 此时将其读出（ version=1 ），并从其帐户余额中扣除 $50（ $100-$50 ）。 在操作员 A 操作的过程中，操作员B 也读入此用户信息（ version=1 ），并从其帐户余额中扣除 $20 （ $100-$20 ）。 操作员 A 完成了修改工作，将数据版本号加一（ version=2 ），连同帐户扣除后余额（ balance=$50 ），提交至数据库更新，此时由于提交数据版本大于数据库记录当前版本，数据被更新，数据库记录 version 更新为 2 。 操作员 B 完成了操作，也将版本号加一（ version=2 ）试图向数据库提交数据（ balance=$80 ），但此时比对数据库记录版本时发现，操作员 B 提交的数据版本号为 2 ，数据库记录当前版本也为 2 ，不满足 “ 提交版本必须大于记录当前版本才能执行更新 “ 的乐观锁策略，因此，操作员 B 的提交被驳回。 这样，就避免了操作员 B 用基于 version=1 的旧数据修改的结果覆盖操作员A 的操作结果的可能。 2. CAS算法 即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS算法涉及到三个操作数 需要读写的内存值 V 进行比较的值 A 拟写入的新值 B 当且仅当 V 的值等于 A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。 关于自旋锁，大家可以看一下这篇文章，非常不错：《 面试必备之深入理解自旋锁》 乐观锁的缺点 ABA 问题是乐观锁一个常见的问题 1 ABA 问题 如果一个变量V初次读取的时候是A值，并且在准备赋值的时候检查到它仍然是A值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回A，那CAS操作就会误认为它从来没有被修改过。这个问题被称为CAS操作的 \"ABA\"问题。 JDK 1.5 以后的 AtomicStampedReference 类就提供了此种能力，其中的 compareAndSet 方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 2 循环时间长开销大 自旋CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给CPU带来非常大的执行开销。 如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 3 只能保证一个共享变量的原子操作 CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5开始，提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用AtomicReference类把多个共享变量合并成一个共享变量来操作。 CAS与synchronized的使用情景 简单的来说CAS适用于写比较少的情况下（多读场景，冲突一般较少），synchronized适用于写比较多的情况下（多写场景，冲突一般较多） 对于资源竞争较少（线程冲突较轻）的情况，使用synchronized同步锁进行线程阻塞和唤醒切换以及用户态内核态间的切换操作额外浪费消耗cpu资源；而CAS基于硬件实现，不需要进入内核，不需要切换线程，操作自旋几率较少，因此可以获得更高的性能。 对于资源竞争严重（线程冲突严重）的情况，CAS自旋的概率会比较大，从而浪费更多的CPU资源，效率低于synchronized。 补充： Java并发编程这个领域中synchronized关键字一直都是元老级的角色，很久之前很多人都会称它为 “重量级锁” 。但是，在JavaSE 1.6之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的 偏向锁 和 轻量级锁 以及其它各种优化之后变得在某些情况下并不是那么重了。synchronized的底层实现主要依靠 Lock-Free 的队列，基本思路是 自旋后阻塞，竞争切换后继续竞争锁，稍微牺牲了公平性，但获得了高吞吐量。在线程冲突较少的情况下，可以获得和CAS类似的性能；而线程冲突严重的情况下，性能远高于CAS。 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \"面试突击\" 即可免费领取！ Java工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/github-trending/2018-12.html":{"url":"zother6-JavaGuide/github-trending/2018-12.html","title":"2018 12","keywords":"","body":"本文数据统计于 1.1 号凌晨，由 SnailClimb 整理。 1. JavaGuide Github地址： https://github.com/Snailclimb/JavaGuide star: 18.2k 介绍: 【Java学习+面试指南】 一份涵盖大部分Java程序员所需要掌握的核心知识。 2. mall Github地址： https://github.com/macrozheng/mall star: 3.3k 介绍: mall项目是一套电商系统，包括前台商城系统及后台管理系统，基于SpringBoot+MyBatis实现。 前台商城系统包含首页门户、商品推荐、商品搜索、商品展示、购物车、订单流程、会员中心、客户服务、帮助中心等模块。 后台管理系统包含商品管理、订单管理、会员管理、促销管理、运营管理、内容管理、统计报表、财务管理、权限管理、设置等模块。 3. advanced-java Github地址：https://github.com/doocs/advanced-java star: 3.3k 介绍: 互联网 Java 工程师进阶知识完全扫盲 4. matrix Github地址：https://github.com/Tencent/matrix star: 2.5k 介绍: Matrix 是一款微信研发并日常使用的 APM（Application Performance Manage），当前主要运行在 Android 平台上。 Matrix 的目标是建立统一的应用性能接入框架，通过各种性能监控方案，对性能监控项的异常数据进行采集和分析，输出相应的问题分析、定位与优化建议，从而帮助开发者开发出更高质量的应用。 5. miaosha Github地址：https://github.com/qiurunze123/miaosha star: 2.4k 介绍: 高并发大流量如何进行秒杀架构，我对这部分知识做了一个系统的整理，写了一套系统。 6. arthas Github地址：https://github.com/alibaba/arthas star: 8.2k 介绍: Arthas 是Alibaba开源的Java诊断工具，深受开发者喜爱。 7 spring-boot Github地址： https://github.com/spring-projects/spring-boot star: 32.6k 介绍： 虽然Spring的组件代码是轻量级的，但它的配置却是重量级的（需要大量XML配置）,不过Spring Boot 让这一切成为了过去。 另外Spring Cloud也是基于Spring Boot构建的，我个人非常有必要学习一下。 关于Spring Boot官方的介绍： Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications that you can “just run”…Most Spring Boot applications need very little Spring configuration.(Spring Boot可以轻松创建独立的生产级基于Spring的应用程序,只要通过 “just run”（可能是run ‘Application’或java -jar 或 tomcat 或 maven插件run 或 shell脚本）便可以运行项目。大部分Spring Boot项目只需要少量的配置即可) 8. tutorials Github地址：https://github.com/eugenp/tutorials star: 10k 介绍: 该项目是一系列小而专注的教程 - 每个教程都涵盖Java生态系统中单一且定义明确的开发领域。 当然，它们的重点是Spring Framework - Spring，Spring Boot和Spring Securiyt。 除了Spring之外，还有以下技术：核心Java，Jackson，HttpClient，Guava。 9. qmq Github地址：https://github.com/qunarcorp/qmq star: 1.1k 介绍: QMQ是去哪儿网内部广泛使用的消息中间件，自2012年诞生以来在去哪儿网所有业务场景中广泛的应用，包括跟交易息息相关的订单场景； 也包括报价搜索等高吞吐量场景。 10. symphony Github地址：https://github.com/b3log/symphony star: 9k 介绍: 一款用 Java 实现的现代化社区（论坛/BBS/社交网络/博客）平台。 11. incubator-dubbo Github地址：https://github.com/apache/incubator-dubbo star: 23.6k 介绍: 阿里开源的一个基于Java的高性能开源RPC框架。 12. apollo Github地址：https://github.com/ctripcorp/apollo star: 10k 介绍: Apollo（阿波罗）是携程框架部门研发的分布式配置中心，能够集中化管理应用不同环境、不同集群的配置，配置修改后能够实时推送到应用端，并且具备规范的权限、流程治理等特性，适用于微服务配置管理场景。 "},"zother6-JavaGuide/github-trending/2019-1.html":{"url":"zother6-JavaGuide/github-trending/2019-1.html","title":"2019 1","keywords":"","body":"1. JavaGuide Github地址： https://github.com/Snailclimb/JavaGuide star: 22.8k 介绍: 【Java学习+面试指南】 一份涵盖大部分Java程序员所需要掌握的核心知识。 2. advanced-java Github地址：https://github.com/doocs/advanced-java star: 7.9k 介绍: 互联网 Java 工程师进阶知识完全扫盲 3. fescar Github地址：https://github.com/alibaba/fescar star: 4.6k 介绍: 具有 高性能 和 易用性 的 微服务架构 的 分布式事务 的解决方案。（特点：高性能且易于使用，旨在实现简单并快速的事务提交与回滚。 4. mall Github地址： https://github.com/macrozheng/mall star: 5.6 k 介绍: mall项目是一套电商系统，包括前台商城系统及后台管理系统，基于SpringBoot+MyBatis实现。 前台商城系统包含首页门户、商品推荐、商品搜索、商品展示、购物车、订单流程、会员中心、客户服务、帮助中心等模块。 后台管理系统包含商品管理、订单管理、会员管理、促销管理、运营管理、内容管理、统计报表、财务管理、权限管理、设置等模块。 5. miaosha Github地址：https://github.com/qiurunze123/miaosha star: 4.4k 介绍: 高并发大流量如何进行秒杀架构，我对这部分知识做了一个系统的整理，写了一套系统。 6. flink Github地址：https://github.com/apache/flink star: 7.1 k 介绍: Apache Flink是一个开源流处理框架，具有强大的流和批处理功能。 7. cim Github地址：https://github.com/crossoverJie/cim star: 1.8 k 介绍: cim(cross IM) 适用于开发者的即时通讯系统。 8. symphony Github地址：https://github.com/b3log/symphony star: 10k 介绍: 一款用 Java 实现的现代化社区（论坛/BBS/社交网络/博客）平台。 9. spring-boot Github地址： https://github.com/spring-projects/spring-boot star: 32.6k 介绍： 虽然Spring的组件代码是轻量级的，但它的配置却是重量级的（需要大量XML配置）,不过Spring Boot 让这一切成为了过去。 另外Spring Cloud也是基于Spring Boot构建的，我个人非常有必要学习一下。 关于Spring Boot官方的介绍： Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications that you can “just run”…Most Spring Boot applications need very little Spring configuration.(Spring Boot可以轻松创建独立的生产级基于Spring的应用程序,只要通过 “just run”（可能是run ‘Application’或java -jar 或 tomcat 或 maven插件run 或 shell脚本）便可以运行项目。大部分Spring Boot项目只需要少量的配置即可) 10. arthas Github地址：https://github.com/alibaba/arthas star: 9.5k 介绍: Arthas 是Alibaba开源的Java诊断工具。 概览： 当你遇到以下类似问题而束手无策时，Arthas可以帮助你解决： 这个类从哪个 jar 包加载的？为什么会报各种类相关的 Exception？ 我改的代码为什么没有执行到？难道是我没 commit？分支搞错了？ 遇到问题无法在线上 debug，难道只能通过加日志再重新发布吗？ 线上遇到某个用户的数据处理有问题，但线上同样无法 debug，线下无法重现！ 是否有一个全局视角来查看系统的运行状况？ 有什么办法可以监控到JVM的实时运行状态？ Arthas支持JDK 6+，支持Linux/Mac/Winodws，采用命令行交互模式，同时提供丰富的 Tab 自动补全功能，进一步方便进行问题的定位和诊断。 "},"zother6-JavaGuide/github-trending/2019-12.html":{"url":"zother6-JavaGuide/github-trending/2019-12.html","title":"2019 12","keywords":"","body":"年末将至，值得你关注的16个Java 开源项目！ Star 的数量统计于 2019-12-29。 1.JavaGuide Guide 哥大三开始维护的，目前算是纯 Java 类型项目中 Star 数量最多的项目了。但是，本仓库的价值远远（+N次 ）比不上像 Spring Boot、Elasticsearch 等等这样非常非常非常优秀的项目。希望以后我也有能力为这些项目贡献一些有价值的代码。 Github 地址：https://github.com/Snailclimb/JavaGuide Star: 66.3k 介绍: 【Java 学习+面试指南】 一份涵盖大部分 Java 程序员所需要掌握的核心知识。 2.java-design-patterns 感觉还不错。根据官网介绍： 设计模式是程序员在设计应用程序或系统时可以用来解决常见问题的最佳形式化实践。 设计模式可以通过提供经过测试的，经过验证的开发范例来加快开发过程。 重用设计模式有助于防止引起重大问题的细微问题，并且还可以提高熟悉模式的编码人员和架构师的代码可读性。 Github 地址 : https://github.com/iluwatar/java-design-patterns Star: 53.8k 介绍: 用 Java 实现的设计模式。https://java-design-patterns.com。 3.elasticsearch 搜索引擎界的扛把子，但不仅仅是搜素引擎那么简单。 Github 地址 : https://github.com/elastic/elasticsearch Star: 46.2k 介绍: 开源，分布式，RESTful 搜索引擎。 4.spring-boot 必须好好学啊，一定要好好学！现在 Java 后端新项目有不用 Spring Boot 开发的有吗？如果有的话，请把这个人的联系方式告诉我，我有很多话想给他交流交流！ Github地址： https://github.com/spring-projects/spring-boot star: 34.8k (1,073 stars this month) 介绍： 虽然Spring的组件代码是轻量级的，但它的配置却是重量级的（需要大量XML配置）,不过Spring Boot 让这一切成为了过去。 另外Spring Cloud也是基于Spring Boot构建的，我个人非常有必要学习一下。 5.RxJava 这个没怎么用过，不做太多评价。 Github 地址 : https://github.com/ReactiveX/RxJava Star: 41.5k 介绍: RxJava 是一个 基于事件流、实现异步操作的库。 6.advanced-java 本项目大部分内容来自中华石杉的一个课程，内容涵盖高并发、分布式、高可用、微服务、海量数据处理等领域知识，非常不错了！ Github 地址：https://github.com/doocs/advanced-java Star: 36.7k 介绍: 互联网 Java 工程师进阶知识完全扫盲：涵盖高并发、分布式、高可用、微服务等领域知识，后端同学必看，前端同学也可学习。 7.mall 很牛逼的实战项目，还附有详细的文档，作为毕设或者练手项目都再好不过了。 Github地址： https://github.com/macrozheng/mall star: 27.6k 介绍: mall项目是一套电商系统，包括前台商城系统及后台管理系统，基于SpringBoot+MyBatis实现。 前台商城系统包含首页门户、商品推荐、商品搜索、商品展示、购物车、订单流程、会员中心、客户服务、帮助中心等模块。 后台管理系统包含商品管理、订单管理、会员管理、促销管理、运营管理、内容管理、统计报表、财务管理、权限管理、设置等模块。 8.okhttp 给我感觉是安卓项目中用的居多。当然，Java 后端项目也会经常用，但是一般使用 Spring Boot 进行开发的时候，如果需要远程调用的话建议使用 Spring 封装的 RestTemplate。 Github地址：https://github.com/square/okhttp star: 35.4k 介绍: 适用于Android，Kotlin和Java的HTTP客户端。https://square.github.io/okhttp/。 9.guava 很厉害很厉害！提供了很多非常实用的工具类、更加实用的集合类、一些常用的数据结构比如布隆过滤器、缓存等等。 Github地址：https://github.com/google/guava star: 35.3k 介绍: Guava是一组核心库，其中包括新的集合类型（例如 multimap 和 multiset），不可变集合，图形库以及用于并发，I / O，哈希，基元，字符串等的实用程序！ 10.Spark 我木有用过，留下了没有技术的眼泪。 Github地址：https://github.com/apache/spark star: 24.7k 介绍: Spark 是一个快速、通用的大规模数据处理引擎，和Hadoop的MapReduce计算框架类似，但是相对于MapReduce，Spark凭借其可伸缩、基于内存计算等特点，以及可以直接读写Hadoop上任何格式数据的优势，进行批处理时更加高效，并有更低的延迟。 11.arthas 虽然我自己没有亲身用过，但是身边用过的朋友评价都还挺好的。根据官网介绍，这家伙可以解决下面这些让人脑壳疼的问题： 这个类从哪个 jar 包加载的？为什么会报各种类相关的 Exception？ 我改的代码为什么没有执行到？难道是我没 commit？分支搞错了？ 遇到问题无法在线上 debug，难道只能通过加日志再重新发布吗？ 线上遇到某个用户的数据处理有问题，但线上同样无法 debug，线下无法重现！ 是否有一个全局视角来查看系统的运行状况？ 有什么办法可以监控到JVM的实时运行状态？ 怎么快速定位应用的热点，生成火焰图？ Github 地址：https://github.com/alibaba/arthas star: 18.8 k 介绍: Arthas 是 Alibaba 开源的 Java 诊断工具。 12.spring-boot-examples 学习 Spring Boot 必备！配合上我的 springboot-guide ：https://github.com/Snailclimb/springboot-guide，效果杠杠滴！ Github 地址：https://github.com/ityouknow/spring-boot-examples star: 20.2 k 介绍: Spring Boot 教程、技术栈示例代码，快速简单上手教程。 13.lombok 使用 Lombok 我们可以简化我们的 Java 代码，比如使用它之后我们通过注释就可以实现 getter/setter、equals等方法。 Github 地址：https://github.com/rzwitserloot/lombok star: 20.2 k 介绍: 对 Java 编程语言的非常刺激的补充。https://projectlombok.org/ 。 14.p3c 与我而言，没有特别惊艳，但是一些提供的一些代码规范确实挺有用的！ Github 地址：https://github.com/alibaba/p3c star: 19.8 k 介绍: 阿里巴巴Java编码指南pmd实现和IDE插件。 15.spring-boot-demo Github 地址：https://github.com/xkcoding/spring-boot-demo Star: 8.8k 介绍: spring boot demo 是一个用来深度学习并实战 spring boot 的项目。 16. awesome-java Guide 哥半个多月前开始维护的，虽然现在 Star 数量比较少，我相信后面一定会有更多人喜欢上这个项目，我也会继续认真维护下去。 Github 地址：https://github.com/Snailclimb/awesome-java Star: 0.3 k 介绍: Github 上非常棒的 Java 开源项目集合。 "},"zother6-JavaGuide/github-trending/2019-2.html":{"url":"zother6-JavaGuide/github-trending/2019-2.html","title":"2019 2","keywords":"","body":"1. JavaGuide Github地址： https://github.com/Snailclimb/JavaGuide Star: 27.2k (4,437 stars this month) 介绍: 【Java学习+面试指南】 一份涵盖大部分Java程序员所需要掌握的核心知识。 2.DoraemonKit Github地址： https://github.com/didi/DoraemonKit Star: 5.2k (3,786 stars this month) 介绍: 简称 \"DoKit\" 。一款功能齐全的客户端（ iOS 、Android ）研发助手，你值得拥有。 3.advanced-java Github地址：https://github.com/doocs/advanced-java Star:11.2k (3,042 stars this month) 介绍: 互联网 Java 工程师进阶知识完全扫盲。 4. spring-boot-examples Github地址：https://github.com/ityouknow/spring-boot-examples star: 9.6 k (1,764 stars this month) 介绍: Spring Boot 教程、技术栈示例代码，快速简单上手教程。 5. mall Github地址： https://github.com/macrozheng/mall star: 7.4 k (1,736 stars this month) 介绍: mall项目是一套电商系统，包括前台商城系统及后台管理系统，基于SpringBoot+MyBatis实现。 前台商城系统包含首页门户、商品推荐、商品搜索、商品展示、购物车、订单流程、会员中心、客户服务、帮助中心等模块。 后台管理系统包含商品管理、订单管理、会员管理、促销管理、运营管理、内容管理、统计报表、财务管理、权限管理、设置等模块。 6. fescar Github地址：https://github.com/alibaba/fescar star: 6.0 k (1,308 stars this month) 介绍: 具有 高性能 和 易用性 的 微服务架构 的 分布式事务 的解决方案。（特点：高性能且易于使用，旨在实现简单并快速的事务提交与回滚。） 7. h4cker Github地址：https://github.com/The-Art-of-Hacking/h4cker star: 2.1 k (1,303 stars this month) 介绍: 该仓库主要由Omar Santos维护，包括与道德黑客/渗透测试，数字取证和事件响应（DFIR），漏洞研究，漏洞利用开发，逆向工程等相关的资源。 8. spring-boot Github地址： https://github.com/spring-projects/spring-boot star: 34.8k (1,073 stars this month) 介绍： 虽然Spring的组件代码是轻量级的，但它的配置却是重量级的（需要大量XML配置）,不过Spring Boot 让这一切成为了过去。 另外Spring Cloud也是基于Spring Boot构建的，我个人非常有必要学习一下。 关于Spring Boot官方的介绍： Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications that you can “just run”…Most Spring Boot applications need very little Spring configuration.(Spring Boot可以轻松创建独立的生产级基于Spring的应用程序,只要通过 “just run”（可能是run ‘Application’或java -jar 或 tomcat 或 maven插件run 或 shell脚本）便可以运行项目。大部分Spring Boot项目只需要少量的配置即可) 9. arthas Github地址：https://github.com/alibaba/arthas star: 10.5 k (970 stars this month) 介绍: Arthas 是Alibaba开源的Java诊断工具。 10. tutorials Github地址：https://github.com/eugenp/tutorials star: 12.1 k (789 stars this month) 介绍: 该项目是一系列小而专注的教程 - 每个教程都涵盖Java生态系统中单一且定义明确的开发领域。 当然，它们的重点是Spring Framework - Spring，Spring Boot和Spring Securiyt。 除了Spring之外，还有以下技术：核心Java，Jackson，HttpClient，Guava。 "},"zother6-JavaGuide/github-trending/2019-3.html":{"url":"zother6-JavaGuide/github-trending/2019-3.html","title":"2019 3","keywords":"","body":"1. JavaGuide Github 地址： https://github.com/Snailclimb/JavaGuide Star: 32.9k (6,196 stars this month) 介绍: 【Java 学习+面试指南】 一份涵盖大部分 Java 程序员所需要掌握的核心知识。 2.advanced-java Github 地址：https://github.com/doocs/advanced-java Star: 15.1k (4,012 stars this month) 介绍: 互联网 Java 工程师进阶知识完全扫盲。 3.spring-boot-examples Github 地址：https://github.com/ityouknow/spring-boot-examples Star: 12.8k (3,462 stars this month) 介绍: Spring Boot 教程、技术栈示例代码，快速简单上手教程。 4. mall Github 地址： https://github.com/macrozheng/mall star: 9.7 k (2,418 stars this month) 介绍: mall 项目是一套电商系统，包括前台商城系统及后台管理系统，基于 SpringBoot+MyBatis 实现。 前台商城系统包含首页门户、商品推荐、商品搜索、商品展示、购物车、订单流程、会员中心、客户服务、帮助中心等模块。 后台管理系统包含商品管理、订单管理、会员管理、促销管理、运营管理、内容管理、统计报表、财务管理、权限管理、设置等模块。 5. seata Github 地址 : https://github.com/seata/seata star: 7.2 k (1359 stars this month) 介绍: Seata 是一种易于使用，高性能，基于 Java 的开源分布式事务解决方案。 6. quarkus Github 地址：https://github.com/quarkusio/quarkus star: 12 k (1,224 stars this month) 介绍: Quarkus 是为 GraalVM 和 HotSpot 量身定制的 Kubernetes Native Java 框架，由最佳的 Java 库和标准精心打造而成。Quarkus 的目标是使 Java 成为 Kubernetes 和无服务器环境中的领先平台，同时为开发人员提供统一的反应式和命令式编程模型，以优化地满足更广泛的分布式应用程序架构。 7. arthas Github 地址：https://github.com/alibaba/arthas star: 11.6 k (1,199 stars this month) 介绍: Arthas 是 Alibaba 开源的 Java 诊断工具。 8.DoraemonKit Github 地址： https://github.com/didi/DoraemonKit Star: 6.2k (1,177 stars this month) 介绍: 简称 \"DoKit\" 。一款功能齐全的客户端（ iOS 、Android ）研发助手，你值得拥有。 9.elasticsearch Github 地址 https://github.com/elastic/elasticsearch Star: 39.7k (1,069 stars this month) 介绍: 开源，分布式，RESTful 搜索引擎。 10. tutorials Github 地址：https://github.com/eugenp/tutorials star: 13 k (998 stars this month) 介绍: 该项目是一系列小而专注的教程 - 每个教程都涵盖 Java 生态系统中单一且定义明确的开发领域。 当然，它们的重点是 Spring Framework - Spring，Spring Boot 和 Spring Securiyt。 除了 Spring 之外，还有以下技术：核心 Java，Jackson，HttpClient，Guava。 "},"zother6-JavaGuide/github-trending/2019-4.html":{"url":"zother6-JavaGuide/github-trending/2019-4.html","title":"2019 4","keywords":"","body":"以下涉及到的数据统计与 2019 年 5 月 1 日 12 点，数据来源：https://github.com/trending/java?since=monthly 。 下面的内容从 Java 学习文档到最热门的框架再到热门的工具应有尽有，比如下面推荐到的开源项目 Hutool 就是近期比较热门的项目之一，它是 Java 工具包，能够帮助我们简化代码！我觉得下面这些项目对于学习 Java 的朋友还是很有帮助的！ 1. JavaGuide Github 地址： https://github.com/Snailclimb/JavaGuide Star: 37.9k (5,660 stars this month) 介绍: 【Java 学习+面试指南】 一份涵盖大部分 Java 程序员所需要掌握的核心知识。 2. advanced-java Github 地址：https://github.com/doocs/advanced-java Star: 15.1k (4,654 stars this month) 介绍: 互联网 Java 工程师进阶知识完全扫盲。 3. CS-Notes Github 地址：https://github.com/CyC2018/CS-Notes Star: 59.2k (4,012 stars this month) 介绍: 技术面试必备基础知识。 4. ghidra Github 地址：https://github.com/NationalSecurityAgency/ghidra Star: 15.0k (2,995 stars this month) 介绍: Ghidra是一个软件逆向工程（SRE）框架。 5. mall Github 地址： https://github.com/macrozheng/mall star: 11.6 k (2,100 stars this month) 介绍: mall 项目是一套电商系统，包括前台商城系统及后台管理系统，基于 SpringBoot+MyBatis 实现。 前台商城系统包含首页门户、商品推荐、商品搜索、商品展示、购物车、订单流程、会员中心、客户服务、帮助中心等模块。 后台管理系统包含商品管理、订单管理、会员管理、促销管理、运营管理、内容管理、统计报表、财务管理、权限管理、设置等模块。 6. ZXBlog Github 地址： https://github.com/ZXZxin/ZXBlog star: 2.1 k (2,086 stars this month) 介绍: 记录各种学习笔记(算法、Java、数据库、并发......)。 7.DoraemonKit Github地址： https://github.com/didi/DoraemonKit Star: 7.6k (1,541 stars this month) 介绍: 简称 \"DoKit\" 。一款功能齐全的客户端（ iOS 、Android ）研发助手，你值得拥有。 8. spring-boot Github地址： https://github.com/spring-projects/spring-boot star: 37.3k (1,489 stars this month) 介绍： 虽然Spring的组件代码是轻量级的，但它的配置却是重量级的（需要大量XML配置）,不过Spring Boot 让这一切成为了过去。 另外Spring Cloud也是基于Spring Boot构建的，我个人非常有必要学习一下。 Spring Boot官方的介绍： Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications that you can “just run”…Most Spring Boot applications need very little Spring configuration.(Spring Boot可以轻松创建独立的生产级基于Spring的应用程序,只要通过 “just run”（可能是run ‘Application’或java -jar 或 tomcat 或 maven插件run 或 shell脚本）便可以运行项目。大部分Spring Boot项目只需要少量的配置即可) 9. spring-boot-examples Github 地址：https://github.com/ityouknow/spring-boot-examples Star: 12.8k (1,453 stars this month) 介绍: Spring Boot 教程、技术栈示例代码，快速简单上手教程。 10. seata Github 地址 : https://github.com/seata/seata star: 8.4 k (1441 stars this month) 介绍: Seata 是一种易于使用，高性能，基于 Java 的开源分布式事务解决方案。 11. litemall Github 地址：https://github.com/ityouknow/spring-boot-examples Star: 6.0k (1,427 stars this month) 介绍: 又一个小商城。litemall = Spring Boot后端 + Vue管理员前端 + 微信小程序用户前端 + Vue用户移动端。 12. skywalking Github 地址：https://github.com/apache/skywalking Star: 8.0k (1,381 stars this month) 介绍: 针对分布式系统的应用性能监控，尤其是针对微服务、云原生和面向容器的分布式系统架构。 13. elasticsearch Github 地址 https://github.com/elastic/elasticsearch Star: 4.0k (1,068stars this month) 介绍: 开源，分布式，RESTful 搜索引擎。 14. arthas Github地址：https://github.com/alibaba/arthas star: 12.6 k (1,080 stars this month) 介绍: Arthas 是Alibaba开源的Java诊断工具。 15. hutool Github地址：https://github.com/looly/hutool star: 4.5 k (1,031 stars this month) 介绍: Hutool是一个Java工具包，也只是一个工具包，它帮助我们简化每一行代码，减少每一个方法，让Java语言也可以“甜甜的”。Hutool最初是我项目中“util”包的一个整理，后来慢慢积累并加入更多非业务相关功能，并广泛学习其它开源项目精髓，经过自己整理修改，最终形成丰富的开源工具集。官网:https://www.hutool.cn/ 。 "},"zother6-JavaGuide/github-trending/2019-5.html":{"url":"zother6-JavaGuide/github-trending/2019-5.html","title":"2019 5","keywords":"","body":"以下涉及到的数据统计与 2019 年 6 月 1 日 18 点，数据来源：https://github.com/trending/java?since=monthly 。下面推荐的内容从 Java 学习文档到最热门的框架再到热门的工具应有尽有，建议收藏+在看！ 1.LeetCodeAnimation Github 地址： https://github.com/MisterBooo/LeetCodeAnimation Star: 29.0k (11,492 stars this month) 介绍: Demonstrate all the questions on LeetCode in the form of animation.（用动画的形式呈现解LeetCode题目的思路）。 2.CS-Notes Github 地址：https://github.com/CyC2018/CS-Notes Star: 64.4k (5513 stars this month) 介绍: 技术面试必备基础知识、Leetcode 题解、后端面试、Java 面试、春招、秋招、操作系统、计算机网络、系统设计。 3.JavaGuide Github 地址：https://github.com/Snailclimb/JavaGuide Star: 42.0k (4,442 stars this month) 介绍: 【Java 学习+面试指南】 一份涵盖大部分 Java 程序员所需要掌握的核心知识。 4.mall Github 地址： https://github.com/macrozheng/mall star: 14.6 k (3,086 stars this month) 介绍: mall 项目是一套电商系统，包括前台商城系统及后台管理系统，基于 SpringBoot+MyBatis 实现。 前台商城系统包含首页门户、商品推荐、商品搜索、商品展示、购物车、订单流程、会员中心、客户服务、帮助中心等模块。 后台管理系统包含商品管理、订单管理、会员管理、促销管理、运营管理、内容管理、统计报表、财务管理、权限管理、设置等模块。 5.advanced-java Github 地址：https://github.com/doocs/advanced-java Star: 20.8k (2,394 stars this month) 介绍: 互联网 Java 工程师进阶知识完全扫盲。 6.spring-boot Github地址： https://github.com/spring-projects/spring-boot star: 38.5k (1,339 stars this month) 介绍： 虽然Spring的组件代码是轻量级的，但它的配置却是重量级的（需要大量XML配置）,不过Spring Boot 让这一切成为了过去。 另外Spring Cloud也是基于Spring Boot构建的，我个人非常有必要学习一下。 Spring Boot官方的介绍： Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications that you can “just run”…Most Spring Boot applications need very little Spring configuration.(Spring Boot可以轻松创建独立的生产级基于Spring的应用程序,只要通过 “just run”（可能是run ‘Application’或java -jar 或 tomcat 或 maven插件run 或 shell脚本）便可以运行项目。大部分Spring Boot项目只需要少量的配置即可) 7. Java Github 地址：https://github.com/TheAlgorithms/Java Star:14.3k (1,334 stars this month) 介绍: All Algorithms implemented in Java。 8.server Github 地址：https://github.com/wildfirechat/server star: 2.2 k (1,275 stars this month) 介绍: 全开源即时通讯(IM)系统。 9.litemall Github 地址：https://github.com/linlinjava/litemall Star: 7.1k (1,114 stars this month) 介绍: 又一个小商城。litemall = Spring Boot后端 + Vue管理员前端 + 微信小程序用户前端 + Vue用户移动端。 10.Linkage-RecyclerView Github 地址：https://github.com/KunMinX/Linkage-RecyclerView Star: 10.0k (1,093 stars this month) 介绍: 即使不用饿了么订餐，也请务必收藏好该库！🔥 一行代码即可接入，二级联动订餐列表 - Even if you don't order food by PrubHub, be sure to collect this library, please! 🔥 This secondary linkage list widget can be accessed by only one line of code. Supporting by RecyclerView & AndroidX. 11.toBeTopJavaer Github 地址 : https://github.com/hollischuang/toBeTopJavaer Star: 3.3k (1,007 stars this month) 介绍: To Be Top Javaer - Java工程师成神之路 12.elasticsearch Github 地址 : https://github.com/elastic/elasticsearch Star: 48.0k (968 stars this month) 介绍: 开源，分布式，RESTful 搜索引擎。 13.java-design-patterns Github 地址 : https://github.com/iluwatar/java-design-patterns Star: 41.5k (955 stars this month) 介绍: Design patterns implemented in Java。 14.apollo Github 地址 : https://github.com/ctripcorp/apollo Star: 14.5k (927 stars this month) 介绍: Apollo（阿波罗）是携程框架部门研发的分布式配置中心，能够集中化管理应用不同环境、不同集群的配置，配置修改后能够实时推送到应用端，并且具备规范的权限、流程治理等特性，适用于微服务配置管理场景。 15.arthas Github地址：https://github.com/alibaba/arthas star: 13.5 k (933 stars this month) 介绍: Arthas 是Alibaba开源的Java诊断工具。 16.dubbo Github地址：https://github.com/apache/dubbo star: 26.9 k (769 stars this month) 介绍: Apache Dubbo是一个基于Java的高性能开源RPC框架。 17.DoraemonKit Github地址： https://github.com/didi/DoraemonKit Star: 8.5k (909 stars this month) 介绍: 简称 \"DoKit\" 。一款功能齐全的客户端（ iOS 、Android ）研发助手，你值得拥有。 18.halo Github地址： https://github.com/halo-dev/halo Star: 4.1k (829 stars this month) 介绍: Halo 可能是最好的 Java 博客系统。 19.seata Github 地址 : https://github.com/seata/seata star: 9.2 k (776 stars this month) 介绍: Seata 是一种易于使用，高性能，基于 Java 的开源分布式事务解决方案。 20.hutool Github地址：https://github.com/looly/hutool star: 5,3 k (812 stars this month) 介绍: Hutool是一个Java工具包，也只是一个工具包，它帮助我们简化每一行代码，减少每一个方法，让Java语言也可以“甜甜的”。Hutool最初是我项目中“util”包的一个整理，后来慢慢积累并加入更多非业务相关功能，并广泛学习其它开源项目精髓，经过自己整理修改，最终形成丰富的开源工具集。官网:https://www.hutool.cn/ 。 "},"zother6-JavaGuide/github-trending/2019-6.html":{"url":"zother6-JavaGuide/github-trending/2019-6.html","title":"2019 6","keywords":"","body":"1.CS-Notes Github 地址：https://github.com/CyC2018/CS-Notes Star: 69.8k 介绍: 技术面试必备基础知识、Leetcode 题解、后端面试、Java 面试、春招、秋招、操作系统、计算机网络、系统设计。 2.toBeTopJavaer Github 地址：https://github.com/hollischuang/toBeTopJavaer Star: 4.7k 介绍: To Be Top Javaer - Java工程师成神之路。 3.p3c Github 地址： https://github.com/alibaba/p3c Star: 16.6k 介绍: Alibaba Java Coding Guidelines pmd implements and IDE plugin。Eclipse 和 IDEA 上都有该插件，推荐使用！ 4.SpringCloudLearning Github 地址： https://github.com/forezp/SpringCloudLearning Star: 8.7k 介绍: 史上最简单的Spring Cloud教程源码。 5.dubbo Github地址：https://github.com/apache/dubbo star: 27.6 k 介绍: Apache Dubbo是一个基于Java的高性能开源RPC框架。 6.jeecg-boot Github地址： https://github.com/zhangdaiscott/jeecg-boot star: 3.3 k 介绍: 一款基于代码生成器的JAVA快速开发平台！全新架构前后端分离：SpringBoot 2.x，Ant Design&Vue，Mybatis，Shiro，JWT。强大的代码生成器让前后端代码一键生成，无需写任何代码，绝对是全栈开发福音！！ JeecgBoot的宗旨是提高UI能力的同时,降低前后分离的开发成本，JeecgBoot还独创在线开发模式，No代码概念，一系列在线智能开发：在线配置表单、在线配置报表、在线设计流程等等。 7.advanced-java Github 地址：https://github.com/doocs/advanced-java Star: 24.2k 介绍: 互联网 Java 工程师进阶知识完全扫盲：涵盖高并发、分布式、高可用、微服务等领域知识，后端同学必看，前端同学也可学习。 8.FEBS-Shiro Github 地址：https://github.com/wuyouzhuguli/FEBS-Shiro Star: 2.6k 介绍: Spring Boot 2.1.3，Shiro1.4.0 & Layui 2.5.4 权限管理系统。预览地址：http://49.234.20.223:8080/login。 9.SpringAll Github 地址: https://github.com/wuyouzhuguli/SpringAll Star: 5.4k 介绍: 循序渐进，学习Spring Boot、Spring Boot & Shiro、Spring Cloud、Spring Security & Spring Security OAuth2，博客Spring系列源码。 10.JavaGuide Github 地址：https://github.com/Snailclimb/JavaGuide Star: 47.2k 介绍: 【Java 学习+面试指南】 一份涵盖大部分 Java 程序员所需要掌握的核心知识。 11.vhr Github 地址：https://github.com/lenve/vhr Star: 4.9k 介绍: 微人事是一个前后端分离的人力资源管理系统，项目采用SpringBoot+Vue开发。 12. tutorials Github 地址：https://github.com/eugenp/tutorials star: 15.4 k 介绍: 该项目是一系列小而专注的教程 - 每个教程都涵盖 Java 生态系统中单一且定义明确的开发领域。 当然，它们的重点是 Spring Framework - Spring，Spring Boot 和 Spring Securiyt。 除了 Spring 之外，还有以下技术：核心 Java，Jackson，HttpClient，Guava。 13.EasyScheduler Github 地址：https://github.com/analysys/EasyScheduler star: 1.1 k 介绍: Easy Scheduler是一个分布式工作流任务调度系统，主要解决“复杂任务依赖但无法直接监控任务健康状态”的问题。Easy Scheduler以DAG方式组装任务，可以实时监控任务的运行状态。同时，它支持重试，重新运行等操作... 。https://analysys.github.io/easyscheduler_docs_cn/ 14.thingsboard Github 地址：https://github.com/thingsboard/thingsboard star: 3.7 k 介绍: 开源物联网平台 - 设备管理，数据收集，处理和可视化。 https://thingsboard.io 15.mall-learning Github 地址: https://github.com/macrozheng/mall-learning star: 0.6 k 介绍: mall学习教程，架构、业务、技术要点全方位解析。mall项目（16k+star）是一套电商系统，使用现阶段主流技术实现。 涵盖了SpringBoot2.1.3、MyBatis3.4.6、Elasticsearch6.2.2、RabbitMQ3.7.15、Redis3.2、Mongodb3.2、Mysql5.7等技术，采用Docker容器化部署。 https://github.com/macrozheng/mall 16. flink Github地址：https://github.com/apache/flink star: 9.3 k 介绍: Apache Flink是一个开源流处理框架，具有强大的流和批处理功能。 17.spring-cloud-kubernetes Github地址：https://github.com/spring-cloud/spring-cloud-kubernetes star: 1.4 k 介绍: Kubernetes 集成 Spring Cloud Discovery Client, Configuration, etc... 18.springboot-learning-example Github地址：https://github.com/JeffLi1993/springboot-learning-example star: 10.0 k 介绍: spring boot 实践学习案例，是 spring boot 初学者及核心技术巩固的最佳实践。 19.canal Github地址：https://github.com/alibaba/canal star: 9.3 k 介绍: 阿里巴巴 MySQL binlog 增量订阅&消费组件。 20.react-native-device-info Github地址：https://github.com/react-native-community/react-native-device-info star: 4.0 k 介绍: React Native iOS和Android的设备信息。 "},"zother6-JavaGuide/github-trending/JavaGithubTrending.html":{"url":"zother6-JavaGuide/github-trending/JavaGithubTrending.html","title":"Java Github Trending","keywords":"","body":" 2018 年 12 月 2019 年 1 月 2019 年 2 月 2019 年 3 月 2019 年 4 月 2019 年 5 月 2019 年 6 月 "},"zother6-JavaGuide/java/basic/Arrays,CollectionsCommonMethods.html":{"url":"zother6-JavaGuide/java/basic/Arrays,CollectionsCommonMethods.html","title":"Arrays Collections Common Methods","keywords":"","body":" Collections 工具类和 Arrays 工具类常见方法 Collections 排序操作 查找,替换操作 同步控制 Arrays类的常见操作 排序 : sort() 查找 : binarySearch() 比较: equals() 填充 : fill() 转列表 asList() 转字符串 toString() 复制 copyOf() Collections 工具类和 Arrays 工具类常见方法 Collections Collections 工具类常用方法: 排序 查找,替换操作 同步控制(不推荐，需要线程安全的集合类型时请考虑使用 JUC 包下的并发集合) 排序操作 void reverse(List list)//反转 void shuffle(List list)//随机排序 void sort(List list)//按自然排序的升序排序 void sort(List list, Comparator c)//定制排序，由Comparator控制排序逻辑 void swap(List list, int i , int j)//交换两个索引位置的元素 void rotate(List list, int distance)//旋转。当distance为正数时，将list后distance个元素整体移到前面。当distance为负数时，将 list的前distance个元素整体移到后面。 示例代码: ArrayList arrayList = new ArrayList(); arrayList.add(-1); arrayList.add(3); arrayList.add(3); arrayList.add(-5); arrayList.add(7); arrayList.add(4); arrayList.add(-9); arrayList.add(-7); System.out.println(\"原始数组:\"); System.out.println(arrayList); // void reverse(List list)：反转 Collections.reverse(arrayList); System.out.println(\"Collections.reverse(arrayList):\"); System.out.println(arrayList); Collections.rotate(arrayList, 4); System.out.println(\"Collections.rotate(arrayList, 4):\"); System.out.println(arrayList); // void sort(List list),按自然排序的升序排序 Collections.sort(arrayList); System.out.println(\"Collections.sort(arrayList):\"); System.out.println(arrayList); // void shuffle(List list),随机排序 Collections.shuffle(arrayList); System.out.println(\"Collections.shuffle(arrayList):\"); System.out.println(arrayList); // void swap(List list, int i , int j),交换两个索引位置的元素 Collections.swap(arrayList, 2, 5); System.out.println(\"Collections.swap(arrayList, 2, 5):\"); System.out.println(arrayList); // 定制排序的用法 Collections.sort(arrayList, new Comparator() { @Override public int compare(Integer o1, Integer o2) { return o2.compareTo(o1); } }); System.out.println(\"定制排序后：\"); System.out.println(arrayList); 查找,替换操作 int binarySearch(List list, Object key)//对List进行二分查找，返回索引，注意List必须是有序的 int max(Collection coll)//根据元素的自然顺序，返回最大的元素。 类比int min(Collection coll) int max(Collection coll, Comparator c)//根据定制排序，返回最大元素，排序规则由Comparatator类控制。类比int min(Collection coll, Comparator c) void fill(List list, Object obj)//用指定的元素代替指定list中的所有元素。 int frequency(Collection c, Object o)//统计元素出现次数 int indexOfSubList(List list, List target)//统计target在list中第一次出现的索引，找不到则返回-1，类比int lastIndexOfSubList(List source, list target). boolean replaceAll(List list, Object oldVal, Object newVal), 用新元素替换旧元素 示例代码： ArrayList arrayList = new ArrayList(); arrayList.add(-1); arrayList.add(3); arrayList.add(3); arrayList.add(-5); arrayList.add(7); arrayList.add(4); arrayList.add(-9); arrayList.add(-7); ArrayList arrayList2 = new ArrayList(); arrayList2.add(-3); arrayList2.add(-5); arrayList2.add(7); System.out.println(\"原始数组:\"); System.out.println(arrayList); System.out.println(\"Collections.max(arrayList):\"); System.out.println(Collections.max(arrayList)); System.out.println(\"Collections.min(arrayList):\"); System.out.println(Collections.min(arrayList)); System.out.println(\"Collections.replaceAll(arrayList, 3, -3):\"); Collections.replaceAll(arrayList, 3, -3); System.out.println(arrayList); System.out.println(\"Collections.frequency(arrayList, -3):\"); System.out.println(Collections.frequency(arrayList, -3)); System.out.println(\"Collections.indexOfSubList(arrayList, arrayList2):\"); System.out.println(Collections.indexOfSubList(arrayList, arrayList2)); System.out.println(\"Collections.binarySearch(arrayList, 7):\"); // 对List进行二分查找，返回索引，List必须是有序的 Collections.sort(arrayList); System.out.println(Collections.binarySearch(arrayList, 7)); 同步控制 Collections提供了多个synchronizedXxx()方法·，该方法可以将指定集合包装成线程同步的集合，从而解决多线程并发访问集合时的线程安全问题。 我们知道 HashSet，TreeSet，ArrayList,LinkedList,HashMap,TreeMap 都是线程不安全的。Collections提供了多个静态方法可以把他们包装成线程同步的集合。 最好不要用下面这些方法，效率非常低，需要线程安全的集合类型时请考虑使用 JUC 包下的并发集合。 方法如下： synchronizedCollection(Collection c) //返回指定 collection 支持的同步（线程安全的）collection。 synchronizedList(List list)//返回指定列表支持的同步（线程安全的）List。 synchronizedMap(Map m) //返回由指定映射支持的同步（线程安全的）Map。 synchronizedSet(Set s) //返回指定 set 支持的同步（线程安全的）set。 Collections还可以设置不可变集合，提供了如下三类方法： emptyXxx(): 返回一个空的、不可变的集合对象，此处的集合既可以是List，也可以是Set，还可以是Map。 singletonXxx(): 返回一个只包含指定对象（只有一个或一个元素）的不可变的集合对象，此处的集合可以是：List，Set，Map。 unmodifiableXxx(): 返回指定集合对象的不可变视图，此处的集合可以是：List，Set，Map。 上面三类方法的参数是原有的集合对象，返回值是该集合的”只读“版本。 示例代码： ArrayList arrayList = new ArrayList(); arrayList.add(-1); arrayList.add(3); arrayList.add(3); arrayList.add(-5); arrayList.add(7); arrayList.add(4); arrayList.add(-9); arrayList.add(-7); HashSet integers1 = new HashSet<>(); integers1.add(1); integers1.add(3); integers1.add(2); Map scores = new HashMap(); scores.put(\"语文\" , 80); scores.put(\"Java\" , 82); //Collections.emptyXXX();创建一个空的、不可改变的XXX对象 List list = Collections.emptyList(); System.out.println(list);//[] Set objects = Collections.emptySet(); System.out.println(objects);//[] Map objectObjectMap = Collections.emptyMap(); System.out.println(objectObjectMap);//{} //Collections.singletonXXX(); List> arrayLists = Collections.singletonList(arrayList); System.out.println(arrayLists);//[[-1, 3, 3, -5, 7, 4, -9, -7]] //创建一个只有一个元素，且不可改变的Set对象 Set> singleton = Collections.singleton(arrayList); System.out.println(singleton);//[[-1, 3, 3, -5, 7, 4, -9, -7]] Map nihao = Collections.singletonMap(\"1\", \"nihao\"); System.out.println(nihao);//{1=nihao} //unmodifiableXXX();创建普通XXX对象对应的不可变版本 List integers = Collections.unmodifiableList(arrayList); System.out.println(integers);//[-1, 3, 3, -5, 7, 4, -9, -7] Set integers2 = Collections.unmodifiableSet(integers1); System.out.println(integers2);//[1, 2, 3] Map objectObjectMap2 = Collections.unmodifiableMap(scores); System.out.println(objectObjectMap2);//{Java=82, 语文=80} //添加出现异常：java.lang.UnsupportedOperationException // list.add(1); // arrayLists.add(arrayList); // integers.add(1); Arrays类的常见操作 排序 : sort() 查找 : binarySearch() 比较: equals() 填充 : fill() 转列表: asList() 转字符串 : toString() 复制: copyOf() 排序 : sort() // *************排序 sort**************** int a[] = { 1, 3, 2, 7, 6, 5, 4, 9 }; // sort(int[] a)方法按照数字顺序排列指定的数组。 Arrays.sort(a); System.out.println(\"Arrays.sort(a):\"); for (int i : a) { System.out.print(i); } // 换行 System.out.println(); // sort(int[] a,int fromIndex,int toIndex)按升序排列数组的指定范围 int b[] = { 1, 3, 2, 7, 6, 5, 4, 9 }; Arrays.sort(b, 2, 6); System.out.println(\"Arrays.sort(b, 2, 6):\"); for (int i : b) { System.out.print(i); } // 换行 System.out.println(); int c[] = { 1, 3, 2, 7, 6, 5, 4, 9 }; // parallelSort(int[] a) 按照数字顺序排列指定的数组(并行的)。同sort方法一样也有按范围的排序 Arrays.parallelSort(c); System.out.println(\"Arrays.parallelSort(c)：\"); for (int i : c) { System.out.print(i); } // 换行 System.out.println(); // parallelSort给字符数组排序，sort也可以 char d[] = { 'a', 'f', 'b', 'c', 'e', 'A', 'C', 'B' }; Arrays.parallelSort(d); System.out.println(\"Arrays.parallelSort(d)：\"); for (char d2 : d) { System.out.print(d2); } // 换行 System.out.println(); 在做算法面试题的时候，我们还可能会经常遇到对字符串排序的情况,Arrays.sort() 对每个字符串的特定位置进行比较，然后按照升序排序。 String[] strs = { \"abcdehg\", \"abcdefg\", \"abcdeag\" }; Arrays.sort(strs); System.out.println(Arrays.toString(strs));//[abcdeag, abcdefg, abcdehg] 查找 : binarySearch() // *************查找 binarySearch()**************** char[] e = { 'a', 'f', 'b', 'c', 'e', 'A', 'C', 'B' }; // 排序后再进行二分查找，否则找不到 Arrays.sort(e); System.out.println(\"Arrays.sort(e)\" + Arrays.toString(e)); System.out.println(\"Arrays.binarySearch(e, 'c')：\"); int s = Arrays.binarySearch(e, 'c'); System.out.println(\"字符c在数组的位置：\" + s); 比较: equals() // *************比较 equals**************** char[] e = { 'a', 'f', 'b', 'c', 'e', 'A', 'C', 'B' }; char[] f = { 'a', 'f', 'b', 'c', 'e', 'A', 'C', 'B' }; /* * 元素数量相同，并且相同位置的元素相同。 另外，如果两个数组引用都是null，则它们被认为是相等的 。 */ // 输出true System.out.println(\"Arrays.equals(e, f):\" + Arrays.equals(e, f)); 填充 : fill() // *************填充fill(批量初始化)**************** int[] g = { 1, 2, 3, 3, 3, 3, 6, 6, 6 }; // 数组中所有元素重新分配值 Arrays.fill(g, 3); System.out.println(\"Arrays.fill(g, 3)：\"); // 输出结果：333333333 for (int i : g) { System.out.print(i); } // 换行 System.out.println(); int[] h = { 1, 2, 3, 3, 3, 3, 6, 6, 6, }; // 数组中指定范围元素重新分配值 Arrays.fill(h, 0, 2, 9); System.out.println(\"Arrays.fill(h, 0, 2, 9);：\"); // 输出结果：993333666 for (int i : h) { System.out.print(i); } 转列表 asList() // *************转列表 asList()**************** /* * 返回由指定数组支持的固定大小的列表。 * （将返回的列表更改为“写入数组”。）该方法作为基于数组和基于集合的API之间的桥梁，与Collection.toArray()相结合 。 * 返回的列表是可序列化的，并实现RandomAccess 。 * 此方法还提供了一种方便的方式来创建一个初始化为包含几个元素的固定大小的列表如下： */ List stooges = Arrays.asList(\"Larry\", \"Moe\", \"Curly\"); System.out.println(stooges); 转字符串 toString() // *************转字符串 toString()**************** /* * 返回指定数组的内容的字符串表示形式。 */ char[] k = { 'a', 'f', 'b', 'c', 'e', 'A', 'C', 'B' }; System.out.println(Arrays.toString(k));// [a, f, b, c, e, A, C, B] 复制 copyOf() // *************复制 copy**************** // copyOf 方法实现数组复制,h为数组，6为复制的长度 int[] h = { 1, 2, 3, 3, 3, 3, 6, 6, 6, }; int i[] = Arrays.copyOf(h, 6); System.out.println(\"Arrays.copyOf(h, 6);：\"); // 输出结果：123333 for (int j : i) { System.out.print(j); } // 换行 System.out.println(); // copyOfRange将指定数组的指定范围复制到新数组中 int j[] = Arrays.copyOfRange(h, 6, 11); System.out.println(\"Arrays.copyOfRange(h, 6, 11)：\"); // 输出结果66600(h数组只有9个元素这里是从索引6到索引11复制所以不足的就为0) for (int j2 : j) { System.out.print(j2); } // 换行 System.out.println(); "},"zother6-JavaGuide/java/basic/final,static,this,super.html":{"url":"zother6-JavaGuide/java/basic/final,static,this,super.html","title":"Final Static This Super","keywords":"","body":" final,static,this,super 关键字总结 final 关键字 static 关键字 this 关键字 super 关键字 参考 static 关键字详解 static 关键字主要有以下四种使用场景 修饰成员变量和成员方法(常用) 静态代码块 静态内部类 静态导包 补充内容 静态方法与非静态方法 static{}静态代码块与{}非静态代码块(构造代码块) 参考 final,static,this,super 关键字总结 final 关键字 final关键字主要用在三个地方：变量、方法、类。 对于一个final变量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改；如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象。 当用final修饰一个类时，表明这个类不能被继承。final类中的所有成员方法都会被隐式地指定为final方法。 使用final方法的原因有两个。第一个原因是把方法锁定，以防任何继承类修改它的含义；第二个原因是效率。在早期的Java实现版本中，会将final方法转为内嵌调用。但是如果方法过于庞大，可能看不到内嵌调用带来的任何性能提升（现在的Java版本已经不需要使用final方法进行这些优化了）。类中所有的private方法都隐式地指定为final。 static 关键字 static 关键字主要有以下四种使用场景： 修饰成员变量和成员方法: 被 static 修饰的成员属于类，不属于单个这个类的某个对象，被类中所有对象共享，可以并且建议通过类名调用。被static 声明的成员变量属于静态成员变量，静态变量 存放在 Java 内存区域的方法区。调用格式：类名.静态变量名 类名.静态方法名() 静态代码块: 静态代码块定义在类中方法外, 静态代码块在非静态代码块之前执行(静态代码块—>非静态代码块—>构造方法)。 该类不管创建多少对象，静态代码块只执行一次. 静态内部类（static修饰类的话只能修饰内部类）： 静态内部类与非静态内部类之间存在一个最大的区别: 非静态内部类在编译完成之后会隐含地保存着一个引用，该引用是指向创建它的外围类，但是静态内部类却没有。没有这个引用就意味着：1. 它的创建是不需要依赖外围类的创建。2. 它不能使用任何外围类的非static成员变量和方法。 静态导包(用来导入类中的静态资源，1.5之后的新特性): 格式为：import static 这两个关键字连用可以指定导入某个类中的指定静态资源，并且不需要使用类名调用类中静态成员，可以直接使用类中静态成员变量和成员方法。 this 关键字 this关键字用于引用类的当前实例。 例如： class Manager { Employees[] employees; void manageEmployees() { int totalEmp = this.employees.length; System.out.println(\"Total employees: \" + totalEmp); this.report(); } void report() { } } 在上面的示例中，this关键字用于两个地方： this.employees.length：访问类Manager的当前实例的变量。 this.report（）：调用类Manager的当前实例的方法。 此关键字是可选的，这意味着如果上面的示例在不使用此关键字的情况下表现相同。 但是，使用此关键字可能会使代码更易读或易懂。 super 关键字 super关键字用于从子类访问父类的变量和方法。 例如： public class Super { protected int number; protected showNumber() { System.out.println(\"number = \" + number); } } public class Sub extends Super { void bar() { super.number = 10; super.showNumber(); } } 在上面的例子中，Sub 类访问父类成员变量 number 并调用其其父类 Super 的 showNumber（） 方法。 使用 this 和 super 要注意的问题： 在构造器中使用 super（） 调用父类中的其他构造方法时，该语句必须处于构造器的首行，否则编译器会报错。另外，this 调用本类中的其他构造方法时，也要放在首行。 this、super不能用在static方法中。 简单解释一下： 被 static 修饰的成员属于类，不属于单个这个类的某个对象，被类中所有对象共享。而 this 代表对本类对象的引用，指向本类对象；而 super 代表对父类对象的引用，指向父类对象；所以， this和super是属于对象范畴的东西，而静态方法是属于类范畴的东西。 参考 https://www.codejava.net/java-core/the-java-language/java-keywords https://blog.csdn.net/u013393958/article/details/79881037 static 关键字详解 static 关键字主要有以下四种使用场景 修饰成员变量和成员方法 静态代码块 修饰类(只能修饰内部类) 静态导包(用来导入类中的静态资源，1.5之后的新特性) 修饰成员变量和成员方法(常用) 被 static 修饰的成员属于类，不属于单个这个类的某个对象，被类中所有对象共享，可以并且建议通过类名调用。被static 声明的成员变量属于静态成员变量，静态变量 存放在 Java 内存区域的方法区。 方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。 HotSpot 虚拟机中方法区也常被称为 “永久代”，本质上两者并不等价。仅仅是因为 HotSpot 虚拟机设计团队用永久代来实现方法区而已，这样 HotSpot 虚拟机的垃圾收集器就可以像管理 Java 堆一样管理这部分内存了。但是这并不是一个好主意，因为这样更容易遇到内存溢出问题。 调用格式： 类名.静态变量名 类名.静态方法名() 如果变量或者方法被 private 则代表该属性或者该方法只能在类的内部被访问而不能在类的外部被访问。 测试方法： public class StaticBean { String name; //静态变量 static int age; public StaticBean(String name) { this.name = name; } //静态方法 static void SayHello() { System.out.println(\"Hello i am java\"); } @Override public String toString() { return \"StaticBean{\"+ \"name=\" + name + \",age=\" + age + \"}\"; } } public class StaticDemo { public static void main(String[] args) { StaticBean staticBean = new StaticBean(\"1\"); StaticBean staticBean2 = new StaticBean(\"2\"); StaticBean staticBean3 = new StaticBean(\"3\"); StaticBean staticBean4 = new StaticBean(\"4\"); StaticBean.age = 33; System.out.println(staticBean + \" \" + staticBean2 + \" \" + staticBean3 + \" \" + staticBean4); //StaticBean{name=1,age=33} StaticBean{name=2,age=33} StaticBean{name=3,age=33} StaticBean{name=4,age=33} StaticBean.SayHello();//Hello i am java } } 静态代码块 静态代码块定义在类中方法外, 静态代码块在非静态代码块之前执行(静态代码块—非静态代码块—构造方法)。 该类不管创建多少对象，静态代码块只执行一次. 静态代码块的格式是 static { 语句体; } 一个类中的静态代码块可以有多个，位置可以随便放，它不在任何的方法体内，JVM加载类时会执行这些静态的代码块，如果静态代码块有多个，JVM将按照它们在类中出现的先后顺序依次执行它们，每个代码块只会被执行一次。 静态代码块对于定义在它之后的静态变量，可以赋值，但是不能访问. 静态内部类 静态内部类与非静态内部类之间存在一个最大的区别，我们知道非静态内部类在编译完成之后会隐含地保存着一个引用，该引用是指向创建它的外围类，但是静态内部类却没有。没有这个引用就意味着： 它的创建是不需要依赖外围类的创建。 它不能使用任何外围类的非static成员变量和方法。 Example（静态内部类实现单例模式） public class Singleton { //声明为 private 避免调用默认构造方法创建对象 private Singleton() { } // 声明为 private 表明静态内部该类只能在该 Singleton 类中被访问 private static class SingletonHolder { private static final Singleton INSTANCE = new Singleton(); } public static Singleton getUniqueInstance() { return SingletonHolder.INSTANCE; } } 当 Singleton 类加载时，静态内部类 SingletonHolder 没有被加载进内存。只有当调用 getUniqueInstance()方法从而触发 SingletonHolder.INSTANCE 时 SingletonHolder 才会被加载，此时初始化 INSTANCE 实例，并且 JVM 能确保 INSTANCE 只被实例化一次。 这种方式不仅具有延迟初始化的好处，而且由 JVM 提供了对线程安全的支持。 静态导包 格式为：import static 这两个关键字连用可以指定导入某个类中的指定静态资源，并且不需要使用类名调用类中静态成员，可以直接使用类中静态成员变量和成员方法 //将Math中的所有静态资源导入，这时候可以直接使用里面的静态方法，而不用通过类名进行调用 //如果只想导入单一某个静态方法，只需要将换成对应的方法名即可 import static java.lang.Math.*;//换成import static java.lang.Math.max;具有一样的效果 public class Demo { public static void main(String[] args) { int max = max(1,2); System.out.println(max); } } 补充内容 静态方法与非静态方法 静态方法属于类本身，非静态方法属于从该类生成的每个对象。 如果您的方法执行的操作不依赖于其类的各个变量和方法，请将其设置为静态（这将使程序的占用空间更小）。 否则，它应该是非静态的。 Example class Foo { int i; public Foo(int i) { this.i = i; } public static String method1() { return \"An example string that doesn't depend on i (an instance variable)\"; } public int method2() { return this.i + 1; //Depends on i } } 你可以像这样调用静态方法：Foo.method1()。 如果您尝试使用这种方法调用 method2 将失败。 但这样可行：Foo bar = new Foo(1);bar.method2(); 总结： 在外部调用静态方法时，可以使用”类名.方法名”的方式，也可以使用”对象名.方法名”的方式。而实例方法只有后面这种方式。也就是说，调用静态方法可以无需创建对象。 静态方法在访问本类的成员时，只允许访问静态成员（即静态成员变量和静态方法），而不允许访问实例成员变量和实例方法；实例方法则无此限制 static{}静态代码块与{}非静态代码块(构造代码块) 相同点： 都是在JVM加载类时且在构造方法执行之前执行，在类中都可以定义多个，定义多个时按定义的顺序执行，一般在代码块中对一些static变量进行赋值。 不同点： 静态代码块在非静态代码块之前执行(静态代码块—非静态代码块—构造方法)。静态代码块只在第一次new执行一次，之后不再执行，而非静态代码块在每new一次就执行一次。 非静态代码块可在普通方法中定义(不过作用不大)；而静态代码块不行。 修正 issue #677：静态代码块可能在第一次new的时候执行，但不一定只在第一次new的时候执行。比如通过 Class.forName(\"ClassDemo\")创建 Class 对象的时候也会执行。 一般情况下,如果有些代码比如一些项目最常用的变量或对象必须在项目启动的时候就执行的时候,需要使用静态代码块,这种代码是主动执行的。如果我们想要设计不需要创建对象就可以调用类中的方法，例如：Arrays类，Character类，String类等，就需要使用静态方法, 两者的区别是 静态代码块是自动执行的而静态方法是被调用的时候才执行的. Example： public class Test { public Test() { System.out.print(\"默认构造方法！--\"); } //非静态代码块 { System.out.print(\"非静态代码块！--\"); } //静态代码块 static { System.out.print(\"静态代码块！--\"); } private static void test() { System.out.print(\"静态方法中的内容! --\"); { System.out.print(\"静态方法中的代码块！--\"); } } public static void main(String[] args) { Test test = new Test(); Test.test();//静态代码块！--静态方法中的内容! --静态方法中的代码块！-- } } 上述代码输出： 静态代码块！--非静态代码块！--默认构造方法！--静态方法中的内容! --静态方法中的代码块！-- 当只执行 Test.test(); 时输出： 静态代码块！--静态方法中的内容! --静态方法中的代码块！-- 当只执行 Test test = new Test(); 时输出： 静态代码块！--非静态代码块！--默认构造方法！-- 非静态代码块与构造函数的区别是： 非静态代码块是给所有对象进行统一初始化，而构造函数是给对应的对象初始化，因为构造函数是可以多个的，运行哪个构造函数就会建立什么样的对象，但无论建立哪个对象，都会先执行相同的构造代码块。也就是说，构造代码块中定义的是不同对象共性的初始化内容。 参考 httpsblog.csdn.netchen13579867831articledetails78995480 httpwww.cnblogs.comchenssyp3388487.html httpwww.cnblogs.comQian123p5713440.html "},"zother6-JavaGuide/java/basic/reflection.html":{"url":"zother6-JavaGuide/java/basic/reflection.html","title":"Reflection","keywords":"","body":"反射机制介绍 JAVA 反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为 java 语言的反射机制。 获取 Class 对象的两种方式 如果我们动态获取到这些信息，我们需要依靠 Class 对象。Class 类对象将一个类的方法、变量等信息告诉运行的程序。Java 提供了两种方式获取 Class 对象: 1.知道具体类的情况下可以使用： Class alunbarClass = TargetObject.class; 但是我们一般是不知道具体类的，基本都是通过遍历包下面的类来获取 Class 对象 2.通过 Class.forName()传入类的路径获取： Class alunbarClass1 = Class.forName(\"cn.javaguide.TargetObject\"); 代码实例 简单用代码演示一下反射的一些操作! 1.创建一个我们要使用反射操作的类 TargetObject： package cn.javaguide; public class TargetObject { private String value; public TargetObject() { value = \"JavaGuide\"; } public void publicMethod(String s) { System.out.println(\"I love \" + s); } private void privateMethod() { System.out.println(\"value is \" + value); } } 2.使用反射操作这个类的方法以及参数 package cn.javaguide; import java.lang.reflect.Field; import java.lang.reflect.InvocationTargetException; import java.lang.reflect.Method; public class Main { public static void main(String[] args) throws ClassNotFoundException, NoSuchMethodException, IllegalAccessException, InstantiationException, InvocationTargetException, NoSuchFieldException { /** * 获取TargetObject类的Class对象并且创建TargetObject类实例 */ Class tagetClass = Class.forName(\"cn.javaguide.TargetObject\"); TargetObject targetObject = (TargetObject) tagetClass.newInstance(); /** * 获取所有类中所有定义的方法 */ Method[] methods = tagetClass.getDeclaredMethods(); for (Method method : methods) { System.out.println(method.getName()); } /** * 获取指定方法并调用 */ Method publicMethod = tagetClass.getDeclaredMethod(\"publicMethod\", String.class); publicMethod.invoke(targetObject, \"JavaGuide\"); /** * 获取指定参数并对参数进行修改 */ Field field = tagetClass.getDeclaredField(\"value\"); //为了对类中的参数进行修改我们取消安全检查 field.setAccessible(true); field.set(targetObject, \"JavaGuide\"); /** * 调用 private 方法 */ Method privateMethod = tagetClass.getDeclaredMethod(\"privateMethod\"); //为了调用private方法我们取消安全检查 privateMethod.setAccessible(true); privateMethod.invoke(targetObject); } } 输出内容： publicMethod privateMethod I love JavaGuide value is JavaGuide 静态编译和动态编译 静态编译：在编译时确定类型，绑定对象 动态编译：运行时确定类型，绑定对象 反射机制优缺点 优点： 运行期类型的判断，动态加载类，提高代码灵活度。 缺点： 1,性能瓶颈：反射相当于一系列解释操作，通知 JVM 要做的事情，性能比直接的 java 代码要慢很多。2,安全问题，让我们可以动态操作改变类的属性同时也增加了类的安全隐患。 反射的应用场景 反射是框架设计的灵魂。 在我们平时的项目开发过程中，基本上很少会直接使用到反射机制，但这不能说明反射机制没有用，实际上有很多设计、开发都与反射机制有关，例如模块化的开发，通过反射去调用对应的字节码；动态代理设计模式也采用了反射机制，还有我们日常使用的 Spring／Hibernate 等框架也大量使用到了反射机制。 举例： 我们在使用 JDBC 连接数据库时使用 Class.forName()通过反射加载数据库的驱动程序； Spring 框架的 IOC（动态加载管理 Bean）创建对象以及 AOP（动态代理）功能都和反射有联系； 动态配置实例的属性； ...... 推荐阅读： Java反射使用总结 Reflection：Java 反射机制的应用场景 Java 基础之—反射（非常重要） "},"zother6-JavaGuide/java/basic/用好Java中的枚举真的没有那么简单.html":{"url":"zother6-JavaGuide/java/basic/用好Java中的枚举真的没有那么简单.html","title":"用好Java中的枚举真的没有那么简单","keywords":"","body":" 最近重看 Java 枚举，看到这篇觉得还不错的文章，于是简单翻译和完善了一些内容，分享给大家，希望你们也能有所收获。另外，不要忘了文末还有补充哦！ ps: 这里发一篇枚举的文章，也是因为后面要发一篇非常实用的关于 SpringBoot 全局异常处理的比较好的实践，里面就用到了枚举。 这篇文章由 JavaGuide 翻译，公众号: JavaGuide,原文地址：https://www.baeldung.com/a-guide-to-java-enums 。 转载请注明上面这段文字。 1.概览 在本文中，我们将看到什么是 Java 枚举，它们解决了哪些问题以及如何在实践中使用 Java 枚举实现一些设计模式。 enum关键字在 java5 中引入，表示一种特殊类型的类，其总是继承java.lang.Enum类，更多内容可以自行查看其官方文档。 枚举在很多时候会和常量拿来对比，可能因为本身我们大量实际使用枚举的地方就是为了替代常量。那么这种方式由什么优势呢？ 以这种方式定义的常量使代码更具可读性，允许进行编译时检查，预先记录可接受值的列表，并避免由于传入无效值而引起的意外行为。 下面示例定义一个简单的枚举类型 pizza 订单的状态，共有三种 ORDERED, READY, DELIVERED状态: package shuang.kou.enumdemo.enumtest; public enum PizzaStatus { ORDERED, READY, DELIVERED; } 简单来说，我们通过上面的代码避免了定义常量，我们将所有和 pizza 订单的状态的常量都统一放到了一个枚举类型里面。 System.out.println(PizzaStatus.ORDERED.name());//ORDERED System.out.println(PizzaStatus.ORDERED);//ORDERED System.out.println(PizzaStatus.ORDERED.name().getClass());//class java.lang.String System.out.println(PizzaStatus.ORDERED.getClass());//class shuang.kou.enumdemo.enumtest.PizzaStatus 2.自定义枚举方法 现在我们对枚举是什么以及如何使用它们有了基本的了解，让我们通过在枚举上定义一些额外的API方法，将上一个示例提升到一个新的水平： public class Pizza { private PizzaStatus status; public enum PizzaStatus { ORDERED, READY, DELIVERED; } public boolean isDeliverable() { if (getStatus() == PizzaStatus.READY) { return true; } return false; } // Methods that set and get the status variable. } 3.使用 == 比较枚举类型 由于枚举类型确保JVM中仅存在一个常量实例，因此我们可以安全地使用“ ==”运算符比较两个变量，如上例所示；此外，“ ==”运算符可提供编译时和运行时的安全性。 首先，让我们看一下以下代码段中的运行时安全性，其中“ ==”运算符用于比较状态，并且如果两个值均为null 都不会引发 NullPointerException。相反，如果使用equals方法，将抛出 NullPointerException： if(testPz.getStatus().equals(Pizza.PizzaStatus.DELIVERED)); if(testPz.getStatus() == Pizza.PizzaStatus.DELIVERED); 对于编译时安全性，我们看另一个示例，两个不同枚举类型进行比较，使用equal方法比较结果确定为true，因为getStatus方法的枚举值与另一个类型枚举值一致，但逻辑上应该为false。这个问题可以使用==操作符避免。因为编译器会表示类型不兼容错误： if(testPz.getStatus().equals(TestColor.GREEN)); if(testPz.getStatus() == TestColor.GREEN); 4.在 switch 语句中使用枚举类型 public int getDeliveryTimeInDays() { switch (status) { case ORDERED: return 5; case READY: return 2; case DELIVERED: return 0; } return 0; } 5.枚举类型的属性,方法和构造函数 文末有我(JavaGuide)的补充。 你可以通过在枚举类型中定义属性,方法和构造函数让它变得更加强大。 下面，让我们扩展上面的示例，实现从比萨的一个阶段到另一个阶段的过渡，并了解如何摆脱之前使用的if语句和switch语句： public class Pizza { private PizzaStatus status; public enum PizzaStatus { ORDERED (5){ @Override public boolean isOrdered() { return true; } }, READY (2){ @Override public boolean isReady() { return true; } }, DELIVERED (0){ @Override public boolean isDelivered() { return true; } }; private int timeToDelivery; public boolean isOrdered() {return false;} public boolean isReady() {return false;} public boolean isDelivered(){return false;} public int getTimeToDelivery() { return timeToDelivery; } PizzaStatus (int timeToDelivery) { this.timeToDelivery = timeToDelivery; } } public boolean isDeliverable() { return this.status.isReady(); } public void printTimeToDeliver() { System.out.println(\"Time to delivery is \" + this.getStatus().getTimeToDelivery()); } // Methods that set and get the status variable. } 下面这段代码展示它是如何 work 的： @Test public void givenPizaOrder_whenReady_thenDeliverable() { Pizza testPz = new Pizza(); testPz.setStatus(Pizza.PizzaStatus.READY); assertTrue(testPz.isDeliverable()); } 6.EnumSet and EnumMap 6.1. EnumSet EnumSet 是一种专门为枚举类型所设计的 Set 类型。 与HashSet相比，由于使用了内部位向量表示，因此它是特定 Enum 常量集的非常有效且紧凑的表示形式。 它提供了类型安全的替代方法，以替代传统的基于int的“位标志”，使我们能够编写更易读和易于维护的简洁代码。 EnumSet 是抽象类，其有两个实现：RegularEnumSet 、JumboEnumSet，选择哪一个取决于实例化时枚举中常量的数量。 在很多场景中的枚举常量集合操作（如：取子集、增加、删除、containsAll和removeAll批操作）使用EnumSet非常合适；如果需要迭代所有可能的常量则使用Enum.values()。 public class Pizza { private static EnumSet undeliveredPizzaStatuses = EnumSet.of(PizzaStatus.ORDERED, PizzaStatus.READY); private PizzaStatus status; public enum PizzaStatus { ... } public boolean isDeliverable() { return this.status.isReady(); } public void printTimeToDeliver() { System.out.println(\"Time to delivery is \" + this.getStatus().getTimeToDelivery() + \" days\"); } public static List getAllUndeliveredPizzas(List input) { return input.stream().filter( (s) -> undeliveredPizzaStatuses.contains(s.getStatus())) .collect(Collectors.toList()); } public void deliver() { if (isDeliverable()) { PizzaDeliverySystemConfiguration.getInstance().getDeliveryStrategy() .deliver(this); this.setStatus(PizzaStatus.DELIVERED); } } // Methods that set and get the status variable. } 下面的测试演示了展示了 EnumSet 在某些场景下的强大功能： @Test public void givenPizaOrders_whenRetrievingUnDeliveredPzs_thenCorrectlyRetrieved() { List pzList = new ArrayList<>(); Pizza pz1 = new Pizza(); pz1.setStatus(Pizza.PizzaStatus.DELIVERED); Pizza pz2 = new Pizza(); pz2.setStatus(Pizza.PizzaStatus.ORDERED); Pizza pz3 = new Pizza(); pz3.setStatus(Pizza.PizzaStatus.ORDERED); Pizza pz4 = new Pizza(); pz4.setStatus(Pizza.PizzaStatus.READY); pzList.add(pz1); pzList.add(pz2); pzList.add(pz3); pzList.add(pz4); List undeliveredPzs = Pizza.getAllUndeliveredPizzas(pzList); assertTrue(undeliveredPzs.size() == 3); } 6.2. EnumMap EnumMap是一个专门化的映射实现，用于将枚举常量用作键。与对应的 HashMap 相比，它是一个高效紧凑的实现，并且在内部表示为一个数组: EnumMap map; 让我们快速看一个真实的示例，该示例演示如何在实践中使用它： public static EnumMap> groupPizzaByStatus(List pizzaList) { EnumMap> pzByStatus = new EnumMap>(PizzaStatus.class); for (Pizza pz : pizzaList) { PizzaStatus status = pz.getStatus(); if (pzByStatus.containsKey(status)) { pzByStatus.get(status).add(pz); } else { List newPzList = new ArrayList(); newPzList.add(pz); pzByStatus.put(status, newPzList); } } return pzByStatus; } 下面的测试演示了展示了 EnumMap 在某些场景下的强大功能： @Test public void givenPizaOrders_whenGroupByStatusCalled_thenCorrectlyGrouped() { List pzList = new ArrayList<>(); Pizza pz1 = new Pizza(); pz1.setStatus(Pizza.PizzaStatus.DELIVERED); Pizza pz2 = new Pizza(); pz2.setStatus(Pizza.PizzaStatus.ORDERED); Pizza pz3 = new Pizza(); pz3.setStatus(Pizza.PizzaStatus.ORDERED); Pizza pz4 = new Pizza(); pz4.setStatus(Pizza.PizzaStatus.READY); pzList.add(pz1); pzList.add(pz2); pzList.add(pz3); pzList.add(pz4); EnumMap> map = Pizza.groupPizzaByStatus(pzList); assertTrue(map.get(Pizza.PizzaStatus.DELIVERED).size() == 1); assertTrue(map.get(Pizza.PizzaStatus.ORDERED).size() == 2); assertTrue(map.get(Pizza.PizzaStatus.READY).size() == 1); } 7. 通过枚举实现一些设计模式 7.1 单例模式 通常，使用类实现 Singleton 模式并非易事，枚举提供了一种实现单例的简便方法。 《Effective Java 》和《Java与模式》都非常推荐这种方式，使用这种方式方式实现枚举可以有什么好处呢？ 《Effective Java》 这种方法在功能上与公有域方法相近，但是它更加简洁，无偿提供了序列化机制，绝对防止多次实例化，即使是在面对复杂序列化或者反射攻击的时候。虽然这种方法还没有广泛采用，但是单元素的枚举类型已经成为实现 Singleton的最佳方法。 —-《Effective Java 中文版 第二版》 《Java与模式》 《Java与模式》中，作者这样写道，使用枚举来实现单实例控制会更加简洁，而且无偿地提供了序列化机制，并由JVM从根本上提供保障，绝对防止多次实例化，是更简洁、高效、安全的实现单例的方式。 下面的代码段显示了如何使用枚举实现单例模式： public enum PizzaDeliverySystemConfiguration { INSTANCE; PizzaDeliverySystemConfiguration() { // Initialization configuration which involves // overriding defaults like delivery strategy } private PizzaDeliveryStrategy deliveryStrategy = PizzaDeliveryStrategy.NORMAL; public static PizzaDeliverySystemConfiguration getInstance() { return INSTANCE; } public PizzaDeliveryStrategy getDeliveryStrategy() { return deliveryStrategy; } } 如何使用呢？请看下面的代码： PizzaDeliveryStrategy deliveryStrategy = PizzaDeliverySystemConfiguration.getInstance().getDeliveryStrategy(); 通过 PizzaDeliverySystemConfiguration.getInstance() 获取的就是单例的 PizzaDeliverySystemConfiguration 7.2 策略模式 通常，策略模式由不同类实现同一个接口来实现的。 这也就意味着添加新策略意味着添加新的实现类。使用枚举，可以轻松完成此任务，添加新的实现意味着只定义具有某个实现的另一个实例。 下面的代码段显示了如何使用枚举实现策略模式： public enum PizzaDeliveryStrategy { EXPRESS { @Override public void deliver(Pizza pz) { System.out.println(\"Pizza will be delivered in express mode\"); } }, NORMAL { @Override public void deliver(Pizza pz) { System.out.println(\"Pizza will be delivered in normal mode\"); } }; public abstract void deliver(Pizza pz); } 给 Pizza增加下面的方法： public void deliver() { if (isDeliverable()) { PizzaDeliverySystemConfiguration.getInstance().getDeliveryStrategy() .deliver(this); this.setStatus(PizzaStatus.DELIVERED); } } 如何使用呢？请看下面的代码： @Test public void givenPizaOrder_whenDelivered_thenPizzaGetsDeliveredAndStatusChanges() { Pizza pz = new Pizza(); pz.setStatus(Pizza.PizzaStatus.READY); pz.deliver(); assertTrue(pz.getStatus() == Pizza.PizzaStatus.DELIVERED); } 8. Java 8 与枚举 Pizza 类可以用Java 8重写，您可以看到方法 lambda 和Stream API如何使 getAllUndeliveredPizzas（）和groupPizzaByStatus（）方法变得如此简洁： getAllUndeliveredPizzas（）: public static List getAllUndeliveredPizzas(List input) { return input.stream().filter( (s) -> !deliveredPizzaStatuses.contains(s.getStatus())) .collect(Collectors.toList()); } groupPizzaByStatus（） : public static EnumMap> groupPizzaByStatus(List pzList) { EnumMap> map = pzList.stream().collect( Collectors.groupingBy(Pizza::getStatus, () -> new EnumMap<>(PizzaStatus.class), Collectors.toList())); return map; } 9. Enum 类型的 JSON 表现形式 使用Jackson库，可以将枚举类型的JSON表示为POJO。下面的代码段显示了可以用于同一目的的Jackson批注： @JsonFormat(shape = JsonFormat.Shape.OBJECT) public enum PizzaStatus { ORDERED (5){ @Override public boolean isOrdered() { return true; } }, READY (2){ @Override public boolean isReady() { return true; } }, DELIVERED (0){ @Override public boolean isDelivered() { return true; } }; private int timeToDelivery; public boolean isOrdered() {return false;} public boolean isReady() {return false;} public boolean isDelivered(){return false;} @JsonProperty(\"timeToDelivery\") public int getTimeToDelivery() { return timeToDelivery; } private PizzaStatus (int timeToDelivery) { this.timeToDelivery = timeToDelivery; } } 我们可以按如下方式使用 Pizza 和 PizzaStatus： Pizza pz = new Pizza(); pz.setStatus(Pizza.PizzaStatus.READY); System.out.println(Pizza.getJsonString(pz)); 生成 Pizza 状态以以下JSON展示： { \"status\" : { \"timeToDelivery\" : 2, \"ready\" : true, \"ordered\" : false, \"delivered\" : false }, \"deliverable\" : true } 有关枚举类型的JSON序列化/反序列化（包括自定义）的更多信息，请参阅Jackson-将枚举序列化为JSON对象。 10.总结 本文我们讨论了Java枚举类型，从基础知识到高级应用以及实际应用场景，让我们感受到枚举的强大功能。 11. 补充 我们在上面讲到了，我们可以通过在枚举类型中定义属性,方法和构造函数让它变得更加强大。 下面我通过一个实际的例子展示一下，当我们调用短信验证码的时候可能有几种不同的用途，我们在下面这样定义： public enum PinType { REGISTER(100000, \"注册使用\"), FORGET_PASSWORD(100001, \"忘记密码使用\"), UPDATE_PHONE_NUMBER(100002, \"更新手机号码使用\"); private final int code; private final String message; PinType(int code, String message) { this.code = code; this.message = message; } public int getCode() { return code; } public String getMessage() { return message; } @Override public String toString() { return \"PinType{\" + \"code=\" + code + \", message='\" + message + '\\'' + '}'; } } 实际使用： System.out.println(PinType.FORGET_PASSWORD.getCode()); System.out.println(PinType.FORGET_PASSWORD.getMessage()); System.out.println(PinType.FORGET_PASSWORD.toString()); Output: 100001 忘记密码使用 PinType{code=100001, message='忘记密码使用'} 这样的话，在实际使用起来就会非常灵活方便！ "},"zother6-JavaGuide/java/collection/ArrayList-Grow.html":{"url":"zother6-JavaGuide/java/collection/ArrayList-Grow.html","title":"Array List Grow","keywords":"","body":"一 先从 ArrayList 的构造函数说起 ArrayList有三种方式来初始化，构造方法源码如下： /** * 默认初始容量大小 */ private static final int DEFAULT_CAPACITY = 10; private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}; /** *默认构造函数，使用初始容量10构造一个空列表(无参数构造) */ public ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; } /** * 带初始容量参数的构造函数。（用户自己指定容量） */ public ArrayList(int initialCapacity) { if (initialCapacity > 0) {//初始容量大于0 //创建initialCapacity大小的数组 this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) {//初始容量等于0 //创建空数组 this.elementData = EMPTY_ELEMENTDATA; } else {//初始容量小于0，抛出异常 throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); } } /** *构造包含指定collection元素的列表，这些元素利用该集合的迭代器按顺序返回 *如果指定的集合为null，throws NullPointerException。 */ public ArrayList(Collection c) { elementData = c.toArray(); if ((size = elementData.length) != 0) { // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); } else { // replace with empty array. this.elementData = EMPTY_ELEMENTDATA; } } 细心的同学一定会发现 ：以无参数构造方法创建 ArrayList 时，实际上初始化赋值的是一个空数组。当真正对数组进行添加元素操作时，才真正分配容量。即向数组中添加第一个元素时，数组容量扩为10。 下面在我们分析 ArrayList 扩容时会讲到这一点内容！ 二 一步一步分析 ArrayList 扩容机制 这里以无参构造函数创建的 ArrayList 为例分析 1. 先来看 add 方法 /** * 将指定的元素追加到此列表的末尾。 */ public boolean add(E e) { //添加元素之前，先调用ensureCapacityInternal方法 ensureCapacityInternal(size + 1); // Increments modCount!! //这里看到ArrayList添加元素的实质就相当于为数组赋值 elementData[size++] = e; return true; } 2. 再来看看 ensureCapacityInternal() 方法 可以看到 add 方法 首先调用了ensureCapacityInternal(size + 1) //得到最小扩容量 private void ensureCapacityInternal(int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { // 获取默认的容量和传入参数的较大值 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity); } 当 要 add 进第1个元素时，minCapacity为1，在Math.max()方法比较后，minCapacity 为10。 3. ensureExplicitCapacity() 方法 如果调用 ensureCapacityInternal() 方法就一定会进过（执行）这个方法，下面我们来研究一下这个方法的源码！ //判断是否需要扩容 private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code if (minCapacity - elementData.length > 0) //调用grow方法进行扩容，调用此方法代表已经开始扩容了 grow(minCapacity); } 我们来仔细分析一下： 当我们要 add 进第1个元素到 ArrayList 时，elementData.length 为0 （因为还是一个空的 list），因为执行了 ensureCapacityInternal() 方法 ，所以 minCapacity 此时为10。此时，minCapacity - elementData.length > 0成立，所以会进入 grow(minCapacity) 方法。 当add第2个元素时，minCapacity 为2，此时e lementData.length(容量)在添加第一个元素后扩容成 10 了。此时，minCapacity - elementData.length > 0 不成立，所以不会进入 （执行）grow(minCapacity) 方法。 添加第3、4···到第10个元素时，依然不会执行grow方法，数组容量都为10。 直到添加第11个元素，minCapacity(为11)比elementData.length（为10）要大。进入grow方法进行扩容。 4. grow() 方法 /** * 要分配的最大数组大小 */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; /** * ArrayList扩容的核心方法。 */ private void grow(int minCapacity) { // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2， //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity >> 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量， if (newCapacity - minCapacity 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); } int newCapacity = oldCapacity + (oldCapacity >> 1),所以 ArrayList 每次扩容之后容量都会变为原来的 1.5 倍左右（oldCapacity为偶数就是1.5倍，否则是1.5倍左右）！ 奇偶不同，比如 ：10+10/2 = 15, 33+33/2=49。如果是奇数的话会丢掉小数. \">>\"（移位运算符）：>>1 右移一位相当于除2，右移n位相当于除以 2 的 n 次方。这里 oldCapacity 明显右移了1位所以相当于oldCapacity /2。对于大数据的2进制运算,位移运算符比那些普通运算符的运算要快很多,因为程序仅仅移动一下而已,不去计算,这样提高了效率,节省了资源 　 我们再来通过例子探究一下grow() 方法 ： 当add第1个元素时，oldCapacity 为0，经比较后第一个if判断成立，newCapacity = minCapacity(为10)。但是第二个if判断不会成立，即newCapacity 不比 MAX_ARRAY_SIZE大，则不会进入 hugeCapacity 方法。数组容量为10，add方法中 return true,size增为1。 当add第11个元素进入grow方法时，newCapacity为15，比minCapacity（为11）大，第一个if判断不成立。新容量没有大于数组最大size，不会进入hugeCapacity方法。数组容量扩为15，add方法中return true,size增为11。 以此类推······ 这里补充一点比较重要，但是容易被忽视掉的知识点： java 中的 length属性是针对数组说的,比如说你声明了一个数组,想知道这个数组的长度则用到了 length 这个属性. java 中的 length() 方法是针对字符串说的,如果想看这个字符串的长度则用到 length() 这个方法. java 中的 size() 方法是针对泛型集合说的,如果想看这个泛型有多少个元素,就调用此方法来查看! 5. hugeCapacity() 方法。 从上面 grow() 方法源码我们知道： 如果新容量大于 MAX_ARRAY_SIZE,进入(执行) hugeCapacity() 方法来比较 minCapacity 和 MAX_ARRAY_SIZE，如果minCapacity大于最大容量，则新容量则为Integer.MAX_VALUE，否则，新容量大小则为 MAX_ARRAY_SIZE 即为 Integer.MAX_VALUE - 8。 private static int hugeCapacity(int minCapacity) { if (minCapacity MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; } 三 System.arraycopy() 和 Arrays.copyOf()方法 阅读源码的话，我们就会发现 ArrayList 中大量调用了这两个方法。比如：我们上面讲的扩容操作以及add(int index, E element)、toArray() 等方法中都用到了该方法！ 3.1 System.arraycopy() 方法 /** * 在此列表中的指定位置插入指定的元素。 *先调用 rangeCheckForAdd 对index进行界限检查；然后调用 ensureCapacityInternal 方法保证capacity足够大； *再将从index开始之后的所有成员后移一个位置；将element插入index位置；最后size加1。 */ public void add(int index, E element) { rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //arraycopy()方法实现数组自己复制自己 //elementData:源数组;index:源数组中的起始位置;elementData：目标数组；index + 1：目标数组中的起始位置； size - index：要复制的数组元素的数量； System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; } 我们写一个简单的方法测试以下： public class ArraycopyTest { public static void main(String[] args) { // TODO Auto-generated method stub int[] a = new int[10]; a[0] = 0; a[1] = 1; a[2] = 2; a[3] = 3; System.arraycopy(a, 2, a, 3, 3); a[2]=99; for (int i = 0; i 结果： 0 1 99 2 3 0 0 0 0 0 3.2 Arrays.copyOf()方法 /** 以正确的顺序返回一个包含此列表中所有元素的数组（从第一个到最后一个元素）; 返回的数组的运行时类型是指定数组的运行时类型。 */ public Object[] toArray() { //elementData：要复制的数组；size：要复制的长度 return Arrays.copyOf(elementData, size); } 个人觉得使用 Arrays.copyOf()方法主要是为了给原有数组扩容，测试代码如下： public class ArrayscopyOfTest { public static void main(String[] args) { int[] a = new int[3]; a[0] = 0; a[1] = 1; a[2] = 2; int[] b = Arrays.copyOf(a, 10); System.out.println(\"b.length\"+b.length); } } 结果： 10 3.3 两者联系和区别 联系： 看两者源代码可以发现 copyOf() 内部实际调用了 System.arraycopy() 方法 区别： arraycopy() 需要目标数组，将原数组拷贝到你自己定义的数组里或者原数组，而且可以选择拷贝的起点和长度以及放入新数组中的位置 copyOf() 是系统自动在内部新建一个数组，并返回该数组。 四 ensureCapacity方法 ArrayList 源码中有一个 ensureCapacity 方法不知道大家注意到没有，这个方法 ArrayList 内部没有被调用过，所以很显然是提供给用户调用的，那么这个方法有什么作用呢？ /** 如有必要，增加此 ArrayList 实例的容量，以确保它至少可以容纳由minimum capacity参数指定的元素数。 * * @param minCapacity 所需的最小容量 */ public void ensureCapacity(int minCapacity) { int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table ? 0 // larger than default for default empty table. It's already // supposed to be at default size. : DEFAULT_CAPACITY; if (minCapacity > minExpand) { ensureExplicitCapacity(minCapacity); } } 最好在 add 大量元素之前用 ensureCapacity 方法，以减少增量重新分配的次数 我们通过下面的代码实际测试以下这个方法的效果： public class EnsureCapacityTest { public static void main(String[] args) { ArrayList list = new ArrayList(); final int N = 10000000; long startTime = System.currentTimeMillis(); for (int i = 0; i 运行结果： 使用ensureCapacity方法前：2158 public class EnsureCapacityTest { public static void main(String[] args) { ArrayList list = new ArrayList(); final int N = 10000000; list = new ArrayList(); long startTime1 = System.currentTimeMillis(); list.ensureCapacity(N); for (int i = 0; i 运行结果： 使用ensureCapacity方法前：1773 通过运行结果，我们可以看出向 ArrayList 添加大量元素之前最好先使用ensureCapacity 方法，以减少增量重新分配的次数。 "},"zother6-JavaGuide/java/collection/ArrayList.html":{"url":"zother6-JavaGuide/java/collection/ArrayList.html","title":"Array List","keywords":"","body":" ArrayList简介 ArrayList核心源码 ArrayList源码分析 System.arraycopy()和Arrays.copyOf()方法 两者联系与区别 ArrayList核心扩容技术 内部类 ArrayList经典Demo ArrayList简介 　　ArrayList 的底层是数组队列，相当于动态数组。与 Java 中的数组相比，它的容量能动态增长。在添加大量元素前，应用程序可以使用ensureCapacity操作来增加 ArrayList 实例的容量。这可以减少递增式再分配的数量。 它继承于 AbstractList，实现了 List, RandomAccess, Cloneable, java.io.Serializable 这些接口。 在我们学数据结构的时候就知道了线性表的顺序存储，插入删除元素的时间复杂度为O（n）,求表长以及增加元素，取第 i 元素的时间复杂度为O（1） 　 ArrayList 继承了AbstractList，实现了List。它是一个数组队列，提供了相关的添加、删除、修改、遍历等功能。 　　ArrayList 实现了RandomAccess 接口， RandomAccess 是一个标志接口，表明实现这个这个接口的 List 集合是支持快速随机访问的。在 ArrayList 中，我们即可以通过元素的序号快速获取元素对象，这就是快速随机访问。 　　ArrayList 实现了Cloneable 接口，即覆盖了函数 clone()，能被克隆。 　　ArrayList 实现java.io.Serializable 接口，这意味着ArrayList支持序列化，能通过序列化去传输。 　　和 Vector 不同，ArrayList 中的操作不是线程安全的！所以，建议在单线程中才使用 ArrayList，而在多线程中可以选择 Vector 或者 CopyOnWriteArrayList。 ArrayList核心源码 package java.util; import java.util.function.Consumer; import java.util.function.Predicate; import java.util.function.UnaryOperator; public class ArrayList extends AbstractList implements List, RandomAccess, Cloneable, java.io.Serializable { private static final long serialVersionUID = 8683452581122892189L; /** * 默认初始容量大小 */ private static final int DEFAULT_CAPACITY = 10; /** * 空数组（用于空实例）。 */ private static final Object[] EMPTY_ELEMENTDATA = {}; //用于默认大小空实例的共享空数组实例。 //我们把它从EMPTY_ELEMENTDATA数组中区分出来，以知道在添加第一个元素时容量需要增加多少。 private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}; /** * 保存ArrayList数据的数组 */ transient Object[] elementData; // non-private to simplify nested class access /** * ArrayList 所包含的元素个数 */ private int size; /** * 带初始容量参数的构造函数。（用户自己指定容量） */ public ArrayList(int initialCapacity) { if (initialCapacity > 0) { //创建initialCapacity大小的数组 this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) { //创建空数组 this.elementData = EMPTY_ELEMENTDATA; } else { throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); } } /** *默认构造函数，DEFAULTCAPACITY_EMPTY_ELEMENTDATA 为0.初始化为10，也就是说初始其实是空数组 当添加第一个元素的时候数组容量才变成10 */ public ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; } /** * 构造一个包含指定集合的元素的列表，按照它们由集合的迭代器返回的顺序。 */ public ArrayList(Collection c) { // elementData = c.toArray(); //如果指定集合元素个数不为0 if ((size = elementData.length) != 0) { // c.toArray 可能返回的不是Object类型的数组所以加上下面的语句用于判断， //这里用到了反射里面的getClass()方法 if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); } else { // 用空数组代替 this.elementData = EMPTY_ELEMENTDATA; } } /** * 修改这个ArrayList实例的容量是列表的当前大小。 应用程序可以使用此操作来最小化ArrayList实例的存储。 */ public void trimToSize() { modCount++; if (size minExpand) { ensureExplicitCapacity(minCapacity); } } //得到最小扩容量 private void ensureCapacityInternal(int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { // 获取默认的容量和传入参数的较大值 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity); } //判断是否需要扩容 private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code if (minCapacity - elementData.length > 0) //调用grow方法进行扩容，调用此方法代表已经开始扩容了 grow(minCapacity); } /** * 要分配的最大数组大小 */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; /** * ArrayList扩容的核心方法。 */ private void grow(int minCapacity) { // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2， //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity >> 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量， if (newCapacity - minCapacity 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); } //比较minCapacity和 MAX_ARRAY_SIZE private static int hugeCapacity(int minCapacity) { if (minCapacity MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; } /** *返回此列表中的元素数。 */ public int size() { return size; } /** * 如果此列表不包含元素，则返回 true 。 */ public boolean isEmpty() { //注意=和==的区别 return size == 0; } /** * 如果此列表包含指定的元素，则返回true 。 */ public boolean contains(Object o) { //indexOf()方法：返回此列表中指定元素的首次出现的索引，如果此列表不包含此元素，则为-1 return indexOf(o) >= 0; } /** *返回此列表中指定元素的首次出现的索引，如果此列表不包含此元素，则为-1 */ public int indexOf(Object o) { if (o == null) { for (int i = 0; i = 0; i--) if (elementData[i]==null) return i; } else { for (int i = size-1; i >= 0; i--) if (o.equals(elementData[i])) return i; } return -1; } /** * 返回此ArrayList实例的浅拷贝。 （元素本身不被复制。） */ public Object clone() { try { ArrayList v = (ArrayList) super.clone(); //Arrays.copyOf功能是实现数组的复制，返回复制后的数组。参数是被复制的数组和复制的长度 v.elementData = Arrays.copyOf(elementData, size); v.modCount = 0; return v; } catch (CloneNotSupportedException e) { // 这不应该发生，因为我们是可以克隆的 throw new InternalError(e); } } /** *以正确的顺序（从第一个到最后一个元素）返回一个包含此列表中所有元素的数组。 *返回的数组将是“安全的”，因为该列表不保留对它的引用。 （换句话说，这个方法必须分配一个新的数组）。 *因此，调用者可以自由地修改返回的数组。 此方法充当基于阵列和基于集合的API之间的桥梁。 */ public Object[] toArray() { return Arrays.copyOf(elementData, size); } /** * 以正确的顺序返回一个包含此列表中所有元素的数组（从第一个到最后一个元素）; *返回的数组的运行时类型是指定数组的运行时类型。 如果列表适合指定的数组，则返回其中。 *否则，将为指定数组的运行时类型和此列表的大小分配一个新数组。 *如果列表适用于指定的数组，其余空间（即数组的列表数量多于此元素），则紧跟在集合结束后的数组中的元素设置为null 。 *（这仅在调用者知道列表不包含任何空元素的情况下才能确定列表的长度。） */ @SuppressWarnings(\"unchecked\") public T[] toArray(T[] a) { if (a.length size) a[size] = null; return a; } // Positional Access Operations @SuppressWarnings(\"unchecked\") E elementData(int index) { return (E) elementData[index]; } /** * 返回此列表中指定位置的元素。 */ public E get(int index) { rangeCheck(index); return elementData(index); } /** * 用指定的元素替换此列表中指定位置的元素。 */ public E set(int index, E element) { //对index进行界限检查 rangeCheck(index); E oldValue = elementData(index); elementData[index] = element; //返回原来在这个位置的元素 return oldValue; } /** * 将指定的元素追加到此列表的末尾。 */ public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! //这里看到ArrayList添加元素的实质就相当于为数组赋值 elementData[size++] = e; return true; } /** * 在此列表中的指定位置插入指定的元素。 *先调用 rangeCheckForAdd 对index进行界限检查；然后调用 ensureCapacityInternal 方法保证capacity足够大； *再将从index开始之后的所有成员后移一个位置；将element插入index位置；最后size加1。 */ public void add(int index, E element) { rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //arraycopy()这个实现数组之间复制的方法一定要看一下，下面就用到了arraycopy()方法实现数组自己复制自己 System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; } /** * 删除该列表中指定位置的元素。 将任何后续元素移动到左侧（从其索引中减去一个元素）。 */ public E remove(int index) { rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved > 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work //从列表中删除的元素 return oldValue; } /** * 从列表中删除指定元素的第一个出现（如果存在）。 如果列表不包含该元素，则它不会更改。 *返回true，如果此列表包含指定的元素 */ public boolean remove(Object o) { if (o == null) { for (int index = 0; index 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work } /** * 从列表中删除所有元素。 */ public void clear() { modCount++; // 把数组中所有的元素的值设为null for (int i = 0; i c) { Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount System.arraycopy(a, 0, elementData, size, numNew); size += numNew; return numNew != 0; } /** * 将指定集合中的所有元素插入到此列表中，从指定的位置开始。 */ public boolean addAll(int index, Collection c) { rangeCheckForAdd(index); Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount int numMoved = size - index; if (numMoved > 0) System.arraycopy(elementData, index, elementData, index + numNew, numMoved); System.arraycopy(a, 0, elementData, index, numNew); size += numNew; return numNew != 0; } /** * 从此列表中删除所有索引为fromIndex （含）和toIndex之间的元素。 *将任何后续元素移动到左侧（减少其索引）。 */ protected void removeRange(int fromIndex, int toIndex) { modCount++; int numMoved = size - toIndex; System.arraycopy(elementData, toIndex, elementData, fromIndex, numMoved); // clear to let GC do its work int newSize = size - (toIndex-fromIndex); for (int i = newSize; i = size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); } /** * add和addAll使用的rangeCheck的一个版本 */ private void rangeCheckForAdd(int index) { if (index > size || index c) { Objects.requireNonNull(c); //如果此列表被修改则返回true return batchRemove(c, false); } /** * 仅保留此列表中包含在指定集合中的元素。 *换句话说，从此列表中删除其中不包含在指定集合中的所有元素。 */ public boolean retainAll(Collection c) { Objects.requireNonNull(c); return batchRemove(c, true); } /** * 从列表中的指定位置开始，返回列表中的元素（按正确顺序）的列表迭代器。 *指定的索引表示初始调用将返回的第一个元素为next 。 初始调用previous将返回指定索引减1的元素。 *返回的列表迭代器是fail-fast 。 */ public ListIterator listIterator(int index) { if (index size) throw new IndexOutOfBoundsException(\"Index: \"+index); return new ListItr(index); } /** *返回列表中的列表迭代器（按适当的顺序）。 *返回的列表迭代器是fail-fast 。 */ public ListIterator listIterator() { return new ListItr(0); } /** *以正确的顺序返回该列表中的元素的迭代器。 *返回的迭代器是fail-fast 。 */ public Iterator iterator() { return new Itr(); } ArrayList源码分析 System.arraycopy()和Arrays.copyOf()方法 　　通过上面源码我们发现这两个实现数组复制的方法被广泛使用而且很多地方都特别巧妙。比如下面add(int index, E element)方法就很巧妙的用到了arraycopy()方法让数组自己复制自己实现让index开始之后的所有成员后移一个位置: /** * 在此列表中的指定位置插入指定的元素。 *先调用 rangeCheckForAdd 对index进行界限检查；然后调用 ensureCapacityInternal 方法保证capacity足够大； *再将从index开始之后的所有成员后移一个位置；将element插入index位置；最后size加1。 */ public void add(int index, E element) { rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //arraycopy()方法实现数组自己复制自己 //elementData:源数组;index:源数组中的起始位置;elementData：目标数组；index + 1：目标数组中的起始位置； size - index：要复制的数组元素的数量； System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; } 又如toArray()方法中用到了copyOf()方法 /** *以正确的顺序（从第一个到最后一个元素）返回一个包含此列表中所有元素的数组。 *返回的数组将是“安全的”，因为该列表不保留对它的引用。 （换句话说，这个方法必须分配一个新的数组）。 *因此，调用者可以自由地修改返回的数组。 此方法充当基于阵列和基于集合的API之间的桥梁。 */ public Object[] toArray() { //elementData：要复制的数组；size：要复制的长度 return Arrays.copyOf(elementData, size); } 两者联系与区别 联系： 看两者源代码可以发现copyOf()内部调用了System.arraycopy()方法 区别： arraycopy()需要目标数组，将原数组拷贝到你自己定义的数组里，而且可以选择拷贝的起点和长度以及放入新数组中的位置 copyOf()是系统自动在内部新建一个数组，并返回该数组。 ArrayList 核心扩容技术 ```java //下面是ArrayList的扩容机制 //ArrayList的扩容机制提高了性能，如果每次只扩充一个， //那么频繁的插入会导致频繁的拷贝，降低性能，而ArrayList的扩容机制避免了这种情况。 /** 如有必要，增加此ArrayList实例的容量，以确保它至少能容纳元素的数量 @param minCapacity 所需的最小容量 */ public void ensureCapacity(int minCapacity) { int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table ? 0 // larger than default for default empty table. It's already // supposed to be at default size. : DEFAULT_CAPACITY; if (minCapacity > minExpand) { ensureExplicitCapacity(minCapacity); } } //得到最小扩容量 private void ensureCapacityInternal(int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { // 获取默认的容量和传入参数的较大值 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity); } //判断是否需要扩容,上面两个方法都要调用 private void ensureExplicitCapacity(int minCapacity) { modCount++; // 如果说minCapacity也就是所需的最小容量大于保存ArrayList数据的数组的长度的话，就需要调用grow(minCapacity)方法扩容。 //这个minCapacity到底为多少呢？举个例子在添加元素(add)方法中这个minCapacity的大小就为现在数组的长度加1 if (minCapacity - elementData.length > 0) //调用grow方法进行扩容，调用此方法代表已经开始扩容了 grow(minCapacity); } ```java /** * ArrayList扩容的核心方法。 */ private void grow(int minCapacity) { //elementData为保存ArrayList数据的数组 ///elementData.length求数组长度elementData.size是求数组中的元素个数 // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2， //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity >> 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量， if (newCapacity - minCapacity 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); } 　　扩容机制代码已经做了详细的解释。另外值得注意的是大家很容易忽略的一个运算符：移位运算符 　　简介：移位运算符就是在二进制的基础上对数字进行平移。按照平移的方向和填充数字的规则分为三种:、>>(带符号右移)和>>>(无符号右移)。 　　作用：对于大数据的2进制运算,位移运算符比那些普通运算符的运算要快很多,因为程序仅仅移动一下而已,不去计算,这样提高了效率,节省了资源 　　比如这里：int newCapacity = oldCapacity + (oldCapacity >> 1); 右移一位相当于除2，右移n位相当于除以 2 的 n 次方。这里 oldCapacity 明显右移了1位所以相当于oldCapacity /2。 另外需要注意的是： java 中的length 属性是针对数组说的,比如说你声明了一个数组,想知道这个数组的长度则用到了 length 这个属性. java 中的length()方法是针对字 符串String说的,如果想看这个字符串的长度则用到 length()这个方法. .java 中的size()方法是针对泛型集合说的,如果想看这个泛型有多少个元素,就调用此方法来查看! 内部类 (1)private class Itr implements Iterator (2)private class ListItr extends Itr implements ListIterator (3)private class SubList extends AbstractList implements RandomAccess (4)static final class ArrayListSpliterator implements Spliterator 　　ArrayList有四个内部类，其中的Itr是实现了Iterator接口，同时重写了里面的hasNext()， next()， remove() 等方法；其中的ListItr 继承 Itr，实现了ListIterator接口，同时重写了hasPrevious()， nextIndex()， previousIndex()， previous()， set(E e)， add(E e) 等方法，所以这也可以看出了 Iterator和ListIterator的区别: ListIterator在Iterator的基础上增加了添加对象，修改对象，逆向遍历等方法，这些是Iterator不能实现的。 ArrayList经典Demo package list; import java.util.ArrayList; import java.util.Iterator; public class ArrayListDemo { public static void main(String[] srgs){ ArrayList arrayList = new ArrayList(); System.out.printf(\"Before add:arrayList.size() = %d\\n\",arrayList.size()); arrayList.add(1); arrayList.add(3); arrayList.add(5); arrayList.add(7); arrayList.add(9); System.out.printf(\"After add:arrayList.size() = %d\\n\",arrayList.size()); System.out.println(\"Printing elements of arrayList\"); // 三种遍历方式打印元素 // 第一种：通过迭代器遍历 System.out.print(\"通过迭代器遍历:\"); Iterator it = arrayList.iterator(); while(it.hasNext()){ System.out.print(it.next() + \" \"); } System.out.println(); // 第二种：通过索引值遍历 System.out.print(\"通过索引值遍历:\"); for(int i = 0; i "},"zother6-JavaGuide/java/collection/HashMap.html":{"url":"zother6-JavaGuide/java/collection/HashMap.html","title":"Hash Map","keywords":"","body":" HashMap 简介 底层数据结构分析 JDK1.8之前 JDK1.8之后 HashMap源码分析 构造方法 put方法 get方法 resize方法 HashMap常用方法测试 感谢 changfubai 对本文的改进做出的贡献！ HashMap 简介 HashMap 主要用来存放键值对，它基于哈希表的Map接口实现，是常用的Java集合之一。 JDK1.8 之前 HashMap 由 数组+链表 组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）.JDK1.8 以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）时，将链表转化为红黑树（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树），以减少搜索时间，具体可以参考 treeifyBin方法。 底层数据结构分析 JDK1.8之前 JDK1.8 之前 HashMap 底层是 数组和链表 结合在一起使用也就是 链表散列。HashMap 通过 key 的 hashCode 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) & hash 判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。 所谓扰动函数指的就是 HashMap 的 hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法 换句话说使用扰动函数之后可以减少碰撞。 JDK 1.8 HashMap 的 hash 方法源码: JDK 1.8 的 hash方法 相比于 JDK 1.7 hash 方法更加简化，但是原理不变。 static final int hash(Object key) { int h; // key.hashCode()：返回散列值也就是hashcode // ^ ：按位异或 // >>>:无符号右移，忽略符号位，空位都以0补齐 return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16); } 对比一下 JDK1.7的 HashMap 的 hash 方法源码. static int hash(int h) { // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h >>> 20) ^ (h >>> 12); return h ^ (h >>> 7) ^ (h >>> 4); } 相比于 JDK1.8 的 hash 方法 ，JDK 1.7 的 hash 方法的性能会稍差一点点，因为毕竟扰动了 4 次。 所谓 “拉链法” 就是：将链表和数组相结合。也就是说创建一个链表数组，数组中每一格就是一个链表。若遇到哈希冲突，则将冲突的值加到链表中即可。 JDK1.8之后 相比于之前的版本，jdk1.8在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间。 类的属性： public class HashMap extends AbstractMap implements Map, Cloneable, Serializable { // 序列号 private static final long serialVersionUID = 362498820763181265L; // 默认的初始容量是16 static final int DEFAULT_INITIAL_CAPACITY = 1 [] table; // 存放具体元素的集 transient Set> entrySet; // 存放元素的个数，注意这个不等于数组的长度。 transient int size; // 每次扩容和更改map结构的计数器 transient int modCount; // 临界值 当实际大小(容量*填充因子)超过临界值时，会进行扩容 int threshold; // 加载因子 final float loadFactor; } loadFactor加载因子 loadFactor加载因子是控制数组存放数据的疏密程度，loadFactor越趋近于1，那么 数组中存放的数据(entry)也就越多，也就越密，也就是会让链表的长度增加，loadFactor越小，也就是趋近于0，数组中存放的数据(entry)也就越少，也就越稀疏。 loadFactor太大导致查找元素效率低，太小导致数组的利用率低，存放的数据会很分散。loadFactor的默认值为0.75f是官方给出的一个比较好的临界值。 给定的默认容量为 16，负载因子为 0.75。Map 在使用过程中不断的往里面存放数据，当数量达到了 16 * 0.75 = 12 就需要将当前 16 的容量进行扩容，而扩容这个过程涉及到 rehash、复制数据等操作，所以非常消耗性能。 threshold threshold = capacity * loadFactor，当Size>=threshold的时候，那么就要考虑对数组的扩增了，也就是说，这个的意思就是 衡量数组是否需要扩增的一个标准。 Node节点类源码: // 继承自 Map.Entry static class Node implements Map.Entry { final int hash;// 哈希值，存放元素到hashmap中时用来与其他元素hash值比较 final K key;//键 V value;//值 // 指向下一个节点 Node next; Node(int hash, K key, V value, Node next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } public final K getKey() { return key; } public final V getValue() { return value; } public final String toString() { return key + \"=\" + value; } // 重写hashCode()方法 public final int hashCode() { return Objects.hashCode(key) ^ Objects.hashCode(value); } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } // 重写 equals() 方法 public final boolean equals(Object o) { if (o == this) return true; if (o instanceof Map.Entry) { Map.Entry e = (Map.Entry)o; if (Objects.equals(key, e.getKey()) && Objects.equals(value, e.getValue())) return true; } return false; } } 树节点类源码: static final class TreeNode extends LinkedHashMap.Entry { TreeNode parent; // 父 TreeNode left; // 左 TreeNode right; // 右 TreeNode prev; // needed to unlink next upon deletion boolean red; // 判断颜色 TreeNode(int hash, K key, V val, Node next) { super(hash, key, val, next); } // 返回根节点 final TreeNode root() { for (TreeNode r = this, p;;) { if ((p = r.parent) == null) return r; r = p; } HashMap源码分析 构造方法 HashMap 中有四个构造方法，它们分别如下： // 默认构造函数。 public HashMap() { this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted } // 包含另一个“Map”的构造函数 public HashMap(Map m) { this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false);//下面会分析到这个方法 } // 指定“容量大小”的构造函数 public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR); } // 指定“容量大小”和“加载因子”的构造函数 public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor putMapEntries方法： final void putMapEntries(Map m, boolean evict) { int s = m.size(); if (s > 0) { // 判断table是否已经初始化 if (table == null) { // pre-size // 未初始化，s为m的实际元素个数 float ft = ((float)s / loadFactor) + 1.0F; int t = ((ft threshold) threshold = tableSizeFor(t); } // 已初始化，并且m元素个数大于阈值，进行扩容处理 else if (s > threshold) resize(); // 将m中的所有元素添加至HashMap中 for (Map.Entry e : m.entrySet()) { K key = e.getKey(); V value = e.getValue(); putVal(hash(key), key, value, false, evict); } } } put方法 HashMap只提供了put用于添加元素，putVal方法只是给put方法调用的一个方法，并没有提供给用户使用。 对putVal方法添加元素的分析如下： ①如果定位到的数组位置没有元素 就直接插入。 ②如果定位到的数组位置有元素就和要插入的key比较，如果key相同就直接覆盖，如果key不相同，就判断p是否是一个树节点，如果是就调用e = ((TreeNode)p).putTreeVal(this, tab, hash, key, value)将元素添加进入。如果不是就遍历链表插入(插入的是链表尾部)。 ps:下图有一个小问题，来自 issue#608指出：直接覆盖之后应该就会 return，不会有后续操作。参考 JDK8 HashMap.java 658 行。 public V put(K key, V value) { return putVal(hash(key), key, value, false, true); } final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node[] tab; Node p; int n, i; // table未初始化或者长度为0，进行扩容 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // (n - 1) & hash 确定元素存放在哪个桶中，桶为空，新生成结点放入桶中(此时，这个结点是放在数组中) if ((p = tab[i = (n - 1) & hash]) == null) tab[i] = newNode(hash, key, value, null); // 桶中已经存在元素 else { Node e; K k; // 比较桶中第一个元素(数组中的结点)的hash值相等，key相等 if (p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k)))) // 将第一个元素赋值给e，用e来记录 e = p; // hash值不相等，即key不相等；为红黑树结点 else if (p instanceof TreeNode) // 放入树中 e = ((TreeNode)p).putTreeVal(this, tab, hash, key, value); // 为链表结点 else { // 在链表最末插入结点 for (int binCount = 0; ; ++binCount) { // 到达链表的尾部 if ((e = p.next) == null) { // 在尾部插入新结点 p.next = newNode(hash, key, value, null); // 结点数量达到阈值，转化为红黑树 if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); // 跳出循环 break; } // 判断链表中结点的key值与插入的元素的key值是否相等 if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) // 相等，跳出循环 break; // 用于遍历桶中的链表，与前面的e = p.next组合，可以遍历链表 p = e; } } // 表示在桶中找到key值、hash值与插入元素相等的结点 if (e != null) { // 记录e的value V oldValue = e.value; // onlyIfAbsent为false或者旧值为null if (!onlyIfAbsent || oldValue == null) //用新值替换旧值 e.value = value; // 访问后回调 afterNodeAccess(e); // 返回旧值 return oldValue; } } // 结构性修改 ++modCount; // 实际大小大于阈值则扩容 if (++size > threshold) resize(); // 插入后回调 afterNodeInsertion(evict); return null; } 我们再来对比一下 JDK1.7 put方法的代码 对于put方法的分析如下： ①如果定位到的数组位置没有元素 就直接插入。 ②如果定位到的数组位置有元素，遍历以这个元素为头结点的链表，依次和插入的key比较，如果key相同就直接覆盖，不同就采用头插法插入元素。 public V put(K key, V value) if (table == EMPTY_TABLE) { inflateTable(threshold); } if (key == null) return putForNullKey(value); int hash = hash(key); int i = indexFor(hash, table.length); for (Entry e = table[i]; e != null; e = e.next) { // 先遍历 Object k; if (e.hash == hash && ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; addEntry(hash, key, value, i); // 再插入 return null; } get方法 public V get(Object key) { Node e; return (e = getNode(hash(key), key)) == null ? null : e.value; } final Node getNode(int hash, Object key) { Node[] tab; Node first, e; int n; K k; if ((tab = table) != null && (n = tab.length) > 0 && (first = tab[(n - 1) & hash]) != null) { // 数组元素相等 if (first.hash == hash && // always check first node ((k = first.key) == key || (key != null && key.equals(k)))) return first; // 桶中不止一个节点 if ((e = first.next) != null) { // 在树中get if (first instanceof TreeNode) return ((TreeNode)first).getTreeNode(hash, key); // 在链表中get do { if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) return e; } while ((e = e.next) != null); } } return null; } resize方法 进行扩容，会伴随着一次重新hash分配，并且会遍历hash表中所有的元素，是非常耗时的。在编写程序中，要尽量避免resize。 final Node[] resize() { Node[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap > 0) { // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap >= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap = DEFAULT_INITIAL_CAPACITY) newThr = oldThr 0) // initial capacity was placed in threshold newCap = oldThr; else { // signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } // 计算新的resize上限 if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap [] newTab = (Node[])new Node[newCap]; table = newTab; if (oldTab != null) { // 把每个bucket都移动到新的buckets中 for (int j = 0; j e; if ((e = oldTab[j]) != null) { oldTab[j] = null; if (e.next == null) newTab[e.hash & (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode)e).split(this, newTab, j, oldCap); else { Node loHead = null, loTail = null; Node hiHead = null, hiTail = null; Node next; do { next = e.next; // 原索引 if ((e.hash & oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } // 原索引+oldCap else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) { loTail.next = null; newTab[j] = loHead; } // 原索引+oldCap放到bucket里 if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; } HashMap常用方法测试 package map; import java.util.Collection; import java.util.HashMap; import java.util.Set; public class HashMapDemo { public static void main(String[] args) { HashMap map = new HashMap(); // 键不能重复，值可以重复 map.put(\"san\", \"张三\"); map.put(\"si\", \"李四\"); map.put(\"wu\", \"王五\"); map.put(\"wang\", \"老王\"); map.put(\"wang\", \"老王2\");// 老王被覆盖 map.put(\"lao\", \"老王\"); System.out.println(\"-------直接输出hashmap:-------\"); System.out.println(map); /** * 遍历HashMap */ // 1.获取Map中的所有键 System.out.println(\"-------foreach获取Map中所有的键:------\"); Set keys = map.keySet(); for (String key : keys) { System.out.print(key+\" \"); } System.out.println();//换行 // 2.获取Map中所有值 System.out.println(\"-------foreach获取Map中所有的值:------\"); Collection values = map.values(); for (String value : values) { System.out.print(value+\" \"); } System.out.println();//换行 // 3.得到key的值的同时得到key所对应的值 System.out.println(\"-------得到key的值的同时得到key所对应的值:-------\"); Set keys2 = map.keySet(); for (String key : keys2) { System.out.print(key + \"：\" + map.get(key)+\" \"); } /** * 如果既要遍历key又要value，那么建议这种方式，应为如果先获取keySet然后再执行map.get(key)，map内部会执行两次遍历。 * 一次是在获取keySet的时候，一次是在遍历所有key的时候。 */ // 当我调用put(key,value)方法的时候，首先会把key和value封装到 // Entry这个静态内部类对象中，把Entry对象再添加到数组中，所以我们想获取 // map中的所有键值对，我们只要获取数组中的所有Entry对象，接下来 // 调用Entry对象中的getKey()和getValue()方法就能获取键值对了 Set> entrys = map.entrySet(); for (java.util.Map.Entry entry : entrys) { System.out.println(entry.getKey() + \"--\" + entry.getValue()); } /** * HashMap其他常用方法 */ System.out.println(\"after map.size()：\"+map.size()); System.out.println(\"after map.isEmpty()：\"+map.isEmpty()); System.out.println(map.remove(\"san\")); System.out.println(\"after map.remove()：\"+map); System.out.println(\"after map.get(si)：\"+map.get(\"si\")); System.out.println(\"after map.containsKey(si)：\"+map.containsKey(\"si\")); System.out.println(\"after containsValue(李四)：\"+map.containsValue(\"李四\")); System.out.println(map.replace(\"si\", \"李四2\")); System.out.println(\"after map.replace(si, 李四2):\"+map); } } "},"zother6-JavaGuide/java/collection/Java集合框架常见面试题.html":{"url":"zother6-JavaGuide/java/collection/Java集合框架常见面试题.html","title":"Java集合框架常见面试题","keywords":"","body":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。 剖析面试最常见问题之Java集合框架 说说List,Set,Map三者的区别？ Arraylist 与 LinkedList 区别? 补充内容:RandomAccess接口 补充内容:双向链表和双向循环链表 ArrayList 与 Vector 区别呢?为什么要用Arraylist取代Vector呢？ 说一说 ArrayList 的扩容机制吧 HashMap 和 Hashtable 的区别 HashMap 和 HashSet区别 HashSet如何检查重复 HashMap的底层实现 JDK1.8之前 JDK1.8之后 HashMap 的长度为什么是2的幂次方 HashMap 多线程操作导致死循环问题 ConcurrentHashMap 和 Hashtable 的区别 ConcurrentHashMap线程安全的具体实现方式/底层具体实现 JDK1.7（上面有示意图） JDK1.8 （上面有示意图） comparable 和 Comparator的区别 Comparator定制排序 重写compareTo方法实现按年龄来排序 集合框架底层数据结构总结 Collection 1. List 2. Set Map 如何选用集合? 剖析面试最常见问题之Java集合框架 说说List,Set,Map三者的区别？ List(对付顺序的好帮手)： List接口存储一组不唯一（可以有多个元素引用相同的对象），有序的对象 Set(注重独一无二的性质): 不允许重复的集合。不会有多个元素引用相同的对象。 Map(用Key来搜索的专家): 使用键值对存储。Map会维护与Key有关联的值。两个Key可以引用相同的对象，但Key不能重复，典型的Key是String类型，但也可以是任何对象。 Arraylist 与 LinkedList 区别? 1. 是否保证线程安全： ArrayList 和 LinkedList 都是不同步的，也就是不保证线程安全； 2. 底层数据结构： Arraylist 底层使用的是 Object 数组；LinkedList 底层使用的是 双向链表 数据结构（JDK1.6之前为循环链表，JDK1.7取消了循环。注意双向链表和双向循环链表的区别，下面有介绍到！） 3. 插入和删除是否受元素位置的影响： ① ArrayList 采用数组存储，所以插入和删除元素的时间复杂度受元素位置的影响。 比如：执行add(E e)方法的时候， ArrayList 会默认在将指定的元素追加到此列表的末尾，这种情况时间复杂度就是O(1)。但是如果要在指定位置 i 插入和删除元素的话（add(int index, E element)）时间复杂度就为 O(n-i)。因为在进行上述操作的时候集合中第 i 和第 i 个元素之后的(n-i)个元素都要执行向后位/向前移一位的操作。 ② LinkedList 采用链表存储，所以对于add(E e)方法的插入，删除元素时间复杂度不受元素位置的影响，近似 O(1)，如果是要在指定位置i插入和删除元素的话（(add(int index, E element)） 时间复杂度近似为o(n))因为需要先移动到指定位置再插入。 4. 是否支持快速随机访问： LinkedList 不支持高效的随机元素访问，而 ArrayList 支持。快速随机访问就是通过元素的序号快速获取元素对象(对应于get(int index)方法)。 5. 内存空间占用： ArrayList的空 间浪费主要体现在在list列表的结尾会预留一定的容量空间，而LinkedList的空间花费则体现在它的每一个元素都需要消耗比ArrayList更多的空间（因为要存放直接后继和直接前驱以及数据）。 补充内容:RandomAccess接口 public interface RandomAccess { } 查看源码我们发现实际上 RandomAccess 接口中什么都没有定义。所以，在我看来 RandomAccess 接口不过是一个标识罢了。标识什么？ 标识实现这个接口的类具有随机访问功能。 在 binarySearch（) 方法中，它要判断传入的list 是否 RamdomAccess 的实例，如果是，调用indexedBinarySearch()方法，如果不是，那么调用iteratorBinarySearch()方法 public static int binarySearch(List> list, T key) { if (list instanceof RandomAccess || list.size() ArrayList 实现了 RandomAccess 接口， 而 LinkedList 没有实现。为什么呢？我觉得还是和底层数据结构有关！ArrayList 底层是数组，而 LinkedList 底层是链表。数组天然支持随机访问，时间复杂度为 O(1)，所以称为快速随机访问。链表需要遍历到特定位置才能访问特定位置的元素，时间复杂度为 O(n)，所以不支持快速随机访问。，ArrayList 实现了 RandomAccess 接口，就表明了他具有快速随机访问功能。 RandomAccess 接口只是标识，并不是说 ArrayList 实现 RandomAccess 接口才具有快速随机访问功能的！ 下面再总结一下 list 的遍历方式选择： 实现了 RandomAccess 接口的list，优先选择普通 for 循环 ，其次 foreach, 未实现 RandomAccess接口的list，优先选择iterator遍历（foreach遍历底层也是通过iterator实现的），大size的数据，千万不要使用普通for循环 补充内容:双向链表和双向循环链表 双向链表： 包含两个指针，一个prev指向前一个节点，一个next指向后一个节点。 双向循环链表： 最后一个节点的 next 指向head，而 head 的prev指向最后一个节点，构成一个环。 ArrayList 与 Vector 区别呢?为什么要用Arraylist取代Vector呢？ Vector类的所有方法都是同步的。可以由两个线程安全地访问一个Vector对象、但是一个线程访问Vector的话代码要在同步操作上耗费大量的时间。 Arraylist不是同步的，所以在不需要保证线程安全时建议使用Arraylist。 说一说 ArrayList 的扩容机制吧 详见笔主的这篇文章:通过源码一步一步分析ArrayList 扩容机制 HashMap 和 Hashtable 的区别 线程是否安全： HashMap 是非线程安全的，HashTable 是线程安全的；HashTable 内部的方法基本都经过synchronized 修饰。（如果你要保证线程安全的话就使用 ConcurrentHashMap 吧！）； 效率： 因为线程安全的问题，HashMap 要比 HashTable 效率高一点。另外，HashTable 基本被淘汰，不要在代码中使用它； 对Null key 和Null value的支持： HashMap 中，null 可以作为键，这样的键只有一个，可以有一个或多个键所对应的值为 null。。但是在 HashTable 中 put 进的键值只要有一个 null，直接抛出 NullPointerException。 初始容量大小和每次扩充容量大小的不同 ： ①创建时如果不指定容量初始值，Hashtable 默认的初始大小为11，之后每次扩充，容量变为原来的2n+1。HashMap 默认的初始化大小为16。之后每次扩充，容量变为原来的2倍。②创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为2的幂次方大小（HashMap 中的tableSizeFor()方法保证，下面给出了源代码）。也就是说 HashMap 总是使用2的幂作为哈希表的大小,后面会介绍到为什么是2的幂次方。 底层数据结构： JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。Hashtable 没有这样的机制。 HashMap 中带有初始容量的构造函数： public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor 下面这个方法保证了 HashMap 总是使用2的幂作为哈希表的大小。 /** * Returns a power of two size for the given target capacity. */ static final int tableSizeFor(int cap) { int n = cap - 1; n |= n >>> 1; n |= n >>> 2; n |= n >>> 4; n |= n >>> 8; n |= n >>> 16; return (n = MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } HashMap 和 HashSet区别 如果你看过 HashSet 源码的话就应该知道：HashSet 底层就是基于 HashMap 实现的。（HashSet 的源码非常非常少，因为除了 clone()、writeObject()、readObject()是 HashSet 自己不得不实现之外，其他方法都是直接调用 HashMap 中的方法。 HashMap HashSet 实现了Map接口 实现Set接口 存储键值对 仅存储对象 调用 put()向map中添加元素 调用 add()方法向Set中添加元素 HashMap使用键（Key）计算Hashcode HashSet使用成员对象来计算hashcode值，对于两个对象来说hashcode可能相同，所以equals()方法用来判断对象的相等性， HashSet如何检查重复 当你把对象加入HashSet时，HashSet会先计算对象的hashcode值来判断对象加入的位置，同时也会与其他加入的对象的hashcode值作比较，如果没有相符的hashcode，HashSet会假设对象没有重复出现。但是如果发现有相同hashcode值的对象，这时会调用equals()方法来检查hashcode相等的对象是否真的相同。如果两者相同，HashSet就不会让加入操作成功。（摘自我的Java启蒙书《Head fist java》第二版） hashCode()与equals()的相关规定： 如果两个对象相等，则hashcode一定也是相同的 两个对象相等,对两个equals方法返回true 两个对象有相同的hashcode值，它们也不一定是相等的 综上，equals方法被覆盖过，则hashCode方法也必须被覆盖 hashCode()的默认行为是对堆上的对象产生独特值。如果没有重写hashCode()，则该class的两个对象无论如何都不会相等（即使这两个对象指向相同的数据）。 ==与equals的区别 ==是判断两个变量或实例是不是指向同一个内存空间 equals是判断两个变量或实例所指向的内存空间的值是不是相同 ==是指对内存地址进行比较 equals()是对字符串的内容进行比较 ==指引用是否相同 equals()指的是值是否相同 HashMap的底层实现 JDK1.8之前 JDK1.8 之前 HashMap 底层是 数组和链表 结合在一起使用也就是 链表散列。HashMap 通过 key 的 hashCode 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) & hash 判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。 所谓扰动函数指的就是 HashMap 的 hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法 换句话说使用扰动函数之后可以减少碰撞。 JDK 1.8 HashMap 的 hash 方法源码: JDK 1.8 的 hash方法 相比于 JDK 1.7 hash 方法更加简化，但是原理不变。 static final int hash(Object key) { int h; // key.hashCode()：返回散列值也就是hashcode // ^ ：按位异或 // >>>:无符号右移，忽略符号位，空位都以0补齐 return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16); } 对比一下 JDK1.7的 HashMap 的 hash 方法源码. static int hash(int h) { // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h >>> 20) ^ (h >>> 12); return h ^ (h >>> 7) ^ (h >>> 4); } 相比于 JDK1.8 的 hash 方法 ，JDK 1.7 的 hash 方法的性能会稍差一点点，因为毕竟扰动了 4 次。 所谓 “拉链法” 就是：将链表和数组相结合。也就是说创建一个链表数组，数组中每一格就是一个链表。若遇到哈希冲突，则将冲突的值加到链表中即可。 JDK1.8之后 相比于之前的版本， JDK1.8之后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。 TreeMap、TreeSet以及JDK1.8之后的HashMap底层都用到了红黑树。红黑树就是为了解决二叉查找树的缺陷，因为二叉查找树在某些情况下会退化成一个线性结构。 推荐阅读： 《Java 8系列之重新认识HashMap》 ：https://zhuanlan.zhihu.com/p/21673805 HashMap 的长度为什么是2的幂次方 为了能让 HashMap 存取高效，尽量较少碰撞，也就是要尽量把数据分配均匀。我们上面也讲到了过了，Hash 值的范围值-2147483648到2147483647，前后加起来大概40亿的映射空间，只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。但问题是一个40亿长度的数组，内存是放不下的。所以这个散列值是不能直接拿来用的。用之前还要先做对数组的长度取模运算，得到的余数才能用来要存放的位置也就是对应的数组下标。这个数组下标的计算方法是“ (n - 1) & hash”。（n代表数组长度）。这也就解释了 HashMap 的长度为什么是2的幂次方。 这个算法应该如何设计呢？ 我们首先可能会想到采用%取余的操作来实现。但是，重点来了：“取余(%)操作中如果除数是2的幂次则等价于与其除数减一的与(&)操作（也就是说 hash%length==hash&(length-1)的前提是 length 是2的 n 次方；）。” 并且 采用二进制位操作 &，相对于%能够提高运算效率，这就解释了 HashMap 的长度为什么是2的幂次方。 HashMap 多线程操作导致死循环问题 主要原因在于并发下的Rehash 会造成元素之间会形成一个循环链表。不过，jdk 1.8 后解决了这个问题，但是还是不建议在多线程下使用 HashMap,因为多线程下使用 HashMap 还是会存在其他问题比如数据丢失。并发环境下推荐使用 ConcurrentHashMap 。 详情请查看：https://coolshell.cn/articles/9606.html ConcurrentHashMap 和 Hashtable 的区别 ConcurrentHashMap 和 Hashtable 的区别主要体现在实现线程安全的方式上不同。 底层数据结构： JDK1.7的 ConcurrentHashMap 底层采用 分段的数组+链表 实现，JDK1.8 采用的数据结构跟HashMap1.8的结构一样，数组+链表/红黑二叉树。Hashtable 和 JDK1.8 之前的 HashMap 的底层数据结构类似都是采用 数组+链表 的形式，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的； 实现线程安全的方式（重要）： ① 在JDK1.7的时候，ConcurrentHashMap（分段锁） 对整个桶数组进行了分割分段(Segment)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。 到了 JDK1.8 的时候已经摒弃了Segment的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并发控制使用 synchronized 和 CAS 来操作。（JDK1.6以后 对 synchronized锁做了很多优化） 整个看起来就像是优化过且线程安全的 HashMap，虽然在JDK1.8中还能看到 Segment 的数据结构，但是已经简化了属性，只是为了兼容旧版本；② Hashtable(同一把锁) :使用 synchronized 来保证线程安全，效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用 put 添加元素，另一个线程不能使用 put 添加元素，也不能使用 get，竞争会越来越激烈效率越低。 两者的对比图： 图片来源：http://www.cnblogs.com/chengxiao/p/6842045.html HashTable: JDK1.7的ConcurrentHashMap： JDK1.8的ConcurrentHashMap（TreeBin: 红黑二叉树节点 Node: 链表节点）： ConcurrentHashMap线程安全的具体实现方式/底层具体实现 JDK1.7（上面有示意图） 首先将数据分为一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据时，其他段的数据也能被其他线程访问。 ConcurrentHashMap 是由 Segment 数组结构和 HashEntry 数组结构组成。 Segment 实现了 ReentrantLock,所以 Segment 是一种可重入锁，扮演锁的角色。HashEntry 用于存储键值对数据。 static class Segment extends ReentrantLock implements Serializable { } 一个 ConcurrentHashMap 里包含一个 Segment 数组。Segment 的结构和HashMap类似，是一种数组和链表结构，一个 Segment 包含一个 HashEntry 数组，每个 HashEntry 是一个链表结构的元素，每个 Segment 守护着一个HashEntry数组里的元素，当对 HashEntry 数组的数据进行修改时，必须首先获得对应的 Segment的锁。 JDK1.8 （上面有示意图） ConcurrentHashMap取消了Segment分段锁，采用CAS和synchronized来保证并发安全。数据结构跟HashMap1.8的结构类似，数组+链表/红黑二叉树。Java 8在链表长度超过一定阈值（8）时将链表（寻址时间复杂度为O(N)）转换为红黑树（寻址时间复杂度为O(log(N))） synchronized只锁定当前链表或红黑二叉树的首节点，这样只要hash不冲突，就不会产生并发，效率又提升N倍。 comparable 和 Comparator的区别 comparable接口实际上是出自java.lang包 它有一个 compareTo(Object obj)方法用来排序 comparator接口实际上是出自 java.util 包它有一个compare(Object obj1, Object obj2)方法用来排序 一般我们需要对一个集合使用自定义排序时，我们就要重写compareTo()方法或compare()方法，当我们需要对某一个集合实现两种排序方式，比如一个song对象中的歌名和歌手名分别采用一种排序方法的话，我们可以重写compareTo()方法和使用自制的Comparator方法或者以两个Comparator来实现歌名排序和歌星名排序，第二种代表我们只能使用两个参数版的 Collections.sort(). Comparator定制排序 ArrayList arrayList = new ArrayList(); arrayList.add(-1); arrayList.add(3); arrayList.add(3); arrayList.add(-5); arrayList.add(7); arrayList.add(4); arrayList.add(-9); arrayList.add(-7); System.out.println(\"原始数组:\"); System.out.println(arrayList); // void reverse(List list)：反转 Collections.reverse(arrayList); System.out.println(\"Collections.reverse(arrayList):\"); System.out.println(arrayList); // void sort(List list),按自然排序的升序排序 Collections.sort(arrayList); System.out.println(\"Collections.sort(arrayList):\"); System.out.println(arrayList); // 定制排序的用法 Collections.sort(arrayList, new Comparator() { @Override public int compare(Integer o1, Integer o2) { return o2.compareTo(o1); } }); System.out.println(\"定制排序后：\"); System.out.println(arrayList); Output: 原始数组: [-1, 3, 3, -5, 7, 4, -9, -7] Collections.reverse(arrayList): [-7, -9, 4, 7, -5, 3, 3, -1] Collections.sort(arrayList): [-9, -7, -5, -1, 3, 3, 4, 7] 定制排序后： [7, 4, 3, 3, -1, -5, -7, -9] 重写compareTo方法实现按年龄来排序 // person对象没有实现Comparable接口，所以必须实现，这样才不会出错，才可以使treemap中的数据按顺序排列 // 前面一个例子的String类已经默认实现了Comparable接口，详细可以查看String类的API文档，另外其他 // 像Integer类等都已经实现了Comparable接口，所以不需要另外实现了 public class Person implements Comparable { private String name; private int age; public Person(String name, int age) { super(); this.name = name; this.age = age; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } /** * TODO重写compareTo方法实现按年龄来排序 */ @Override public int compareTo(Person o) { // TODO Auto-generated method stub if (this.age > o.getAge()) { return 1; } else if (this.age public static void main(String[] args) { TreeMap pdata = new TreeMap(); pdata.put(new Person(\"张三\", 30), \"zhangsan\"); pdata.put(new Person(\"李四\", 20), \"lisi\"); pdata.put(new Person(\"王五\", 10), \"wangwu\"); pdata.put(new Person(\"小红\", 5), \"xiaohong\"); // 得到key的值的同时得到key所对应的值 Set keys = pdata.keySet(); for (Person key : keys) { System.out.println(key.getAge() + \"-\" + key.getName()); } } Output： 5-小红 10-王五 20-李四 30-张三 集合框架底层数据结构总结 Collection 1. List Arraylist： Object数组 Vector： Object数组 LinkedList： 双向链表(JDK1.6之前为循环链表，JDK1.7取消了循环) 2. Set HashSet（无序，唯一）: 基于 HashMap 实现的，底层采用 HashMap 来保存元素 LinkedHashSet： LinkedHashSet 继承于 HashSet，并且其内部是通过 LinkedHashMap 来实现的。有点类似于我们之前说的LinkedHashMap 其内部是基于 HashMap 实现一样，不过还是有一点点区别的 TreeSet（有序，唯一）： 红黑树(自平衡的排序二叉树) Map HashMap： JDK1.8之前HashMap由数组+链表组成的，数组是HashMap的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）。JDK1.8以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间 LinkedHashMap： LinkedHashMap 继承自 HashMap，所以它的底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成。另外，LinkedHashMap 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。详细可以查看：《LinkedHashMap 源码详细分析（JDK1.8）》 Hashtable： 数组+链表组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的 TreeMap： 红黑树（自平衡的排序二叉树） 如何选用集合? 主要根据集合的特点来选用，比如我们需要根据键值获取到元素值时就选用Map接口下的集合，需要排序时选择TreeMap,不需要排序时就选择HashMap,需要保证线程安全就选用ConcurrentHashMap.当我们只需要存放元素值时，就选择实现Collection接口的集合，需要保证元素唯一时选择实现Set接口的集合比如TreeSet或HashSet，不需要就选择实现List接口的比如ArrayList或LinkedList，然后再根据实现这些接口的集合的特点来选用。 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \"Java面试突击\" 即可免费领取！ Java工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/java/collection/LinkedList.html":{"url":"zother6-JavaGuide/java/collection/LinkedList.html","title":"Linked List","keywords":"","body":" 简介 内部结构分析 LinkedList源码分析 构造方法 添加（add）方法 根据位置取数据的方法 根据对象得到索引的方法 检查链表是否包含某对象的方法： 删除（remove/pop）方法 LinkedList类常用方法测试： 简介 LinkedList是一个实现了List接口和Deque接口的双端链表。 LinkedList底层的链表结构使它支持高效的插入和删除操作，另外它实现了Deque接口，使得LinkedList类也具有队列的特性; LinkedList不是线程安全的，如果想使LinkedList变成线程安全的，可以调用静态类Collections类中的synchronizedList方法： ```java List list=Collections.synchronizedList(new LinkedList(...)); ``` ## 内部结构分析 **如下图所示：** ![LinkedList内部结构](https://user-gold-cdn.xitu.io/2018/3/19/1623e363fe0450b0?w=600&h=481&f=jpeg&s=18502) 看完了图之后，我们再看LinkedList类中的一个**内部私有类Node**就很好理解了： ```java private static class Node { E item;//节点值 Node next;//后继节点 Node prev;//前驱节点 Node(Node prev, E element, Node next) { this.item = element; this.next = next; this.prev = prev; } } ``` 这个类就代表双端链表的节点Node。这个类有三个属性，分别是前驱节点，本节点的值，后继结点。 ## LinkedList源码分析 ### 构造方法 **空构造方法：** ```java public LinkedList() { } ``` **用已有的集合创建链表的构造方法：** ```java public LinkedList(Collection c) { this(); addAll(c); } ``` ### add方法 **add(E e)** 方法：将元素添加到链表尾部 ```java public boolean add(E e) { linkLast(e);//这里就只调用了这一个方法 return true; } ``` ```java /** * 链接使e作为最后一个元素。 */ void linkLast(E e) { final Node l = last; final Node newNode = new Node<>(l, e, null); last = newNode;//新建节点 if (l == null) first = newNode; else l.next = newNode;//指向后继元素也就是指向下一个元素 size++; modCount++; } ``` **add(int index,E e)**：在指定位置添加元素 ```java public void add(int index, E element) { checkPositionIndex(index); //检查索引是否处于[0-size]之间 if (index == size)//添加在链表尾部 linkLast(element); else//添加在链表中间 linkBefore(element, node(index)); } ``` linkBefore方法需要给定两个参数，一个插入节点的值，一个指定的node，所以我们又调用了Node(index)去找到index对应的node addAll(Collection c )：将集合插入到链表尾部 public boolean addAll(Collection c) { return addAll(size, c); } addAll(int index, Collection c)： 将集合从指定位置开始插入 public boolean addAll(int index, Collection c) { //1:检查index范围是否在size之内 checkPositionIndex(index); //2:toArray()方法把集合的数据存到对象数组中 Object[] a = c.toArray(); int numNew = a.length; if (numNew == 0) return false; //3：得到插入位置的前驱节点和后继节点 Node pred, succ; //如果插入位置为尾部，前驱节点为last，后继节点为null if (index == size) { succ = null; pred = last; } //否则，调用node()方法得到后继节点，再得到前驱节点 else { succ = node(index); pred = succ.prev; } // 4：遍历数据将数据插入 for (Object o : a) { @SuppressWarnings(\"unchecked\") E e = (E) o; //创建新节点 Node newNode = new Node<>(pred, e, null); //如果插入位置在链表头部 if (pred == null) first = newNode; else pred.next = newNode; pred = newNode; } //如果插入位置在尾部，重置last节点 if (succ == null) { last = pred; } //否则，将插入的链表与先前链表连接起来 else { pred.next = succ; succ.prev = pred; } size += numNew; modCount++; return true; } 上面可以看出addAll方法通常包括下面四个步骤： 检查index范围是否在size之内 toArray()方法把集合的数据存到对象数组中 得到插入位置的前驱和后继节点 遍历数据，将数据插入到指定位置 addFirst(E e)： 将元素添加到链表头部 public void addFirst(E e) { linkFirst(e); } private void linkFirst(E e) { final Node f = first; final Node newNode = new Node<>(null, e, f);//新建节点，以头节点为后继节点 first = newNode; //如果链表为空，last节点也指向该节点 if (f == null) last = newNode; //否则，将头节点的前驱指针指向新节点，也就是指向前一个元素 else f.prev = newNode; size++; modCount++; } addLast(E e)： 将元素添加到链表尾部，与 add(E e) 方法一样 public void addLast(E e) { linkLast(e); } 根据位置取数据的方法 get(int index)： 根据指定索引返回数据 public E get(int index) { //检查index范围是否在size之内 checkElementIndex(index); //调用Node(index)去找到index对应的node然后返回它的值 return node(index).item; } 获取头节点（index=0）数据方法: public E getFirst() { final Node f = first; if (f == null) throw new NoSuchElementException(); return f.item; } public E element() { return getFirst(); } public E peek() { final Node f = first; return (f == null) ? null : f.item; } public E peekFirst() { final Node f = first; return (f == null) ? null : f.item; } 区别： getFirst(),element(),peek(),peekFirst() 这四个获取头结点方法的区别在于对链表为空时的处理，是抛出异常还是返回null，其中getFirst() 和element() 方法将会在链表为空时，抛出异常 element()方法的内部就是使用getFirst()实现的。它们会在链表为空时，抛出NoSuchElementException获取尾节点（index=-1）数据方法: public E getLast() { final Node l = last; if (l == null) throw new NoSuchElementException(); return l.item; } public E peekLast() { final Node l = last; return (l == null) ? null : l.item; } 两者区别： getLast() 方法在链表为空时，会抛出NoSuchElementException，而peekLast() 则不会，只是会返回 null。 根据对象得到索引的方法 int indexOf(Object o)： 从头遍历找 public int indexOf(Object o) { int index = 0; if (o == null) { //从头遍历 for (Node x = first; x != null; x = x.next) { if (x.item == null) return index; index++; } } else { //从头遍历 for (Node x = first; x != null; x = x.next) { if (o.equals(x.item)) return index; index++; } } return -1; } int lastIndexOf(Object o)： 从尾遍历找 public int lastIndexOf(Object o) { int index = size; if (o == null) { //从尾遍历 for (Node x = last; x != null; x = x.prev) { index--; if (x.item == null) return index; } } else { //从尾遍历 for (Node x = last; x != null; x = x.prev) { index--; if (o.equals(x.item)) return index; } } return -1; } 检查链表是否包含某对象的方法： contains(Object o)： 检查对象o是否存在于链表中 public boolean contains(Object o) { return indexOf(o) != -1; } 删除方法 remove() ,removeFirst(),pop(): 删除头节点 public E pop() { return removeFirst(); } public E remove() { return removeFirst(); } public E removeFirst() { final Node f = first; if (f == null) throw new NoSuchElementException(); return unlinkFirst(f); } removeLast(),pollLast(): 删除尾节点 public E removeLast() { final Node l = last; if (l == null) throw new NoSuchElementException(); return unlinkLast(l); } public E pollLast() { final Node l = last; return (l == null) ? null : unlinkLast(l); } 区别： removeLast()在链表为空时将抛出NoSuchElementException，而pollLast()方法返回null。 remove(Object o): 删除指定元素 public boolean remove(Object o) { //如果删除对象为null if (o == null) { //从头开始遍历 for (Node x = first; x != null; x = x.next) { //找到元素 if (x.item == null) { //从链表中移除找到的元素 unlink(x); return true; } } } else { //从头开始遍历 for (Node x = first; x != null; x = x.next) { //找到元素 if (o.equals(x.item)) { //从链表中移除找到的元素 unlink(x); return true; } } } return false; } 当删除指定对象时，只需调用remove(Object o)即可，不过该方法一次只会删除一个匹配的对象，如果删除了匹配对象，返回true，否则false。 unlink(Node x) 方法： E unlink(Node x) { // assert x != null; final E element = x.item; final Node next = x.next;//得到后继节点 final Node prev = x.prev;//得到前驱节点 //删除前驱指针 if (prev == null) { first = next;//如果删除的节点是头节点,令头节点指向该节点的后继节点 } else { prev.next = next;//将前驱节点的后继节点指向后继节点 x.prev = null; } //删除后继指针 if (next == null) { last = prev;//如果删除的节点是尾节点,令尾节点指向该节点的前驱节点 } else { next.prev = prev; x.next = null; } x.item = null; size--; modCount++; return element; } remove(int index)：删除指定位置的元素 public E remove(int index) { //检查index范围 checkElementIndex(index); //将节点删除 return unlink(node(index)); } LinkedList类常用方法测试 package list; import java.util.Iterator; import java.util.LinkedList; public class LinkedListDemo { public static void main(String[] srgs) { //创建存放int类型的linkedList LinkedList linkedList = new LinkedList<>(); /************************** linkedList的基本操作 ************************/ linkedList.addFirst(0); // 添加元素到列表开头 linkedList.add(1); // 在列表结尾添加元素 linkedList.add(2, 2); // 在指定位置添加元素 linkedList.addLast(3); // 添加元素到列表结尾 System.out.println(\"LinkedList（直接输出的）: \" + linkedList); System.out.println(\"getFirst()获得第一个元素: \" + linkedList.getFirst()); // 返回此列表的第一个元素 System.out.println(\"getLast()获得第最后一个元素: \" + linkedList.getLast()); // 返回此列表的最后一个元素 System.out.println(\"removeFirst()删除第一个元素并返回: \" + linkedList.removeFirst()); // 移除并返回此列表的第一个元素 System.out.println(\"removeLast()删除最后一个元素并返回: \" + linkedList.removeLast()); // 移除并返回此列表的最后一个元素 System.out.println(\"After remove:\" + linkedList); System.out.println(\"contains()方法判断列表是否包含1这个元素:\" + linkedList.contains(1)); // 判断此列表包含指定元素，如果是，则返回true System.out.println(\"该linkedList的大小 : \" + linkedList.size()); // 返回此列表的元素个数 /************************** 位置访问操作 ************************/ System.out.println(\"-----------------------------------------\"); linkedList.set(1, 3); // 将此列表中指定位置的元素替换为指定的元素 System.out.println(\"After set(1, 3):\" + linkedList); System.out.println(\"get(1)获得指定位置（这里为1）的元素: \" + linkedList.get(1)); // 返回此列表中指定位置处的元素 /************************** Search操作 ************************/ System.out.println(\"-----------------------------------------\"); linkedList.add(3); System.out.println(\"indexOf(3): \" + linkedList.indexOf(3)); // 返回此列表中首次出现的指定元素的索引 System.out.println(\"lastIndexOf(3): \" + linkedList.lastIndexOf(3));// 返回此列表中最后出现的指定元素的索引 /************************** Queue操作 ************************/ System.out.println(\"-----------------------------------------\"); System.out.println(\"peek(): \" + linkedList.peek()); // 获取但不移除此列表的头 System.out.println(\"element(): \" + linkedList.element()); // 获取但不移除此列表的头 linkedList.poll(); // 获取并移除此列表的头 System.out.println(\"After poll():\" + linkedList); linkedList.remove(); System.out.println(\"After remove():\" + linkedList); // 获取并移除此列表的头 linkedList.offer(4); System.out.println(\"After offer(4):\" + linkedList); // 将指定元素添加到此列表的末尾 /************************** Deque操作 ************************/ System.out.println(\"-----------------------------------------\"); linkedList.offerFirst(2); // 在此列表的开头插入指定的元素 System.out.println(\"After offerFirst(2):\" + linkedList); linkedList.offerLast(5); // 在此列表末尾插入指定的元素 System.out.println(\"After offerLast(5):\" + linkedList); System.out.println(\"peekFirst(): \" + linkedList.peekFirst()); // 获取但不移除此列表的第一个元素 System.out.println(\"peekLast(): \" + linkedList.peekLast()); // 获取但不移除此列表的第一个元素 linkedList.pollFirst(); // 获取并移除此列表的第一个元素 System.out.println(\"After pollFirst():\" + linkedList); linkedList.pollLast(); // 获取并移除此列表的最后一个元素 System.out.println(\"After pollLast():\" + linkedList); linkedList.push(2); // 将元素推入此列表所表示的堆栈（插入到列表的头） System.out.println(\"After push(2):\" + linkedList); linkedList.pop(); // 从此列表所表示的堆栈处弹出一个元素（获取并移除列表第一个元素） System.out.println(\"After pop():\" + linkedList); linkedList.add(3); linkedList.removeFirstOccurrence(3); // 从此列表中移除第一次出现的指定元素（从头部到尾部遍历列表） System.out.println(\"After removeFirstOccurrence(3):\" + linkedList); linkedList.removeLastOccurrence(3); // 从此列表中移除最后一次出现的指定元素（从尾部到头部遍历列表） System.out.println(\"After removeFirstOccurrence(3):\" + linkedList); /************************** 遍历操作 ************************/ System.out.println(\"-----------------------------------------\"); linkedList.clear(); for (int i = 0; i iterator = linkedList.iterator(); while (iterator.hasNext()) { iterator.next(); } long end = System.currentTimeMillis(); System.out.println(\"Iterator：\" + (end - start) + \" ms\"); // 顺序遍历(随机遍历) start = System.currentTimeMillis(); for (int i = 0; i temp1 = new LinkedList<>(); temp1.addAll(linkedList); start = System.currentTimeMillis(); while (temp1.size() != 0) { temp1.pollFirst(); } end = System.currentTimeMillis(); System.out.println(\"pollFirst()或pollLast()：\" + (end - start) + \" ms\"); // 通过removeFirst()或removeLast()来遍历LinkedList LinkedList temp2 = new LinkedList<>(); temp2.addAll(linkedList); start = System.currentTimeMillis(); while (temp2.size() != 0) { temp2.removeFirst(); } end = System.currentTimeMillis(); System.out.println(\"removeFirst()或removeLast()：\" + (end - start) + \" ms\"); } } "},"zother6-JavaGuide/java/java-programming-problem/a-thread-safe-implementation-of-lru-cache.html":{"url":"zother6-JavaGuide/java/java-programming-problem/a-thread-safe-implementation-of-lru-cache.html","title":"A Thread Safe Implementation Of Lru Cache","keywords":"","body":" 1. LRU 缓存介绍 2. ConcurrentLinkedQueue简单介绍 3. ReadWriteLock简单介绍 4. ScheduledExecutorService 简单介绍 5. 徒手撸一个线程安全的 LRU 缓存 5.1. 实现方法 5.2. 原理 5.3. put方法具体流程分析 5.4. 源码 6. 实现一个线程安全并且带有过期时间的 LRU 缓存 最近被读者问到“不用LinkedHashMap的话，如何实现一个线程安全的 LRU 缓存？网上的代码太杂太乱，Guide哥哥能不能帮忙写一个？”。 划重点，手写一个 LRU 缓存在面试中还是挺常见的！ 很多人就会问了：“网上已经有这么多现成的缓存了！为什么面试官还要我们自己实现一个呢？” 。咳咳咳，当然是为了面试需要。哈哈！开个玩笑，我个人觉得更多地是为了学习吧！今天Guide哥教大家： 实现一个线程安全的 LRU 缓存 实现一个线程安全并且带有过期时间的 LRU 缓存 考虑到了线程安全性我们使用了 ConcurrentHashMap 、ConcurrentLinkedQueue 这两个线程安全的集合。另外，还用到 ReadWriteLock（读写锁）。为了实现带有过期时间的缓存，我们用到了 ScheduledExecutorService来做定时任务执行。 如果有任何不对或者需要完善的地方，请帮忙指出！ 1. LRU 缓存介绍 LRU (Least Recently Used,最近最少使用)是一种缓存淘汰策略。 LRU缓存指的是当缓存大小已达到最大分配容量的时候，如果再要去缓存新的对象数据的话，就需要将缓存中最近访问最少的对象删除掉以便给新来的数据腾出空间。 2. ConcurrentLinkedQueue简单介绍 ConcurrentLinkedQueue是一个基于单向链表的无界无锁线程安全的队列，适合在高并发环境下使用，效率比较高。 我们在使用的时候，可以就把它理解为我们经常接触的数据结构——队列，不过是增加了多线程下的安全性保证罢了。和普通队列一样，它也是按照先进先出(FIFO)的规则对接点进行排序。 另外，队列元素中不可以放置null元素。 ConcurrentLinkedQueue 整个继承关系如下图所示： ConcurrentLinkedQueue中最主要的两个方法是：offer(value)和poll()，分别实现队列的两个重要的操作：入队和出队(offer(value)等价于 add(value))。 我们添加一个元素到队列的时候，它会添加到队列的尾部，当我们获取一个元素时，它会返回队列头部的元素。 利用ConcurrentLinkedQueue队列先进先出的特性，每当我们 put/get(缓存被使用)元素的时候，我们就将这个元素存放在队列尾部，这样就能保证队列头部的元素是最近最少使用的。 3. ReadWriteLock简单介绍 ReadWriteLock 是一个接口，位于java.util.concurrent.locks包下，里面只有两个方法分别返回读锁和写锁： public interface ReadWriteLock { /** * 返回读锁 */ Lock readLock(); /** * 返回写锁 */ Lock writeLock(); } ReentrantReadWriteLock 是ReadWriteLock接口的具体实现类。 读写锁还是比较适合缓存这种读多写少的场景。读写锁可以保证多个线程和同时读取，但是只有一个线程可以写入。 读写锁的特点是：写锁和写锁互斥，读锁和写锁互斥，读锁之间不互斥。也就说：同一时刻只能有一个线程写，但是可以有多个线程 读。读写之间是互斥的，两者不能同时发生（当进行写操作时，同一时刻其他线程的读操作会被阻塞；当进行读操作时，同一时刻所有线程的写操作会被阻塞）。 另外，同一个线程持有写锁时是可以申请读锁，但是持有读锁的情况下不可以申请写锁。 4. ScheduledExecutorService 简单介绍 ScheduledExecutorService 是一个接口，ScheduledThreadPoolExecutor 是其主要实现类。 ScheduledThreadPoolExecutor 主要用来在给定的延迟后运行任务，或者定期执行任务。 这个在实际项目用到的比较少，因为有其他方案选择比如quartz。但是，在一些需求比较简单的场景下还是非常有用的！ ScheduledThreadPoolExecutor 使用的任务队列 DelayQueue 封装了一个 PriorityQueue，PriorityQueue 会对队列中的任务进行排序，执行所需时间短的放在前面先被执行，如果执行所需时间相同则先提交的任务将被先执行。 5. 徒手撸一个线程安全的 LRU 缓存 5.1. 实现方法 ConcurrentHashMap + ConcurrentLinkedQueue +ReadWriteLock 5.2. 原理 ConcurrentHashMap 是线程安全的Map，我们可以利用它缓存 key,value形式的数据。ConcurrentLinkedQueue是一个线程安全的基于链表的队列（先进先出），我们可以用它来维护 key 。每当我们put/get(缓存被使用)元素的时候，我们就将这个元素对应的 key 存放在队列尾部，这样就能保证队列头部的元素是最近最少使用的。当我们的缓存容量不够的时候，我们直接移除队列头部对应的key以及这个key对应的缓存即可！ 另外，我们用到了ReadWriteLock(读写锁)来保证线程安全。 5.3. put方法具体流程分析 为了方便大家理解，我将代码中比较重要的 put(key,value)方法的原理图画了出来，如下图所示： 5.4. 源码 /** * @author shuang.kou * * 使用 ConcurrentHashMap+ConcurrentLinkedQueue+ReadWriteLock实现线程安全的 LRU 缓存 * 这里只是为了学习使用，本地缓存推荐使用 Guava 自带的,使用 Spring 的话，推荐使用Spring Cache */ public class MyLruCache { /** * 缓存的最大容量 */ private final int maxCapacity; private ConcurrentHashMap cacheMap; private ConcurrentLinkedQueue keys; /** * 读写锁 */ private ReadWriteLock readWriteLock = new ReentrantReadWriteLock(); private Lock writeLock = readWriteLock.writeLock(); private Lock readLock = readWriteLock.readLock(); public MyLruCache(int maxCapacity) { if (maxCapacity (maxCapacity); keys = new ConcurrentLinkedQueue<>(); } public V put(K key, V value) { // 加写锁 writeLock.lock(); try { //1.key是否存在于当前缓存 if (cacheMap.containsKey(key)) { moveToTailOfQueue(key); cacheMap.put(key, value); return value; } //2.是否超出缓存容量，超出的话就移除队列头部的元素以及其对应的缓存 if (cacheMap.size() == maxCapacity) { System.out.println(\"maxCapacity of cache reached\"); removeOldestKey(); } //3.key不存在于当前缓存。将key添加到队列的尾部并且缓存key及其对应的元素 keys.add(key); cacheMap.put(key, value); return value; } finally { writeLock.unlock(); } } public V get(K key) { //加读锁 readLock.lock(); try { //key是否存在于当前缓存 if (cacheMap.containsKey(key)) { // 存在的话就将key移动到队列的尾部 moveToTailOfQueue(key); return cacheMap.get(key); } //不存在于当前缓存中就返回Null return null; } finally { readLock.unlock(); } } public V remove(K key) { writeLock.lock(); try { //key是否存在于当前缓存 if (cacheMap.containsKey(key)) { // 存在移除队列和Map中对应的Key keys.remove(key); return cacheMap.remove(key); } //不存在于当前缓存中就返回Null return null; } finally { writeLock.unlock(); } } /** * 将元素添加到队列的尾部(put/get的时候执行) */ private void moveToTailOfQueue(K key) { keys.remove(key); keys.add(key); } /** * 移除队列头部的元素以及其对应的缓存 (缓存容量已满的时候执行) */ private void removeOldestKey() { K oldestKey = keys.poll(); if (oldestKey != null) { cacheMap.remove(oldestKey); } } public int size() { return cacheMap.size(); } } 非并发环境测试： MyLruCache myLruCache = new MyLruCache<>(3); myLruCache.put(1, \"Java\"); System.out.println(myLruCache.get(1));// Java myLruCache.remove(1); System.out.println(myLruCache.get(1));// null myLruCache.put(2, \"C++\"); myLruCache.put(3, \"Python\"); System.out.println(myLruCache.get(2));//C++ myLruCache.put(4, \"C\"); myLruCache.put(5, \"PHP\"); System.out.println(myLruCache.get(2));// C++ 并发环境测试： 我们初始化了一个固定容量为 10 的线程池和count为10的CountDownLatch。我们将1000000次操作分10次添加到线程池，然后我们等待线程池执行完成这10次操作。 int threadNum = 10; int batchSize = 100000; //init cache MyLruCache myLruCache = new MyLruCache<>(batchSize * 10); //init thread pool with 10 threads ExecutorService fixedThreadPool = Executors.newFixedThreadPool(threadNum); //init CountDownLatch with 10 count CountDownLatch latch = new CountDownLatch(threadNum); AtomicInteger atomicInteger = new AtomicInteger(0); long startTime = System.currentTimeMillis(); for (int t = 0; t { for (int i = 0; i 6. 实现一个线程安全并且带有过期时间的 LRU 缓存 实际上就是在我们上面时间的LRU缓存的基础上加上一个定时任务去删除缓存，单纯利用 JDK 提供的类，我们实现定时任务的方式有很多种： Timer :不被推荐，多线程会存在问题。 ScheduledExecutorService ：定时器线程池，可以用来替代 Timer DelayQueue ：延时队列 quartz ：一个很火的开源任务调度框架，很多其他框架都是基于 quartz 开发的，比如当当网的elastic-job就是基于quartz二次开发之后的分布式调度解决方案 ...... 最终我们选择了 ScheduledExecutorService，主要原因是它易用（基于DelayQueue做了很多封装）并且基本能满足我们的大部分需求。 我们在我们上面实现的线程安全的 LRU 缓存基础上，简单稍作修改即可！我们增加了一个方法： private void removeAfterExpireTime(K key, long expireTime) { scheduledExecutorService.schedule(() -> { //过期后清除该键值对 cacheMap.remove(key); keys.remove(key); }, expireTime, TimeUnit.MILLISECONDS); } 我们put元素的时候，如果通过这个方法就能直接设置过期时间。 完整源码如下： /** * @author shuang.kou * * 使用 ConcurrentHashMap+ConcurrentLinkedQueue+ReadWriteLock+ScheduledExecutorService实现线程安全的 LRU 缓存 * 这里只是为了学习使用，本地缓存推荐使用 Guava 自带的，使用 Spring 的话，推荐使用Spring Cache */ public class MyLruCacheWithExpireTime { /** * 缓存的最大容量 */ private final int maxCapacity; private ConcurrentHashMap cacheMap; private ConcurrentLinkedQueue keys; /** * 读写锁 */ private ReadWriteLock readWriteLock = new ReentrantReadWriteLock(); private Lock writeLock = readWriteLock.writeLock(); private Lock readLock = readWriteLock.readLock(); private ScheduledExecutorService scheduledExecutorService; public MyLruCacheWithExpireTime(int maxCapacity) { if (maxCapacity (maxCapacity); keys = new ConcurrentLinkedQueue<>(); scheduledExecutorService = Executors.newScheduledThreadPool(3); } public V put(K key, V value, long expireTime) { // 加写锁 writeLock.lock(); try { //1.key是否存在于当前缓存 if (cacheMap.containsKey(key)) { moveToTailOfQueue(key); cacheMap.put(key, value); return value; } //2.是否超出缓存容量，超出的话就移除队列头部的元素以及其对应的缓存 if (cacheMap.size() == maxCapacity) { System.out.println(\"maxCapacity of cache reached\"); removeOldestKey(); } //3.key不存在于当前缓存。将key添加到队列的尾部并且缓存key及其对应的元素 keys.add(key); cacheMap.put(key, value); if (expireTime > 0) { removeAfterExpireTime(key, expireTime); } return value; } finally { writeLock.unlock(); } } public V get(K key) { //加读锁 readLock.lock(); try { //key是否存在于当前缓存 if (cacheMap.containsKey(key)) { // 存在的话就将key移动到队列的尾部 moveToTailOfQueue(key); return cacheMap.get(key); } //不存在于当前缓存中就返回Null return null; } finally { readLock.unlock(); } } public V remove(K key) { writeLock.lock(); try { //key是否存在于当前缓存 if (cacheMap.containsKey(key)) { // 存在移除队列和Map中对应的Key keys.remove(key); return cacheMap.remove(key); } //不存在于当前缓存中就返回Null return null; } finally { writeLock.unlock(); } } /** * 将元素添加到队列的尾部(put/get的时候执行) */ private void moveToTailOfQueue(K key) { keys.remove(key); keys.add(key); } /** * 移除队列头部的元素以及其对应的缓存 (缓存容量已满的时候执行) */ private void removeOldestKey() { K oldestKey = keys.poll(); if (oldestKey != null) { cacheMap.remove(oldestKey); } } private void removeAfterExpireTime(K key, long expireTime) { scheduledExecutorService.schedule(() -> { //过期后清除该键值对 cacheMap.remove(key); keys.remove(key); }, expireTime, TimeUnit.MILLISECONDS); } public int size() { return cacheMap.size(); } } 测试效果： MyLruCacheWithExpireTime myLruCache = new MyLruCacheWithExpireTime<>(3); myLruCache.put(1,\"Java\",3000); myLruCache.put(2,\"C++\",3000); myLruCache.put(3,\"Python\",1500); System.out.println(myLruCache.size());//3 Thread.sleep(2000); System.out.println(myLruCache.size());//2 "},"zother6-JavaGuide/java/java-programming-problem/Java程序设计题.html":{"url":"zother6-JavaGuide/java/java-programming-problem/Java程序设计题.html","title":"Java程序设计题","keywords":"","body":" 0.0.1. 泛型的实际应用:实现最小值函数 0.0.2. 使用数组实现栈 0.0.3. 实现线程安全的 LRU 缓存 0.0.1. 泛型的实际应用:实现最小值函数 自己设计一个泛型的获取数组最小值的函数.并且这个方法只能接受Number的子类并且实现了Comparable接口。 //注意：Number并没有实现Comparable private static > T min(T[] values) { if (values == null || values.length == 0) return null; T min = values[0]; for (int i = 1; i 0) min = values[i]; } return min; } 测试： int minInteger = min(new Integer[]{1, 2, 3});//result:1 double minDouble = min(new Double[]{1.2, 2.2, -1d});//result:-1d String typeError = min(new String[]{\"1\",\"3\"});//报错 0.0.2. 使用数组实现栈 自己实现一个栈，要求这个栈具有push()、pop()（返回栈顶元素并出栈）、peek() （返回栈顶元素不出栈）、isEmpty()、size()这些基本的方法。 提示：每次入栈之前先判断栈的容量是否够用，如果不够用就用Arrays.copyOf()进行扩容； public class MyStack { private int[] storage;//存放栈中元素的数组 private int capacity;//栈的容量 private int count;//栈中元素数量 private static final int GROW_FACTOR = 2; //不带初始容量的构造方法。默认容量为8 public MyStack() { this.capacity = 8; this.storage=new int[8]; this.count = 0; } //带初始容量的构造方法 public MyStack(int initialCapacity) { if (initialCapacity 验证 MyStack myStack = new MyStack(3); myStack.push(1); myStack.push(2); myStack.push(3); myStack.push(4); myStack.push(5); myStack.push(6); myStack.push(7); myStack.push(8); System.out.println(myStack.peek());//8 System.out.println(myStack.size());//8 for (int i = 0; i "},"zother6-JavaGuide/java/jdk-new-features/new-features-from-jdk8-to-jdk14.html":{"url":"zother6-JavaGuide/java/jdk-new-features/new-features-from-jdk8-to-jdk14.html","title":"New Features From Jdk 8 To Jdk 14","keywords":"","body":"大家好，我是Guide哥！这篇文章来自读者的投稿，经过了两次较大的改动，两周的完善终于完成。Java 8新特性见这里：Java8新特性最佳指南 。 Guide 哥：别人家的特性都用了几年了，我 Java 才出来，哈哈！真实！ Java9 发布于 2017 年 9 月 21 日 。作为 Java8 之后 3 年半才发布的新版本，Java 9 带 来了很多重大的变化其中最重要的改动是 Java 平台模块系统的引入,其他还有诸如集合、Stream 流 Java 平台模块系统 Java 平台模块系统，也就是 Project Jigsaw，把模块化开发实践引入到了 Java 平台中。在引入了模块系统之后，JDK 被重新组织成 94 个模块。Java 应用可以通过新增的 jlink 工具，创建出只包含所依赖的 JDK 模块的自定义运行时镜像。这样可以极大的减少 Java 运行时环境的大小。 Java 9 模块的重要特征是在其工件（artifact）的根目录中包含了一个描述模块的 module-info.class 文 件。 工件的格式可以是传统的 JAR 文件或是 Java 9 新增的 JMOD 文件。 Jshell jshell 是 Java 9 新增的一个实用工具。为 Java 提供了类似于 Python 的实时命令行交互工具。 在 Jshell 中可以直接输入表达式并查看其执行结果 集合、Stream 和 Optional 增加 了 List.of()、Set.of()、Map.of() 和 Map.ofEntries()等工厂方法来创建不可变集合，比如List.of(\"Java\", \"C++\");、Map.of(\"Java\", 1, \"C++\", 2);（这部分内容有点参考 Guava 的味道） Stream 中增加了新的方法 ofNullable、dropWhile、takeWhile 和 iterate 方法。Collectors 中增加了新的方法 filtering 和 flatMapping Optional 类中新增了 ifPresentOrElse、or 和 stream 等方法 进程 API Java 9 增加了 ProcessHandle 接口，可以对原生进程进行管理，尤其适合于管理长时间运行的进程 平台日志 API 和服务 Java 9 允许为 JDK 和应用配置同样的日志实现。新增了 System.LoggerFinder 用来管理 JDK 使 用的日志记录器实现。JVM 在运行时只有一个系统范围的 LoggerFinder 实例。 我们可以通过添加自己的 System.LoggerFinder 实现来让 JDK 和应用使用 SLF4J 等其他日志记录框架。 反应式流 （ Reactive Streams ） 在 Java9 中的 java.util.concurrent.Flow 类中新增了反应式流规范的核心接口 Flow 中包含了 Flow.Publisher、Flow.Subscriber、Flow.Subscription 和 Flow.Processor 等 4 个核心接口。Java 9 还提供了SubmissionPublisher 作为Flow.Publisher 的一个实现。 变量句柄 变量句柄是一个变量或一组变量的引用，包括静态域，非静态域，数组元素和堆外数据结构中的组成部分等 变量句柄的含义类似于已有的方法句柄MethodHandle 由 Java 类java.lang.invoke.VarHandle 来表示，可以使用类 java.lang.invoke.MethodHandles.Lookup 中的静态工厂方法来创建 VarHandle 对 象 改进方法句柄（Method Handle） 方法句柄从 Java7 开始引入，Java9 在类java.lang.invoke.MethodHandles 中新增了更多的静态方法来创建不同类型的方法句柄 其它新特性 接口私有方法 ：Java 9 允许在接口中使用私有方法 try-with-resources 增强 ：在 try-with-resources 语句中可以使用 effectively-final 变量（什么是 effectively-final 变量，见这篇文章 http://ilkinulas.github.io/programming/java/2016/03/27/effectively-final-java.html） 类 CompletableFuture 中增加了几个新的方法（completeAsync ，orTimeout 等） Nashorn 引擎的增强 ：Nashorn 从 Java8 开始引入的 JavaScript 引擎，Java9 对 Nashorn 做了些增强，实现了一些 ES6 的新特性 I/O 流的新特性 ：增加了新的方法来读取和复制 InputStream 中包含的数据 改进应用的安全性能 ：Java 9 新增了 4 个 SHA- 3 哈希算法，SHA3-224、SHA3-256、SHA3-384 和 S HA3-512 ...... Java10 发布于 2018 年 3 月 20 日，最知名的特性应该是 var 关键字（局部变量类型推断）的引入了，其他还有垃圾收集器改善、GC 改进、性能提升、线程管控等一批新特性 var 关键字 介绍 :提供了 var 关键字声明局部变量：var list = new ArrayList(); // ArrayList 局限性 ：只能用于带有构造器的局部变量和 for 循环中 Guide 哥：实际上 Lombok 早就体用了一个类似的关键字，使用它可以简化代码，但是可能会降低程序的易读性、可维护性。一般情况下，我个人都不太推荐使用。 不可变集合 list，set，map 提供了静态方法copyOf()返回入参集合的一个不可变拷贝（以下为 JDK 的源码） static List copyOf(Collection coll) { return ImmutableCollections.listCopy(coll); } java.util.stream.Collectors中新增了静态方法，用于将流中的元素收集为不可变的集合 Optional 新增了orElseThrow()方法来在没有值时抛出异常 并行全垃圾回收器 G1 从 Java9 开始 G1 就了默认的垃圾回收器，G1 是以一种低延时的垃圾回收器来设计的，旨在避免进行 Full GC,但是 Java9 的 G1 的 FullGC 依然是使用单线程去完成标记清除算法,这可能会导致垃圾回收期在无法回收内存的时候触发 Full GC。 为了最大限度地减少 Full GC 造成的应用停顿的影响，从 Java10 开始，G1 的 FullGC 改为并行的标记清除算法，同时会使用与年轻代回收和混合回收相同的并行工作线程数量，从而减少了 Full GC 的发生，以带来更好的性能提升、更大的吞吐量。 应用程序类数据共享 在 Java 5 中就已经引入了类数据共享机制 (Class Data Sharing，简称 CDS)，允许将一组类预处理为共享归档文件，以便在运行时能够进行内存映射以减少 Java 程序的启动时间，当多个 Java 虚拟机（JVM）共享相同的归档文件时，还可以减少动态内存的占用量，同时减少多个虚拟机在同一个物理或虚拟的机器上运行时的资源占用 Java 10 在现有的 CDS 功能基础上再次拓展，以允许应用类放置在共享存档中。CDS 特性在原来的 bootstrap 类基础之上，扩展加入了应用类的 CDS (Application Class-Data Sharing) 支持。其原理为：在启动时记录加载类的过程，写入到文本文件中，再次启动时直接读取此启动文本并加载。设想如果应用环境没有大的变化，启动速度就会得到提升 其他特性 线程-局部管控：Java 10 中线程管控引入 JVM 安全点的概念，将允许在不运行全局 JVM 安全点的情况下实现线程回调，由线程本身或者 JVM 线程来执行，同时保持线程处于阻塞状态，这种方式使得停止单个线程变成可能，而不是只能启用或停止所有线程 备用存储装置上的堆分配：Java 10 中将使得 JVM 能够使用适用于不同类型的存储机制的堆，在可选内存设备上进行堆内存分配 统一的垃圾回收接口：Java 10 中，hotspot/gc 代码实现方面，引入一个干净的 GC 接口，改进不同 GC 源代码的隔离性，多个 GC 之间共享的实现细节代码应该存在于辅助类中。统一垃圾回收接口的主要原因是：让垃圾回收器（GC）这部分代码更加整洁，便于新人上手开发，便于后续排查相关问题。 Java11 Java11 于 2018 年 9 月 25 日正式发布，这是很重要的一个版本！Java 11 和 2017 年 9 月份发布的 Java 9 以及 2018 年 3 月份发布的 Java 10 相比，其最大的区别就是：在长期支持(Long-Term-Support)方面，Oracle 表示会对 Java 11 提供大力支持，这一支持将会持续至 2026 年 9 月。这是据 Java 8 以后支持的首个长期版本。 字符串加强 Java 11 增加了一系列的字符串处理方法，如以下所示。 Guide 哥：说白点就是多了层封装，JDK 开发组的人没少看市面上常见的工具类框架啊! //判断字符串是否为空 \" \".isBlank();//true //去除字符串首尾空格 \" Java \".strip();// \"Java\" //去除字符串首部空格 \" Java \".stripLeading(); // \"Java \" //去除字符串尾部空格 \" Java \".stripTrailing(); // \" Java\" //重复字符串多少次 \"Java\".repeat(3); // \"JavaJavaJava\" //返回由行终止符分隔的字符串集合。 \"A\\nB\\nC\".lines().count(); // 3 \"A\\nB\\nC\".lines().collect(Collectors.toList()); ZGC：可伸缩低延迟垃圾收集器 ZGC 即 Z Garbage Collector，是一个可伸缩的、低延迟的垃圾收集器。 ZGC 主要为了满足如下目标进行设计： GC 停顿时间不超过 10ms 即能处理几百 MB 的小堆，也能处理几个 TB 的大堆 应用吞吐能力不会下降超过 15%（与 G1 回收算法相比） 方便在此基础上引入新的 GC 特性和利用 colord 针以及 Load barriers 优化奠定基础 当前只支持 Linux/x64 位平台 ZGC 目前 处在实验阶段，只支持 Linux/x64 平台 标准 HTTP Client 升级 Java 11 对 Java 9 中引入并在 Java 10 中进行了更新的 Http Client API 进行了标准化，在前两个版本中进行孵化的同时，Http Client 几乎被完全重写，并且现在完全支持异步非阻塞。 并且，Java11 中，Http Client 的包名由 jdk.incubator.http 改为java.net.http，该 API 通过 CompleteableFuture 提供非阻塞请求和响应语义。 使用起来也很简单，如下： var request = HttpRequest.newBuilder() .uri(URI.create(\"https://javastack.cn\")) .GET() .build(); var client = HttpClient.newHttpClient(); // 同步 HttpResponse response = client.send(request, HttpResponse.BodyHandlers.ofString()); System.out.println(response.body()); // 异步 client.sendAsync(request, HttpResponse.BodyHandlers.ofString()) .thenApply(HttpResponse::body) .thenAccept(System.out::println); 简化启动单个源代码文件的方法 增强了 Java 启动器，使其能够运行单一文件的 Java 源代码。此功能允许使用 Java 解释器直接执行 Java 源代码。源代码在内存中编译，然后由解释器执行。唯一的约束在于所有相关的类必须定义在同一个 Java 文件中 对于 Java 初学者并希望尝试简单程序的人特别有用，并且能和 jshell 一起使用 一定能程度上增强了使用 Java 来写脚本程序的能力 用于 Lambda 参数的局部变量语法 从 Java 10 开始，便引入了局部变量类型推断这一关键特性。类型推断允许使用关键字 var 作为局部变量的类型而不是实际类型，编译器根据分配给变量的值推断出类型 Java 10 中对 var 关键字存在几个限制 只能用于局部变量上 声明时必须初始化 不能用作方法参数 不能在 Lambda 表达式中使用 Java11 开始允许开发者在 Lambda 表达式中使用 var 进行参数声明 其他特性 新的垃圾回收器 Epsilon，一个完全消极的 GC 实现，分配有限的内存资源，最大限度的降低内存占用和内存吞吐延迟时间 低开销的 Heap Profiling：Java 11 中提供一种低开销的 Java 堆分配采样方法，能够得到堆分配的 Java 对象信息，并且能够通过 JVMTI 访问堆信息 TLS1.3 协议：Java 11 中包含了传输层安全性（TLS）1.3 规范（RFC 8446）的实现，替换了之前版本中包含的 TLS，包括 TLS 1.2，同时还改进了其他 TLS 功能，例如 OCSP 装订扩展（RFC 6066，RFC 6961），以及会话散列和扩展主密钥扩展（RFC 7627），在安全性和性能方面也做了很多提升 飞行记录器：飞行记录器之前是商业版 JDK 的一项分析工具，但在 Java 11 中，其代码被包含到公开代码库中，这样所有人都能使用该功能了 Java12 增强 Switch 传统的 switch 语法存在容易漏写 break 的问题，而且从代码整洁性层面来看，多个 break 本质也是一种重复 Java12 提供了 swtich 表达式，使用类似 lambda 语法条件匹配成功后的执行块，不需要多写 break 作为预览特性加入，需要在javac编译和java运行时增加参数--enable-preview switch (day) { case MONDAY, FRIDAY, SUNDAY -> System.out.println(6); case TUESDAY -> System.out.println(7); case THURSDAY, SATURDAY -> System.out.println(8); case WEDNESDAY -> System.out.println(9); } 数字格式化工具类 NumberFormat 新增了对复杂的数字进行格式化的支持 NumberFormat fmt = NumberFormat.getCompactNumberInstance(Locale.US, NumberFormat.Style.SHORT); String result = fmt.format(1000); System.out.println(result); // 输出为 1K，计算工资是多少K更方便了。。。 Shenandoah GC Redhat 主导开发的 Pauseless GC 实现，主要目标是 99.9% 的暂停小于 10ms，暂停与堆大小无关等 和 Java11 开源的 ZGC 相比（需要升级到 JDK11 才能使用），Shenandoah GC 有稳定的 JDK8u 版本，在 Java8 占据主要市场份额的今天有更大的可落地性 G1 收集器提升 Java12 为默认的垃圾收集器 G1 带来了两项更新: 可中止的混合收集集合：JEP344 的实现，为了达到用户提供的停顿时间目标，JEP 344 通过把要被回收的区域集（混合收集集合）拆分为强制和可选部分，使 G1 垃圾回收器能中止垃圾回收过程。 G1 可以中止可选部分的回收以达到停顿时间目标 及时返回未使用的已分配内存：JEP346 的实现，增强 G1 GC，以便在空闲时自动将 Java 堆内存返回给操作系统 Java13 引入 yield 关键字到 Switch 中 Switch 表达式中就多了一个关键字用于跳出 Switch 块的关键字 yield，主要用于返回一个值 yield和 return 的区别在于：return 会直接跳出当前循环或者方法，而 yield 只会跳出当前 Switch 块，同时在使用 yield 时，需要有 default 条件 private static String descLanguage(String name) { return switch (name) { case \"Java\": yield \"object-oriented, platform independent and secured\"; case \"Ruby\": yield \"a programmer's best friend\"; default: yield name +\" is a good language\"; }; } 文本块 解决 Java 定义多行字符串时只能通过换行转义或者换行连接符来变通支持的问题，引入三重双引号来定义多行文本 两个\"\"\"中间的任何内容都会被解释为字符串的一部分，包括换行符 String json =\"{\\n\" + \" \\\"name\\\":\\\"mkyong\\\",\\n\" + \" \\\"age\\\":38\\n\" + \"}\\n\"; // 未支持文本块之前 String json = \"\"\" { \"name\":\"mkyong\", \"age\":38 } \"\"\"; 增强 ZGC 释放未使用内存 在 Java 11 中是实验性的引入的 ZGC 在实际的使用中存在未能主动将未使用的内存释放给操作系统的问题 ZGC 堆由一组称为 ZPages 的堆区域组成。在 GC 周期中清空 ZPages 区域时，它们将被释放并返回到页面缓存 ZPageCache 中，此缓存中的 ZPages 按最近最少使用（LRU）的顺序，并按照大小进行组织 在 Java 13 中，ZGC 将向操作系统返回被标识为长时间未使用的页面，这样它们将可以被其他进程重用 SocketAPI 重构 Java 13 为 Socket API 带来了新的底层实现方法，并且在 Java 13 中是默认使用新的 Socket 实现，使其易于发现并在排除问题同时增加可维护性 动态应用程序类-数据共享 Java 13 中对 Java 10 中引入的 应用程序类数据共享进行了进一步的简化、改进和扩展，即：允许在 Java 应用程序执行结束时动态进行类归档，具体能够被归档的类包括：所有已被加载，但不属于默认基层 CDS 的应用程序类和引用类库中的类 Java14 record 关键字 简化数据类的定义方式，使用 record 代替 class 定义的类，只需要声明属性，就可以在获得属性的访问方法，以及 toString，hashCode,equals 方法 类似于使用 Class 定义类，同时使用了 lomobok 插件，并打上了@Getter,@ToString,@EqualsAndHashCode注解 作为预览特性引入 /** * 这个类具有两个特征 * 1. 所有成员属性都是final * 2. 全部方法由构造方法，和两个成员属性访问器组成（共三个） * 那么这种类就很适合使用record来声明 */ final class Rectangle implements Shape { final double length; final double width; public Rectangle(double length, double width) { this.length = length; this.width = width; } double length() { return length; } double width() { return width; } } /** * 1. 使用record声明的类会自动拥有上面类中的三个方法 * 2. 在这基础上还附赠了equals()，hashCode()方法以及toString()方法 * 3. toString方法中包括所有成员属性的字符串表示形式及其名称 */ record Rectangle(float length, float width) { } 空指针异常精准提示 通过 JVM 参数中添加-XX:+ShowCodeDetailsInExceptionMessages，可以在空指针异常中获取更为详细的调用信息，更快的定位和解决问题 a.b.c.i = 99; // 假设这段代码会发生空指针 Exception in thread \"main\" java.lang.NullPointerException: Cannot read field 'c' because 'a.b' is null. at Prog.main(Prog.java:5) // 增加参数后提示的异常中很明确的告知了哪里为空导致 switch 的增强终于转正 JDK12 引入的 switch（预览特性）在 JDK14 变为正式版本，不需要增加参数来启用，直接在 JDK14 中就能使用 主要是用->来替代以前的:+break；另外就是提供了 yield 来在 block 中返回值 Before Java 14 switch (day) { case MONDAY: case FRIDAY: case SUNDAY: System.out.println(6); break; case TUESDAY: System.out.println(7); break; case THURSDAY: case SATURDAY: System.out.println(8); break; case WEDNESDAY: System.out.println(9); break; } Java 14 enhancements switch (day) { case MONDAY, FRIDAY, SUNDAY -> System.out.println(6); case TUESDAY -> System.out.println(7); case THURSDAY, SATURDAY -> System.out.println(8); case WEDNESDAY -> System.out.println(9); } instanceof 增强 instanceof 主要在类型强转前探测对象的具体类型，然后执行具体的强转 新版的 instanceof 可以在判断的是否属于具体的类型同时完成转换 Object obj = \"我是字符串\"; if(obj instanceof String str){ System.out.println(str); } 其他特性 从 Java11 引入的 ZGC 作为继 G1 过后的下一代 GC 算法，从支持 Linux 平台到 Java14 开始支持 MacOS 和 Window（个人感觉是终于可以在日常开发工具中先体验下 ZGC 的效果了，虽然其实 G1 也够用） 移除了 CMS 垃圾收集器（功成而退） 新增了 jpackage 工具，标配将应用打成 jar 包外，还支持不同平台的特性包，比如 linux 下的deb和rpm，window 平台下的msi和exe 总结 关于预览特性 先贴一段 oracle 官网原文：This is a preview feature, which is a feature whose design, specification, and implementation are complete, but is not permanent, which means that the feature may exist in a different form or not at all in future JDK releases. To compile and run code that contains preview features, you must specify additional command-line options. 这是一个预览功能，该功能的设计，规格和实现是完整的，但不是永久性的，这意味着该功能可能以其他形式存在或在将来的 JDK 版本中根本不存在。 要编译和运行包含预览功能的代码，必须指定其他命令行选项。 就以switch的增强为例子，从 Java12 中推出，到 Java13 中将继续增强，直到 Java14 才正式转正进入 JDK 可以放心使用，不用考虑后续 JDK 版本对其的改动或修改 一方面可以看出 JDK 作为标准平台在增加新特性的严谨态度，另一方面个人认为是对于预览特性应该采取审慎使用的态度。特性的设计和实现容易，但是其实际价值依然需要在使用中去验证 JVM 虚拟机优化 每次 Java 版本的发布都伴随着对 JVM 虚拟机的优化，包括对现有垃圾回收算法的改进，引入新的垃圾回收算法，移除老旧的不再适用于今天的垃圾回收算法等 整体优化的方向是高效，低时延的垃圾回收表现 对于日常的应用开发者可能比较关注新的语法特性，但是从一个公司角度来说，在考虑是否升级 Java 平台时更加考虑的是JVM 运行时的提升 参考信息 IBM Developer Java9 https://www.ibm.com/developerworks/cn/java/the-new-features-of-Java-9/ Guide to Java10 https://www.baeldung.com/java-10-overview Java 10 新特性介绍https://www.ibm.com/developerworks/cn/java/the-new-features-of-Java-10/index.html IBM Devloper Java11 https://www.ibm.com/developerworks/cn/java/the-new-features-of-Java-11/index.html Java 11 – Features and Comparison： https://www.geeksforgeeks.org/java-11-features-and-comparison/ Oracle Java12 ReleaseNote https://www.oracle.com/technetwork/java/javase/12all-relnotes-5211423.html#NewFeature Oracle Java13 ReleaseNote https://www.oracle.com/technetwork/java/javase/13all-relnotes-5461743.html#NewFeature New Java13 Features https://www.baeldung.com/java-13-new-features Java13 新特性概述 https://www.ibm.com/developerworks/cn/java/the-new-features-of-Java-13/index.html Oracle Java14 record https://docs.oracle.com/en/java/javase/14/language/records.html java14-features https://www.techgeeknext.com/java/java14-features "},"zother6-JavaGuide/java/jvm/[加餐]大白话带你认识JVM.html":{"url":"zother6-JavaGuide/java/jvm/[加餐]大白话带你认识JVM.html","title":"[加餐]大白话带你认识JVM","keywords":"","body":" 来自掘金用户：说出你的愿望吧丷投稿，原文地址：https://juejin.im/post/5e1505d0f265da5d5d744050#heading-28 前言 如果在文中用词或者理解方面出现问题，欢迎指出。此文旨在提及和而不深究，但会尽量效率地把知识点都抛出来 一、JVM的基本介绍 JVM 是 Java Virtual Machine 的缩写，它是一个虚构出来的计算机，一种规范。通过在实际的计算机上仿真模拟各类计算机功能实现··· 好，其实抛开这么专业的句子不说，就知道JVM其实就类似于一台小电脑运行在windows或者linux这些操作系统环境下即可。它直接和操作系统进行交互，与硬件不直接交互，可操作系统可以帮我们完成和硬件进行交互的工作。 1.1 Java文件是如何被运行的 比如我们现在写了一个 HelloWorld.java 好了，那这个 HelloWorld.java 抛开所有东西不谈，那是不是就类似于一个文本文件，只是这个文本文件它写的都是英文，而且有一定的缩进而已。 那我们的 JVM 是不认识文本文件的，所以它需要一个 编译 ，让其成为一个它会读二进制文件的 HelloWorld.class ① 类加载器 如果 JVM 想要执行这个 .class 文件，我们需要将其装进一个 类加载器 中，它就像一个搬运工一样，会把所有的 .class 文件全部搬进JVM里面来。 ② 方法区 方法区 是用于存放类似于元数据信息方面的数据的，比如类信息，常量，静态变量，编译后代码···等 类加载器将 .class 文件搬过来就是先丢到这一块上 ③ 堆 堆 主要放了一些存储的数据，比如对象实例，数组···等，它和方法区都同属于 线程共享区域 。也就是说它们都是 线程不安全 的 ④ 栈 栈 这是我们的代码运行空间。我们编写的每一个方法都会放到 栈 里面运行。 我们会听说过 本地方法栈 或者 本地方法接口 这两个名词，不过我们基本不会涉及这两块的内容，它俩底层是使用C来进行工作的，和Java没有太大的关系。 ⑤ 程序计数器 主要就是完成一个加载工作，类似于一个指针一样的，指向下一行我们需要执行的代码。和栈一样，都是 线程独享 的，就是说每一个线程都会有自己对应的一块区域而不会存在并发和多线程的问题。 小总结 Java文件经过编译后变成 .class 字节码文件 字节码文件通过类加载器被搬运到 JVM 虚拟机中 虚拟机主要的5大块：方法区，堆都为线程共享区域，有线程安全问题，栈和本地方法栈和计数器都是独享区域，不存在线程安全问题，而 JVM 的调优主要就是围绕堆，栈两大块进行 1.2 简单的代码例子 一个简单的学生类 一个main方法 执行main方法的步骤如下: 编译好 App.java 后得到 App.class 后，执行 App.class，系统会启动一个 JVM 进程，从 classpath 路径中找到一个名为 App.class 的二进制文件，将 App 的类信息加载到运行时数据区的方法区内，这个过程叫做 App 类的加载 JVM 找到 App 的主程序入口，执行main方法 这个main中的第一条语句为 Student student = new Student(\"tellUrDream\") ，就是让 JVM 创建一个Student对象，但是这个时候方法区中是没有 Student 类的信息的，所以 JVM 马上加载 Student 类，把 Student 类的信息放到方法区中 加载完 Student 类后，JVM 在堆中为一个新的 Student 实例分配内存，然后调用构造函数初始化 Student 实例，这个 Student 实例持有 指向方法区中的 Student 类的类型信息 的引用 执行student.sayName();时，JVM 根据 student 的引用找到 student 对象，然后根据 student 对象持有的引用定位到方法区中 student 类的类型信息的方法表，获得 sayName() 的字节码地址。 执行sayName() 其实也不用管太多，只需要知道对象实例初始化时会去方法区中找类信息，完成后再到栈那里去运行方法。找方法就在方法表中找。 二、类加载器的介绍 之前也提到了它是负责加载.class文件的，它们在文件开头会有特定的文件标示，将class文件字节码内容加载到内存中，并将这些内容转换成方法区中的运行时数据结构，并且ClassLoader只负责class文件的加载，而是否能够运行则由 Execution Engine 来决定 2.1 类加载器的流程 从类被加载到虚拟机内存中开始，到释放内存总共有7个步骤：加载，验证，准备，解析，初始化，使用，卸载。其中验证，准备，解析三个部分统称为连接 2.1.1 加载 将class文件加载到内存 将静态数据结构转化成方法区中运行时的数据结构 在堆中生成一个代表这个类的 java.lang.Class对象作为数据访问的入口 2.1.2 链接 验证：确保加载的类符合 JVM 规范和安全，保证被校验类的方法在运行时不会做出危害虚拟机的事件，其实就是一个安全检查 准备：为static变量在方法区中分配内存空间，设置变量的初始值，例如 static int a = 3 （注意：准备阶段只设置类中的静态变量（方法区中），不包括实例变量（堆内存中），实例变量是对象初始化时赋值的） 解析：虚拟机将常量池内的符号引用替换为直接引用的过程（符号引用比如我现在import java.util.ArrayList这就算符号引用，直接引用就是指针或者对象地址，注意引用对象一定是在内存进行） 2.1.3 初始化 初始化其实就是一个赋值的操作，它会执行一个类构造器的clinit>()方法。由编译器自动收集类中所有变量的赋值动作，此时准备阶段时的那个 static int a = 3 的例子，在这个时候就正式赋值为3 2.1.4 卸载 GC将无用对象从内存中卸载 2.2 类加载器的加载顺序 加载一个Class类的顺序也是有优先级的，类加载器从最底层开始往上的顺序是这样的 BootStrap ClassLoader：rt.jar Extention ClassLoader: 加载扩展的jar包 App ClassLoader：指定的classpath下面的jar包 Custom ClassLoader：自定义的类加载器 2.3 双亲委派机制 当一个类收到了加载请求时，它是不会先自己去尝试加载的，而是委派给父类去完成，比如我现在要new一个Person，这个Person是我们自定义的类，如果我们要加载它，就会先委派App ClassLoader，只有当父类加载器都反馈自己无法完成这个请求（也就是父类加载器都没有找到加载所需的Class）时，子类加载器才会自行尝试加载 这样做的好处是，加载位于rt.jar包中的类时不管是哪个加载器加载，最终都会委托到BootStrap ClassLoader进行加载，这样保证了使用不同的类加载器得到的都是同一个结果。 其实这个也是一个隔离的作用，避免了我们的代码影响了JDK的代码，比如我现在要来一个 public class String(){ public static void main(){sout;} } 这种时候，我们的代码肯定会报错，因为在加载的时候其实是找到了rt.jar中的String.class，然后发现这也没有main方法 三、运行时数据区 3.1 本地方法栈和程序计数器 比如说我们现在点开Thread类的源码，会看到它的start0方法带有一个native关键字修饰，而且不存在方法体，这种用native修饰的方法就是本地方法，这是使用C来实现的，然后一般这些方法都会放到一个叫做本地方法栈的区域。 程序计数器其实就是一个指针，它指向了我们程序中下一句需要执行的指令，它也是内存区域中唯一一个不会出现OutOfMemoryError的区域，而且占用内存空间小到基本可以忽略不计。这个内存仅代表当前线程所执行的字节码的行号指示器，字节码解析器通过改变这个计数器的值选取下一条需要执行的字节码指令。 如果执行的是native方法，那这个指针就不工作了。 3.2 方法区 方法区主要的作用技术存放类的元数据信息，常量和静态变量···等。当它存储的信息过大时，会在无法满足内存分配时报错。 3.3 虚拟机栈和虚拟机堆 一句话便是：栈管运行，堆管存储。则虚拟机栈负责运行代码，而虚拟机堆负责存储数据。 3.3.1 虚拟机栈的概念 它是Java方法执行的内存模型。里面会对局部变量，动态链表，方法出口，栈的操作（入栈和出栈）进行存储，且线程独享。同时如果我们听到局部变量表，那也是在说虚拟机栈 public class Person{ int a = 1; public void doSomething(){ int b = 2; } } 3.3.2 虚拟机栈存在的异常 如果线程请求的栈的深度大于虚拟机栈的最大深度，就会报 StackOverflowError （这种错误经常出现在递归中）。Java虚拟机也可以动态扩展，但随着扩展会不断地申请内存，当无法申请足够内存时就会报错 OutOfMemoryError。 3.3.3 虚拟机栈的生命周期 对于栈来说，不存在垃圾回收。只要程序运行结束，栈的空间自然就会释放了。栈的生命周期和所处的线程是一致的。 这里补充一句：8种基本类型的变量+对象的引用变量+实例方法都是在栈里面分配内存。 3.3.4 虚拟机栈的执行 我们经常说的栈帧数据，说白了在JVM中叫栈帧，放到Java中其实就是方法，它也是存放在栈中的。 栈中的数据都是以栈帧的格式存在，它是一个关于方法和运行期数据的数据集。比如我们执行一个方法a，就会对应产生一个栈帧A1，然后A1会被压入栈中。同理方法b会有一个B1，方法c会有一个C1，等到这个线程执行完毕后，栈会先弹出C1，后B1,A1。它是一个先进后出，后进先出原则。 3.3.5 局部变量的复用 局部变量表用于存放方法参数和方法内部所定义的局部变量。它的容量是以Slot为最小单位，一个slot可以存放32位以内的数据类型。 虚拟机通过索引定位的方式使用局部变量表，范围为[0,局部变量表的slot的数量]。方法中的参数就会按一定顺序排列在这个局部变量表中，至于怎么排的我们可以先不关心。而为了节省栈帧空间，这些slot是可以复用的，当方法执行位置超过了某个变量，那么这个变量的slot可以被其它变量复用。当然如果需要复用，那我们的垃圾回收自然就不会去动这些内存。 3.3.6 虚拟机堆的概念 JVM内存会划分为堆内存和非堆内存，堆内存中也会划分为年轻代和老年代，而非堆内存则为永久代。年轻代又会分为Eden和Survivor区。Survivor也会分为FromPlace和ToPlace，toPlace的survivor区域是空的。Eden，FromPlace和ToPlace的默认占比为 8:1:1。当然这个东西其实也可以通过一个 -XX:+UsePSAdaptiveSurvivorSizePolicy 参数来根据生成对象的速率动态调整 堆内存中存放的是对象，垃圾收集就是收集这些对象然后交给GC算法进行回收。非堆内存其实我们已经说过了，就是方法区。在1.8中已经移除永久代，替代品是一个元空间(MetaSpace)，最大区别是metaSpace是不存在于JVM中的，它使用的是本地内存。并有两个参数 MetaspaceSize：初始化元空间大小，控制发生GC MaxMetaspaceSize：限制元空间大小上限，防止占用过多物理内存。 移除的原因可以大致了解一下：融合HotSpot JVM和JRockit VM而做出的改变，因为JRockit是没有永久代的，不过这也间接性地解决了永久代的OOM问题。 3.3.7 Eden年轻代的介绍 当我们new一个对象后，会先放到Eden划分出来的一块作为存储空间的内存，但是我们知道对堆内存是线程共享的，所以有可能会出现两个对象共用一个内存的情况。这里JVM的处理是每个线程都会预先申请好一块连续的内存空间并规定了对象存放的位置，而如果空间不足会再申请多块内存空间。这个操作我们会称作TLAB，有兴趣可以了解一下。 当Eden空间满了之后，会触发一个叫做Minor GC（就是一个发生在年轻代的GC）的操作，存活下来的对象移动到Survivor0区。Survivor0区满后触发 Minor GC，就会将存活对象移动到Survivor1区，此时还会把from和to两个指针交换，这样保证了一段时间内总有一个survivor区为空且to所指向的survivor区为空。经过多次的 Minor GC后仍然存活的对象（这里的存活判断是15次，对应到虚拟机参数为 -XX:MaxTenuringThreshold 。为什么是15，因为HotSpot会在对象投中的标记字段里记录年龄，分配到的空间仅有4位，所以最多只能记录到15）会移动到老年代。老年代是存储长期存活的对象的，占满时就会触发我们最常听说的Full GC，期间会停止所有线程等待GC的完成。所以对于响应要求高的应用应该尽量去减少发生Full GC从而避免响应超时的问题。 而且当老年区执行了full gc之后仍然无法进行对象保存的操作，就会产生OOM，这时候就是虚拟机中的堆内存不足，原因可能会是堆内存设置的大小过小，这个可以通过参数-Xms、-Xms来调整。也可能是代码中创建的对象大且多，而且它们一直在被引用从而长时间垃圾收集无法收集它们。 补充说明：关于-XX:TargetSurvivorRatio参数的问题。其实也不一定是要满足-XX:MaxTenuringThreshold才移动到老年代。可以举个例子：如对象年龄5的占30%，年龄6的占36%，年龄7的占34%，加入某个年龄段（如例子中的年龄6）后，总占用超过Survivor空间*TargetSurvivorRatio的时候，从该年龄段开始及大于的年龄对象就要进入老年代（即例子中的年龄6对象，就是年龄6和年龄7晋升到老年代），这时候无需等到MaxTenuringThreshold中要求的15 3.3.8 如何判断一个对象需要被干掉 图中程序计数器、虚拟机栈、本地方法栈，3个区域随着线程的生存而生存的。内存分配和回收都是确定的。随着线程的结束内存自然就被回收了，因此不需要考虑垃圾回收的问题。而Java堆和方法区则不一样，各线程共享，内存的分配和回收都是动态的。因此垃圾收集器所关注的都是堆和方法这部分内存。 在进行回收前就要判断哪些对象还存活，哪些已经死去。下面介绍两个基础的计算方法 1.引用计数器计算：给对象添加一个引用计数器，每次引用这个对象时计数器加一，引用失效时减一，计数器等于0时就是不会再次使用的。不过这个方法有一种情况就是出现对象的循环引用时GC没法回收。 2.可达性分析计算：这是一种类似于二叉树的实现，将一系列的GC ROOTS作为起始的存活对象集，从这个节点往下搜索，搜索所走过的路径成为引用链，把能被该集合引用到的对象加入到集合中。搜索当一个对象到GC Roots没有使用任何引用链时，则说明该对象是不可用的。主流的商用程序语言，例如Java，C#等都是靠这招去判定对象是否存活的。 （了解一下即可）在Java语言汇总能作为GC Roots的对象分为以下几种： 虚拟机栈（栈帧中的本地方法表）中引用的对象（局部变量） 方法区中静态变量所引用的对象（静态变量） 方法区中常量引用的对象 本地方法栈（即native修饰的方法）中JNI引用的对象（JNI是Java虚拟机调用对应的C函数的方式，通过JNI函数也可以创建新的Java对象。且JNI对于对象的局部引用或者全局引用都会把它们指向的对象都标记为不可回收） 已启动的且未终止的Java线程 这种方法的优点是能够解决循环引用的问题，可它的实现需要耗费大量资源和时间，也需要GC（它的分析过程引用关系不能发生变化，所以需要停止所有进程） 3.3.9 如何宣告一个对象的真正死亡 首先必须要提到的是一个名叫 finalize() 的方法 finalize()是Object类的一个方法、一个对象的finalize()方法只会被系统自动调用一次，经过finalize()方法逃脱死亡的对象，第二次不会再调用。 补充一句：并不提倡在程序中调用finalize()来进行自救。建议忘掉Java程序中该方法的存在。因为它执行的时间不确定，甚至是否被执行也不确定（Java程序的不正常退出），而且运行代价高昂，无法保证各个对象的调用顺序（甚至有不同线程中调用）。在Java9中已经被标记为 deprecated ，且java.lang.ref.Cleaner（也就是强、软、弱、幻象引用的那一套）中已经逐步替换掉它，会比finalize来的更加的轻量及可靠。 　　 判断一个对象的死亡至少需要两次标记 如果对象进行可达性分析之后没发现与GC Roots相连的引用链，那它将会第一次标记并且进行一次筛选。判断的条件是决定这个对象是否有必要执行finalize()方法。如果对象有必要执行finalize()方法，则被放入F-Queue队列中。 GC对F-Queue队列中的对象进行二次标记。如果对象在finalize()方法中重新与引用链上的任何一个对象建立了关联，那么二次标记时则会将它移出“即将回收”集合。如果此时对象还没成功逃脱，那么只能被回收了。 如果确定对象已经死亡，我们又该如何回收这些垃圾呢 3.4 垃圾回收算法 不会非常详细的展开，常用的有标记清除，复制，标记整理和分代收集算法 3.4.1 标记清除算法 标记清除算法就是分为“标记”和“清除”两个阶段。标记出所有需要回收的对象，标记结束后统一回收。这个套路很简单，也存在不足，后续的算法都是根据这个基础来加以改进的。 其实它就是把已死亡的对象标记为空闲内存，然后记录在一个空闲列表中，当我们需要new一个对象时，内存管理模块会从空闲列表中寻找空闲的内存来分给新的对象。 不足的方面就是标记和清除的效率比较低下。且这种做法会让内存中的碎片非常多。这个导致了如果我们需要使用到较大的内存块时，无法分配到足够的连续内存。比如下图 此时可使用的内存块都是零零散散的，导致了刚刚提到的大内存对象问题 3.4.2 复制算法 为了解决效率问题，复制算法就出现了。它将可用内存按容量划分成两等分，每次只使用其中的一块。和survivor一样也是用from和to两个指针这样的玩法。fromPlace存满了，就把存活的对象copy到另一块toPlace上，然后交换指针的内容。这样就解决了碎片的问题。 这个算法的代价就是把内存缩水了，这样堆内存的使用效率就会变得十分低下了 不过它们分配的时候也不是按照1:1这样进行分配的，就类似于Eden和Survivor也不是等价分配是一个道理。 3.4.3 标记整理算法 复制算法在对象存活率高的时候会有一定的效率问题，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉边界以外的内存 3.4.4 分代收集算法 这种算法并没有什么新的思想，只是根据对象存活周期的不同将内存划分为几块。一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记-清理”或者“标记-整理”算法来进行回收。 说白了就是八仙过海各显神通，具体问题具体分析了而已。 3.5 （了解）各种各样的垃圾回收器 HotSpot VM中的垃圾回收器，以及适用场景 到jdk8为止，默认的垃圾收集器是Parallel Scavenge 和 Parallel Old 从jdk9开始，G1收集器成为默认的垃圾收集器 目前来看，G1回收器停顿时间最短而且没有明显缺点，非常适合Web应用。在jdk8中测试Web应用，堆内存6G，新生代4.5G的情况下，Parallel Scavenge 回收新生代停顿长达1.5秒。G1回收器回收同样大小的新生代只停顿0.2秒。 3.6 （了解）JVM的常用参数 JVM的参数非常之多，这里只列举比较重要的几个，通过各种各样的搜索引擎也可以得知这些信息。 参数名称 含义 默认值 说明 -Xms 初始堆大小 物理内存的1/64( 默认(MinHeapFreeRatio参数可以调整)空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制. -Xmx 最大堆大小 物理内存的1/4( 默认(MaxHeapFreeRatio参数可以调整)空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制 -Xmn 年轻代大小(1.4or lator) 注意：此处的大小是（eden+ 2 survivor space).与jmap -heap中显示的New gen是不同的。整个堆大小=年轻代大小 + 老年代大小 + 持久代（永久代）大小.增大年轻代后,将会减小年老代大小.此值对系统性能影响较大,Sun官方推荐配置为整个堆的3/8 -XX:NewSize 设置年轻代大小(for 1.3/1.4) -XX:MaxNewSize 年轻代最大值(for 1.3/1.4) -XX:PermSize 设置持久代(perm gen)初始值 物理内存的1/64 -XX:MaxPermSize 设置持久代最大值 物理内存的1/4 -Xss 每个线程的堆栈大小 JDK5.0以后每个线程堆栈大小为1M,以前每个线程堆栈大小为256K.更具应用的线程所需内存大小进行 调整.在相同物理内存下,减小这个值能生成更多的线程.但是操作系统对一个进程内的线程数还是有限制的,不能无限生成,经验值在3000~5000左右一般小的应用， 如果栈不是很深， 应该是128k够用的 大的应用建议使用256k。这个选项对性能影响比较大，需要严格的测试。（校长）和threadstacksize选项解释很类似,官方文档似乎没有解释,在论坛中有这样一句话:-Xss is translated in a VM flag named ThreadStackSize”一般设置这个值就可以了 -XX:NewRatio 年轻代(包括Eden和两个Survivor区)与年老代的比值(除去持久代) -XX:NewRatio=4表示年轻代与年老代所占比值为1:4,年轻代占整个堆栈的1/5Xms=Xmx并且设置了Xmn的情况下，该参数不需要进行设置。 -XX:SurvivorRatio Eden区与Survivor区的大小比值 设置为8,则两个Survivor区与一个Eden区的比值为2:8,一个Survivor区占整个年轻代的1/10 -XX:+DisableExplicitGC 关闭System.gc() 这个参数需要严格的测试 -XX:PretenureSizeThreshold 对象超过多大是直接在旧生代分配 0 单位字节 新生代采用Parallel ScavengeGC时无效另一种直接在旧生代分配的情况是大的数组对象,且数组中无外部引用对象. -XX:ParallelGCThreads 并行收集器的线程数 此值最好配置与处理器数目相等 同样适用于CMS -XX:MaxGCPauseMillis 每次年轻代垃圾回收的最长时间(最大暂停时间) 如果无法满足此时间,JVM会自动调整年轻代大小,以满足此值. 其实还有一些打印及CMS方面的参数，这里就不以一一列举了 四、关于JVM调优的一些方面 根据刚刚涉及的jvm的知识点，我们可以尝试对JVM进行调优，主要就是堆内存那块 所有线程共享数据区大小=新生代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为64m。所以java堆中增大年轻代后，将会减小年老代大小（因为老年代的清理是使用fullgc，所以老年代过小的话反而是会增多fullgc的）。此值对系统性能影响较大，Sun官方推荐配置为java堆的3/8。 4.1 调整最大堆内存和最小堆内存 -Xmx –Xms：指定java堆最大值（默认值是物理内存的1/4( 默认(MinHeapFreeRatio参数可以调整)空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制.，默认(MaxHeapFreeRatio参数可以调整)空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制。简单点来说，你不停地往堆内存里面丢数据，等它剩余大小小于40%了，JVM就会动态申请内存空间不过会小于-Xmx，如果剩余大小大于70%，又会动态缩小不过不会小于–Xms。就这么简单 开发过程中，通常会将 -Xms 与 -Xmx两个参数的配置相同的值，其目的是为了能够在java垃圾回收机制清理完堆区后不需要重新分隔计算堆区的大小而浪费资源。 我们执行下面的代码 System.out.println(\"Xmx=\" + Runtime.getRuntime().maxMemory() / 1024.0 / 1024 + \"M\"); //系统的最大空间 System.out.println(\"free mem=\" + Runtime.getRuntime().freeMemory() / 1024.0 / 1024 + \"M\"); //系统的空闲空间 System.out.println(\"total mem=\" + Runtime.getRuntime().totalMemory() / 1024.0 / 1024 + \"M\"); //当前可用的总空间 注意：此处设置的是Java堆大小，也就是新生代大小 + 老年代大小 设置一个VM options的参数 -Xmx20m -Xms5m -XX:+PrintGCDetails 再次启动main方法 这里GC弹出了一个Allocation Failure分配失败，这个事情发生在PSYoungGen，也就是年轻代中 这时候申请到的内存为18M，空闲内存为4.214195251464844M 我们此时创建一个字节数组看看，执行下面的代码 byte[] b = new byte[1 * 1024 * 1024]; System.out.println(\"分配了1M空间给数组\"); System.out.println(\"Xmx=\" + Runtime.getRuntime().maxMemory() / 1024.0 / 1024 + \"M\"); //系统的最大空间 System.out.println(\"free mem=\" + Runtime.getRuntime().freeMemory() / 1024.0 / 1024 + \"M\"); //系统的空闲空间 System.out.println(\"total mem=\" + Runtime.getRuntime().totalMemory() / 1024.0 / 1024 + \"M\"); 此时free memory就又缩水了，不过total memory是没有变化的。Java会尽可能将total mem的值维持在最小堆内存大小 byte[] b = new byte[10 * 1024 * 1024]; System.out.println(\"分配了10M空间给数组\"); System.out.println(\"Xmx=\" + Runtime.getRuntime().maxMemory() / 1024.0 / 1024 + \"M\"); //系统的最大空间 System.out.println(\"free mem=\" + Runtime.getRuntime().freeMemory() / 1024.0 / 1024 + \"M\"); //系统的空闲空间 System.out.println(\"total mem=\" + Runtime.getRuntime().totalMemory() / 1024.0 / 1024 + \"M\"); //当前可用的总空间 这时候我们创建了一个10M的字节数据，这时候最小堆内存是顶不住的。我们会发现现在的total memory已经变成了15M，这就是已经申请了一次内存的结果。 此时我们再跑一下这个代码 System.gc(); System.out.println(\"Xmx=\" + Runtime.getRuntime().maxMemory() / 1024.0 / 1024 + \"M\"); //系统的最大空间 System.out.println(\"free mem=\" + Runtime.getRuntime().freeMemory() / 1024.0 / 1024 + \"M\"); //系统的空闲空间 System.out.println(\"total mem=\" + Runtime.getRuntime().totalMemory() / 1024.0 / 1024 + \"M\"); //当前可用的总空间 此时我们手动执行了一次fullgc，此时total memory的内存空间又变回5.5M了，此时又是把申请的内存释放掉的结果。 4.2 调整新生代和老年代的比值 -XX:NewRatio --- 新生代（eden+2*Survivor）和老年代（不包含永久区）的比值 例如：-XX:NewRatio=4，表示新生代:老年代=1:4，即新生代占整个堆的1/5。在Xms=Xmx并且设置了Xmn的情况下，该参数不需要进行设置。 4.3 调整Survivor区和Eden区的比值 -XX:SurvivorRatio（幸存代）--- 设置两个Survivor区和eden的比值 例如：8，表示两个Survivor:eden=2:8，即一个Survivor占年轻代的1/10 4.4 设置年轻代和老年代的大小 -XX:NewSize --- 设置年轻代大小 -XX:MaxNewSize --- 设置年轻代最大值 可以通过设置不同参数来测试不同的情况，反正最优解当然就是官方的Eden和Survivor的占比为8:1:1，然后在刚刚介绍这些参数的时候都已经附带了一些说明，感兴趣的也可以看看。反正最大堆内存和最小堆内存如果数值不同会导致多次的gc，需要注意。 4.5 小总结 根据实际事情调整新生代和幸存代的大小，官方推荐新生代占java堆的3/8，幸存代占新生代的1/10 在OOM时，记得Dump出堆，确保可以排查现场问题，通过下面命令你可以输出一个.dump文件，这个文件可以使用VisualVM或者Java自带的Java VisualVM工具。 -Xmx20m -Xms5m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=你要输出的日志路径 一般我们也可以通过编写脚本的方式来让OOM出现时给我们报个信，可以通过发送邮件或者重启程序等来解决。 4.6 永久区的设置 -XX:PermSize -XX:MaxPermSize 初始空间（默认为物理内存的1/64）和最大空间（默认为物理内存的1/4）。也就是说，jvm启动时，永久区一开始就占用了PermSize大小的空间，如果空间还不够，可以继续扩展，但是不能超过MaxPermSize，否则会OOM。 tips：如果堆空间没有用完也抛出了OOM，有可能是永久区导致的。堆空间实际占用非常少，但是永久区溢出 一样抛出OOM。 4.7 JVM的栈参数调优 4.7.1 调整每个线程栈空间的大小 可以通过-Xss：调整每个线程栈空间的大小 JDK5.0以后每个线程堆栈大小为1M，以前每个线程堆栈大小为256K。在相同物理内存下,减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右 4.7.2 设置线程栈的大小 -XXThreadStackSize： 设置线程栈的大小(0 means use default stack size) 这些参数都是可以通过自己编写程序去简单测试的，这里碍于篇幅问题就不再提供demo了 4.8 (可以直接跳过了)JVM其他参数介绍 形形色色的参数很多，就不会说把所有都扯个遍了，因为大家其实也不会说一定要去深究到底。 4.8.1 设置内存页的大小 -XXThreadStackSize： 设置内存页的大小，不可设置过大，会影响Perm的大小 4.8.2 设置原始类型的快速优化 -XX:+UseFastAccessorMethods： 设置原始类型的快速优化 4.8.3 设置关闭手动GC -XX:+DisableExplicitGC： 设置关闭System.gc()(这个参数需要严格的测试) 4.8.4 设置垃圾最大年龄 -XX:MaxTenuringThreshold 设置垃圾最大年龄。如果设置为0的话,则年轻代对象不经过Survivor区,直接进入年老代. 对于年老代比较多的应用,可以提高效率。如果将此值设置为一个较大值, 则年轻代对象会在Survivor区进行多次复制,这样可以增加对象再年轻代的存活时间, 增加在年轻代即被回收的概率。该参数只有在串行GC时才有效. 4.8.5 加快编译速度 -XX:+AggressiveOpts 加快编译速度 4.8.6 改善锁机制性能 -XX:+UseBiasedLocking 4.8.7 禁用垃圾回收 -Xnoclassgc 4.8.8 设置堆空间存活时间 -XX:SoftRefLRUPolicyMSPerMB 设置每兆堆空闲空间中SoftReference的存活时间，默认值是1s。 4.8.9 设置对象直接分配在老年代 -XX:PretenureSizeThreshold 设置对象超过多大时直接在老年代分配，默认值是0。 4.8.10 设置TLAB占eden区的比例 -XX:TLABWasteTargetPercent 设置TLAB占eden区的百分比，默认值是1% 。 4.8.11设置是否优先YGC -XX:+CollectGen0First 设置FullGC时是否先YGC，默认值是false。 finally 真的扯了很久这东西，参考了多方的资料，有极客时间的《深入拆解虚拟机》和《Java核心技术面试精讲》，也有百度，也有自己在学习的一些线上课程的总结。希望对你有所帮助，谢谢。 "},"zother6-JavaGuide/java/jvm/GC调优参数.html":{"url":"zother6-JavaGuide/java/jvm/GC调优参数.html","title":"GC调优参数","keywords":"","body":" 原文地址： https://juejin.im/post/5c94a123f265da610916081f。 JVM 配置常用参数 堆参数； 回收器参数； 项目中常用配置； 常用组合； 堆参数 回收器参数 如上表所示，目前主要有串行、并行和并发三种，对于大内存的应用而言，串行的性能太低，因此使用到的主要是并行和并发两种。并行和并发 GC 的策略通过 UseParallelGC和UseConcMarkSweepGC 来指定，还有一些细节的配置参数用来配置策略的执行方式。例如：XX:ParallelGCThreads， XX:CMSInitiatingOccupancyFraction 等。 通常：Young 区对象回收只可选择并行（耗时间），Old 区选择并发（耗 CPU）。 项目中常用配置 备注：在Java8中永久代的参数-XX:PermSize 和-XX：MaxPermSize已经失效。 常用组合 常用 GC 调优策略 GC 调优原则； GC 调优目的； GC 调优策略； GC 调优原则 在调优之前，我们需要记住下面的原则： 多数的 Java 应用不需要在服务器上进行 GC 优化； 多数导致 GC 问题的 Java 应用，都不是因为我们参数设置错误，而是代码问题； 在应用上线之前，先考虑将机器的 JVM 参数设置到最优（最适合）； 减少创建对象的数量； 减少使用全局变量和大对象； GC 优化是到最后不得已才采用的手段； 在实际使用中，分析 GC 情况优化代码比优化 GC 参数要多得多。 GC 调优目的 将转移到老年代的对象数量降低到最小； 减少 GC 的执行时间。 GC 调优策略 策略 1：将新对象预留在新生代，由于 Full GC 的成本远高于 Minor GC，因此尽可能将对象分配在新生代是明智的做法，实际项目中根据 GC 日志分析新生代空间大小分配是否合理，适当通过“-Xmn”命令调节新生代大小，最大限度降低新对象直接进入老年代的情况。 策略 2：大对象进入老年代，虽然大部分情况下，将对象分配在新生代是合理的。但是对于大对象这种做法却值得商榷，大对象如果首次在新生代分配可能会出现空间不足导致很多年龄不够的小对象被分配的老年代，破坏新生代的对象结构，可能会出现频繁的 full gc。因此，对于大对象，可以设置直接进入老年代（当然短命的大对象对于垃圾回收来说简直就是噩梦）。-XX:PretenureSizeThreshold 可以设置直接进入老年代的对象大小。 策略 3：合理设置进入老年代对象的年龄，-XX:MaxTenuringThreshold 设置对象进入老年代的年龄大小，减少老年代的内存占用，降低 full gc 发生的频率。 策略 4：设置稳定的堆大小，堆大小设置有两个参数：-Xms 初始化堆大小，-Xmx 最大堆大小。 策略5：注意： 如果满足下面的指标，则一般不需要进行 GC 优化： MinorGC 执行时间不到50ms； Minor GC 执行不频繁，约10秒一次； Full GC 执行时间不到1s； Full GC 执行频率不算频繁，不低于10分钟1次。 "},"zother6-JavaGuide/java/jvm/Java内存区域.html":{"url":"zother6-JavaGuide/java/jvm/Java内存区域.html","title":"Java内存区域","keywords":"","body":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。 Java 内存区域详解 写在前面 (常见面试题) 基本问题 拓展问题 一 概述 二 运行时数据区域 2.1 程序计数器 2.2 Java 虚拟机栈 2.3 本地方法栈 2.4 堆 2.5 方法区 2.5.1 方法区和永久代的关系 2.5.2 常用参数 2.5.3 为什么要将永久代 (PermGen) 替换为元空间 (MetaSpace) 呢? 2.6 运行时常量池 2.7 直接内存 三 HotSpot 虚拟机对象探秘 3.1 对象的创建 Step1:类加载检查 Step2:分配内存 Step3:初始化零值 Step4:设置对象头 Step5:执行 init 方法 3.2 对象的内存布局 3.3 对象的访问定位 四 重点补充内容 4.1 String 类和常量池 4.2 String s1 = new String(\"abc\");这句话创建了几个字符串对象？ 4.3 8 种基本类型的包装类和常量池 参考 公众号 Java 内存区域详解 如果没有特殊说明，都是针对的是 HotSpot 虚拟机。 写在前面 (常见面试题) 基本问题 介绍下 Java 内存区域（运行时数据区） Java 对象的创建过程（五步，建议能默写出来并且要知道每一步虚拟机做了什么） 对象的访问定位的两种方式（句柄和直接指针两种方式） 拓展问题 String 类和常量池 8 种基本类型的包装类和常量池 一 概述 对于 Java 程序员来说，在虚拟机自动内存管理机制下，不再需要像 C/C++程序开发程序员这样为每一个 new 操作去写对应的 delete/free 操作，不容易出现内存泄漏和内存溢出问题。正是因为 Java 程序员把内存控制权利交给 Java 虚拟机，一旦出现内存泄漏和溢出方面的问题，如果不了解虚拟机是怎样使用内存的，那么排查错误将会是一个非常艰巨的任务。 二 运行时数据区域 Java 虚拟机在执行 Java 程序的过程中会把它管理的内存划分成若干个不同的数据区域。JDK. 1.8 和之前的版本略有不同，下面会介绍到。 JDK 1.8 之前： **JDK 1.8 ：** 线程私有的： 程序计数器 虚拟机栈 本地方法栈 线程共享的： 堆 方法区 直接内存 (非运行时数据区的一部分) 2.1 程序计数器 程序计数器是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完成。 另外，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 从上面的介绍中我们知道程序计数器主要有两个作用： 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 注意：程序计数器是唯一一个不会出现 OutOfMemoryError 的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。 2.2 Java 虚拟机栈 与程序计数器一样，Java 虚拟机栈也是线程私有的，它的生命周期和线程相同，描述的是 Java 方法执行的内存模型，每次方法调用的数据都是通过栈传递的。 Java 内存可以粗糙的区分为堆内存（Heap）和栈内存 (Stack),其中栈就是现在说的虚拟机栈，或者说是虚拟机栈中局部变量表部分。 （实际上，Java 虚拟机栈是由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法出口信息。） 局部变量表主要存放了编译器可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference 类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。 Java 虚拟机栈会出现两种错误：StackOverFlowError 和 OutOfMemoryError。 StackOverFlowError： 若 Java 虚拟机栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度的时候，就抛出 StackOverFlowError 错误。 OutOfMemoryError： 若 Java 虚拟机栈的内存大小允许动态扩展，且当线程请求栈时内存用完了，无法再动态扩展了，此时抛出 OutOfMemoryError 错误。 Java 虚拟机栈也是线程私有的，每个线程都有各自的 Java 虚拟机栈，而且随着线程的创建而创建，随着线程的死亡而死亡。 扩展：那么方法/函数如何调用？ Java 栈可用类比数据结构中栈，Java 栈中保存的主要内容是栈帧，每一次函数调用都会有一个对应的栈帧被压入 Java 栈，每一个函数调用结束后，都会有一个栈帧被弹出。 Java 方法有两种返回方式： return 语句。 抛出异常。 不管哪种返回方式都会导致栈帧被弹出。 2.3 本地方法栈 和虚拟机栈所发挥的作用非常相似，区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。 本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。 方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现 StackOverFlowError 和 OutOfMemoryError 两种错误。 2.4 堆 Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。 Java世界中“几乎”所有的对象都在堆中分配，但是，随着JIT编译期的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换优化技术将会导致一些微妙的变化，所有的对象都分配到堆上也渐渐变得不那么“绝对”了。从jdk 1.7开始已经默认开启逃逸分析，如果某些方法中的对象引用没有被返回或者未被外面使用（也就是未逃逸出去），那么对象可以直接在栈上分配内存。 Java 堆是垃圾收集器管理的主要区域，因此也被称作GC 堆（Garbage Collected Heap）.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代：再细致一点有：Eden 空间、From Survivor、To Survivor 空间等。进一步划分的目的是更好地回收内存，或者更快地分配内存。 在 JDK 7 版本及JDK 7 版本之前，堆内存被通常被分为下面三部分： 新生代内存(Young Generation) 老生代(Old Generation) 永生代(Permanent Generation) JDK 8 版本之后方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。 上图所示的 Eden 区、两个 Survivor 区都属于新生代（为了区分，这两个 Survivor 区域按照顺序被命名为 from 和 to），中间一层属于老年代。 大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden 区->Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 修正（issue552）：“Hotspot遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了survivor区的一半时，取这个年龄和MaxTenuringThreshold中更小的一个值，作为新的晋升年龄阈值”。 动态年龄计算的代码如下 uint ageTable::compute_tenuring_threshold(size_t survivor_capacity) { //survivor_capacity是survivor空间的大小 size_t desired_survivor_size = (size_t)((((double) survivor_capacity)*TargetSurvivorRatio)/100); size_t total = 0; uint age = 1; while (age desired_survivor_size) break; age++; } uint result = age 堆这里最容易出现的就是 OutOfMemoryError 错误，并且出现这种错误之后的表现形式还会有几种，比如： OutOfMemoryError: GC Overhead Limit Exceeded ： 当JVM花太多时间执行垃圾回收并且只能回收很少的堆空间时，就会发生此错误。 java.lang.OutOfMemoryError: Java heap space :假如在创建新的对象时, 堆内存中的空间不足以存放新创建的对象, 就会引发java.lang.OutOfMemoryError: Java heap space 错误。(和本机物理内存无关，和你配置的内存大小有关！) ...... 2.5 方法区 方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然 Java 虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。 方法区也被称为永久代。很多人都会分不清方法区和永久代的关系，为此我也查阅了文献。 2.5.1 方法区和永久代的关系 《Java 虚拟机规范》只是规定了有方法区这么个概念和它的作用，并没有规定如何去实现它。那么，在不同的 JVM 上方法区的实现肯定是不同的了。 方法区和永久代的关系很像 Java 中接口和类的关系，类实现了接口，而永久代就是 HotSpot 虚拟机对虚拟机规范中方法区的一种实现方式。 也就是说，永久代是 HotSpot 的概念，方法区是 Java 虚拟机规范中的定义，是一种规范，而永久代是一种实现，一个是标准一个是实现，其他的虚拟机实现并没有永久代这一说法。 2.5.2 常用参数 JDK 1.8 之前永久代还没被彻底移除的时候通常通过下面这些参数来调节方法区大小 -XX:PermSize=N //方法区 (永久代) 初始大小 -XX:MaxPermSize=N //方法区 (永久代) 最大大小,超过这个值将会抛出 OutOfMemoryError 异常:java.lang.OutOfMemoryError: PermGen 相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。 JDK 1.8 的时候，方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。 下面是一些常用参数： -XX:MetaspaceSize=N //设置 Metaspace 的初始（和最小大小） -XX:MaxMetaspaceSize=N //设置 Metaspace 的最大大小 与永久代很大的不同就是，如果不指定大小的话，随着更多类的创建，虚拟机会耗尽所有可用的系统内存。 2.5.3 为什么要将永久代 (PermGen) 替换为元空间 (MetaSpace) 呢? 整个永久代有一个 JVM 本身设置固定大小上限，无法进行调整，而元空间使用的是直接内存，受本机可用内存的限制，虽然元空间仍旧可能溢出，但是比原来出现的几率会更小。 当你元空间溢出时会得到如下错误： java.lang.OutOfMemoryError: MetaSpace 你可以使用 -XX：MaxMetaspaceSize 标志设置最大元空间大小，默认值为 unlimited，这意味着它只受系统内存的限制。-XX：MetaspaceSize 调整标志定义元空间的初始大小如果未指定此标志，则 Metaspace 将根据运行时的应用程序需求动态地重新调整大小。 元空间里面存放的是类的元数据，这样加载多少类的元数据就不由 MaxPermSize 控制了, 而由系统的实际可用空间来控制，这样能加载的类就更多了。 在 JDK8，合并 HotSpot 和 JRockit 的代码时, JRockit 从来没有一个叫永久代的东西, 合并之后就没有必要额外的设置这么一个永久代的地方了。 2.6 运行时常量池 运行时常量池是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池表（用于存放编译期生成的各种字面量和符号引用） 既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 错误。 JDK1.7 及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。 修正(issue747，reference)： JDK1.7之前运行时常量池逻辑包含字符串常量池存放在方法区, 此时hotspot虚拟机对方法区的实现为永久代 JDK1.7 字符串常量池被从方法区拿到了堆中, 这里没有提到运行时常量池,也就是说字符串常量池被单独拿到堆,运行时常量池剩下的东西还在方法区, 也就是hotspot中的永久代 。 JDK1.8 hotspot移除了永久代用元空间(Metaspace)取而代之, 这时候字符串常量池还在堆, 运行时常量池还在方法区, 只不过方法区的实现从永久代变成了元空间(Metaspace) 相关问题：JVM 常量池中存储的是对象还是引用呢？： https://www.zhihu.com/question/57109429/answer/151717241 by RednaxelaFX 2.7 直接内存 直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用。而且也可能导致 OutOfMemoryError 错误出现。 JDK1.4 中新加入的 NIO(New Input/Output) 类，引入了一种基于通道（Channel） 与缓存区（Buffer） 的 I/O 方式，它可以直接使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样就能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆之间来回复制数据。 本机直接内存的分配不会受到 Java 堆的限制，但是，既然是内存就会受到本机总内存大小以及处理器寻址空间的限制。 三 HotSpot 虚拟机对象探秘 通过上面的介绍我们大概知道了虚拟机的内存情况，下面我们来详细的了解一下 HotSpot 虚拟机在 Java 堆中对象分配、布局和访问的全过程。 3.1 对象的创建 下图便是 Java 对象的创建过程，我建议最好是能默写出来，并且要掌握每一步在做什么。 Step1:类加载检查 虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 Step2:分配内存 在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。分配方式有 “指针碰撞” 和 “空闲列表” 两种，选择那种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 内存分配的两种方式：（补充内容，需要掌握） 选择以上两种方式中的哪一种，取决于 Java 堆内存是否规整。而 Java 堆内存是否规整，取决于 GC 收集器的算法是\"标记-清除\"，还是\"标记-整理\"（也称作\"标记-压缩\"），值得注意的是，复制算法内存也是规整的 内存分配并发问题（补充内容，需要掌握） 在创建对象的时候有一个很重要的问题，就是线程安全，因为在实际开发过程中，创建对象是很频繁的事情，作为虚拟机来说，必须要保证线程是安全的，通常来讲，虚拟机采用两种方式来保证线程安全： CAS+失败重试： CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性。 TLAB： 为每一个线程预先在 Eden 区分配一块儿内存，JVM 在给线程中的对象分配内存时，首先在 TLAB 分配，当对象大于 TLAB 中的剩余内存或 TLAB 的内存已用尽时，再采用上述的 CAS 进行内存分配 Step3:初始化零值 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 Step4:设置对象头 初始化零值完成之后，虚拟机要对对象进行必要的设置，例如这个对象是那个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。 Step5:执行 init 方法 在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始， 方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。 3.2 对象的内存布局 在 Hotspot 虚拟机中，对象在内存中的布局可以分为 3 块区域：对象头、实例数据和对齐填充。 Hotspot 虚拟机的对象头包括两部分信息，第一部分用于存储对象自身的运行时数据（哈希码、GC 分代年龄、锁状态标志等等），另一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是那个类的实例。 实例数据部分是对象真正存储的有效信息，也是在程序中所定义的各种类型的字段内容。 对齐填充部分不是必然存在的，也没有什么特别的含义，仅仅起占位作用。 因为 Hotspot 虚拟机的自动内存管理系统要求对象起始地址必须是 8 字节的整数倍，换句话说就是对象的大小必须是 8 字节的整数倍。而对象头部分正好是 8 字节的倍数（1 倍或 2 倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。 3.3 对象的访问定位 建立对象就是为了使用对象，我们的 Java 程序通过栈上的 reference 数据来操作堆上的具体对象。对象的访问方式由虚拟机实现而定，目前主流的访问方式有①使用句柄和②直接指针两种： 句柄： 如果使用句柄的话，那么 Java 堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息； 直接指针： 如果使用直接指针访问，那么 Java 堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而 reference 中存储的直接就是对象的地址。 这两种对象访问方式各有优势。使用句柄来访问的最大好处是 reference 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而 reference 本身不需要修改。使用直接指针访问方式最大的好处就是速度快，它节省了一次指针定位的时间开销。 四 重点补充内容 4.1 String 类和常量池 String 对象的两种创建方式： String str1 = \"abcd\";//先检查字符串常量池中有没有\"abcd\"，如果字符串常量池中没有，则创建一个，然后 str1 指向字符串常量池中的对象，如果有，则直接将 str1 指向\"abcd\"\"； String str2 = new String(\"abcd\");//堆中创建一个新的对象 String str3 = new String(\"abcd\");//堆中创建一个新的对象 System.out.println(str1==str2);//false System.out.println(str2==str3);//false 这两种不同的创建方法是有差别的。 第一种方式是在常量池中拿对象； 第二种方式是直接在堆内存空间创建一个新的对象。 记住一点：只要使用 new 方法，便需要创建新的对象。 再给大家一个图应该更容易理解，图片来源：https://www.journaldev.com/797/what-is-java-string-pool： String 类型的常量池比较特殊。它的主要使用方法有两种： 直接使用双引号声明出来的 String 对象会直接存储在常量池中。 如果不是用双引号声明的 String 对象，可以使用 String 提供的 intern 方法。String.intern() 是一个 Native 方法，它的作用是：如果运行时常量池中已经包含一个等于此 String 对象内容的字符串，则返回常量池中该字符串的引用；如果没有，JDK1.7之前（不包含1.7）的处理方式是在常量池中创建与此 String 内容相同的字符串，并返回常量池中创建的字符串的引用，JDK1.7以及之后的处理方式是在常量池中记录此字符串的引用，并返回该引用。 String s1 = new String(\"计算机\"); String s2 = s1.intern(); String s3 = \"计算机\"; System.out.println(s2);//计算机 System.out.println(s1 == s2);//false，因为一个是堆内存中的 String 对象一个是常量池中的 String 对象， System.out.println(s3 == s2);//true，因为两个都是常量池中的 String 对象 字符串拼接: String str1 = \"str\"; String str2 = \"ing\"; String str3 = \"str\" + \"ing\";//常量池中的对象 String str4 = str1 + str2; //在堆上创建的新的对象 String str5 = \"string\";//常量池中的对象 System.out.println(str3 == str4);//false System.out.println(str3 == str5);//true System.out.println(str4 == str5);//false 尽量避免多个字符串拼接，因为这样会重新创建对象。如果需要改变字符串的话，可以使用 StringBuilder 或者 StringBuffer。 4.2 String s1 = new String(\"abc\");这句话创建了几个字符串对象？ 将创建 1 或 2 个字符串。如果池中已存在字符串常量“abc”，则只会在堆空间创建一个字符串常量“abc”。如果池中没有字符串常量“abc”，那么它将首先在池中创建，然后在堆空间中创建，因此将创建总共 2 个字符串对象。 验证： String s1 = new String(\"abc\");// 堆内存的地址值 String s2 = \"abc\"; System.out.println(s1 == s2);// 输出 false,因为一个是堆内存，一个是常量池的内存，故两者是不同的。 System.out.println(s1.equals(s2));// 输出 true 结果： false true 4.3 8 种基本类型的包装类和常量池 Java 基本类型的包装类的大部分都实现了常量池技术，即 Byte,Short,Integer,Long,Character,Boolean；前面 4 种包装类默认创建了数值[-128，127] 的相应类型的缓存数据，Character创建了数值在[0,127]范围的缓存数据，Boolean 直接返回True Or False。如果超出对应范围仍然会去创建新的对象。 为啥把缓存设置为[-128，127]区间？（参见issue/461）性能和资源之间的权衡。 public static Boolean valueOf(boolean b) { return (b ? TRUE : FALSE); } private static class CharacterCache { private CharacterCache(){} static final Character cache[] = new Character[127 + 1]; static { for (int i = 0; i 两种浮点数类型的包装类 Float,Double 并没有实现常量池技术。** Integer i1 = 33; Integer i2 = 33; System.out.println(i1 == i2);// 输出 true Integer i11 = 333; Integer i22 = 333; System.out.println(i11 == i22);// 输出 false Double i3 = 1.2; Double i4 = 1.2; System.out.println(i3 == i4);// 输出 false Integer 缓存源代码： /** *此方法将始终缓存-128 到 127（包括端点）范围内的值，并可以缓存此范围之外的其他值。 */ public static Integer valueOf(int i) { if (i >= IntegerCache.low && i 应用场景： Integer i1=40；Java 在编译的时候会直接将代码封装成 Integer i1=Integer.valueOf(40);，从而使用常量池中的对象。 Integer i1 = new Integer(40);这种情况下会创建新的对象。 Integer i1 = 40; Integer i2 = new Integer(40); System.out.println(i1==i2);//输出 false Integer 比较更丰富的一个例子: Integer i1 = 40; Integer i2 = 40; Integer i3 = 0; Integer i4 = new Integer(40); Integer i5 = new Integer(40); Integer i6 = new Integer(0); System.out.println(\"i1=i2 \" + (i1 == i2)); System.out.println(\"i1=i2+i3 \" + (i1 == i2 + i3)); System.out.println(\"i1=i4 \" + (i1 == i4)); System.out.println(\"i4=i5 \" + (i4 == i5)); System.out.println(\"i4=i5+i6 \" + (i4 == i5 + i6)); System.out.println(\"40=i5+i6 \" + (40 == i5 + i6)); 结果： i1=i2 true i1=i2+i3 true i1=i4 false i4=i5 false i4=i5+i6 true 40=i5+i6 true 解释： 语句 i4 == i5 + i6，因为+这个操作符不适用于 Integer 对象，首先 i5 和 i6 进行自动拆箱操作，进行数值相加，即 i4 == 40。然后 Integer 对象无法与数值进行直接比较，所以 i4 自动拆箱转为 int 值 40，最终这条语句转为 40 == 40 进行数值比较。 参考 《深入理解 Java 虚拟机：JVM 高级特性与最佳实践（第二版》 《实战 java 虚拟机》 https://docs.oracle.com/javase/specs/index.html http://www.pointsoftware.ch/en/under-the-hood-runtime-data-areas-javas-memory-model/ https://dzone.com/articles/jvm-permgen-%E2%80%93-where-art-thou https://stackoverflow.com/questions/9095748/method-area-and-permgen 深入解析String#internhttps://tech.meituan.com/2014/03/06/in-depth-understanding-string-intern.html 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \"Java面试突击\" 即可免费领取！ Java工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/java/jvm/JDK监控和故障处理工具总结.html":{"url":"zother6-JavaGuide/java/jvm/JDK监控和故障处理工具总结.html","title":"JDK监控和故障处理工具总结","keywords":"","body":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。 JDK 监控和故障处理工具总结 JDK 命令行工具 jps:查看所有 Java 进程 jstat: 监视虚拟机各种运行状态信息 jinfo: 实时地查看和调整虚拟机各项参数 jmap:生成堆转储快照 jhat: 分析 heapdump 文件 jstack :生成虚拟机当前时刻的线程快照 JDK 可视化分析工具 JConsole:Java 监视与管理控制台 连接 Jconsole 查看 Java 程序概况 内存监控 线程监控 Visual VM:多合一故障处理工具 JDK 监控和故障处理工具总结 JDK 命令行工具 这些命令在 JDK 安装目录下的 bin 目录下： jps (JVM Process Status）: 类似 UNIX 的 ps 命令。用户查看所有 Java 进程的启动类、传入参数和 Java 虚拟机参数等信息； jstat（ JVM Statistics Monitoring Tool）: 用于收集 HotSpot 虚拟机各方面的运行数据; jinfo (Configuration Info for Java) : Configuration Info forJava,显示虚拟机配置信息; jmap (Memory Map for Java) :生成堆转储快照; jhat (JVM Heap Dump Browser ) : 用于分析 heapdump 文件，它会建立一个 HTTP/HTML 服务器，让用户可以在浏览器上查看分析结果; jstack (Stack Trace for Java):生成虚拟机当前时刻的线程快照，线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合。 jps:查看所有 Java 进程 jps(JVM Process Status) 命令类似 UNIX 的 ps 命令。 jps：显示虚拟机执行主类名称以及这些进程的本地虚拟机唯一 ID（Local Virtual Machine Identifier,LVMID）。jps -q ：只输出进程的本地虚拟机唯一 ID。 C:\\Users\\SnailClimb>jps 7360 NettyClient2 17396 7972 Launcher 16504 Jps 17340 NettyServer jps -l:输出主类的全名，如果进程执行的是 Jar 包，输出 Jar 路径。 C:\\Users\\SnailClimb>jps -l 7360 firstNettyDemo.NettyClient2 17396 7972 org.jetbrains.jps.cmdline.Launcher 16492 sun.tools.jps.Jps 17340 firstNettyDemo.NettyServer jps -v：输出虚拟机进程启动时 JVM 参数。 jps -m：输出传递给 Java 进程 main() 函数的参数。 jstat: 监视虚拟机各种运行状态信息 jstat（JVM Statistics Monitoring Tool） 使用于监视虚拟机各种运行状态信息的命令行工具。 它可以显示本地或者远程（需要远程主机提供 RMI 支持）虚拟机进程中的类信息、内存、垃圾收集、JIT 编译等运行数据，在没有 GUI，只提供了纯文本控制台环境的服务器上，它将是运行期间定位虚拟机性能问题的首选工具。 jstat 命令使用格式： jstat - [-t] [-h] [ []] 比如 jstat -gc -h3 31736 1000 10表示分析进程 id 为 31736 的 gc 情况，每隔 1000ms 打印一次记录，打印 10 次停止，每 3 行后打印指标头部。 常见的 option 如下： jstat -class vmid ：显示 ClassLoader 的相关信息； jstat -compiler vmid ：显示 JIT 编译的相关信息； jstat -gc vmid ：显示与 GC 相关的堆信息； jstat -gccapacity vmid ：显示各个代的容量及使用情况； jstat -gcnew vmid ：显示新生代信息； jstat -gcnewcapcacity vmid ：显示新生代大小与使用情况； jstat -gcold vmid ：显示老年代和永久代的行为统计，从jdk1.8开始,该选项仅表示老年代，因为永久代被移除了； jstat -gcoldcapacity vmid ：显示老年代的大小； jstat -gcpermcapacity vmid ：显示永久代大小，从jdk1.8开始,该选项不存在了，因为永久代被移除了； jstat -gcutil vmid ：显示垃圾收集信息； 另外，加上 -t参数可以在输出信息上加一个 Timestamp 列，显示程序的运行时间。 jinfo: 实时地查看和调整虚拟机各项参数 jinfo vmid :输出当前 jvm 进程的全部参数和系统属性 (第一部分是系统的属性，第二部分是 JVM 的参数)。 jinfo -flag name vmid :输出对应名称的参数的具体值。比如输出 MaxHeapSize、查看当前 jvm 进程是否开启打印 GC 日志 ( -XX:PrintGCDetails :详细 GC 日志模式，这两个都是默认关闭的)。 C:\\Users\\SnailClimb>jinfo -flag MaxHeapSize 17340 -XX:MaxHeapSize=2124414976 C:\\Users\\SnailClimb>jinfo -flag PrintGC 17340 -XX:-PrintGC 使用 jinfo 可以在不重启虚拟机的情况下，可以动态的修改 jvm 的参数。尤其在线上的环境特别有用,请看下面的例子： jinfo -flag [+|-]name vmid 开启或者关闭对应名称的参数。 C:\\Users\\SnailClimb>jinfo -flag PrintGC 17340 -XX:-PrintGC C:\\Users\\SnailClimb>jinfo -flag +PrintGC 17340 C:\\Users\\SnailClimb>jinfo -flag PrintGC 17340 -XX:+PrintGC jmap:生成堆转储快照 jmap（Memory Map for Java）命令用于生成堆转储快照。 如果不使用 jmap 命令，要想获取 Java 堆转储，可以使用 “-XX:+HeapDumpOnOutOfMemoryError” 参数，可以让虚拟机在 OOM 异常出现之后自动生成 dump 文件，Linux 命令下可以通过 kill -3 发送进程退出信号也能拿到 dump 文件。 jmap 的作用并不仅仅是为了获取 dump 文件，它还可以查询 finalizer 执行队列、Java 堆和永久代的详细信息，如空间使用率、当前使用的是哪种收集器等。和jinfo一样，jmap有不少功能在 Windows 平台下也是受限制的。 示例：将指定应用程序的堆快照输出到桌面。后面，可以通过 jhat、Visual VM 等工具分析该堆文件。 C:\\Users\\SnailClimb>jmap -dump:format=b,file=C:\\Users\\SnailClimb\\Desktop\\heap.hprof 17340 Dumping heap to C:\\Users\\SnailClimb\\Desktop\\heap.hprof ... Heap dump file created jhat: 分析 heapdump 文件 jhat 用于分析 heapdump 文件，它会建立一个 HTTP/HTML 服务器，让用户可以在浏览器上查看分析结果。 C:\\Users\\SnailClimb>jhat C:\\Users\\SnailClimb\\Desktop\\heap.hprof Reading from C:\\Users\\SnailClimb\\Desktop\\heap.hprof... Dump file created Sat May 04 12:30:31 CST 2019 Snapshot read, resolving... Resolving 131419 objects... Chasing references, expect 26 dots.......................... Eliminating duplicate references.......................... Snapshot resolved. Started HTTP server on port 7000 Server is ready. 访问 http://localhost:7000/ jstack :生成虚拟机当前时刻的线程快照 jstack（Stack Trace for Java）命令用于生成虚拟机当前时刻的线程快照。线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合. 生成线程快照的目的主要是定位线程长时间出现停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等都是导致线程长时间停顿的原因。线程出现停顿的时候通过jstack来查看各个线程的调用堆栈，就可以知道没有响应的线程到底在后台做些什么事情，或者在等待些什么资源。 下面是一个线程死锁的代码。我们下面会通过 jstack 命令进行死锁检查，输出死锁信息，找到发生死锁的线程。 public class DeadLockDemo { private static Object resource1 = new Object();//资源 1 private static Object resource2 = new Object();//资源 2 public static void main(String[] args) { new Thread(() -> { synchronized (resource1) { System.out.println(Thread.currentThread() + \"get resource1\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \"waiting get resource2\"); synchronized (resource2) { System.out.println(Thread.currentThread() + \"get resource2\"); } } }, \"线程 1\").start(); new Thread(() -> { synchronized (resource2) { System.out.println(Thread.currentThread() + \"get resource2\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \"waiting get resource1\"); synchronized (resource1) { System.out.println(Thread.currentThread() + \"get resource1\"); } } }, \"线程 2\").start(); } } Output Thread[线程 1,5,main]get resource1 Thread[线程 2,5,main]get resource2 Thread[线程 1,5,main]waiting get resource2 Thread[线程 2,5,main]waiting get resource1 线程 A 通过 synchronized (resource1) 获得 resource1 的监视器锁，然后通过Thread.sleep(1000);让线程 A 休眠 1s 为的是让线程 B 得到执行然后获取到 resource2 的监视器锁。线程 A 和线程 B 休眠结束了都开始企图请求获取对方的资源，然后这两个线程就会陷入互相等待的状态，这也就产生了死锁。 通过 jstack 命令分析： C:\\Users\\SnailClimb>jps 13792 KotlinCompileDaemon 7360 NettyClient2 17396 7972 Launcher 8932 Launcher 9256 DeadLockDemo 10764 Jps 17340 NettyServer C:\\Users\\SnailClimb>jstack 9256 输出的部分内容如下： Found one Java-level deadlock: ============================= \"线程 2\": waiting to lock monitor 0x000000000333e668 (object 0x00000000d5efe1c0, a java.lang.Object), which is held by \"线程 1\" \"线程 1\": waiting to lock monitor 0x000000000333be88 (object 0x00000000d5efe1d0, a java.lang.Object), which is held by \"线程 2\" Java stack information for the threads listed above: =================================================== \"线程 2\": at DeadLockDemo.lambda$main$1(DeadLockDemo.java:31) - waiting to lock (a java.lang.Object) - locked (a java.lang.Object) at DeadLockDemo$$Lambda$2/1078694789.run(Unknown Source) at java.lang.Thread.run(Thread.java:748) \"线程 1\": at DeadLockDemo.lambda$main$0(DeadLockDemo.java:16) - waiting to lock (a java.lang.Object) - locked (a java.lang.Object) at DeadLockDemo$$Lambda$1/1324119927.run(Unknown Source) at java.lang.Thread.run(Thread.java:748) Found 1 deadlock. 可以看到 jstack 命令已经帮我们找到发生死锁的线程的具体信息。 JDK 可视化分析工具 JConsole:Java 监视与管理控制台 JConsole 是基于 JMX 的可视化监视、管理工具。可以很方便的监视本地及远程服务器的 java 进程的内存使用情况。你可以在控制台输出console命令启动或者在 JDK 目录下的 bin 目录找到jconsole.exe然后双击启动。 连接 Jconsole 如果需要使用 JConsole 连接远程进程，可以在远程 Java 程序启动时加上下面这些参数: -Djava.rmi.server.hostname=外网访问 ip 地址 -Dcom.sun.management.jmxremote.port=60001 //监控的端口号 -Dcom.sun.management.jmxremote.authenticate=false //关闭认证 -Dcom.sun.management.jmxremote.ssl=false 在使用 JConsole 连接时，远程进程地址如下： 外网访问 ip 地址:60001 查看 Java 程序概况 内存监控 JConsole 可以显示当前内存的详细信息。不仅包括堆内存/非堆内存的整体信息，还可以细化到 eden 区、survivor 区等的使用情况，如下图所示。 点击右边的“执行 GC(G)”按钮可以强制应用程序执行一个 Full GC。 新生代 GC（Minor GC）:指发生新生代的的垃圾收集动作，Minor GC 非常频繁，回收速度一般也比较快。 老年代 GC（Major GC/Full GC）:指发生在老年代的 GC，出现了 Major GC 经常会伴随至少一次的 Minor GC（并非绝对），Major GC 的速度一般会比 Minor GC 的慢 10 倍以上。 线程监控 类似我们前面讲的 jstack 命令，不过这个是可视化的。 最下面有一个\"检测死锁 (D)\"按钮，点击这个按钮可以自动为你找到发生死锁的线程以及它们的详细信息 。 Visual VM:多合一故障处理工具 VisualVM 提供在 Java 虚拟机 (Java Virutal Machine, JVM) 上运行的 Java 应用程序的详细信息。在 VisualVM 的图形用户界面中，您可以方便、快捷地查看多个 Java 应用程序的相关信息。Visual VM 官网：https://visualvm.github.io/ 。Visual VM 中文文档:https://visualvm.github.io/documentation.html。 下面这段话摘自《深入理解 Java 虚拟机》。 VisualVM（All-in-One Java Troubleshooting Tool）是到目前为止随 JDK 发布的功能最强大的运行监视和故障处理程序，官方在 VisualVM 的软件说明中写上了“All-in-One”的描述字样，预示着他除了运行监视、故障处理外，还提供了很多其他方面的功能，如性能分析（Profiling）。VisualVM 的性能分析功能甚至比起 JProfiler、YourKit 等专业且收费的 Profiling 工具都不会逊色多少，而且 VisualVM 还有一个很大的优点：不需要被监视的程序基于特殊 Agent 运行，因此他对应用程序的实际性能的影响很小，使得他可以直接应用在生产环境中。这个优点是 JProfiler、YourKit 等工具无法与之媲美的。 VisualVM 基于 NetBeans 平台开发，因此他一开始就具备了插件扩展功能的特性，通过插件扩展支持，VisualVM 可以做到： 显示虚拟机进程以及进程的配置、环境信息（jps、jinfo）。 监视应用程序的 CPU、GC、堆、方法区以及线程的信息（jstat、jstack）。 dump 以及分析堆转储快照（jmap、jhat）。 方法级的程序运行性能分析，找到被调用最多、运行时间最长的方法。 离线程序快照：收集程序的运行时配置、线程 dump、内存 dump 等信息建立一个快照，可以将快照发送开发者处进行 Bug 反馈。 其他 plugins 的无限的可能性...... 这里就不具体介绍 VisualVM 的使用，如果想了解的话可以看: https://visualvm.github.io/documentation.html https://www.ibm.com/developerworks/cn/java/j-lo-visualvm/index.html 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \"Java面试突击\" 即可免费领取！ Java工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/java/jvm/jvm 知识点汇总.html":{"url":"zother6-JavaGuide/java/jvm/jvm 知识点汇总.html","title":"jvm 知识点汇总","keywords":"","body":"无论什么级别的Java从业者，JVM都是进阶时必须迈过的坎。不管是工作还是面试中，JVM都是必考题。如果不懂JVM的话，薪酬会非常吃亏（近70%的面试者挂在JVM上了）。 掌握了JVM机制，就等于学会了深层次解决问题的方法。对于Java开发者而言，只有熟悉底层虚拟机的运行机制，才能通过JVM日志深入到字节码的层次去分析排查问题，发现隐性的系统缺陷，进而提升系统性能。 一些技术人员开发工具用得很熟练，触及JVM问题时却是模棱两可，甚至连内存模型和内存区域，HotSpot和JVM规范，都混淆不清。工作很长时间，在生产时还在用缺省参数来直接启动，以致系统运行时出现性能、稳定性等问题时束手无措，不知该如何追踪排查。久而久之，这对自己的职业成长是极为不利的。 掌握JVM，是深入Java技术栈的必经之路。 "},"zother6-JavaGuide/java/jvm/JVM垃圾回收.html":{"url":"zother6-JavaGuide/java/jvm/JVM垃圾回收.html","title":"JVM垃圾回收","keywords":"","body":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。 JVM 垃圾回收 写在前面 本节常见面试题 本文导火索 1 揭开 JVM 内存分配与回收的神秘面纱 1.1 对象优先在 eden 区分配 1.2 大对象直接进入老年代 1.3 长期存活的对象将进入老年代 1.4 动态对象年龄判定 2 对象已经死亡？ 2.1 引用计数法 2.2 可达性分析算法 2.3 再谈引用 2.4 不可达的对象并非“非死不可” 2.5 如何判断一个常量是废弃常量 2.6 如何判断一个类是无用的类 3 垃圾收集算法 3.1 标记-清除算法 3.2 复制算法 3.3 标记-整理算法 3.4 分代收集算法 4 垃圾收集器 4.1 Serial 收集器 4.2 ParNew 收集器 4.3 Parallel Scavenge 收集器 4.4.Serial Old 收集器 4.5 Parallel Old 收集器 4.6 CMS 收集器 4.7 G1 收集器 参考 JVM 垃圾回收 写在前面 本节常见面试题 问题答案在文中都有提到 如何判断对象是否死亡（两种方法）。 简单的介绍一下强引用、软引用、弱引用、虚引用（虚引用与软引用和弱引用的区别、使用软引用能带来的好处）。 如何判断一个常量是废弃常量 如何判断一个类是无用的类 垃圾收集有哪些算法，各自的特点？ HotSpot 为什么要分为新生代和老年代？ 常见的垃圾回收器有哪些？ 介绍一下 CMS,G1 收集器。 Minor Gc 和 Full GC 有什么不同呢？ 本文导火索 当需要排查各种内存溢出问题、当垃圾收集成为系统达到更高并发的瓶颈时，我们就需要对这些“自动化”的技术实施必要的监控和调节。 1 揭开 JVM 内存分配与回收的神秘面纱 Java 的自动内存管理主要是针对对象内存的回收和对象内存的分配。同时，Java 自动内存管理最核心的功能是 堆 内存中对象的分配与回收。 Java 堆是垃圾收集器管理的主要区域，因此也被称作GC 堆（Garbage Collected Heap）.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代：再细致一点有：Eden 空间、From Survivor、To Survivor 空间等。进一步划分的目的是更好地回收内存，或者更快地分配内存。 堆空间的基本结构： 上图所示的 eden 区、s0(\"From\") 区、s1(\"To\") 区都属于新生代，tentired 区属于老年代。大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s1(\"To\")，并且对象的年龄还会加 1(Eden 区->Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。经过这次GC后，Eden区和\"From\"区已经被清空。这个时候，\"From\"和\"To\"会交换他们的角色，也就是新的\"To\"就是上次GC前的“From”，新的\"From\"就是上次GC前的\"To\"。不管怎样，都会保证名为To的Survivor区域是空的。Minor GC会一直重复这样的过程，直到“To”区被填满，\"To\"区被填满之后，会将所有对象移动到老年代中。 1.1 对象优先在 eden 区分配 目前主流的垃圾收集器都会采用分代回收算法，因此需要将堆内存分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。 大多数情况下，对象在新生代中 eden 区分配。当 eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC.下面我们来进行实际测试以下。 在测试之前我们先来看看 Minor GC 和 Full GC 有什么不同呢？ 新生代 GC（Minor GC）:指发生新生代的的垃圾收集动作，Minor GC 非常频繁，回收速度一般也比较快。 老年代 GC（Major GC/Full GC）:指发生在老年代的 GC，出现了 Major GC 经常会伴随至少一次的 Minor GC（并非绝对），Major GC 的速度一般会比 Minor GC 的慢 10 倍以上。 issue#664 :guang19 补充：个人在网上查阅相关资料的时候发现如题所说的观点。有的文章说 Full GC与Major GC一样是属于对老年代的GC，也有的文章说 Full GC 是对整个堆区的GC，所以这点需要各位同学自行分辨Full GC语义。见: 知乎讨论 测试： public class GCTest { public static void main(String[] args) { byte[] allocation1, allocation2; allocation1 = new byte[30900*1024]; //allocation2 = new byte[900*1024]; } } 通过以下方式运行： 添加的参数：-XX:+PrintGCDetails 运行结果 (红色字体描述有误，应该是对应于 JDK1.7 的永久代)： 从上图我们可以看出 eden 区内存几乎已经被分配完全（即使程序什么也不做，新生代也会使用 2000 多 k 内存）。假如我们再为 allocation2 分配内存会出现什么情况呢？ allocation2 = new byte[900*1024]; 简单解释一下为什么会出现这种情况： 因为给 allocation2 分配内存的时候 eden 区内存几乎已经被分配完了，我们刚刚讲了当 Eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC.GC 期间虚拟机又发现 allocation1 无法存入 Survivor 空间，所以只好通过 分配担保机制 把新生代的对象提前转移到老年代中去，老年代上的空间足够存放 allocation1，所以不会出现 Full GC。执行 Minor GC 后，后面分配的对象如果能够存在 eden 区的话，还是会在 eden 区分配内存。可以执行如下代码验证： public class GCTest { public static void main(String[] args) { byte[] allocation1, allocation2,allocation3,allocation4,allocation5; allocation1 = new byte[32000*1024]; allocation2 = new byte[1000*1024]; allocation3 = new byte[1000*1024]; allocation4 = new byte[1000*1024]; allocation5 = new byte[1000*1024]; } } 1.2 大对象直接进入老年代 大对象就是需要大量连续内存空间的对象（比如：字符串、数组）。 为什么要这样呢？ 为了避免为大对象分配内存时由于分配担保机制带来的复制而降低效率。 1.3 长期存活的对象将进入老年代 既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别哪些对象应放在新生代，哪些对象应放在老年代中。为了做到这一点，虚拟机给每个对象一个对象年龄（Age）计数器。 如果对象在 Eden 出生并经过第一次 Minor GC 后仍然能够存活，并且能被 Survivor 容纳的话，将被移动到 Survivor 空间中，并将对象年龄设为 1.对象在 Survivor 中每熬过一次 MinorGC,年龄就增加 1 岁，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 1.4 动态对象年龄判定 大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden 区->Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 修正（issue552）：“Hotspot遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了survivor区的一半时，取这个年龄和MaxTenuringThreshold中更小的一个值，作为新的晋升年龄阈值”。 动态年龄计算的代码如下 uint ageTable::compute_tenuring_threshold(size_t survivor_capacity) { //survivor_capacity是survivor空间的大小 size_t desired_survivor_size = (size_t)((((double) survivor_capacity)*TargetSurvivorRatio)/100); size_t total = 0; uint age = 1; while (age desired_survivor_size) break; age++; } uint result = age 额外补充说明(issue672)：关于默认的晋升年龄是15，这个说法的来源大部分都是《深入理解Java虚拟机》这本书。 如果你去Oracle的官网阅读相关的虚拟机参数，你会发现-XX:MaxTenuringThreshold=threshold这里有个说明 Sets the maximum tenuring threshold for use in adaptive GC sizing. The largest value is 15. The default value is 15 for the parallel (throughput) collector, and 6 for the CMS collector.默认晋升年龄并不都是15，这个是要区分垃圾收集器的，CMS就是6. 2 对象已经死亡？ 堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断那些对象已经死亡（即不能再被任何途径使用的对象）。 2.1 引用计数法 给对象中添加一个引用计数器，每当有一个地方引用它，计数器就加 1；当引用失效，计数器就减 1；任何时候计数器为 0 的对象就是不可能再被使用的。 这个方法实现简单，效率高，但是目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决对象之间相互循环引用的问题。 所谓对象之间的相互引用问题，如下面代码所示：除了对象 objA 和 objB 相互引用着对方之外，这两个对象之间再无任何引用。但是他们因为互相引用对方，导致它们的引用计数器都不为 0，于是引用计数算法无法通知 GC 回收器回收他们。 public class ReferenceCountingGc { Object instance = null; public static void main(String[] args) { ReferenceCountingGc objA = new ReferenceCountingGc(); ReferenceCountingGc objB = new ReferenceCountingGc(); objA.instance = objB; objB.instance = objA; objA = null; objB = null; } } 2.2 可达性分析算法 这个算法的基本思想就是通过一系列的称为 “GC Roots” 的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的。 2.3 再谈引用 无论是通过引用计数法判断对象引用数量，还是通过可达性分析法判断对象的引用链是否可达，判定对象的存活都与“引用”有关。 JDK1.2 之前，Java 中引用的定义很传统：如果 reference 类型的数据存储的数值代表的是另一块内存的起始地址，就称这块内存代表一个引用。 JDK1.2 以后，Java 对引用的概念进行了扩充，将引用分为强引用、软引用、弱引用、虚引用四种（引用强度逐渐减弱） 1．强引用（StrongReference） 以前我们使用的大部分引用实际上都是强引用，这是使用最普遍的引用。如果一个对象具有强引用，那就类似于必不可少的生活用品，垃圾回收器绝不会回收它。当内存空间不足，Java 虚拟机宁愿抛出 OutOfMemoryError 错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。 2．软引用（SoftReference） 如果一个对象只具有软引用，那就类似于可有可无的生活用品。如果内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。 软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收，JAVA 虚拟机就会把这个软引用加入到与之关联的引用队列中。 3．弱引用（WeakReference） 如果一个对象只具有弱引用，那就类似于可有可无的生活用品。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。 弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java 虚拟机就会把这个弱引用加入到与之关联的引用队列中。 4．虚引用（PhantomReference） \"虚引用\"顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。 虚引用主要用来跟踪对象被垃圾回收的活动。 虚引用与软引用和弱引用的一个区别在于： 虚引用必须和引用队列（ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 特别注意，在程序设计中一般很少使用弱引用与虚引用，使用软引用的情况较多，这是因为软引用可以加速 JVM 对垃圾内存的回收速度，可以维护系统的运行安全，防止内存溢出（OutOfMemory）等问题的产生。 2.4 不可达的对象并非“非死不可” 即使在可达性分析法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑阶段”，要真正宣告一个对象死亡，至少要经历两次标记过程；可达性分析法中不可达的对象被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行 finalize 方法。当对象没有覆盖 finalize 方法，或 finalize 方法已经被虚拟机调用过时，虚拟机将这两种情况视为没有必要执行。 被判定为需要执行的对象将会被放在一个队列中进行第二次标记，除非这个对象与引用链上的任何一个对象建立关联，否则就会被真的回收。 2.5 如何判断一个常量是废弃常量 运行时常量池主要回收的是废弃的常量。那么，我们如何判断一个常量是废弃常量呢？ 假如在常量池中存在字符串 \"abc\"，如果当前没有任何 String 对象引用该字符串常量的话，就说明常量 \"abc\" 就是废弃常量，如果这时发生内存回收的话而且有必要的话，\"abc\" 就会被系统清理出常量池。 注意：我们在 可能是把 Java 内存区域讲的最清楚的一篇文章 也讲了 JDK1.7 及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。 2.6 如何判断一个类是无用的类 方法区主要回收的是无用的类，那么如何判断一个类是无用的类的呢？ 判定一个常量是否是“废弃常量”比较简单，而要判定一个类是否是“无用的类”的条件则相对苛刻许多。类需要同时满足下面 3 个条件才能算是 “无用的类” ： 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 虚拟机可以对满足上述 3 个条件的无用类进行回收，这里说的仅仅是“可以”，而并不是和对象一样不使用了就会必然被回收。 3 垃圾收集算法 3.1 标记-清除算法 该算法分为“标记”和“清除”阶段：首先比较出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。它是最基础的收集算法，后续的算法都是对其不足进行改进得到。这种垃圾收集算法会带来两个明显的问题： 效率问题 空间问题（标记清除后会产生大量不连续的碎片） 3.2 复制算法 为了解决效率问题，“复制”收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。 3.3 标记-整理算法 根据老年代的特点提出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。 3.4 分代收集算法 当前虚拟机的垃圾收集都采用分代收集算法，这种算法没有什么新的思想，只是根据对象存活周期的不同将内存分为几块。一般将 java 堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。 比如在新生代中，每次收集都会有大量对象死去，所以可以选择复制算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。 延伸面试问题： HotSpot 为什么要分为新生代和老年代？ 根据上面的对分代收集算法的介绍回答。 4 垃圾收集器 如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。 虽然我们对各个收集器进行比较，但并非要挑选出一个最好的收集器。因为直到现在为止还没有最好的垃圾收集器出现，更加没有万能的垃圾收集器，我们能做的就是根据具体应用场景选择适合自己的垃圾收集器。试想一下：如果有一种四海之内、任何场景下都适用的完美收集器存在，那么我们的 HotSpot 虚拟机就不会实现那么多不同的垃圾收集器了。 4.1 Serial 收集器 Serial（串行）收集器收集器是最基本、历史最悠久的垃圾收集器了。大家看名字就知道这个收集器是一个单线程收集器了。它的 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ \"Stop The World\" ），直到它收集结束。 新生代采用复制算法，老年代采用标记-整理算法。 虚拟机的设计者们当然知道 Stop The World 带来的不良用户体验，所以在后续的垃圾收集器设计中停顿时间在不断缩短（仍然还有停顿，寻找最优秀的垃圾收集器的过程仍然在继续）。 但是 Serial 收集器有没有优于其他垃圾收集器的地方呢？当然有，它简单而高效（与其他收集器的单线程相比）。Serial 收集器由于没有线程交互的开销，自然可以获得很高的单线程收集效率。Serial 收集器对于运行在 Client 模式下的虚拟机来说是个不错的选择。 4.2 ParNew 收集器 ParNew 收集器其实就是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和 Serial 收集器完全一样。 新生代采用复制算法，老年代采用标记-整理算法。 它是许多运行在 Server 模式下的虚拟机的首要选择，除了 Serial 收集器外，只有它能与 CMS 收集器（真正意义上的并发收集器，后面会介绍到）配合工作。 并行和并发概念补充： 并行（Parallel） ：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。 并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行，可能会交替执行），用户程序在继续运行，而垃圾收集器运行在另一个 CPU 上。 4.3 Parallel Scavenge 收集器 Parallel Scavenge 收集器也是使用复制算法的多线程收集器，它看上去几乎和ParNew都一样。 那么它有什么特别之处呢？ -XX:+UseParallelGC 使用 Parallel 收集器+ 老年代串行 -XX:+UseParallelOldGC 使用 Parallel 收集器+ 老年代并行 Parallel Scavenge 收集器关注点是吞吐量（高效率的利用 CPU）。CMS 等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是 CPU 中用于运行用户代码的时间与 CPU 总消耗时间的比值。 Parallel Scavenge 收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解的话，手工优化存在困难的话可以选择把内存管理优化交给虚拟机去完成也是一个不错的选择。 新生代采用复制算法，老年代采用标记-整理算法。 4.4.Serial Old 收集器 Serial 收集器的老年代版本，它同样是一个单线程收集器。它主要有两大用途：一种用途是在 JDK1.5 以及以前的版本中与 Parallel Scavenge 收集器搭配使用，另一种用途是作为 CMS 收集器的后备方案。 4.5 Parallel Old 收集器 Parallel Scavenge 收集器的老年代版本。使用多线程和“标记-整理”算法。在注重吞吐量以及 CPU 资源的场合，都可以优先考虑 Parallel Scavenge 收集器和 Parallel Old 收集器。 4.6 CMS 收集器 CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它非常符合在注重用户体验的应用上使用。 CMS（Concurrent Mark Sweep）收集器是 HotSpot 虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。 从名字中的Mark Sweep这两个词可以看出，CMS 收集器是一种 “标记-清除”算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤： 初始标记： 暂停所有的其他线程，并记录下直接与 root 相连的对象，速度很快 ； 并发标记： 同时开启 GC 和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以 GC 线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。 重新标记： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短 并发清除： 开启用户线程，同时 GC 线程开始对未标记的区域做清扫。 从它的名字就可以看出它是一款优秀的垃圾收集器，主要优点：并发收集、低停顿。但是它有下面三个明显的缺点： 对 CPU 资源敏感； 无法处理浮动垃圾； 它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。 4.7 G1 收集器 G1 (Garbage-First) 是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足 GC 停顿时间要求的同时,还具备高吞吐量性能特征. 被视为 JDK1.7 中 HotSpot 虚拟机的一个重要进化特征。它具备一下特点： 并行与并发：G1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间。部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。 分代收集：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。 空间整合：与 CMS 的“标记--清理”算法不同，G1 从整体来看是基于“标记整理”算法实现的收集器；从局部上来看是基于“复制”算法实现的。 可预测的停顿：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内。 G1 收集器的运作大致分为以下几个步骤： 初始标记 并发标记 最终标记 筛选回收 G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region(这也就是它的名字 Garbage-First 的由来)。这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 G1 收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。 参考 《深入理解 Java 虚拟机：JVM 高级特性与最佳实践（第二版》 https://my.oschina.net/hosee/blog/644618 https://docs.oracle.com/javase/specs/jvms/se8/html/index.html 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \"Java面试突击\" 即可免费领取！ Java工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/java/jvm/最重要的JVM参数指南.html":{"url":"zother6-JavaGuide/java/jvm/最重要的JVM参数指南.html","title":"最重要的JVM参数指南","keywords":"","body":" 本文由 JavaGuide 翻译自 https://www.baeldung.com/jvm-parameters，并对文章进行了大量的完善补充。翻译不易，如需转载请注明出处为： 作者： 。 1.概述 在本篇文章中，你将掌握最常用的 JVM 参数配置。如果对于下面提到了一些概念比如堆、 2.堆内存相关 Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。 2.1.显式指定堆内存–Xms和-Xmx 与性能有关的最常见实践之一是根据应用程序要求初始化堆内存。如果我们需要指定最小和最大堆大小（推荐显示指定大小），以下参数可以帮助你实现： -Xms[unit] -Xmx[unit] heap size 表示要初始化内存的具体大小。 unit 表示要初始化内存的单位。单位为“ g” (GB) 、“ m”（MB）、“ k”（KB）。 举个栗子🌰，如果我们要为JVM分配最小2 GB和最大5 GB的堆内存大小，我们的参数应该这样来写： -Xms2G -Xmx5G 2.2.显式新生代内存(Young Ceneration) 根据Oracle官方文档，在堆总可用内存配置完成之后，第二大影响因素是为 Young Generation 在堆内存所占的比例。默认情况下，YG 的最小大小为 1310 MB，最大大小为无限制。 一共有两种指定 新生代内存(Young Ceneration)大小的方法： 1.通过-XX:NewSize和-XX:MaxNewSize指定 -XX:NewSize=[unit] -XX:MaxNewSize=[unit] 举个栗子🌰，如果我们要为 新生代分配 最小256m 的内存，最大 1024m的内存我们的参数应该这样来写： -XX:NewSize=256m -XX:MaxNewSize=1024m 2.通过-Xmn[unit]指定 举个栗子🌰，如果我们要为 新生代分配256m的内存（NewSize与MaxNewSize设为一致），我们的参数应该这样来写： -Xmn256m GC 调优策略中很重要的一条经验总结是这样说的： 将新对象预留在新生代，由于 Full GC 的成本远高于 Minor GC，因此尽可能将对象分配在新生代是明智的做法，实际项目中根据 GC 日志分析新生代空间大小分配是否合理，适当通过“-Xmn”命令调节新生代大小，最大限度降低新对象直接进入老年代的情况。 另外，你还可以通过-XX:NewRatio=来设置新生代和老年代内存的比值。 比如下面的参数就是设置新生代（包括Eden和两个Survivor区）与老年代的比值为1。也就是说：新生代与老年代所占比值为1：1，新生代占整个堆栈的 1/2。 -XX:NewRatio=1 2.3.显示指定永久代/元空间的大小 从Java 8开始，如果我们没有指定 Metaspace 的大小，随着更多类的创建，虚拟机会耗尽所有可用的系统内存（永久代并不会出现这种情况）。 JDK 1.8 之前永久代还没被彻底移除的时候通常通过下面这些参数来调节方法区大小 -XX:PermSize=N //方法区 (永久代) 初始大小 -XX:MaxPermSize=N //方法区 (永久代) 最大大小,超过这个值将会抛出 OutOfMemoryError 异常:java.lang.OutOfMemoryError: PermGen 相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。 JDK 1.8 的时候，方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。 下面是一些常用参数： -XX:MetaspaceSize=N //设置 Metaspace 的初始（和最小大小） -XX:MaxMetaspaceSize=N //设置 Metaspace 的最大大小，如果不指定大小的话，随着更多类的创建，虚拟机会耗尽所有可用的系统内存。 3.垃圾收集相关 3.1.垃圾回收器 为了提高应用程序的稳定性，选择正确的垃圾收集算法至关重要。 JVM具有四种类型的GC实现： 串行垃圾收集器 并行垃圾收集器 CMS垃圾收集器 G1垃圾收集器 可以使用以下参数声明这些实现： -XX:+UseSerialGC -XX:+UseParallelGC -XX:+USeParNewGC -XX:+UseG1GC 有关垃圾回收实施的更多详细信息，请参见此处。 3.2.GC记录 为了严格监控应用程序的运行状况，我们应该始终检查JVM的垃圾回收性能。最简单的方法是以人类可读的格式记录GC活动。 使用以下参数，我们可以记录GC活动： -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles= -XX:GCLogFileSize=[ unit ] -Xloggc:/path/to/gc.log 推荐阅读 CMS GC 默认新生代是多大？ CMS GC启动参数优化配置 从实际案例聊聊Java应用的GC优化-美团技术团队 JVM性能调优详解 （2019-11-11） JVM参数使用手册 "},"zother6-JavaGuide/java/jvm/类加载器.html":{"url":"zother6-JavaGuide/java/jvm/类加载器.html","title":"类加载器","keywords":"","body":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。 回顾一下类加载过程 类加载器总结 双亲委派模型 双亲委派模型介绍 双亲委派模型实现源码分析 双亲委派模型的好处 如果我们不想要双亲委派模型怎么办？ 自定义类加载器 推荐 公众号JavaGuide 后台回复关键字“1”，免费获取JavaGuide配套的Java工程师必备学习资源(文末有公众号二维码)。 回顾一下类加载过程 类加载过程：加载->连接->初始化。连接过程又可分为三步:验证->准备->解析。 一个非数组类的加载阶段（加载阶段获取类的二进制字节流的动作）是可控性最强的阶段，这一步我们可以去完成还可以自定义类加载器去控制字节流的获取方式（重写一个类加载器的 loadClass() 方法）。数组类型不通过类加载器创建，它由 Java 虚拟机直接创建。 所有的类都由类加载器加载，加载的作用就是将 .class文件加载到内存。 类加载器总结 JVM 中内置了三个重要的 ClassLoader，除了 BootstrapClassLoader 其他类加载器均由 Java 实现且全部继承自java.lang.ClassLoader： BootstrapClassLoader(启动类加载器) ：最顶层的加载类，由C++实现，负责加载 %JAVA_HOME%/lib目录下的jar包和类或者或被 -Xbootclasspath参数指定的路径中的所有类。 ExtensionClassLoader(扩展类加载器) ：主要负责加载目录 %JRE_HOME%/lib/ext 目录下的jar包和类，或被 java.ext.dirs 系统变量所指定的路径下的jar包。 AppClassLoader(应用程序类加载器) :面向我们用户的加载器，负责加载当前应用classpath下的所有jar包和类。 双亲委派模型 双亲委派模型介绍 每一个类都有一个对应它的类加载器。系统中的 ClassLoder 在协同工作的时候会默认使用 双亲委派模型 。即在类加载的时候，系统会首先判断当前类是否被加载过。已经被加载的类会直接返回，否则才会尝试加载。加载的时候，首先会把该请求委派该父类加载器的 loadClass() 处理，因此所有的请求最终都应该传送到顶层的启动类加载器 BootstrapClassLoader 中。当父类加载器无法处理时，才由自己来处理。当父类加载器为null时，会使用启动类加载器 BootstrapClassLoader 作为父类加载器。 每个类加载都有一个父类加载器，我们通过下面的程序来验证。 public class ClassLoaderDemo { public static void main(String[] args) { System.out.println(\"ClassLodarDemo's ClassLoader is \" + ClassLoaderDemo.class.getClassLoader()); System.out.println(\"The Parent of ClassLodarDemo's ClassLoader is \" + ClassLoaderDemo.class.getClassLoader().getParent()); System.out.println(\"The GrandParent of ClassLodarDemo's ClassLoader is \" + ClassLoaderDemo.class.getClassLoader().getParent().getParent()); } } Output ClassLodarDemo's ClassLoader is sun.misc.Launcher$AppClassLoader@18b4aac2 The Parent of ClassLodarDemo's ClassLoader is sun.misc.Launcher$ExtClassLoader@1b6d3586 The GrandParent of ClassLodarDemo's ClassLoader is null AppClassLoader的父类加载器为ExtClassLoader ExtClassLoader的父类加载器为null，null并不代表ExtClassLoader没有父类加载器，而是 BootstrapClassLoader 。 其实这个双亲翻译的容易让别人误解，我们一般理解的双亲都是父母，这里的双亲更多地表达的是“父母这一辈”的人而已，并不是说真的有一个 Mother ClassLoader 和一个 Father ClassLoader 。另外，类加载器之间的“父子”关系也不是通过继承来体现的，是由“优先级”来决定。官方API文档对这部分的描述如下: The Java platform uses a delegation model for loading classes. The basic idea is that every class loader has a \"parent\" class loader. When loading a class, a class loader first \"delegates\" the search for the class to its parent class loader before attempting to find the class itself. 双亲委派模型实现源码分析 双亲委派模型的实现代码非常简单，逻辑非常清晰，都集中在 java.lang.ClassLoader 的 loadClass() 中，相关代码如下所示。 private final ClassLoader parent; protected Class loadClass(String name, boolean resolve) throws ClassNotFoundException { synchronized (getClassLoadingLock(name)) { // 首先，检查请求的类是否已经被加载过 Class c = findLoadedClass(name); if (c == null) { long t0 = System.nanoTime(); try { if (parent != null) {//父加载器不为空，调用父加载器loadClass()方法处理 c = parent.loadClass(name, false); } else {//父加载器为空，使用启动类加载器 BootstrapClassLoader 加载 c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { //抛出异常说明父类加载器无法完成加载请求 } if (c == null) { long t1 = System.nanoTime(); //自己尝试加载 c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); } } if (resolve) { resolveClass(c); } return c; } } 双亲委派模型的好处 双亲委派模型保证了Java程序的稳定运行，可以避免类的重复加载（JVM 区分不同类的方式不仅仅根据类名，相同的类文件被不同的类加载器加载产生的是两个不同的类），也保证了 Java 的核心 API 不被篡改。如果没有使用双亲委派模型，而是每个类加载器加载自己的话就会出现一些问题，比如我们编写一个称为 java.lang.Object 类的话，那么程序运行的时候，系统就会出现多个不同的 Object 类。 如果我们不想用双亲委派模型怎么办？ 为了避免双亲委托机制，我们可以自己定义一个类加载器，然后重写 loadClass() 即可。 自定义类加载器 除了 BootstrapClassLoader 其他类加载器均由 Java 实现且全部继承自java.lang.ClassLoader。如果我们要自定义自己的类加载器，很明显需要继承 ClassLoader。 推荐阅读 https://blog.csdn.net/xyang81/article/details/7292380 https://juejin.im/post/5c04892351882516e70dcc9b http://gityuan.com/2016/01/24/java-classloader/ 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \"Java面试突击\" 即可免费领取！ Java工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/java/jvm/类加载过程.html":{"url":"zother6-JavaGuide/java/jvm/类加载过程.html","title":"类加载过程","keywords":"","body":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。 公众号JavaGuide 后台回复关键字“1”，免费获取JavaGuide配套的Java工程师必备学习资源(文末有公众号二维码)。 类的生命周期 类加载过程 加载 验证 准备 解析 初始化 卸载 公众号 类的生命周期 一个类的完整生命周期如下： 类加载过程 Class 文件需要加载到虚拟机中之后才能运行和使用，那么虚拟机是如何加载这些 Class 文件呢？ 系统加载 Class 类型的文件主要三步:加载->连接->初始化。连接过程又可分为三步:验证->准备->解析。 加载 类加载过程的第一步，主要完成下面3件事情： 通过全类名获取定义此类的二进制字节流 将字节流所代表的静态存储结构转换为方法区的运行时数据结构 在内存中生成一个代表该类的 Class 对象,作为方法区这些数据的访问入口 虚拟机规范多上面这3点并不具体，因此是非常灵活的。比如：\"通过全类名获取定义此类的二进制字节流\" 并没有指明具体从哪里获取、怎样获取。比如：比较常见的就是从 ZIP 包中读取（日后出现的JAR、EAR、WAR格式的基础）、其他文件生成（典型应用就是JSP）等等。 一个非数组类的加载阶段（加载阶段获取类的二进制字节流的动作）是可控性最强的阶段，这一步我们可以去完成还可以自定义类加载器去控制字节流的获取方式（重写一个类加载器的 loadClass() 方法）。数组类型不通过类加载器创建，它由 Java 虚拟机直接创建。 类加载器、双亲委派模型也是非常重要的知识点，这部分内容会在后面的文章中单独介绍到。 加载阶段和连接阶段的部分内容是交叉进行的，加载阶段尚未结束，连接阶段可能就已经开始了。 验证 准备 准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些内存都将在方法区中分配。对于该阶段有以下几点需要注意： 这时候进行内存分配的仅包括类变量（static），而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在 Java 堆中。 这里所设置的初始值\"通常情况\"下是数据类型默认的零值（如0、0L、null、false等），比如我们定义了public static int value=111 ，那么 value 变量在准备阶段的初始值就是 0 而不是111（初始化阶段才会赋值）。特殊情况：比如给 value 变量加上了 fianl 关键字public static final int value=111 ，那么准备阶段 value 的值就被赋值为 111。 基本数据类型的零值： 解析 解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用限定符7类符号引用进行。 符号引用就是一组符号来描述目标，可以是任何字面量。直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。在程序实际运行时，只有符号引用是不够的，举个例子：在程序执行方法时，系统需要明确知道这个方法所在的位置。Java 虚拟机为每个类都准备了一张方法表来存放类中所有的方法。当需要调用一个类的方法的时候，只要知道这个方法在方发表中的偏移量就可以直接调用该方法了。通过解析操作符号引用就可以直接转变为目标方法在类中方法表的位置，从而使得方法可以被调用。 综上，解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，也就是得到类或者字段、方法在内存中的指针或者偏移量。 初始化 初始化是类加载的最后一步，也是真正执行类中定义的 Java 程序代码(字节码)，初始化阶段是执行类构造器 ()方法的过程。 对于（） 方法的调用，虚拟机会自己确保其在多线程环境中的安全性。因为 （） 方法是带锁线程安全，所以在多线程环境下进行类初始化的话可能会引起死锁，并且这种死锁很难被发现。 对于初始化阶段，虚拟机严格规范了有且只有5种情况下，必须对类进行初始化(只有主动去使用类才会初始化类)： 当遇到 new 、 getstatic、putstatic或invokestatic 这4条直接码指令时，比如 new 一个类，读取一个静态字段(未被 final 修饰)、或调用一个类的静态方法时。 当jvm执行new指令时会初始化类。即当程序创建一个类的实例对象。 当jvm执行getstatic指令时会初始化类。即程序访问类的静态变量(不是静态常量，常量会被加载到运行时常量池)。 当jvm执行putstatic指令时会初始化类。即程序给类的静态变量赋值。 当jvm执行invokestatic指令时会初始化类。即程序调用类的静态方法。 使用 java.lang.reflect 包的方法对类进行反射调用时如Class.forname(\"...\"),newInstance()等等。 ，如果类没初始化，需要触发其初始化。 初始化一个类，如果其父类还未初始化，则先触发该父类的初始化。 当虚拟机启动时，用户需要定义一个要执行的主类 (包含 main 方法的那个类)，虚拟机会先初始化这个类。 MethodHandle和VarHandle可以看作是轻量级的反射调用机制，而要想使用这2个调用， 就必须先使用findStaticVarHandle来初始化要调用的类。 「补充，来自issue745」 当一个接口中定义了JDK8新加入的默认方法（被default关键字修饰的接口方法）时，如果有这个接口的实现类发生了初始化，那该接口要在其之前被初始化。 卸载 卸载这部分内容来自 issue#662由 guang19 补充完善。 卸载类即该类的Class对象被GC。 卸载类需要满足3个要求: 该类的所有的实例对象都已被GC，也就是说堆不存在该类的实例对象。 该类没有在其他任何地方被引用 该类的类加载器的实例已被GC 所以，在JVM生命周期类，由jvm自带的类加载器加载的类是不会被卸载的。但是由我们自定义的类加载器加载的类是可能被卸载的。 只要想通一点就好了，jdk自带的BootstrapClassLoader,PlatformClassLoader,AppClassLoader负责加载jdk提供的类，所以它们(类加载器的实例)肯定不会被回收。而我们自定义的类加载器的实例是可以被回收的，所以使用我们自定义加载器加载的类是可以被卸载掉的。 参考 《深入理解Java虚拟机》 《实战Java虚拟机》 https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-5.html 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \"Java面试突击\" 即可免费领取！ Java工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/java/jvm/类文件结构.html":{"url":"zother6-JavaGuide/java/jvm/类文件结构.html","title":"类文件结构","keywords":"","body":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。 类文件结构 一 概述 二 Class 文件结构总结 2.1 魔数 2.2 Class 文件版本 2.3 常量池 2.4 访问标志 2.5 当前类索引,父类索引与接口索引集合 2.6 字段表集合 2.7 方法表集合 2.8 属性表集合 参考 类文件结构 一 概述 在 Java 中，JVM 可以理解的代码就叫做字节码（即扩展名为 .class 的文件），它不面向任何特定的处理器，只面向虚拟机。Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。所以 Java 程序运行时比较高效，而且，由于字节码并不针对一种特定的机器，因此，Java 程序无须重新编译便可在多种不同操作系统的计算机上运行。 Clojure（Lisp 语言的一种方言）、Groovy、Scala 等语言都是运行在 Java 虚拟机之上。下图展示了不同的语言被不同的编译器编译成.class文件最终运行在 Java 虚拟机之上。.class文件的二进制格式可以使用 WinHex 查看。 可以说.class文件是不同的语言在 Java 虚拟机之间的重要桥梁，同时也是支持 Java 跨平台很重要的一个原因。 二 Class 文件结构总结 根据 Java 虚拟机规范，类文件由单个 ClassFile 结构组成： ClassFile { u4 magic; //Class 文件的标志 u2 minor_version;//Class 的小版本号 u2 major_version;//Class 的大版本号 u2 constant_pool_count;//常量池的数量 cp_info constant_pool[constant_pool_count-1];//常量池 u2 access_flags;//Class 的访问标记 u2 this_class;//当前类 u2 super_class;//父类 u2 interfaces_count;//接口 u2 interfaces[interfaces_count];//一个类可以实现多个接口 u2 fields_count;//Class 文件的字段属性 field_info fields[fields_count];//一个类会可以有个字段 u2 methods_count;//Class 文件的方法数量 method_info methods[methods_count];//一个类可以有个多个方法 u2 attributes_count;//此类的属性表中的属性数 attribute_info attributes[attributes_count];//属性表集合 } 下面详细介绍一下 Class 文件结构涉及到的一些组件。 Class文件字节码结构组织示意图 （之前在网上保存的，非常不错，原出处不明）： 2.1 魔数 u4 magic; //Class 文件的标志 每个 Class 文件的头四个字节称为魔数（Magic Number）,它的唯一作用是确定这个文件是否为一个能被虚拟机接收的 Class 文件。 程序设计者很多时候都喜欢用一些特殊的数字表示固定的文件类型或者其它特殊的含义。 2.2 Class 文件版本 u2 minor_version;//Class 的小版本号 u2 major_version;//Class 的大版本号 紧接着魔数的四个字节存储的是 Class 文件的版本号：第五和第六是次版本号，第七和第八是主版本号。 高版本的 Java 虚拟机可以执行低版本编译器生成的 Class 文件，但是低版本的 Java 虚拟机不能执行高版本编译器生成的 Class 文件。所以，我们在实际开发的时候要确保开发的的 JDK 版本和生产环境的 JDK 版本保持一致。 2.3 常量池 u2 constant_pool_count;//常量池的数量 cp_info constant_pool[constant_pool_count-1];//常量池 紧接着主次版本号之后的是常量池，常量池的数量是 constant_pool_count-1（常量池计数器是从1开始计数的，将第0项常量空出来是有特殊考虑的，索引值为0代表“不引用任何一个常量池项”）。 常量池主要存放两大常量：字面量和符号引用。字面量比较接近于 Java 语言层面的的常量概念，如文本字符串、声明为 final 的常量值等。而符号引用则属于编译原理方面的概念。包括下面三类常量： 类和接口的全限定名 字段的名称和描述符 方法的名称和描述符 常量池中每一项常量都是一个表，这14种表有一个共同的特点：开始的第一位是一个 u1 类型的标志位 -tag 来标识常量的类型，代表当前这个常量属于哪种常量类型． 类型 标志（tag） 描述 CONSTANT_utf8_info 1 UTF-8编码的字符串 CONSTANT_Integer_info 3 整形字面量 CONSTANT_Float_info 4 浮点型字面量 CONSTANT_Long_info ５ 长整型字面量 CONSTANT_Double_info ６ 双精度浮点型字面量 CONSTANT_Class_info ７ 类或接口的符号引用 CONSTANT_String_info ８ 字符串类型字面量 CONSTANT_Fieldref_info ９ 字段的符号引用 CONSTANT_Methodref_info 10 类中方法的符号引用 CONSTANT_InterfaceMethodref_info 11 接口中方法的符号引用 CONSTANT_NameAndType_info 12 字段或方法的符号引用 CONSTANT_MothodType_info 16 标志方法类型 CONSTANT_MethodHandle_info 15 表示方法句柄 CONSTANT_InvokeDynamic_info 18 表示一个动态方法调用点 .class 文件可以通过javap -v class类名 指令来看一下其常量池中的信息(javap -v class类名-> temp.txt ：将结果输出到 temp.txt 文件)。 2.4 访问标志 在常量池结束之后，紧接着的两个字节代表访问标志，这个标志用于识别一些类或者接口层次的访问信息，包括：这个 Class 是类还是接口，是否为 public 或者 abstract 类型，如果是类的话是否声明为 final 等等。 类访问和属性修饰符: 我们定义了一个 Employee 类 package top.snailclimb.bean; public class Employee { ... } 通过javap -v class类名 指令来看一下类的访问标志。 2.5 当前类索引,父类索引与接口索引集合 u2 this_class;//当前类 u2 super_class;//父类 u2 interfaces_count;//接口 u2 interfaces[interfaces_count];//一个类可以实现多个接口 类索引用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名，由于 Java 语言的单继承，所以父类索引只有一个，除了 java.lang.Object 之外，所有的 java 类都有父类，因此除了 java.lang.Object 外，所有 Java 类的父类索引都不为 0。 接口索引集合用来描述这个类实现了那些接口，这些被实现的接口将按implents(如果这个类本身是接口的话则是extends) 后的接口顺序从左到右排列在接口索引集合中。 2.6 字段表集合 u2 fields_count;//Class 文件的字段的个数 field_info fields[fields_count];//一个类会可以有个字段 字段表（field info）用于描述接口或类中声明的变量。字段包括类级变量以及实例变量，但不包括在方法内部声明的局部变量。 field info(字段表) 的结构: access_flags: 字段的作用域（public ,private,protected修饰符），是实例变量还是类变量（static修饰符）,可否被序列化（transient 修饰符）,可变性（final）,可见性（volatile 修饰符，是否强制从主内存读写）。 name_index: 对常量池的引用，表示的字段的名称； descriptor_index: 对常量池的引用，表示字段和方法的描述符； attributes_count: 一个字段还会拥有一些额外的属性，attributes_count 存放属性的个数； attributes[attributes_count]: 存放具体属性具体内容。 上述这些信息中，各个修饰符都是布尔值，要么有某个修饰符，要么没有，很适合使用标志位来表示。而字段叫什么名字、字段被定义为什么数据类型这些都是无法固定的，只能引用常量池中常量来描述。 字段的 access_flags 的取值: 2.7 方法表集合 u2 methods_count;//Class 文件的方法的数量 method_info methods[methods_count];//一个类可以有个多个方法 methods_count 表示方法的数量，而 method_info 表示的方法表。 Class 文件存储格式中对方法的描述与对字段的描述几乎采用了完全一致的方式。方法表的结构如同字段表一样，依次包括了访问标志、名称索引、描述符索引、属性表集合几项。 method_info(方法表的) 结构: 方法表的 access_flag 取值： 注意：因为volatile修饰符和transient修饰符不可以修饰方法，所以方法表的访问标志中没有这两个对应的标志，但是增加了synchronized、native、abstract等关键字修饰方法，所以也就多了这些关键字对应的标志。 2.8 属性表集合 u2 attributes_count;//此类的属性表中的属性数 attribute_info attributes[attributes_count];//属性表集合 在 Class 文件，字段表，方法表中都可以携带自己的属性表集合，以用于描述某些场景专有的信息。与 Class 文件中其它的数据项目要求的顺序、长度和内容不同，属性表集合的限制稍微宽松一些，不再要求各个属性表具有严格的顺序，并且只要不与已有的属性名重复，任何人实现的编译器都可以向属性表中写 入自己定义的属性信息，Java 虚拟机运行时会忽略掉它不认识的属性。 参考 https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html https://coolshell.cn/articles/9229.html https://blog.csdn.net/luanlouis/article/details/39960815 《实战 Java 虚拟机》 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \"Java面试突击\" 即可免费领取！ Java工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/java/Multithread/AQS.html":{"url":"zother6-JavaGuide/java/Multithread/AQS.html","title":"AQS","keywords":"","body":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java 面试突击》以及 Java 工程师必备学习资源。 1 AQS 简单介绍 2 AQS 原理 2.1 AQS 原理概览 2.2 AQS 对资源的共享方式 2.3 AQS 底层使用了模板方法模式 3 Semaphore(信号量)-允许多个线程同时访问 4 CountDownLatch （倒计时器） 4.1 CountDownLatch 的三种典型用法 4.2 CountDownLatch 的使用示例 4.3 CountDownLatch 的不足 4.4 CountDownLatch 常见面试题 5 CyclicBarrier(循环栅栏) 5.1 CyclicBarrier 的应用场景 5.2 CyclicBarrier 的使用示例 5.3 CyclicBarrier源码分析 5.4 CyclicBarrier 和 CountDownLatch 的区别 6 ReentrantLock 和 ReentrantReadWriteLock 参考 公众号 常见问题：AQS 原理？;CountDownLatch 和 CyclicBarrier 了解吗,两者的区别是什么？用过 Semaphore 吗？ 1 AQS 简单介绍 AQS 的全称为（AbstractQueuedSynchronizer），这个类在 java.util.concurrent.locks 包下面。 AQS 是一个用来构建锁和同步器的框架，使用 AQS 能简单且高效地构造出应用广泛的大量的同步器，比如我们提到的 ReentrantLock，Semaphore，其他的诸如 ReentrantReadWriteLock，SynchronousQueue，FutureTask(jdk1.7) 等等皆是基于 AQS 的。当然，我们自己也能利用 AQS 非常轻松容易地构造出符合我们自己需求的同步器。 2 AQS 原理 在面试中被问到并发知识的时候，大多都会被问到“请你说一下自己对于 AQS 原理的理解”。下面给大家一个示例供大家参考，面试不是背题，大家一定要加入自己的思想，即使加入不了自己的思想也要保证自己能够通俗的讲出来而不是背出来。 下面大部分内容其实在 AQS 类注释上已经给出了，不过是英语看着比较吃力一点，感兴趣的话可以看看源码。 2.1 AQS 原理概览 AQS 核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制 AQS 是用 CLH 队列锁实现的，即将暂时获取不到锁的线程加入到队列中。 CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS 是将每条请求共享资源的线程封装成一个 CLH 锁队列的一个结点（Node）来实现锁的分配。 看个 AQS(AbstractQueuedSynchronizer)原理图： AQS 使用一个 int 成员变量来表示同步状态，通过内置的 FIFO 队列来完成获取资源线程的排队工作。AQS 使用 CAS 对该同步状态进行原子操作实现对其值的修改。 private volatile int state;//共享变量，使用volatile修饰保证线程可见性 状态信息通过 protected 类型的getState，setState，compareAndSetState进行操作 //返回同步状态的当前值 protected final int getState() { return state; } // 设置同步状态的值 protected final void setState(int newState) { state = newState; } //原子地（CAS操作）将同步状态值设置为给定值update如果当前同步状态的值等于expect（期望值） protected final boolean compareAndSetState(int expect, int update) { return unsafe.compareAndSwapInt(this, stateOffset, expect, update); } 2.2 AQS 对资源的共享方式 AQS 定义两种资源共享方式 1)Exclusive（独占） 只有一个线程能执行，如 ReentrantLock。又可分为公平锁和非公平锁,ReentrantLock 同时支持两种锁,下面以 ReentrantLock 对这两种锁的定义做介绍： 公平锁：按照线程在队列中的排队顺序，先到者先拿到锁 非公平锁：当线程要获取锁时，先通过两次 CAS 操作去抢锁，如果没抢到，当前线程再加入到队列中等待唤醒。 说明：下面这部分关于 ReentrantLock 源代码内容节选自：https://www.javadoop.com/post/AbstractQueuedSynchronizer-2 ，这是一篇很不错文章，推荐阅读。 下面来看 ReentrantLock 中相关的源代码： ReentrantLock 默认采用非公平锁，因为考虑获得更好的性能，通过 boolean 来决定是否用公平锁（传入 true 用公平锁）。 /** Synchronizer providing all implementation mechanics */ private final Sync sync; public ReentrantLock() { // 默认非公平锁 sync = new NonfairSync(); } public ReentrantLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); } ReentrantLock 中公平锁的 lock 方法 static final class FairSync extends Sync { final void lock() { acquire(1); } // AbstractQueuedSynchronizer.acquire(int arg) public final void acquire(int arg) { if (!tryAcquire(arg) && acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { // 1. 和非公平锁相比，这里多了一个判断：是否有线程在等待 if (!hasQueuedPredecessors() && compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc 非公平锁的 lock 方法： static final class NonfairSync extends Sync { final void lock() { // 2. 和公平锁相比，这里会直接先进行一次CAS，成功就返回了 if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); } // AbstractQueuedSynchronizer.acquire(int arg) public final void acquire(int arg) { if (!tryAcquire(arg) && acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } protected final boolean tryAcquire(int acquires) { return nonfairTryAcquire(acquires); } } /** * Performs non-fair tryLock. tryAcquire is implemented in * subclasses, but both need nonfair try for trylock method. */ final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { // 这里没有对阻塞队列进行判断 if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc 总结：公平锁和非公平锁只有两处不同： 非公平锁在调用 lock 后，首先就会调用 CAS 进行一次抢锁，如果这个时候恰巧锁没有被占用，那么直接就获取到锁返回了。 非公平锁在 CAS 失败后，和公平锁一样都会进入到 tryAcquire 方法，在 tryAcquire 方法中，如果发现锁这个时候被释放了（state == 0），非公平锁会直接 CAS 抢锁，但是公平锁会判断等待队列是否有线程处于等待状态，如果有则不去抢锁，乖乖排到后面。 公平锁和非公平锁就这两点区别，如果这两次 CAS 都不成功，那么后面非公平锁和公平锁是一样的，都要进入到阻塞队列等待唤醒。 相对来说，非公平锁会有更好的性能，因为它的吞吐量比较大。当然，非公平锁让获取锁的时间变得更加不确定，可能会导致在阻塞队列中的线程长期处于饥饿状态。 2)Share（共享） 多个线程可同时执行，如 Semaphore/CountDownLatch。Semaphore、CountDownLatCh、 CyclicBarrier、ReadWriteLock 我们都会在后面讲到。 ReentrantReadWriteLock 可以看成是组合式，因为 ReentrantReadWriteLock 也就是读写锁允许多个线程同时对某一资源进行读。 不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源 state 的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS 已经在上层已经帮我们实现好了。 2.3 AQS 底层使用了模板方法模式 同步器的设计是基于模板方法模式的，如果需要自定义同步器一般的方式是这样（模板方法模式很经典的一个应用）： 使用者继承 AbstractQueuedSynchronizer 并重写指定的方法。（这些重写方法很简单，无非是对于共享资源 state 的获取和释放） 将 AQS 组合在自定义同步组件的实现中，并调用其模板方法，而这些模板方法会调用使用者重写的方法。 这和我们以往通过实现接口的方式有很大区别，这是模板方法模式很经典的一个运用，下面简单的给大家介绍一下模板方法模式，模板方法模式是一个很容易理解的设计模式之一。 模板方法模式是基于”继承“的，主要是为了在不改变模板结构的前提下在子类中重新定义模板中的内容以实现复用代码。举个很简单的例子假如我们要去一个地方的步骤是：购票buyTicket()->安检securityCheck()->乘坐某某工具回家ride()->到达目的地arrive()。我们可能乘坐不同的交通工具回家比如飞机或者火车，所以除了ride()方法，其他方法的实现几乎相同。我们可以定义一个包含了这些方法的抽象类，然后用户根据自己的需要继承该抽象类然后修改 ride()方法。 AQS 使用了模板方法模式，自定义同步器时需要重写下面几个 AQS 提供的模板方法： isHeldExclusively()//该线程是否正在独占资源。只有用到condition才需要去实现它。 tryAcquire(int)//独占方式。尝试获取资源，成功则返回true，失败则返回false。 tryRelease(int)//独占方式。尝试释放资源，成功则返回true，失败则返回false。 tryAcquireShared(int)//共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。 tryReleaseShared(int)//共享方式。尝试释放资源，成功则返回true，失败则返回false。 默认情况下，每个方法都抛出 UnsupportedOperationException。 这些方法的实现必须是内部线程安全的，并且通常应该简短而不是阻塞。AQS 类中的其他方法都是 final ，所以无法被其他类使用，只有这几个方法可以被其他类使用。 以 ReentrantLock 为例，state 初始化为 0，表示未锁定状态。A 线程 lock()时，会调用 tryAcquire()独占该锁并将 state+1。此后，其他线程再 tryAcquire()时就会失败，直到 A 线程 unlock()到 state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A 线程自己是可以重复获取此锁的（state 会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证 state 是能回到零态的。 再以 CountDownLatch 以例，任务分为 N 个子线程去执行，state 也初始化为 N（注意 N 要与线程个数一致）。这 N 个子线程是并行执行的，每个子线程执行完后 countDown()一次，state 会 CAS(Compare and Swap)减 1。等到所有子线程都执行完后(即 state=0)，会 unpark()主调用线程，然后主调用线程就会从 await()函数返回，继续后余动作。 一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。但 AQS 也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。 推荐两篇 AQS 原理和相关源码分析的文章： http://www.cnblogs.com/waterystone/p/4920797.html https://www.cnblogs.com/chengxiao/archive/2017/07/24/7141160.html 3 Semaphore(信号量)-允许多个线程同时访问 synchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，Semaphore(信号量)可以指定多个线程同时访问某个资源。 示例代码如下： /** * * @author Snailclimb * @date 2018年9月30日 * @Description: 需要一次性拿一个许可的情况 */ public class SemaphoreExample1 { // 请求的数量 private static final int threadCount = 550; public static void main(String[] args) throws InterruptedException { // 创建一个具有固定线程数量的线程池对象（如果这里线程池的线程数量给太少的话你会发现执行的很慢） ExecutorService threadPool = Executors.newFixedThreadPool(300); // 一次只能允许执行的线程数量。 final Semaphore semaphore = new Semaphore(20); for (int i = 0; i {// Lambda 表达式的运用 try { semaphore.acquire();// 获取一个许可，所以可运行线程数量为20/1=20 test(threadnum); semaphore.release();// 释放一个许可 } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } }); } threadPool.shutdown(); System.out.println(\"finish\"); } public static void test(int threadnum) throws InterruptedException { Thread.sleep(1000);// 模拟请求的耗时操作 System.out.println(\"threadnum:\" + threadnum); Thread.sleep(1000);// 模拟请求的耗时操作 } } 执行 acquire 方法阻塞，直到有一个许可证可以获得然后拿走一个许可证；每个 release 方法增加一个许可证，这可能会释放一个阻塞的 acquire 方法。然而，其实并没有实际的许可证这个对象，Semaphore 只是维持了一个可获得许可证的数量。 Semaphore 经常用于限制获取某种资源的线程数量。 当然一次也可以一次拿取和释放多个许可，不过一般没有必要这样做： semaphore.acquire(5);// 获取5个许可，所以可运行线程数量为20/5=4 test(threadnum); semaphore.release(5);// 获取5个许可，所以可运行线程数量为20/5=4 除了 acquire方法之外，另一个比较常用的与之对应的方法是tryAcquire方法，该方法如果获取不到许可就立即返回 false。 Semaphore 有两种模式，公平模式和非公平模式。 公平模式： 调用 acquire 的顺序就是获取许可证的顺序，遵循 FIFO； 非公平模式： 抢占式的。 Semaphore 对应的两个构造方法如下： public Semaphore(int permits) { sync = new NonfairSync(permits); } public Semaphore(int permits, boolean fair) { sync = fair ? new FairSync(permits) : new NonfairSync(permits); } 这两个构造方法，都必须提供许可的数量，第二个构造方法可以指定是公平模式还是非公平模式，默认非公平模式。 issue645补充内容 ：Semaphore与CountDownLatch一样，也是共享锁的一种实现。它默认构造AQS的state为permits。当执行任务的线程数量超出permits,那么多余的线程将会被放入阻塞队列Park,并自旋判断state是否大于0。只有当state大于0的时候，阻塞的线程才能继续执行,此时先前执行任务的线程继续执行release方法，release方法使得state的变量会加1，那么自旋的线程便会判断成功。 如此，每次只有最多不超过permits数量的线程能自旋成功，便限制了执行任务线程的数量。 由于篇幅问题，如果对 Semaphore 源码感兴趣的朋友可以看下这篇文章：https://juejin.im/post/5ae755366fb9a07ab508adc6 4 CountDownLatch （倒计时器） CountDownLatch允许 count 个线程阻塞在一个地方，直至所有线程的任务都执行完毕。在 Java 并发中，countdownlatch 的概念是一个常见的面试题，所以一定要确保你很好的理解了它。 CountDownLatch是共享锁的一种实现,它默认构造 AQS 的 state 值为 count。当线程使用countDown方法时,其实使用了tryReleaseShared方法以CAS的操作来减少state,直至state为0就代表所有的线程都调用了countDown方法。当调用await方法的时候，如果state不为0，就代表仍然有线程没有调用countDown方法，那么就把已经调用过countDown的线程都放入阻塞队列Park,并自旋CAS判断state == 0，直至最后一个线程调用了countDown，使得state == 0，于是阻塞的线程便判断成功，全部往下执行。 4.1 CountDownLatch 的两种典型用法 某一线程在开始运行前等待 n 个线程执行完毕。将 CountDownLatch 的计数器初始化为 n ：new CountDownLatch(n)，每当一个任务线程执行完毕，就将计数器减 1 countdownlatch.countDown()，当计数器的值变为 0 时，在CountDownLatch上 await() 的线程就会被唤醒。一个典型应用场景就是启动一个服务时，主线程需要等待多个组件加载完毕，之后再继续执行。 实现多个线程开始执行任务的最大并行性。注意是并行性，不是并发，强调的是多个线程在某一时刻同时开始执行。类似于赛跑，将多个线程放到起点，等待发令枪响，然后同时开跑。做法是初始化一个共享的 CountDownLatch 对象，将其计数器初始化为 1 ：new CountDownLatch(1)，多个线程在开始执行任务前首先 coundownlatch.await()，当主线程调用 countDown() 时，计数器变为 0，多个线程同时被唤醒。 4.2 CountDownLatch 的使用示例 /** * * @author SnailClimb * @date 2018年10月1日 * @Description: CountDownLatch 使用方法示例 */ public class CountDownLatchExample1 { // 请求的数量 private static final int threadCount = 550; public static void main(String[] args) throws InterruptedException { // 创建一个具有固定线程数量的线程池对象（如果这里线程池的线程数量给太少的话你会发现执行的很慢） ExecutorService threadPool = Executors.newFixedThreadPool(300); final CountDownLatch countDownLatch = new CountDownLatch(threadCount); for (int i = 0; i {// Lambda 表达式的运用 try { test(threadnum); } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } finally { countDownLatch.countDown();// 表示一个请求已经被完成 } }); } countDownLatch.await(); threadPool.shutdown(); System.out.println(\"finish\"); } public static void test(int threadnum) throws InterruptedException { Thread.sleep(1000);// 模拟请求的耗时操作 System.out.println(\"threadnum:\" + threadnum); Thread.sleep(1000);// 模拟请求的耗时操作 } } 上面的代码中，我们定义了请求的数量为 550，当这 550 个请求被处理完成之后，才会执行System.out.println(\"finish\");。 与 CountDownLatch 的第一次交互是主线程等待其他线程。主线程必须在启动其他线程后立即调用 CountDownLatch.await() 方法。这样主线程的操作就会在这个方法上阻塞，直到其他线程完成各自的任务。 其他 N 个线程必须引用闭锁对象，因为他们需要通知 CountDownLatch 对象，他们已经完成了各自的任务。这种通知机制是通过 CountDownLatch.countDown()方法来完成的；每调用一次这个方法，在构造函数中初始化的 count 值就减 1。所以当 N 个线程都调 用了这个方法，count 的值等于 0，然后主线程就能通过 await()方法，恢复执行自己的任务。 再插一嘴：CountDownLatch 的 await() 方法使用不当很容易产生死锁，比如我们上面代码中的 for 循环改为： for (int i = 0; i 这样就导致 count 的值没办法等于 0，然后就会导致一直等待。 如果对CountDownLatch源码感兴趣的朋友，可以查看： 【JUC】JDK1.8源码分析之CountDownLatch（五） 4.3 CountDownLatch 的不足 CountDownLatch 是一次性的，计数器的值只能在构造方法中初始化一次，之后没有任何机制再次对其设置值，当 CountDownLatch 使用完毕后，它不能再次被使用。 4.4 CountDownLatch 相常见面试题 解释一下 CountDownLatch 概念？ CountDownLatch 和 CyclicBarrier 的不同之处？ 给出一些 CountDownLatch 使用的例子？ CountDownLatch 类中主要的方法？ 5 CyclicBarrier(循环栅栏) CyclicBarrier 和 CountDownLatch 非常类似，它也可以实现线程间的技术等待，但是它的功能比 CountDownLatch 更加复杂和强大。主要应用场景和 CountDownLatch 类似。 CountDownLatch的实现是基于AQS的，而CycliBarrier是基于 ReentrantLock(ReentrantLock也属于AQS同步器)和 Condition 的. CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。CyclicBarrier 默认的构造方法是 CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用await方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞。 再来看一下它的构造函数： public CyclicBarrier(int parties) { this(parties, null); } public CyclicBarrier(int parties, Runnable barrierAction) { if (parties 其中，parties 就代表了有拦截的线程的数量，当拦截的线程数量达到这个值的时候就打开栅栏，让所有线程通过。 5.1 CyclicBarrier 的应用场景 CyclicBarrier 可以用于多线程计算数据，最后合并计算结果的应用场景。比如我们用一个 Excel 保存了用户所有银行流水，每个 Sheet 保存一个帐户近一年的每笔银行流水，现在需要统计用户的日均银行流水，先用多线程处理每个 sheet 里的银行流水，都执行完之后，得到每个 sheet 的日均银行流水，最后，再用 barrierAction 用这些线程的计算结果，计算出整个 Excel 的日均银行流水。 5.2 CyclicBarrier 的使用示例 示例 1： /** * * @author Snailclimb * @date 2018年10月1日 * @Description: 测试 CyclicBarrier 类中带参数的 await() 方法 */ public class CyclicBarrierExample2 { // 请求的数量 private static final int threadCount = 550; // 需要同步的线程数量 private static final CyclicBarrier cyclicBarrier = new CyclicBarrier(5); public static void main(String[] args) throws InterruptedException { // 创建线程池 ExecutorService threadPool = Executors.newFixedThreadPool(10); for (int i = 0; i { try { test(threadNum); } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } catch (BrokenBarrierException e) { // TODO Auto-generated catch block e.printStackTrace(); } }); } threadPool.shutdown(); } public static void test(int threadnum) throws InterruptedException, BrokenBarrierException { System.out.println(\"threadnum:\" + threadnum + \"is ready\"); try { /**等待60秒，保证子线程完全执行结束*/ cyclicBarrier.await(60, TimeUnit.SECONDS); } catch (Exception e) { System.out.println(\"-----CyclicBarrierException------\"); } System.out.println(\"threadnum:\" + threadnum + \"is finish\"); } } 运行结果，如下： threadnum:0is ready threadnum:1is ready threadnum:2is ready threadnum:3is ready threadnum:4is ready threadnum:4is finish threadnum:0is finish threadnum:1is finish threadnum:2is finish threadnum:3is finish threadnum:5is ready threadnum:6is ready threadnum:7is ready threadnum:8is ready threadnum:9is ready threadnum:9is finish threadnum:5is finish threadnum:8is finish threadnum:7is finish threadnum:6is finish ...... 可以看到当线程数量也就是请求数量达到我们定义的 5 个的时候， await方法之后的方法才被执行。 另外，CyclicBarrier 还提供一个更高级的构造函数CyclicBarrier(int parties, Runnable barrierAction)，用于在线程到达屏障时，优先执行barrierAction，方便处理更复杂的业务场景。示例代码如下： /** * * @author SnailClimb * @date 2018年10月1日 * @Description: 新建 CyclicBarrier 的时候指定一个 Runnable */ public class CyclicBarrierExample3 { // 请求的数量 private static final int threadCount = 550; // 需要同步的线程数量 private static final CyclicBarrier cyclicBarrier = new CyclicBarrier(5, () -> { System.out.println(\"------当线程数达到之后，优先执行------\"); }); public static void main(String[] args) throws InterruptedException { // 创建线程池 ExecutorService threadPool = Executors.newFixedThreadPool(10); for (int i = 0; i { try { test(threadNum); } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } catch (BrokenBarrierException e) { // TODO Auto-generated catch block e.printStackTrace(); } }); } threadPool.shutdown(); } public static void test(int threadnum) throws InterruptedException, BrokenBarrierException { System.out.println(\"threadnum:\" + threadnum + \"is ready\"); cyclicBarrier.await(); System.out.println(\"threadnum:\" + threadnum + \"is finish\"); } } 运行结果，如下： threadnum:0is ready threadnum:1is ready threadnum:2is ready threadnum:3is ready threadnum:4is ready ------当线程数达到之后，优先执行------ threadnum:4is finish threadnum:0is finish threadnum:2is finish threadnum:1is finish threadnum:3is finish threadnum:5is ready threadnum:6is ready threadnum:7is ready threadnum:8is ready threadnum:9is ready ------当线程数达到之后，优先执行------ threadnum:9is finish threadnum:5is finish threadnum:6is finish threadnum:8is finish threadnum:7is finish ...... 5.3 CyclicBarrier源码分析 当调用 CyclicBarrier 对象调用 await() 方法时，实际上调用的是dowait(false, 0L)方法。 await() 方法就像树立起一个栅栏的行为一样，将线程挡住了，当拦住的线程数量达到 parties 的值时，栅栏才会打开，线程才得以通过执行。 public int await() throws InterruptedException, BrokenBarrierException { try { return dowait(false, 0L); } catch (TimeoutException toe) { throw new Error(toe); // cannot happen } } dowait(false, 0L)： // 当线程数量或者请求数量达到 count 时 await 之后的方法才会被执行。上面的示例中 count 的值就为 5。 private int count; /** * Main barrier code, covering the various policies. */ private int dowait(boolean timed, long nanos) throws InterruptedException, BrokenBarrierException, TimeoutException { final ReentrantLock lock = this.lock; // 锁住 lock.lock(); try { final Generation g = generation; if (g.broken) throw new BrokenBarrierException(); // 如果线程中断了，抛出异常 if (Thread.interrupted()) { breakBarrier(); throw new InterruptedException(); } // cout减1 int index = --count; // 当 count 数量减为 0 之后说明最后一个线程已经到达栅栏了，也就是达到了可以执行await 方法之后的条件 if (index == 0) { // tripped boolean ranAction = false; try { final Runnable command = barrierCommand; if (command != null) command.run(); ranAction = true; // 将 count 重置为 parties 属性的初始化值 // 唤醒之前等待的线程 // 下一波执行开始 nextGeneration(); return 0; } finally { if (!ranAction) breakBarrier(); } } // loop until tripped, broken, interrupted, or timed out for (;;) { try { if (!timed) trip.await(); else if (nanos > 0L) nanos = trip.awaitNanos(nanos); } catch (InterruptedException ie) { if (g == generation && ! g.broken) { breakBarrier(); throw ie; } else { // We're about to finish waiting even if we had not // been interrupted, so this interrupt is deemed to // \"belong\" to subsequent execution. Thread.currentThread().interrupt(); } } if (g.broken) throw new BrokenBarrierException(); if (g != generation) return index; if (timed && nanos 总结：CyclicBarrier 内部通过一个 count 变量作为计数器，cout 的初始值为 parties 属性的初始化值，每当一个线程到了栅栏这里了，那么就将计数器减一。如果 count 值为 0 了，表示这是这一代最后一个线程到达栅栏，就尝试执行我们构造方法中输入的任务。 5.4 CyclicBarrier 和 CountDownLatch 的区别 下面这个是国外一个大佬的回答： CountDownLatch 是计数器，只能使用一次，而 CyclicBarrier 的计数器提供 reset 功能，可以多次使用。但是我不那么认为它们之间的区别仅仅就是这么简单的一点。我们来从 jdk 作者设计的目的来看，javadoc 是这么描述它们的： CountDownLatch: A synchronization aid that allows one or more threads to wait until a set of operations being performed in other threads completes.(CountDownLatch: 一个或者多个线程，等待其他多个线程完成某件事情之后才能执行；) CyclicBarrier : A synchronization aid that allows a set of threads to all wait for each other to reach a common barrier point.(CyclicBarrier : 多个线程互相等待，直到到达同一个同步点，再继续一起执行。) 对于 CountDownLatch 来说，重点是“一个线程（多个线程）等待”，而其他的 N 个线程在完成“某件事情”之后，可以终止，也可以等待。而对于 CyclicBarrier，重点是多个线程，在任意一个线程没有完成，所有的线程都必须等待。 CountDownLatch 是计数器，线程完成一个记录一个，只不过计数不是递增而是递减，而 CyclicBarrier 更像是一个阀门，需要所有线程都到达，阀门才能打开，然后继续执行。 6 ReentrantLock 和 ReentrantReadWriteLock ReentrantLock 和 synchronized 的区别在上面已经讲过了这里就不多做讲解。另外，需要注意的是：读写锁 ReentrantReadWriteLock 可以保证多个线程可以同时读，所以在读操作远大于写操作的时候，读写锁就非常有用了。 参考 https://juejin.im/post/5ae755256fb9a07ac3634067 https://blog.csdn.net/u010185262/article/details/54692886 https://blog.csdn.net/tolcf/article/details/50925145?utm_source=blogxgwz0 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java 面试突击》: 由本文档衍生的专为面试而生的《Java 面试突击》V2.0 PDF 版本公众号后台回复 \"面试突击\" 即可免费领取！ Java 工程师必备学习资源: 一些 Java 工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/java/Multithread/Atomic.html":{"url":"zother6-JavaGuide/java/Multithread/Atomic.html","title":"Atomic","keywords":"","body":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。 个人觉得这一节掌握基本的使用即可！ 1 Atomic 原子类介绍 2 基本类型原子类 2.1 基本类型原子类介绍 2.2 AtomicInteger 常见方法使用 2.3 基本数据类型原子类的优势 2.4 AtomicInteger 线程安全原理简单分析 3 数组类型原子类 3.1 数组类型原子类介绍 3.2 AtomicIntegerArray 常见方法使用 4 引用类型原子类 4.1 引用类型原子类介绍 4.2 AtomicReference 类使用示例 4.3 AtomicStampedReference 类使用示例 4.4 AtomicMarkableReference 类使用示例 5 对象的属性修改类型原子类 5.1 对象的属性修改类型原子类介绍 5.2 AtomicIntegerFieldUpdater 类使用示例 1 Atomic 原子类介绍 Atomic 翻译成中文是原子的意思。在化学上，我们知道原子是构成一般物质的最小单位，在化学反应中是不可分割的。在我们这里 Atomic 是指一个操作是不可中断的。即使是在多个线程一起执行的时候，一个操作一旦开始，就不会被其他线程干扰。 所以，所谓原子类说简单点就是具有原子/原子操作特征的类。 并发包 java.util.concurrent 的原子类都存放在java.util.concurrent.atomic下,如下图所示。 根据操作的数据类型，可以将JUC包中的原子类分为4类 基本类型 使用原子的方式更新基本类型 AtomicInteger：整型原子类 AtomicLong：长整型原子类 AtomicBoolean ：布尔型原子类 数组类型 使用原子的方式更新数组里的某个元素 AtomicIntegerArray：整型数组原子类 AtomicLongArray：长整型数组原子类 AtomicReferenceArray ：引用类型数组原子类 引用类型 AtomicReference：引用类型原子类 AtomicMarkableReference：原子更新带有标记的引用类型。该类将 boolean 标记与引用关联起来，也可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 AtomicStampedReference ：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于解决原子的更新数据和数据的版本号，可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 对象的属性修改类型 AtomicIntegerFieldUpdater:原子更新整型字段的更新器 AtomicLongFieldUpdater：原子更新长整型字段的更新器 AtomicReferenceFieldUpdater：原子更新引用类型里的字段 修正: AtomicMarkableReference 不能解决ABA问题 issue#626 /** AtomicMarkableReference是将一个boolean值作是否有更改的标记，本质就是它的版本号只有两个，true和false， 修改的时候在这两个版本号之间来回切换，这样做并不能解决ABA的问题，只是会降低ABA问题发生的几率而已 @author : mazh @Date : 2020/1/17 14:41 */ public class SolveABAByAtomicMarkableReference { private static AtomicMarkableReference atomicMarkableReference = new AtomicMarkableReference(100, false); public static void main(String[] args) { Thread refT1 = new Thread(() -> { try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } atomicMarkableReference.compareAndSet(100, 101, atomicMarkableReference.isMarked(), !atomicMarkableReference.isMarked()); atomicMarkableReference.compareAndSet(101, 100, atomicMarkableReference.isMarked(), !atomicMarkableReference.isMarked()); }); Thread refT2 = new Thread(() -> { boolean marked = atomicMarkableReference.isMarked(); try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } boolean c3 = atomicMarkableReference.compareAndSet(100, 101, marked, !marked); System.out.println(c3); // 返回true,实际应该返回false }); refT1.start(); refT2.start(); } } CAS ABA 问题 描述: 第一个线程取到了变量 x 的值 A，然后巴拉巴拉干别的事，总之就是只拿到了变量 x 的值 A。这段时间内第二个线程也取到了变量 x 的值 A，然后把变量 x 的值改为 B，然后巴拉巴拉干别的事，最后又把变量 x 的值变为 A （相当于还原了）。在这之后第一个线程终于进行了变量 x 的操作，但是此时变量 x 的值还是 A，所以 compareAndSet 操作是成功。 例子描述(可能不太合适，但好理解): 年初，现金为零，然后通过正常劳动赚了三百万，之后正常消费了（比如买房子）三百万。年末，虽然现金零收入（可能变成其他形式了），但是赚了钱是事实，还是得交税的！ 代码例子（以AtomicInteger为例） import java.util.concurrent.atomic.AtomicInteger; public class AtomicIntegerDefectDemo { public static void main(String[] args) { defectOfABA(); } static void defectOfABA() { final AtomicInteger atomicInteger = new AtomicInteger(1); Thread coreThread = new Thread( () -> { final int currentValue = atomicInteger.get(); System.out.println(Thread.currentThread().getName() + \" ------ currentValue=\" + currentValue); // 这段目的：模拟处理其他业务花费的时间 try { Thread.sleep(300); } catch (InterruptedException e) { e.printStackTrace(); } boolean casResult = atomicInteger.compareAndSet(1, 2); System.out.println(Thread.currentThread().getName() + \" ------ currentValue=\" + currentValue + \", finalValue=\" + atomicInteger.get() + \", compareAndSet Result=\" + casResult); } ); coreThread.start(); // 这段目的：为了让 coreThread 线程先跑起来 try { Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } Thread amateurThread = new Thread( () -> { int currentValue = atomicInteger.get(); boolean casResult = atomicInteger.compareAndSet(1, 2); System.out.println(Thread.currentThread().getName() + \" ------ currentValue=\" + currentValue + \", finalValue=\" + atomicInteger.get() + \", compareAndSet Result=\" + casResult); currentValue = atomicInteger.get(); casResult = atomicInteger.compareAndSet(2, 1); System.out.println(Thread.currentThread().getName() + \" ------ currentValue=\" + currentValue + \", finalValue=\" + atomicInteger.get() + \", compareAndSet Result=\" + casResult); } ); amateurThread.start(); } } 输出内容如下： Thread-0 ------ currentValue=1 Thread-1 ------ currentValue=1, finalValue=2, compareAndSet Result=true Thread-1 ------ currentValue=2, finalValue=1, compareAndSet Result=true Thread-0 ------ currentValue=1, finalValue=2, compareAndSet Result=true 下面我们来详细介绍一下这些原子类。 2 基本类型原子类 2.1 基本类型原子类介绍 使用原子的方式更新基本类型 AtomicInteger：整型原子类 AtomicLong：长整型原子类 AtomicBoolean ：布尔型原子类 上面三个类提供的方法几乎相同，所以我们这里以 AtomicInteger 为例子来介绍。 AtomicInteger 类常用方法 public final int get() //获取当前的值 public final int getAndSet(int newValue)//获取当前的值，并设置新的值 public final int getAndIncrement()//获取当前的值，并自增 public final int getAndDecrement() //获取当前的值，并自减 public final int getAndAdd(int delta) //获取当前的值，并加上预期的值 boolean compareAndSet(int expect, int update) //如果输入的数值等于预期值，则以原子方式将该值设置为输入值（update） public final void lazySet(int newValue)//最终设置为newValue,使用 lazySet 设置之后可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 2.2 AtomicInteger 常见方法使用 import java.util.concurrent.atomic.AtomicInteger; public class AtomicIntegerTest { public static void main(String[] args) { // TODO Auto-generated method stub int temvalue = 0; AtomicInteger i = new AtomicInteger(0); temvalue = i.getAndSet(3); System.out.println(\"temvalue:\" + temvalue + \"; i:\" + i);//temvalue:0; i:3 temvalue = i.getAndIncrement(); System.out.println(\"temvalue:\" + temvalue + \"; i:\" + i);//temvalue:3; i:4 temvalue = i.getAndAdd(5); System.out.println(\"temvalue:\" + temvalue + \"; i:\" + i);//temvalue:4; i:9 } } 2.3 基本数据类型原子类的优势 通过一个简单例子带大家看一下基本数据类型原子类的优势 ①多线程环境不使用原子类保证线程安全（基本数据类型） class Test { private volatile int count = 0; //若要线程安全执行执行count++，需要加锁 public synchronized void increment() { count++; } public int getCount() { return count; } } ②多线程环境使用原子类保证线程安全（基本数据类型） class Test2 { private AtomicInteger count = new AtomicInteger(); public void increment() { count.incrementAndGet(); } //使用AtomicInteger之后，不需要加锁，也可以实现线程安全。 public int getCount() { return count.get(); } } 2.4 AtomicInteger 线程安全原理简单分析 AtomicInteger 类的部分源码： // setup to use Unsafe.compareAndSwapInt for updates（更新操作时提供“比较并替换”的作用） private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static { try { valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(\"value\")); } catch (Exception ex) { throw new Error(ex); } } private volatile int value; AtomicInteger 类主要利用 CAS (compare and swap) + volatile 和 native 方法来保证原子操作，从而避免 synchronized 的高开销，执行效率大为提升。 CAS的原理是拿期望的值和原本的一个值作比较，如果相同则更新成新的值。UnSafe 类的 objectFieldOffset() 方法是一个本地方法，这个方法是用来拿到“原来的值”的内存地址。另外 value 是一个volatile变量，在内存中可见，因此 JVM 可以保证任何时刻任何线程总能拿到该变量的最新值。 3 数组类型原子类 3.1 数组类型原子类介绍 使用原子的方式更新数组里的某个元素 AtomicIntegerArray：整形数组原子类 AtomicLongArray：长整形数组原子类 AtomicReferenceArray ：引用类型数组原子类 上面三个类提供的方法几乎相同，所以我们这里以 AtomicIntegerArray 为例子来介绍。 AtomicIntegerArray 类常用方法 public final int get(int i) //获取 index=i 位置元素的值 public final int getAndSet(int i, int newValue)//返回 index=i 位置的当前的值，并将其设置为新值：newValue public final int getAndIncrement(int i)//获取 index=i 位置元素的值，并让该位置的元素自增 public final int getAndDecrement(int i) //获取 index=i 位置元素的值，并让该位置的元素自减 public final int getAndAdd(int delta) //获取 index=i 位置元素的值，并加上预期的值 boolean compareAndSet(int expect, int update) //如果输入的数值等于预期值，则以原子方式将 index=i 位置的元素值设置为输入值（update） public final void lazySet(int i, int newValue)//最终 将index=i 位置的元素设置为newValue,使用 lazySet 设置之后可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 3.2 AtomicIntegerArray 常见方法使用 import java.util.concurrent.atomic.AtomicIntegerArray; public class AtomicIntegerArrayTest { public static void main(String[] args) { // TODO Auto-generated method stub int temvalue = 0; int[] nums = { 1, 2, 3, 4, 5, 6 }; AtomicIntegerArray i = new AtomicIntegerArray(nums); for (int j = 0; j 4 引用类型原子类 4.1 引用类型原子类介绍 基本类型原子类只能更新一个变量，如果需要原子更新多个变量，需要使用 引用类型原子类。 AtomicReference：引用类型原子类 AtomicStampedReference：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于解决原子的更新数据和数据的版本号，可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 AtomicMarkableReference ：原子更新带有标记的引用类型。该类将 boolean 标记与引用关联起来，也可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 上面三个类提供的方法几乎相同，所以我们这里以 AtomicReference 为例子来介绍。 4.2 AtomicReference 类使用示例 import java.util.concurrent.atomic.AtomicReference; public class AtomicReferenceTest { public static void main(String[] args) { AtomicReference ar = new AtomicReference(); Person person = new Person(\"SnailClimb\", 22); ar.set(person); Person updatePerson = new Person(\"Daisy\", 20); ar.compareAndSet(person, updatePerson); System.out.println(ar.get().getName()); System.out.println(ar.get().getAge()); } } class Person { private String name; private int age; public Person(String name, int age) { super(); this.name = name; this.age = age; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } } 上述代码首先创建了一个 Person 对象，然后把 Person 对象设置进 AtomicReference 对象中，然后调用 compareAndSet 方法，该方法就是通过 CAS 操作设置 ar。如果 ar 的值为 person 的话，则将其设置为 updatePerson。实现原理与 AtomicInteger 类中的 compareAndSet 方法相同。运行上面的代码后的输出结果如下： Daisy 20 4.3 AtomicStampedReference 类使用示例 import java.util.concurrent.atomic.AtomicStampedReference; public class AtomicStampedReferenceDemo { public static void main(String[] args) { // 实例化、取当前值和 stamp 值 final Integer initialRef = 0, initialStamp = 0; final AtomicStampedReference asr = new AtomicStampedReference<>(initialRef, initialStamp); System.out.println(\"currentValue=\" + asr.getReference() + \", currentStamp=\" + asr.getStamp()); // compare and set final Integer newReference = 666, newStamp = 999; final boolean casResult = asr.compareAndSet(initialRef, newReference, initialStamp, newStamp); System.out.println(\"currentValue=\" + asr.getReference() + \", currentStamp=\" + asr.getStamp() + \", casResult=\" + casResult); // 获取当前的值和当前的 stamp 值 int[] arr = new int[1]; final Integer currentValue = asr.get(arr); final int currentStamp = arr[0]; System.out.println(\"currentValue=\" + currentValue + \", currentStamp=\" + currentStamp); // 单独设置 stamp 值 final boolean attemptStampResult = asr.attemptStamp(newReference, 88); System.out.println(\"currentValue=\" + asr.getReference() + \", currentStamp=\" + asr.getStamp() + \", attemptStampResult=\" + attemptStampResult); // 重新设置当前值和 stamp 值 asr.set(initialRef, initialStamp); System.out.println(\"currentValue=\" + asr.getReference() + \", currentStamp=\" + asr.getStamp()); // [不推荐使用，除非搞清楚注释的意思了] weak compare and set // 困惑！weakCompareAndSet 这个方法最终还是调用 compareAndSet 方法。[版本: jdk-8u191] // 但是注释上写着 \"May fail spuriously and does not provide ordering guarantees, // so is only rarely an appropriate alternative to compareAndSet.\" // todo 感觉有可能是 jvm 通过方法名在 native 方法里面做了转发 final boolean wCasResult = asr.weakCompareAndSet(initialRef, newReference, initialStamp, newStamp); System.out.println(\"currentValue=\" + asr.getReference() + \", currentStamp=\" + asr.getStamp() + \", wCasResult=\" + wCasResult); } } 输出结果如下： currentValue=0, currentStamp=0 currentValue=666, currentStamp=999, casResult=true currentValue=666, currentStamp=999 currentValue=666, currentStamp=88, attemptStampResult=true currentValue=0, currentStamp=0 currentValue=666, currentStamp=999, wCasResult=true 4.4 AtomicMarkableReference 类使用示例 import java.util.concurrent.atomic.AtomicMarkableReference; public class AtomicMarkableReferenceDemo { public static void main(String[] args) { // 实例化、取当前值和 mark 值 final Boolean initialRef = null, initialMark = false; final AtomicMarkableReference amr = new AtomicMarkableReference<>(initialRef, initialMark); System.out.println(\"currentValue=\" + amr.getReference() + \", currentMark=\" + amr.isMarked()); // compare and set final Boolean newReference1 = true, newMark1 = true; final boolean casResult = amr.compareAndSet(initialRef, newReference1, initialMark, newMark1); System.out.println(\"currentValue=\" + amr.getReference() + \", currentMark=\" + amr.isMarked() + \", casResult=\" + casResult); // 获取当前的值和当前的 mark 值 boolean[] arr = new boolean[1]; final Boolean currentValue = amr.get(arr); final boolean currentMark = arr[0]; System.out.println(\"currentValue=\" + currentValue + \", currentMark=\" + currentMark); // 单独设置 mark 值 final boolean attemptMarkResult = amr.attemptMark(newReference1, false); System.out.println(\"currentValue=\" + amr.getReference() + \", currentMark=\" + amr.isMarked() + \", attemptMarkResult=\" + attemptMarkResult); // 重新设置当前值和 mark 值 amr.set(initialRef, initialMark); System.out.println(\"currentValue=\" + amr.getReference() + \", currentMark=\" + amr.isMarked()); // [不推荐使用，除非搞清楚注释的意思了] weak compare and set // 困惑！weakCompareAndSet 这个方法最终还是调用 compareAndSet 方法。[版本: jdk-8u191] // 但是注释上写着 \"May fail spuriously and does not provide ordering guarantees, // so is only rarely an appropriate alternative to compareAndSet.\" // todo 感觉有可能是 jvm 通过方法名在 native 方法里面做了转发 final boolean wCasResult = amr.weakCompareAndSet(initialRef, newReference1, initialMark, newMark1); System.out.println(\"currentValue=\" + amr.getReference() + \", currentMark=\" + amr.isMarked() + \", wCasResult=\" + wCasResult); } } 输出结果如下： currentValue=null, currentMark=false currentValue=true, currentMark=true, casResult=true currentValue=true, currentMark=true currentValue=true, currentMark=false, attemptMarkResult=true currentValue=null, currentMark=false currentValue=true, currentMark=true, wCasResult=true 5 对象的属性修改类型原子类 5.1 对象的属性修改类型原子类介绍 如果需要原子更新某个类里的某个字段时，需要用到对象的属性修改类型原子类。 AtomicIntegerFieldUpdater:原子更新整形字段的更新器 AtomicLongFieldUpdater：原子更新长整形字段的更新器 AtomicReferenceFieldUpdater ：原子更新引用类型里的字段的更新器 要想原子地更新对象的属性需要两步。第一步，因为对象的属性修改类型原子类都是抽象类，所以每次使用都必须使用静态方法 newUpdater()创建一个更新器，并且需要设置想要更新的类和属性。第二步，更新的对象属性必须使用 public volatile 修饰符。 上面三个类提供的方法几乎相同，所以我们这里以 AtomicIntegerFieldUpdater为例子来介绍。 5.2 AtomicIntegerFieldUpdater 类使用示例 import java.util.concurrent.atomic.AtomicIntegerFieldUpdater; public class AtomicIntegerFieldUpdaterTest { public static void main(String[] args) { AtomicIntegerFieldUpdater a = AtomicIntegerFieldUpdater.newUpdater(User.class, \"age\"); User user = new User(\"Java\", 22); System.out.println(a.getAndIncrement(user));// 22 System.out.println(a.get(user));// 23 } } class User { private String name; public volatile int age; public User(String name, int age) { super(); this.name = name; this.age = age; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } } 输出结果： 22 23 Reference 《Java并发编程的艺术》 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \"面试突击\" 即可免费领取！ Java工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/java/Multithread/JavaConcurrencyAdvancedCommonInterviewQuestions.html":{"url":"zother6-JavaGuide/java/Multithread/JavaConcurrencyAdvancedCommonInterviewQuestions.html","title":"Java Concurrency Advanced Common Interview Questions","keywords":"","body":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。 Java 并发进阶常见面试题总结 1. synchronized 关键字 1.1. 说一说自己对于 synchronized 关键字的了解 1.2. 说说自己是怎么使用 synchronized 关键字，在项目中用到了吗 1.3. 讲一下 synchronized 关键字的底层原理 1.4. 说说 JDK1.6 之后的synchronized 关键字底层做了哪些优化，可以详细介绍一下这些优化吗 1.5. 谈谈 synchronized和ReentrantLock 的区别 2. volatile关键字 2.1. 讲一下Java内存模型 2.2. 说说 synchronized 关键字和 volatile 关键字的区别 3. ThreadLocal 3.1. ThreadLocal简介 3.2. ThreadLocal示例 3.3. ThreadLocal原理 3.4. ThreadLocal 内存泄露问题 4. 线程池 4.1. 为什么要用线程池？ 4.2. 实现Runnable接口和Callable接口的区别 4.3. 执行execute()方法和submit()方法的区别是什么呢？ 4.4. 如何创建线程池 5. Atomic 原子类 5.1. 介绍一下Atomic 原子类 5.2. JUC 包中的原子类是哪4类? 5.3. 讲讲 AtomicInteger 的使用 5.4. 能不能给我简单介绍一下 AtomicInteger 类的原理 6. AQS 6.1. AQS 介绍 6.2. AQS 原理分析 6.2.1. AQS 原理概览 6.2.2. AQS 对资源的共享方式 6.2.3. AQS底层使用了模板方法模式 6.3. AQS 组件总结 7 Reference Java 并发进阶常见面试题总结 1. synchronized 关键字 1.1. 说一说自己对于 synchronized 关键字的了解 synchronized关键字解决的是多个线程之间访问资源的同步性，synchronized关键字可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。 另外，在 Java 早期版本中，synchronized属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的 synchronized 效率低的原因。庆幸的是在 Java 6 之后 Java 官方对从 JVM 层面对synchronized 较大优化，所以现在的 synchronized 锁效率也优化得很不错了。JDK1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。 1.2. 说说自己是怎么使用 synchronized 关键字，在项目中用到了吗 synchronized关键字最主要的三种使用方式： 修饰实例方法: 作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁 修饰静态方法: 也就是给当前类加锁，会作用于类的所有对象实例，因为静态成员不属于任何一个实例对象，是类成员（ static 表明这是该类的一个静态资源，不管new了多少个对象，只有一份）。所以如果一个线程 A 调用一个实例对象的非静态 synchronized 方法，而线程 B 需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。 修饰代码块: 指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。 总结： synchronized 关键字加到 static 静态方法和 synchronized(class)代码块上都是是给 Class 类上锁。synchronized 关键字加到实例方法上是给对象实例上锁。尽量不要使用 synchronized(String a) 因为JVM中，字符串常量池具有缓存功能！ 下面我以一个常见的面试题为例讲解一下 synchronized 关键字的具体使用。 面试中面试官经常会说：“单例模式了解吗？来给我手写一下！给我解释一下双重检验锁方式实现单例模式的原理呗！” 双重校验锁实现对象单例（线程安全） public class Singleton { private volatile static Singleton uniqueInstance; private Singleton() { } public static Singleton getUniqueInstance() { //先判断对象是否已经实例过，没有实例化过才进入加锁代码 if (uniqueInstance == null) { //类对象加锁 synchronized (Singleton.class) { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } } } return uniqueInstance; } } 另外，需要注意 uniqueInstance 采用 volatile 关键字修饰也是很有必要。 uniqueInstance 采用 volatile 关键字修饰也是很有必要的， uniqueInstance = new Singleton(); 这段代码其实是分为三步执行： 为 uniqueInstance 分配内存空间 初始化 uniqueInstance 将 uniqueInstance 指向分配的内存地址 但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1->3->2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化。 使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。 1.3. 讲一下 synchronized 关键字的底层原理 synchronized 关键字底层原理属于 JVM 层面。 ① synchronized 同步语句块的情况 public class SynchronizedDemo { public void method() { synchronized (this) { System.out.println(\"synchronized 代码块\"); } } } 通过 JDK 自带的 javap 命令查看 SynchronizedDemo 类的相关字节码信息：首先切换到类的对应目录执行 javac SynchronizedDemo.java 命令生成编译后的 .class 文件，然后执行javap -c -s -v -l SynchronizedDemo.class。 从上面我们可以看出： synchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指明同步代码块的结束位置。 当执行 monitorenter 指令时，线程试图获取锁也就是获取 monitor(monitor对象存在于每个Java对象的对象头中，synchronized 锁便是通过这种方式获取锁的，也是为什么Java中任意对象可以作为锁的原因) 的持有权。当计数器为0则可以成功获取，获取后将锁计数器设为1也就是加1。相应的在执行 monitorexit 指令后，将锁计数器设为0，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。 ② synchronized 修饰方法的的情况 public class SynchronizedDemo2 { public synchronized void method() { System.out.println(\"synchronized 方法\"); } } synchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法，JVM 通过该 ACC_SYNCHRONIZED 访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。 1.4. 说说 JDK1.6 之后的synchronized 关键字底层做了哪些优化，可以详细介绍一下这些优化吗 JDK1.6 对锁的实现引入了大量的优化，如偏向锁、轻量级锁、自旋锁、适应性自旋锁、锁消除、锁粗化等技术来减少锁操作的开销。 锁主要存在四种状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈而逐渐升级。注意锁可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率。 关于这几种优化的详细信息可以查看笔主的这篇文章：https://gitee.com/SnailClimb/JavaGuide/blob/master/docs/java/Multithread/synchronized.md 1.5. 谈谈 synchronized和ReentrantLock 的区别 ① 两者都是可重入锁 两者都是可重入锁。“可重入锁”概念是：自己可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果不可锁重入的话，就会造成死锁。同一个线程每次获取锁，锁的计数器都自增1，所以要等到锁的计数器下降为0时才能释放锁。 ② synchronized 依赖于 JVM 而 ReentrantLock 依赖于 API synchronized 是依赖于 JVM 实现的，前面我们也讲到了 虚拟机团队在 JDK1.6 为 synchronized 关键字进行了很多优化，但是这些优化都是在虚拟机层面实现的，并没有直接暴露给我们。ReentrantLock 是 JDK 层面实现的（也就是 API 层面，需要 lock() 和 unlock() 方法配合 try/finally 语句块来完成），所以我们可以通过查看它的源代码，来看它是如何实现的。 ③ ReentrantLock 比 synchronized 增加了一些高级功能 相比synchronized，ReentrantLock增加了一些高级功能。主要来说主要有三点：①等待可中断；②可实现公平锁；③可实现选择性通知（锁可以绑定多个条件） ReentrantLock提供了一种能够中断等待锁的线程的机制，通过lock.lockInterruptibly()来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。 ReentrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。 ReentrantLock默认情况是非公平的，可以通过 ReentrantLock类的ReentrantLock(boolean fair)构造方法来制定是否是公平的。 synchronized关键字与wait()和notify()/notifyAll()方法相结合可以实现等待/通知机制，ReentrantLock类当然也可以实现，但是需要借助于Condition接口与newCondition() 方法。Condition是JDK1.5之后才有的，它具有很好的灵活性，比如可以实现多路通知功能也就是在一个Lock对象中可以创建多个Condition实例（即对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 在使用notify()/notifyAll()方法进行通知时，被通知的线程是由 JVM 选择的，用ReentrantLock类结合Condition实例可以实现“选择性通知” ，这个功能非常重要，而且是Condition接口默认提供的。而synchronized关键字就相当于整个Lock对象中只有一个Condition实例，所有的线程都注册在它一个身上。如果执行notifyAll()方法的话就会通知所有处于等待状态的线程这样会造成很大的效率问题，而Condition实例的signalAll()方法 只会唤醒注册在该Condition实例中的所有等待线程。 如果你想使用上述功能，那么选择ReentrantLock是一个不错的选择。 ④ 性能已不是选择标准 2. volatile关键字 2.1. 讲一下Java内存模型 在 JDK1.2 之前，Java的内存模型实现总是从主存（即共享内存）读取变量，是不需要进行特别的注意的。而在当前的 Java 内存模型下，线程可以把变量保存本地内存（比如机器的寄存器）中，而不是直接在主存中进行读写。这就可能造成一个线程在主存中修改了一个变量的值，而另外一个线程还继续使用它在寄存器中的变量值的拷贝，造成数据的不一致。 要解决这个问题，就需要把变量声明为volatile，这就指示 JVM，这个变量是不稳定的，每次使用它都到主存中进行读取。 说白了， volatile 关键字的主要作用就是保证变量的可见性然后还有一个作用是防止指令重排序。 2.2 并发编程的三个重要特性 原子性 : 一个的操作或者多次操作，要么所有的操作全部都得到执行并且不会收到任何因素的干扰而中断，要么所有的操作都执行，要么都不执行。synchronized 可以保证代码片段的原子性。 可见性 ：当一个变量对共享变量进行了修改，那么另外的线程都是立即可以看到修改后的最新值。volatile 关键字可以保证共享变量的可见性。 有序性 ：代码在执行的过程中的先后顺序，Java 在编译器以及运行期间的优化，代码的执行顺序未必就是编写代码时候的顺序。volatile 关键字可以禁止指令进行重排序优化。 2.3. 说说 synchronized 关键字和 volatile 关键字的区别 synchronized 关键字和 volatile 关键字是两个互补的存在，而不是对立的存在： volatile关键字是线程同步的轻量级实现，所以volatile性能肯定比synchronized关键字要好。但是volatile关键字只能用于变量而synchronized关键字可以修饰方法以及代码块。synchronized关键字在JavaSE1.6之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的偏向锁和轻量级锁以及其它各种优化之后执行效率有了显著提升，实际开发中使用 synchronized 关键字的场景还是更多一些。 多线程访问volatile关键字不会发生阻塞，而synchronized关键字可能会发生阻塞 volatile关键字能保证数据的可见性，但不能保证数据的原子性。synchronized关键字两者都能保证。 volatile关键字主要用于解决变量在多个线程之间的可见性，而 synchronized关键字解决的是多个线程之间访问资源的同步性。 3. ThreadLocal 3.1. ThreadLocal简介 通常情况下，我们创建的变量是可以被任何一个线程访问并修改的。如果想实现每一个线程都有自己的专属本地变量该如何解决呢？ JDK中提供的ThreadLocal类正是为了解决这样的问题。 ThreadLocal类主要解决的就是让每个线程绑定自己的值，可以将ThreadLocal类形象的比喻成存放数据的盒子，盒子中可以存储每个线程的私有数据。 如果你创建了一个ThreadLocal变量，那么访问这个变量的每个线程都会有这个变量的本地副本，这也是ThreadLocal变量名的由来。他们可以使用 get（） 和 set（） 方法来获取默认值或将其值更改为当前线程所存的副本的值，从而避免了线程安全问题。 再举个简单的例子： 比如有两个人去宝屋收集宝物，这两个共用一个袋子的话肯定会产生争执，但是给他们两个人每个人分配一个袋子的话就不会出现这样的问题。如果把这两个人比作线程的话，那么ThreadLocal就是用来避免这两个线程竞争的。 3.2. ThreadLocal示例 相信看了上面的解释，大家已经搞懂 ThreadLocal 类是个什么东西了。 import java.text.SimpleDateFormat; import java.util.Random; public class ThreadLocalExample implements Runnable{ // SimpleDateFormat 不是线程安全的，所以每个线程都要有自己独立的副本 private static final ThreadLocal formatter = ThreadLocal.withInitial(() -> new SimpleDateFormat(\"yyyyMMdd HHmm\")); public static void main(String[] args) throws InterruptedException { ThreadLocalExample obj = new ThreadLocalExample(); for(int i=0 ; i Output: Thread Name= 0 default Formatter = yyyyMMdd HHmm Thread Name= 0 formatter = yy-M-d ah:mm Thread Name= 1 default Formatter = yyyyMMdd HHmm Thread Name= 2 default Formatter = yyyyMMdd HHmm Thread Name= 1 formatter = yy-M-d ah:mm Thread Name= 3 default Formatter = yyyyMMdd HHmm Thread Name= 2 formatter = yy-M-d ah:mm Thread Name= 4 default Formatter = yyyyMMdd HHmm Thread Name= 3 formatter = yy-M-d ah:mm Thread Name= 4 formatter = yy-M-d ah:mm Thread Name= 5 default Formatter = yyyyMMdd HHmm Thread Name= 5 formatter = yy-M-d ah:mm Thread Name= 6 default Formatter = yyyyMMdd HHmm Thread Name= 6 formatter = yy-M-d ah:mm Thread Name= 7 default Formatter = yyyyMMdd HHmm Thread Name= 7 formatter = yy-M-d ah:mm Thread Name= 8 default Formatter = yyyyMMdd HHmm Thread Name= 9 default Formatter = yyyyMMdd HHmm Thread Name= 8 formatter = yy-M-d ah:mm Thread Name= 9 formatter = yy-M-d ah:mm 从输出中可以看出，Thread-0已经改变了formatter的值，但仍然是thread-2默认格式化程序与初始化值相同，其他线程也一样。 上面有一段代码用到了创建 ThreadLocal 变量的那段代码用到了 Java8 的知识，它等于下面这段代码，如果你写了下面这段代码的话，IDEA会提示你转换为Java8的格式(IDEA真的不错！)。因为ThreadLocal类在Java 8中扩展，使用一个新的方法withInitial()，将Supplier功能接口作为参数。 private static final ThreadLocal formatter = new ThreadLocal(){ @Override protected SimpleDateFormat initialValue() { return new SimpleDateFormat(\"yyyyMMdd HHmm\"); } }; 3.3. ThreadLocal原理 从 Thread类源代码入手。 public class Thread implements Runnable { ...... //与此线程有关的ThreadLocal值。由ThreadLocal类维护 ThreadLocal.ThreadLocalMap threadLocals = null; //与此线程有关的InheritableThreadLocal值。由InheritableThreadLocal类维护 ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; ...... } 从上面Thread类 源代码可以看出Thread 类中有一个 threadLocals 和 一个 inheritableThreadLocals 变量，它们都是 ThreadLocalMap 类型的变量,我们可以把 ThreadLocalMap 理解为ThreadLocal 类实现的定制化的 HashMap。默认情况下这两个变量都是null，只有当前线程调用 ThreadLocal 类的 set或get方法时才创建它们，实际上调用这两个方法的时候，我们调用的是ThreadLocalMap类对应的 get()、set()方法。 ThreadLocal类的set()方法 public void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); } ThreadLocalMap getMap(Thread t) { return t.threadLocals; } 通过上面这些内容，我们足以通过猜测得出结论：最终的变量是放在了当前线程的 ThreadLocalMap 中，并不是存在 ThreadLocal 上，ThreadLocal 可以理解为只是ThreadLocalMap的封装，传递了变量值。 ThrealLocal 类中可以通过Thread.currentThread()获取到当前线程对象后，直接通过getMap(Thread t)可以访问到该线程的ThreadLocalMap对象。 每个Thread中都具备一个ThreadLocalMap，而ThreadLocalMap可以存储以ThreadLocal为key ，Object 对象为 value的键值对。 ThreadLocalMap(ThreadLocal firstKey, Object firstValue) { ...... } 比如我们在同一个线程中声明了两个 ThreadLocal 对象的话，会使用 Thread内部都是使用仅有那个ThreadLocalMap 存放数据的，ThreadLocalMap的 key 就是 ThreadLocal对象，value 就是 ThreadLocal 对象调用set方法设置的值。 ThreadLocalMap是ThreadLocal的静态内部类。 3.4. ThreadLocal 内存泄露问题 ThreadLocalMap 中使用的 key 为 ThreadLocal 的弱引用,而 value 是强引用。所以，如果 ThreadLocal 没有被外部强引用的情况下，在垃圾回收的时候，key 会被清理掉，而 value 不会被清理掉。这样一来，ThreadLocalMap 中就会出现key为null的Entry。假如我们不做任何措施的话，value 永远无法被GC 回收，这个时候就可能会产生内存泄露。ThreadLocalMap实现中已经考虑了这种情况，在调用 set()、get()、remove() 方法的时候，会清理掉 key 为 null 的记录。使用完 ThreadLocal方法后 最好手动调用remove()方法 static class Entry extends WeakReference> { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal k, Object v) { super(k); value = v; } } 弱引用介绍： 如果一个对象只具有弱引用，那就类似于可有可无的生活用品。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它 所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。 弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。 4. 线程池 4.1. 为什么要用线程池？ 池化技术相比大家已经屡见不鲜了，线程池、数据库连接池、Http 连接池等等都是对这个思想的应用。池化技术的思想主要是为了减少每次获取资源的消耗，提高对资源的利用率。 线程池提供了一种限制和管理资源（包括执行一个任务）。 每个线程池还维护一些基本统计信息，例如已完成任务的数量。 这里借用《Java 并发编程的艺术》提到的来说一下使用线程池的好处： 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要的等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 4.2. 实现Runnable接口和Callable接口的区别 Runnable自Java 1.0以来一直存在，但Callable仅在Java 1.5中引入,目的就是为了来处理Runnable不支持的用例。Runnable 接口不会返回结果或抛出检查异常，但是Callable 接口可以。所以，如果任务不需要返回结果或抛出异常推荐使用 Runnable 接口，这样代码看起来会更加简洁。 工具类 Executors 可以实现 Runnable 对象和 Callable 对象之间的相互转换。（Executors.callable（Runnable task）或 Executors.callable（Runnable task，Object resule））。 Runnable.java @FunctionalInterface public interface Runnable { /** * 被线程执行，没有返回值也无法抛出异常 */ public abstract void run(); } Callable.java @FunctionalInterface public interface Callable { /** * 计算结果，或在无法这样做时抛出异常。 * @return 计算得出的结果 * @throws 如果无法计算结果，则抛出异常 */ V call() throws Exception; } 4.3. 执行execute()方法和submit()方法的区别是什么呢？ execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功与否； submit()方法用于提交需要返回值的任务。线程池会返回一个 Future 类型的对象，通过这个 Future 对象可以判断任务是否执行成功，并且可以通过 Future 的 get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用 get（long timeout，TimeUnit unit）方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。 我们以AbstractExecutorService接口中的一个 submit 方法为例子来看看源代码： public Future submit(Runnable task) { if (task == null) throw new NullPointerException(); RunnableFuture ftask = newTaskFor(task, null); execute(ftask); return ftask; } 上面方法调用的 newTaskFor 方法返回了一个 FutureTask 对象。 protected RunnableFuture newTaskFor(Runnable runnable, T value) { return new FutureTask(runnable, value); } 我们再来看看execute()方法： public void execute(Runnable command) { ... } 4.4. 如何创建线程池 《阿里巴巴Java开发手册》中强制线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险 Executors 返回线程池对象的弊端如下： FixedThreadPool 和 SingleThreadExecutor ： 允许请求的队列长度为 Integer.MAX_VALUE ，可能堆积大量的请求，从而导致OOM。 CachedThreadPool 和 ScheduledThreadPool ： 允许创建的线程数量为 Integer.MAX_VALUE ，可能会创建大量线程，从而导致OOM。 方式一：通过构造方法实现 方式二：通过Executor 框架的工具类Executors来实现 我们可以创建三种类型的ThreadPoolExecutor： FixedThreadPool ： 该方法返回一个固定线程数量的线程池。该线程池中的线程数量始终不变。当有一个新的任务提交时，线程池中若有空闲线程，则立即执行。若没有，则新的任务会被暂存在一个任务队列中，待有线程空闲时，便处理在任务队列中的任务。 SingleThreadExecutor： 方法返回一个只有一个线程的线程池。若多余一个任务被提交到该线程池，任务会被保存在一个任务队列中，待线程空闲，按先入先出的顺序执行队列中的任务。 CachedThreadPool： 该方法返回一个可根据实际情况调整线程数量的线程池。线程池的线程数量不确定，但若有空闲线程可以复用，则会优先使用可复用的线程。若所有线程均在工作，又有新的任务提交，则会创建新的线程处理任务。所有线程在当前任务执行完毕后，将返回线程池进行复用。 对应Executors工具类中的方法如图所示： 4.5 ThreadPoolExecutor 类分析 ThreadPoolExecutor 类中提供的四个构造方法。我们来看最长的那个，其余三个都是在这个构造方法的基础上产生（其他几个构造方法说白点都是给定某些默认参数的构造方法比如默认制定拒绝策略是什么），这里就不贴代码讲了，比较简单。 /** * 用给定的初始参数创建一个新的ThreadPoolExecutor。 */ public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { if (corePoolSize 下面这些对创建 非常重要，在后面使用线程池的过程中你一定会用到！所以，务必拿着小本本记清楚。 4.5.1 ThreadPoolExecutor构造函数重要参数分析 ThreadPoolExecutor 3 个最重要的参数： corePoolSize : 核心线程数线程数定义了最小可以同时运行的线程数量。 maximumPoolSize : 当队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。 workQueue: 当新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中。 ThreadPoolExecutor其他常见参数: keepAliveTime:当线程池中的线程数量大于 corePoolSize 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 keepAliveTime才会被回收销毁； unit : keepAliveTime 参数的时间单位。 threadFactory :executor 创建新线程的时候会用到。 handler :饱和策略。关于饱和策略下面单独介绍一下。 4.5.2 ThreadPoolExecutor 饱和策略 ThreadPoolExecutor 饱和策略定义: 如果当前同时运行的线程数量达到最大线程数量并且队列也已经被放满了任时，ThreadPoolTaskExecutor 定义一些策略: ThreadPoolExecutor.AbortPolicy：抛出 RejectedExecutionException来拒绝新任务的处理。 ThreadPoolExecutor.CallerRunsPolicy：调用执行自己的线程运行任务。您不会任务请求。但是这种策略会降低对于新任务提交速度，影响程序的整体性能。另外，这个策略喜欢增加队列容量。如果您的应用程序可以承受此延迟并且你不能任务丢弃任何一个任务请求的话，你可以选择这个策略。 ThreadPoolExecutor.DiscardPolicy： 不处理新任务，直接丢弃掉。 ThreadPoolExecutor.DiscardOldestPolicy： 此策略将丢弃最早的未处理的任务请求。 举个例子： Spring 通过 ThreadPoolTaskExecutor 或者我们直接通过 ThreadPoolExecutor 的构造函数创建线程池的时候，当我们不指定 RejectedExecutionHandler 饱和策略的话来配置线程池的时候默认使用的是 ThreadPoolExecutor.AbortPolicy。在默认情况下，ThreadPoolExecutor 将抛出 RejectedExecutionException 来拒绝新来的任务 ，这代表你将丢失对这个任务的处理。 对于可伸缩的应用程序，建议使用 ThreadPoolExecutor.CallerRunsPolicy。当最大池被填满时，此策略为我们提供可伸缩队列。（这个直接查看 ThreadPoolExecutor 的构造函数源码就可以看出，比较简单的原因，这里就不贴代码了） 4.6 一个简单的线程池Demo:Runnable+ThreadPoolExecutor 为了让大家更清楚上面的面试题中的一些概念，我写了一个简单的线程池 Demo。 首先创建一个 Runnable 接口的实现类（当然也可以是 Callable 接口，我们上面也说了两者的区别。） MyRunnable.java import java.util.Date; /** * 这是一个简单的Runnable类，需要大约5秒钟来执行其任务。 * @author shuang.kou */ public class MyRunnable implements Runnable { private String command; public MyRunnable(String s) { this.command = s; } @Override public void run() { System.out.println(Thread.currentThread().getName() + \" Start. Time = \" + new Date()); processCommand(); System.out.println(Thread.currentThread().getName() + \" End. Time = \" + new Date()); } private void processCommand() { try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } } @Override public String toString() { return this.command; } } 编写测试程序，我们这里以阿里巴巴推荐的使用 ThreadPoolExecutor 构造函数自定义参数的方式来创建线程池。 ThreadPoolExecutorDemo.java import java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; public class ThreadPoolExecutorDemo { private static final int CORE_POOL_SIZE = 5; private static final int MAX_POOL_SIZE = 10; private static final int QUEUE_CAPACITY = 100; private static final Long KEEP_ALIVE_TIME = 1L; public static void main(String[] args) { //使用阿里巴巴推荐的创建线程池的方式 //通过ThreadPoolExecutor构造函数自定义参数创建 ThreadPoolExecutor executor = new ThreadPoolExecutor( CORE_POOL_SIZE, MAX_POOL_SIZE, KEEP_ALIVE_TIME, TimeUnit.SECONDS, new ArrayBlockingQueue<>(QUEUE_CAPACITY), new ThreadPoolExecutor.CallerRunsPolicy()); for (int i = 0; i 可以看到我们上面的代码指定了： corePoolSize: 核心线程数为 5。 maximumPoolSize ：最大线程数 10 keepAliveTime : 等待时间为 1L。 unit: 等待时间的单位为 TimeUnit.SECONDS。 workQueue：任务队列为 ArrayBlockingQueue，并且容量为 100; handler:饱和策略为 CallerRunsPolicy。 Output： pool-1-thread-2 Start. Time = Tue Nov 12 20:59:44 CST 2019 pool-1-thread-5 Start. Time = Tue Nov 12 20:59:44 CST 2019 pool-1-thread-4 Start. Time = Tue Nov 12 20:59:44 CST 2019 pool-1-thread-1 Start. Time = Tue Nov 12 20:59:44 CST 2019 pool-1-thread-3 Start. Time = Tue Nov 12 20:59:44 CST 2019 pool-1-thread-5 End. Time = Tue Nov 12 20:59:49 CST 2019 pool-1-thread-3 End. Time = Tue Nov 12 20:59:49 CST 2019 pool-1-thread-2 End. Time = Tue Nov 12 20:59:49 CST 2019 pool-1-thread-4 End. Time = Tue Nov 12 20:59:49 CST 2019 pool-1-thread-1 End. Time = Tue Nov 12 20:59:49 CST 2019 pool-1-thread-2 Start. Time = Tue Nov 12 20:59:49 CST 2019 pool-1-thread-1 Start. Time = Tue Nov 12 20:59:49 CST 2019 pool-1-thread-4 Start. Time = Tue Nov 12 20:59:49 CST 2019 pool-1-thread-3 Start. Time = Tue Nov 12 20:59:49 CST 2019 pool-1-thread-5 Start. Time = Tue Nov 12 20:59:49 CST 2019 pool-1-thread-2 End. Time = Tue Nov 12 20:59:54 CST 2019 pool-1-thread-3 End. Time = Tue Nov 12 20:59:54 CST 2019 pool-1-thread-4 End. Time = Tue Nov 12 20:59:54 CST 2019 pool-1-thread-5 End. Time = Tue Nov 12 20:59:54 CST 2019 pool-1-thread-1 End. Time = Tue Nov 12 20:59:54 CST 2019 4.7 线程池原理分析 承接 4.6 节，我们通过代码输出结果可以看出：线程池每次会同时执行 5 个任务，这 5 个任务执行完之后，剩余的 5 个任务才会被执行。 大家可以先通过上面讲解的内容，分析一下到底是咋回事？（自己独立思考一会） 现在，我们就分析上面的输出内容来简单分析一下线程池原理。 为了搞懂线程池的原理，我们需要首先分析一下 execute方法。在 4.6 节中的 Demo 中我们使用 executor.execute(worker)来提交一个任务到线程池中去，这个方法非常重要，下面我们来看看它的源码： // 存放线程池的运行状态 (runState) 和线程池内有效线程的数量 (workerCount) private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); private static int workerCountOf(int c) { return c & CAPACITY; } private final BlockingQueue workQueue; public void execute(Runnable command) { // 如果任务为null，则抛出异常。 if (command == null) throw new NullPointerException(); // ctl 中保存的线程池当前的一些状态信息 int c = ctl.get(); // 下面会涉及到 3 步 操作 // 1.首先判断当前线程池中之行的任务数量是否小于 corePoolSize // 如果小于的话，通过addWorker(command, true)新建一个线程，并将任务(command)添加到该线程中；然后，启动该线程从而执行任务。 if (workerCountOf(c) 通过下图可以更好的对上面这 3 步做一个展示，下图是我为了省事直接从网上找到，原地址不明。 现在，让我们在回到 4.6 节我们写的 Demo， 现在应该是不是很容易就可以搞懂它的原理了呢？ 没搞懂的话，也没关系，可以看看我的分析： 我们在代码中模拟了 10 个任务，我们配置的核心线程数为 5 、等待队列容量为 100 ，所以每次只可能存在 5 个任务同时执行，剩下的 5 个任务会被放到等待队列中去。当前的 5 个任务之行完成后，才会之行剩下的 5 个任务。 5. Atomic 原子类 5.1. 介绍一下Atomic 原子类 Atomic 翻译成中文是原子的意思。在化学上，我们知道原子是构成一般物质的最小单位，在化学反应中是不可分割的。在我们这里 Atomic 是指一个操作是不可中断的。即使是在多个线程一起执行的时候，一个操作一旦开始，就不会被其他线程干扰。 所以，所谓原子类说简单点就是具有原子/原子操作特征的类。 并发包 java.util.concurrent 的原子类都存放在java.util.concurrent.atomic下,如下图所示。 5.2. JUC 包中的原子类是哪4类? 基本类型 使用原子的方式更新基本类型 AtomicInteger：整形原子类 AtomicLong：长整型原子类 AtomicBoolean：布尔型原子类 数组类型 使用原子的方式更新数组里的某个元素 AtomicIntegerArray：整形数组原子类 AtomicLongArray：长整形数组原子类 AtomicReferenceArray：引用类型数组原子类 引用类型 AtomicReference：引用类型原子类 AtomicStampedReference：原子更新引用类型里的字段原子类 AtomicMarkableReference ：原子更新带有标记位的引用类型 对象的属性修改类型 AtomicIntegerFieldUpdater：原子更新整形字段的更新器 AtomicLongFieldUpdater：原子更新长整形字段的更新器 AtomicStampedReference：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于解决原子的更新数据和数据的版本号，可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 5.3. 讲讲 AtomicInteger 的使用 AtomicInteger 类常用方法 public final int get() //获取当前的值 public final int getAndSet(int newValue)//获取当前的值，并设置新的值 public final int getAndIncrement()//获取当前的值，并自增 public final int getAndDecrement() //获取当前的值，并自减 public final int getAndAdd(int delta) //获取当前的值，并加上预期的值 boolean compareAndSet(int expect, int update) //如果输入的数值等于预期值，则以原子方式将该值设置为输入值（update） public final void lazySet(int newValue)//最终设置为newValue,使用 lazySet 设置之后可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 AtomicInteger 类的使用示例 使用 AtomicInteger 之后，不用对 increment() 方法加锁也可以保证线程安全。 class AtomicIntegerTest { private AtomicInteger count = new AtomicInteger(); //使用AtomicInteger之后，不需要对该方法加锁，也可以实现线程安全。 public void increment() { count.incrementAndGet(); } public int getCount() { return count.get(); } } 5.4. 能不能给我简单介绍一下 AtomicInteger 类的原理 AtomicInteger 线程安全原理简单分析 AtomicInteger 类的部分源码： // setup to use Unsafe.compareAndSwapInt for updates（更新操作时提供“比较并替换”的作用） private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static { try { valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(\"value\")); } catch (Exception ex) { throw new Error(ex); } } private volatile int value; AtomicInteger 类主要利用 CAS (compare and swap) + volatile 和 native 方法来保证原子操作，从而避免 synchronized 的高开销，执行效率大为提升。 CAS的原理是拿期望的值和原本的一个值作比较，如果相同则更新成新的值。UnSafe 类的 objectFieldOffset() 方法是一个本地方法，这个方法是用来拿到“原来的值”的内存地址，返回值是 valueOffset。另外 value 是一个volatile变量，在内存中可见，因此 JVM 可以保证任何时刻任何线程总能拿到该变量的最新值。 关于 Atomic 原子类这部分更多内容可以查看我的这篇文章：并发编程面试必备：JUC 中的 Atomic 原子类总结 6. AQS 6.1. AQS 介绍 AQS的全称为（AbstractQueuedSynchronizer），这个类在java.util.concurrent.locks包下面。 AQS是一个用来构建锁和同步器的框架，使用AQS能简单且高效地构造出应用广泛的大量的同步器，比如我们提到的ReentrantLock，Semaphore，其他的诸如ReentrantReadWriteLock，SynchronousQueue，FutureTask等等皆是基于AQS的。当然，我们自己也能利用AQS非常轻松容易地构造出符合我们自己需求的同步器。 6.2. AQS 原理分析 AQS 原理这部分参考了部分博客，在5.2节末尾放了链接。 在面试中被问到并发知识的时候，大多都会被问到“请你说一下自己对于AQS原理的理解”。下面给大家一个示例供大家参加，面试不是背题，大家一定要加入自己的思想，即使加入不了自己的思想也要保证自己能够通俗的讲出来而不是背出来。 下面大部分内容其实在AQS类注释上已经给出了，不过是英语看着比较吃力一点，感兴趣的话可以看看源码。 6.2.1. AQS 原理概览 AQS核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制AQS是用CLH队列锁实现的，即将暂时获取不到锁的线程加入到队列中。 CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS是将每条请求共享资源的线程封装成一个CLH锁队列的一个结点（Node）来实现锁的分配。 看个AQS(AbstractQueuedSynchronizer)原理图： AQS使用一个int成员变量来表示同步状态，通过内置的FIFO队列来完成获取资源线程的排队工作。AQS使用CAS对该同步状态进行原子操作实现对其值的修改。 private volatile int state;//共享变量，使用volatile修饰保证线程可见性 状态信息通过protected类型的getState，setState，compareAndSetState进行操作 //返回同步状态的当前值 protected final int getState() { return state; } // 设置同步状态的值 protected final void setState(int newState) { state = newState; } //原子地（CAS操作）将同步状态值设置为给定值update如果当前同步状态的值等于expect（期望值） protected final boolean compareAndSetState(int expect, int update) { return unsafe.compareAndSwapInt(this, stateOffset, expect, update); } 6.2.2. AQS 对资源的共享方式 AQS定义两种资源共享方式 Exclusive（独占）：只有一个线程能执行，如ReentrantLock。又可分为公平锁和非公平锁： 公平锁：按照线程在队列中的排队顺序，先到者先拿到锁 非公平锁：当线程要获取锁时，无视队列顺序直接去抢锁，谁抢到就是谁的 Share（共享）：多个线程可同时执行，如Semaphore/CountDownLatch。Semaphore、CountDownLatch、 CyclicBarrier、ReadWriteLock 我们都会在后面讲到。 ReentrantReadWriteLock 可以看成是组合式，因为ReentrantReadWriteLock也就是读写锁允许多个线程同时对某一资源进行读。 不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源 state 的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS已经在顶层实现好了。 6.2.3. AQS底层使用了模板方法模式 同步器的设计是基于模板方法模式的，如果需要自定义同步器一般的方式是这样（模板方法模式很经典的一个应用）： 使用者继承AbstractQueuedSynchronizer并重写指定的方法。（这些重写方法很简单，无非是对于共享资源state的获取和释放） 将AQS组合在自定义同步组件的实现中，并调用其模板方法，而这些模板方法会调用使用者重写的方法。 这和我们以往通过实现接口的方式有很大区别，这是模板方法模式很经典的一个运用。 AQS使用了模板方法模式，自定义同步器时需要重写下面几个AQS提供的模板方法： isHeldExclusively()//该线程是否正在独占资源。只有用到condition才需要去实现它。 tryAcquire(int)//独占方式。尝试获取资源，成功则返回true，失败则返回false。 tryRelease(int)//独占方式。尝试释放资源，成功则返回true，失败则返回false。 tryAcquireShared(int)//共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。 tryReleaseShared(int)//共享方式。尝试释放资源，成功则返回true，失败则返回false。 默认情况下，每个方法都抛出 UnsupportedOperationException。 这些方法的实现必须是内部线程安全的，并且通常应该简短而不是阻塞。AQS类中的其他方法都是final ，所以无法被其他类使用，只有这几个方法可以被其他类使用。 以ReentrantLock为例，state初始化为0，表示未锁定状态。A线程lock()时，会调用tryAcquire()独占该锁并将state+1。此后，其他线程再tryAcquire()时就会失败，直到A线程unlock()到state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A线程自己是可以重复获取此锁的（state会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证state是能回到零态的。 再以CountDownLatch以例，任务分为N个子线程去执行，state也初始化为N（注意N要与线程个数一致）。这N个子线程是并行执行的，每个子线程执行完后countDown()一次，state会CAS(Compare and Swap)减1。等到所有子线程都执行完后(即state=0)，会unpark()主调用线程，然后主调用线程就会从await()函数返回，继续后余动作。 一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。但AQS也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。 推荐两篇 AQS 原理和相关源码分析的文章： http://www.cnblogs.com/waterystone/p/4920797.html https://www.cnblogs.com/chengxiao/archive/2017/07/24/7141160.html 6.3. AQS 组件总结 Semaphore(信号量)-允许多个线程同时访问： synchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，Semaphore(信号量)可以指定多个线程同时访问某个资源。 CountDownLatch （倒计时器）： CountDownLatch是一个同步工具类，用来协调多个线程之间的同步。这个工具通常用来控制线程等待，它可以让某一个线程等待直到倒计时结束，再开始执行。 CyclicBarrier(循环栅栏)： CyclicBarrier 和 CountDownLatch 非常类似，它也可以实现线程间的技术等待，但是它的功能比 CountDownLatch 更加复杂和强大。主要应用场景和 CountDownLatch 类似。CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。CyclicBarrier默认的构造方法是 CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用await()方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞。 7 Reference 《深入理解 Java 虚拟机》 《实战 Java 高并发程序设计》 《Java并发编程的艺术》 http://www.cnblogs.com/waterystone/p/4920797.html https://www.cnblogs.com/chengxiao/archive/2017/07/24/7141160.html https://www.journaldev.com/1076/java-threadlocal-example 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \"面试突击\" 即可免费领取！ Java工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/java/Multithread/JavaConcurrencyBasicsCommonInterviewQuestionsSummary.html":{"url":"zother6-JavaGuide/java/Multithread/JavaConcurrencyBasicsCommonInterviewQuestionsSummary.html","title":"Java Concurrency Basics Common Interview Questions Summary","keywords":"","body":" Java 并发基础常见面试题总结 1. 什么是线程和进程? 1.1. 何为进程? 1.2. 何为线程? 2. 请简要描述线程与进程的关系,区别及优缺点？ 2.1. 图解进程和线程的关系 2.2. 程序计数器为什么是私有的? 2.3. 虚拟机栈和本地方法栈为什么是私有的? 2.4. 一句话简单了解堆和方法区 3. 说说并发与并行的区别? 4. 为什么要使用多线程呢? 5. 使用多线程可能带来什么问题? 6. 说说线程的生命周期和状态? 7. 什么是上下文切换? 8. 什么是线程死锁?如何避免死锁? 8.1. 认识线程死锁 8.2. 如何避免线程死锁? 9. 说说 sleep() 方法和 wait() 方法区别和共同点? 10. 为什么我们调用 start() 方法时会执行 run() 方法，为什么我们不能直接调用 run() 方法？ 公众号 Java 并发基础常见面试题总结 1. 什么是线程和进程? 1.1. 何为进程? 进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。 在 Java 中，当我们启动 main 函数时其实就是启动了一个 JVM 的进程，而 main 函数所在的线程就是这个进程中的一个线程，也称主线程。 如下图所示，在 windows 中通过查看任务管理器的方式，我们就可以清楚看到 window 当前运行的进程（.exe 文件的运行）。 1.2. 何为线程? 线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享进程的堆和方法区资源，但每个线程有自己的程序计数器、虚拟机栈和本地方法栈，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。 Java 程序天生就是多线程程序，我们可以通过 JMX 来看一下一个普通的 Java 程序有哪些线程，代码如下。 public class MultiThread { public static void main(String[] args) { // 获取 Java 线程管理 MXBean ThreadMXBean threadMXBean = ManagementFactory.getThreadMXBean(); // 不需要获取同步的 monitor 和 synchronizer 信息，仅获取线程和线程堆栈信息 ThreadInfo[] threadInfos = threadMXBean.dumpAllThreads(false, false); // 遍历线程信息，仅打印线程 ID 和线程名称信息 for (ThreadInfo threadInfo : threadInfos) { System.out.println(\"[\" + threadInfo.getThreadId() + \"] \" + threadInfo.getThreadName()); } } } 上述程序输出如下（输出内容可能不同，不用太纠结下面每个线程的作用，只用知道 main 线程执行 main 方法即可）： [5] Attach Listener //添加事件 [4] Signal Dispatcher // 分发处理给 JVM 信号的线程 [3] Finalizer //调用对象 finalize 方法的线程 [2] Reference Handler //清除 reference 线程 [1] main //main 线程,程序入口 从上面的输出内容可以看出：一个 Java 程序的运行是 main 线程和多个其他线程同时运行。 2. 请简要描述线程与进程的关系,区别及优缺点？ 从 JVM 角度说进程和线程之间的关系 2.1. 图解进程和线程的关系 下图是 Java 内存区域，通过下图我们从 JVM 的角度来说一下线程和进程之间的关系。如果你对 Java 内存区域 (运行时数据区) 这部分知识不太了解的话可以阅读一下这篇文章：《可能是把 Java 内存区域讲的最清楚的一篇文章》 从上图可以看出：一个进程中可以有多个线程，多个线程共享进程的堆和方法区 (JDK1.8 之后的元空间)资源，但是每个线程有自己的程序计数器、虚拟机栈 和 本地方法栈。 总结： 线程 是 进程 划分成的更小的运行单位。线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。线程执行开销小，但不利于资源的管理和保护；而进程正相反 下面是该知识点的扩展内容！ 下面来思考这样一个问题：为什么程序计数器、虚拟机栈和本地方法栈是线程私有的呢？为什么堆和方法区是线程共享的呢？ 2.2. 程序计数器为什么是私有的? 程序计数器主要有下面两个作用： 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 需要注意的是，如果执行的是 native 方法，那么程序计数器记录的是 undefined 地址，只有执行的是 Java 代码时程序计数器记录的才是下一条指令的地址。 所以，程序计数器私有主要是为了线程切换后能恢复到正确的执行位置。 2.3. 虚拟机栈和本地方法栈为什么是私有的? 虚拟机栈： 每个 Java 方法在执行的同时会创建一个栈帧用于存储局部变量表、操作数栈、常量池引用等信息。从方法调用直至执行完成的过程，就对应着一个栈帧在 Java 虚拟机栈中入栈和出栈的过程。 本地方法栈： 和虚拟机栈所发挥的作用非常相似，区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。 所以，为了保证线程中的局部变量不被别的线程访问到，虚拟机栈和本地方法栈是线程私有的。 2.4. 一句话简单了解堆和方法区 堆和方法区是所有线程共享的资源，其中堆是进程中最大的一块内存，主要用于存放新创建的对象 (几乎所有对象都在这里分配内存)，方法区主要用于存放已被加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 3. 说说并发与并行的区别? 并发： 同一时间段，多个任务都在执行 (单位时间内不一定同时执行)； 并行： 单位时间内，多个任务同时执行。 4. 为什么要使用多线程呢? 先从总体上来说： 从计算机底层来说： 线程可以比作是轻量级的进程，是程序执行的最小单位,线程间的切换和调度的成本远远小于进程。另外，多核 CPU 时代意味着多个线程可以同时运行，这减少了线程上下文切换的开销。 从当代互联网发展趋势来说： 现在的系统动不动就要求百万级甚至千万级的并发量，而多线程并发编程正是开发高并发系统的基础，利用好多线程机制可以大大提高系统整体的并发能力以及性能。 再深入到计算机底层来探讨： 单核时代： 在单核时代多线程主要是为了提高 CPU 和 IO 设备的综合利用率。举个例子：当只有一个线程的时候会导致 CPU 计算时，IO 设备空闲；进行 IO 操作时，CPU 空闲。我们可以简单地说这两者的利用率目前都是 50%左右。但是当有两个线程的时候就不一样了，当一个线程执行 CPU 计算时，另外一个线程可以进行 IO 操作，这样两个的利用率就可以在理想情况下达到 100%了。 多核时代: 多核时代多线程主要是为了提高 CPU 利用率。举个例子：假如我们要计算一个复杂的任务，我们只用一个线程的话，CPU 只会一个 CPU 核心被利用到，而创建多个线程就可以让多个 CPU 核心被利用到，这样就提高了 CPU 的利用率。 5. 使用多线程可能带来什么问题? 并发编程的目的就是为了能提高程序的执行效率提高程序运行速度，但是并发编程并不总是能提高程序运行速度的，而且并发编程可能会遇到很多问题，比如：内存泄漏、死锁、线程不安全等等。 6. 说说线程的生命周期和状态? Java 线程在运行的生命周期中的指定时刻只可能处于下面 6 种不同状态的其中一个状态（图源《Java 并发编程艺术》4.1.4 节）。 线程在生命周期中并不是固定处于某一个状态而是随着代码的执行在不同状态之间切换。Java 线程状态变迁如下图所示（图源《Java 并发编程艺术》4.1.4 节）： 由上图可以看出：线程创建之后它将处于 NEW（新建） 状态，调用 start() 方法后开始运行，线程这时候处于 READY（可运行） 状态。可运行状态的线程获得了 CPU 时间片（timeslice）后就处于 RUNNING（运行） 状态。 操作系统隐藏 Java 虚拟机（JVM）中的 RUNNABLE 和 RUNNING 状态，它只能看到 RUNNABLE 状态（图源：HowToDoInJava：Java Thread Life Cycle and Thread States），所以 Java 系统一般将这两个状态统称为 RUNNABLE（运行中） 状态 。 当线程执行 wait()方法之后，线程进入 WAITING（等待） 状态。进入等待状态的线程需要依靠其他线程的通知才能够返回到运行状态，而 TIME_WAITING(超时等待) 状态相当于在等待状态的基础上增加了超时限制，比如通过 sleep（long millis）方法或 wait（long millis）方法可以将 Java 线程置于 TIMED WAITING 状态。当超时时间到达后 Java 线程将会返回到 RUNNABLE 状态。当线程调用同步方法时，在没有获取到锁的情况下，线程将会进入到 BLOCKED（阻塞） 状态。线程在执行 Runnable 的run()方法之后将会进入到 TERMINATED（终止） 状态。 7. 什么是上下文切换? 多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。 概括来说就是：当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换。 上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。 Linux 相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。 8. 什么是线程死锁?如何避免死锁? 8.1. 认识线程死锁 线程死锁描述的是这样一种情况：多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放。由于线程被无限期地阻塞，因此程序不可能正常终止。 如下图所示，线程 A 持有资源 2，线程 B 持有资源 1，他们同时都想申请对方的资源，所以这两个线程就会互相等待而进入死锁状态。 下面通过一个例子来说明线程死锁,代码模拟了上图的死锁的情况 (代码来源于《并发编程之美》)： public class DeadLockDemo { private static Object resource1 = new Object();//资源 1 private static Object resource2 = new Object();//资源 2 public static void main(String[] args) { new Thread(() -> { synchronized (resource1) { System.out.println(Thread.currentThread() + \"get resource1\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \"waiting get resource2\"); synchronized (resource2) { System.out.println(Thread.currentThread() + \"get resource2\"); } } }, \"线程 1\").start(); new Thread(() -> { synchronized (resource2) { System.out.println(Thread.currentThread() + \"get resource2\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \"waiting get resource1\"); synchronized (resource1) { System.out.println(Thread.currentThread() + \"get resource1\"); } } }, \"线程 2\").start(); } } Output Thread[线程 1,5,main]get resource1 Thread[线程 2,5,main]get resource2 Thread[线程 1,5,main]waiting get resource2 Thread[线程 2,5,main]waiting get resource1 线程 A 通过 synchronized (resource1) 获得 resource1 的监视器锁，然后通过Thread.sleep(1000);让线程 A 休眠 1s 为的是让线程 B 得到执行然后获取到 resource2 的监视器锁。线程 A 和线程 B 休眠结束了都开始企图请求获取对方的资源，然后这两个线程就会陷入互相等待的状态，这也就产生了死锁。上面的例子符合产生死锁的四个必要条件。 学过操作系统的朋友都知道产生死锁必须具备以下四个条件： 互斥条件：该资源任意一个时刻只由一个线程占用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件:线程已获得的资源在末使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。 8.2. 如何避免线程死锁? 我上面说了产生死锁的四个必要条件，为了避免死锁，我们只要破坏产生死锁的四个条件中的其中一个就可以了。现在我们来挨个分析一下： 破坏互斥条件 ：这个条件我们没有办法破坏，因为我们用锁本来就是想让他们互斥的（临界资源需要互斥访问）。 破坏请求与保持条件 ：一次性申请所有的资源。 破坏不剥夺条件 ：占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。 破坏循环等待条件 ：靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。 我们对线程 2 的代码修改成下面这样就不会产生死锁了。 new Thread(() -> { synchronized (resource1) { System.out.println(Thread.currentThread() + \"get resource1\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \"waiting get resource2\"); synchronized (resource2) { System.out.println(Thread.currentThread() + \"get resource2\"); } } }, \"线程 2\").start(); Output Thread[线程 1,5,main]get resource1 Thread[线程 1,5,main]waiting get resource2 Thread[线程 1,5,main]get resource2 Thread[线程 2,5,main]get resource1 Thread[线程 2,5,main]waiting get resource2 Thread[线程 2,5,main]get resource2 Process finished with exit code 0 我们分析一下上面的代码为什么避免了死锁的发生? 线程 1 首先获得到 resource1 的监视器锁,这时候线程 2 就获取不到了。然后线程 1 再去获取 resource2 的监视器锁，可以获取到。然后线程 1 释放了对 resource1、resource2 的监视器锁的占用，线程 2 获取到就可以执行了。这样就破坏了破坏循环等待条件，因此避免了死锁。 9. 说说 sleep() 方法和 wait() 方法区别和共同点? 两者最主要的区别在于：sleep 方法没有释放锁，而 wait 方法释放了锁 。 两者都可以暂停线程的执行。 Wait 通常被用于线程间交互/通信，sleep 通常被用于暂停执行。 wait() 方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的 notify() 或者 notifyAll() 方法。sleep() 方法执行完成后，线程会自动苏醒。或者可以使用 wait(long timeout)超时后线程会自动苏醒。 10. 为什么我们调用 start() 方法时会执行 run() 方法，为什么我们不能直接调用 run() 方法？ 这是另一个非常经典的 java 多线程面试问题，而且在面试中会经常被问到。很简单，但是很多人都会答不上来！ new 一个 Thread，线程进入了新建状态;调用 start() 方法，会启动一个线程并使线程进入了就绪状态，当分配到时间片后就可以开始运行了。 start() 会执行线程的相应准备工作，然后自动执行 run() 方法的内容，这是真正的多线程工作。 而直接执行 run() 方法，会把 run 方法当成一个 main 线程下的普通方法去执行，并不会在某个线程中执行它，所以这并不是多线程工作。 总结： 调用 start 方法方可启动线程并使线程进入就绪状态，而 run 方法只是 thread 的一个普通方法调用，还是在主线程里执行。 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java 面试突击》: 由本文档衍生的专为面试而生的《Java 面试突击》V2.0 PDF 版本公众号后台回复 \"面试突击\" 即可免费领取！ Java 工程师必备学习资源: 一些 Java 工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/java/Multithread/java线程池学习总结.html":{"url":"zother6-JavaGuide/java/Multithread/java线程池学习总结.html","title":"java线程池学习总结","keywords":"","body":" 一 使用线程池的好处 二 Executor 框架 2.1 简介 2.2 Executor 框架结构(主要由三大部分组成) 1) 任务(Runnable /Callable) 2) 任务的执行(Executor) 3) 异步计算的结果(Future) 2.3 Executor 框架的使用示意图 三 (重要)ThreadPoolExecutor 类简单介绍 3.1 ThreadPoolExecutor 类分析 3.2 推荐使用 ThreadPoolExecutor 构造函数创建线程池 四 (重要)ThreadPoolExecutor 使用示例 4.1 示例代码:Runnable+ThreadPoolExecutor 4.2 线程池原理分析 4.3 几个常见的对比 4.3.1 Runnable vs Callable 4.3.2 execute() vs submit() 4.3.3 shutdown()VSshutdownNow() 4.3.2 isTerminated() VS isShutdown() 4.4 加餐:Callable+ThreadPoolExecutor示例代码 五 几种常见的线程池详解 5.1 FixedThreadPool 5.1.1 介绍 5.1.2 执行任务过程介绍 5.1.3 为什么不推荐使用FixedThreadPool？ 5.2 SingleThreadExecutor 详解 5.2.1 介绍 5.2.2 执行任务过程介绍 5.2.3 为什么不推荐使用SingleThreadExecutor？ 5.3 CachedThreadPool 详解 5.3.1 介绍 5.3.2 执行任务过程介绍 5.3.3 为什么不推荐使用CachedThreadPool？ 六 ScheduledThreadPoolExecutor 详解 6.1 简介 6.2 运行机制 6.3 ScheduledThreadPoolExecutor 执行周期任务的步骤 七 线程池大小确定 八 参考 九 其他推荐阅读 一 使用线程池的好处 池化技术相比大家已经屡见不鲜了，线程池、数据库连接池、Http 连接池等等都是对这个思想的应用。池化技术的思想主要是为了减少每次获取资源的消耗，提高对资源的利用率。 线程池提供了一种限制和管理资源（包括执行一个任务）。 每个线程池还维护一些基本统计信息，例如已完成任务的数量。 这里借用《Java 并发编程的艺术》提到的来说一下使用线程池的好处： 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要的等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 二 Executor 框架 2.1 简介 Executor 框架是 Java5 之后引进的，在 Java 5 之后，通过 Executor 来启动线程比使用 Thread 的 start 方法更好，除了更易管理，效率更好（用线程池实现，节约开销）外，还有关键的一点：有助于避免 this 逃逸问题。 补充：this 逃逸是指在构造函数返回之前其他线程就持有该对象的引用. 调用尚未构造完全的对象的方法可能引发令人疑惑的错误。 Executor 框架不仅包括了线程池的管理，还提供了线程工厂、队列以及拒绝策略等，Executor 框架让并发编程变得更加简单。 2.2 Executor 框架结构(主要由三大部分组成) 1) 任务(Runnable /Callable) 执行任务需要实现的 Runnable 接口 或 Callable接口。Runnable 接口或 Callable 接口 实现类都可以被 ThreadPoolExecutor 或 ScheduledThreadPoolExecutor 执行。 2) 任务的执行(Executor) 如下图所示，包括任务执行机制的核心接口 Executor ，以及继承自 Executor 接口的 ExecutorService 接口。ThreadPoolExecutor 和 ScheduledThreadPoolExecutor 这两个关键类实现了 ExecutorService 接口。 这里提了很多底层的类关系，但是，实际上我们需要更多关注的是 ThreadPoolExecutor 这个类，这个类在我们实际使用线程池的过程中，使用频率还是非常高的。 注意： 通过查看 ScheduledThreadPoolExecutor 源代码我们发现 ScheduledThreadPoolExecutor 实际上是继承了 ThreadPoolExecutor 并实现了 ScheduledExecutorService ，而 ScheduledExecutorService 又实现了 ExecutorService，正如我们下面给出的类关系图显示的一样。 ThreadPoolExecutor 类描述: //AbstractExecutorService实现了ExecutorService接口 public class ThreadPoolExecutor extends AbstractExecutorService ScheduledThreadPoolExecutor 类描述: //ScheduledExecutorService实现了ExecutorService接口 public class ScheduledThreadPoolExecutor extends ThreadPoolExecutor implements ScheduledExecutorService 3) 异步计算的结果(Future) Future 接口以及 Future 接口的实现类 FutureTask 类都可以代表异步计算的结果。 当我们把 Runnable接口 或 Callable 接口 的实现类提交给 ThreadPoolExecutor 或 ScheduledThreadPoolExecutor 执行。（调用 submit() 方法时会返回一个 FutureTask 对象） 2.3 Executor 框架的使用示意图 主线程首先要创建实现 Runnable 或者 Callable 接口的任务对象。 把创建完成的实现 Runnable/Callable接口的 对象直接交给 ExecutorService 执行: ExecutorService.execute（Runnable command））或者也可以把 Runnable 对象或Callable 对象提交给 ExecutorService 执行（ExecutorService.submit（Runnable task）或 ExecutorService.submit（Callable task））。 如果执行 ExecutorService.submit（…），ExecutorService 将返回一个实现Future接口的对象（我们刚刚也提到过了执行 execute()方法和 submit()方法的区别，submit()会返回一个 FutureTask 对象）。由于 FutureTask 实现了 Runnable，我们也可以创建 FutureTask，然后直接交给 ExecutorService 执行。 最后，主线程可以执行 FutureTask.get()方法来等待任务执行完成。主线程也可以执行 FutureTask.cancel（boolean mayInterruptIfRunning）来取消此任务的执行。 三 (重要)ThreadPoolExecutor 类简单介绍 线程池实现类 ThreadPoolExecutor 是 Executor 框架最核心的类。 3.1 ThreadPoolExecutor 类分析 ThreadPoolExecutor 类中提供的四个构造方法。我们来看最长的那个，其余三个都是在这个构造方法的基础上产生（其他几个构造方法说白点都是给定某些默认参数的构造方法比如默认制定拒绝策略是什么），这里就不贴代码讲了，比较简单。 /** * 用给定的初始参数创建一个新的ThreadPoolExecutor。 */ public ThreadPoolExecutor(int corePoolSize,//线程池的核心线程数量 int maximumPoolSize,//线程池的最大线程数 long keepAliveTime,//当线程数大于核心线程数时，多余的空闲线程存活的最长时间 TimeUnit unit,//时间单位 BlockingQueue workQueue,//任务队列，用来储存等待执行任务的队列 ThreadFactory threadFactory,//线程工厂，用来创建线程，一般默认即可 RejectedExecutionHandler handler//拒绝策略，当提交的任务过多而不能及时处理时，我们可以定制策略来处理任务 ) { if (corePoolSize 下面这些对创建 非常重要，在后面使用线程池的过程中你一定会用到！所以，务必拿着小本本记清楚。 ThreadPoolExecutor 3 个最重要的参数： corePoolSize : 核心线程数线程数定义了最小可以同时运行的线程数量。 maximumPoolSize : 当队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。 workQueue: 当新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，信任就会被存放在队列中。 ThreadPoolExecutor其他常见参数: keepAliveTime:当线程池中的线程数量大于 corePoolSize 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 keepAliveTime才会被回收销毁； unit : keepAliveTime 参数的时间单位。 threadFactory :executor 创建新线程的时候会用到。 handler :饱和策略。关于饱和策略下面单独介绍一下。 下面这张图可以加深你对线程池中各个参数的相互关系的理解（图片来源：《Java 性能调优实战》）： ThreadPoolExecutor 饱和策略定义: 如果当前同时运行的线程数量达到最大线程数量并且队列也已经被放满了任时，ThreadPoolTaskExecutor 定义一些策略: ThreadPoolExecutor.AbortPolicy：抛出 RejectedExecutionException来拒绝新任务的处理。 ThreadPoolExecutor.CallerRunsPolicy：调用执行自己的线程运行任务，也就是直接在调用execute方法的线程中运行(run)被拒绝的任务，如果执行程序已关闭，则会丢弃该任务。因此这种策略会降低对于新任务提交速度，影响程序的整体性能。另外，这个策略喜欢增加队列容量。如果您的应用程序可以承受此延迟并且你不能任务丢弃任何一个任务请求的话，你可以选择这个策略。 ThreadPoolExecutor.DiscardPolicy： 不处理新任务，直接丢弃掉。 ThreadPoolExecutor.DiscardOldestPolicy： 此策略将丢弃最早的未处理的任务请求。 举个例子： Spring 通过 ThreadPoolTaskExecutor 或者我们直接通过 ThreadPoolExecutor 的构造函数创建线程池的时候，当我们不指定 RejectedExecutionHandler 饱和策略的话来配置线程池的时候默认使用的是 ThreadPoolExecutor.AbortPolicy。在默认情况下，ThreadPoolExecutor 将抛出 RejectedExecutionException 来拒绝新来的任务 ，这代表你将丢失对这个任务的处理。 对于可伸缩的应用程序，建议使用 ThreadPoolExecutor.CallerRunsPolicy。当最大池被填满时，此策略为我们提供可伸缩队列。（这个直接查看 ThreadPoolExecutor 的构造函数源码就可以看出，比较简单的原因，这里就不贴代码了。） 3.2 推荐使用 ThreadPoolExecutor 构造函数创建线程池 在《阿里巴巴 Java 开发手册》“并发处理”这一章节，明确指出线程资源必须通过线程池提供，不允许在应用中自行显示创建线程。 为什么呢？ 使用线程池的好处是减少在创建和销毁线程上所消耗的时间以及系统资源开销，解决资源不足的问题。如果不使用线程池，有可能会造成系统创建大量同类线程而导致消耗完内存或者“过度切换”的问题。 另外《阿里巴巴 Java 开发手册》中强制线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 构造函数的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险 Executors 返回线程池对象的弊端如下： FixedThreadPool 和 SingleThreadExecutor ： 允许请求的队列长度为 Integer.MAX_VALUE,可能堆积大量的请求，从而导致 OOM。 CachedThreadPool 和 ScheduledThreadPool ： 允许创建的线程数量为 Integer.MAX_VALUE ，可能会创建大量线程，从而导致 OOM。 方式一：通过ThreadPoolExecutor构造函数实现（推荐） 方式二：通过 Executor 框架的工具类 Executors 来实现 我们可以创建三种类型的 ThreadPoolExecutor： FixedThreadPool SingleThreadExecutor CachedThreadPool 对应 Executors 工具类中的方法如图所示： 四 (重要)ThreadPoolExecutor 使用示例 我们上面讲解了 Executor框架以及 ThreadPoolExecutor 类，下面让我们实战一下，来通过写一个 ThreadPoolExecutor 的小 Demo 来回顾上面的内容。 4.1 示例代码:Runnable+ThreadPoolExecutor 首先创建一个 Runnable 接口的实现类（当然也可以是 Callable 接口，我们上面也说了两者的区别。） MyRunnable.java import java.util.Date; /** * 这是一个简单的Runnable类，需要大约5秒钟来执行其任务。 * @author shuang.kou */ public class MyRunnable implements Runnable { private String command; public MyRunnable(String s) { this.command = s; } @Override public void run() { System.out.println(Thread.currentThread().getName() + \" Start. Time = \" + new Date()); processCommand(); System.out.println(Thread.currentThread().getName() + \" End. Time = \" + new Date()); } private void processCommand() { try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } } @Override public String toString() { return this.command; } } 编写测试程序，我们这里以阿里巴巴推荐的使用 ThreadPoolExecutor 构造函数自定义参数的方式来创建线程池。 ThreadPoolExecutorDemo.java import java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; public class ThreadPoolExecutorDemo { private static final int CORE_POOL_SIZE = 5; private static final int MAX_POOL_SIZE = 10; private static final int QUEUE_CAPACITY = 100; private static final Long KEEP_ALIVE_TIME = 1L; public static void main(String[] args) { //使用阿里巴巴推荐的创建线程池的方式 //通过ThreadPoolExecutor构造函数自定义参数创建 ThreadPoolExecutor executor = new ThreadPoolExecutor( CORE_POOL_SIZE, MAX_POOL_SIZE, KEEP_ALIVE_TIME, TimeUnit.SECONDS, new ArrayBlockingQueue<>(QUEUE_CAPACITY), new ThreadPoolExecutor.CallerRunsPolicy()); for (int i = 0; i 可以看到我们上面的代码指定了： corePoolSize: 核心线程数为 5。 maximumPoolSize ：最大线程数 10 keepAliveTime : 等待时间为 1L。 unit: 等待时间的单位为 TimeUnit.SECONDS。 workQueue：任务队列为 ArrayBlockingQueue，并且容量为 100; handler:饱和策略为 CallerRunsPolicy。 Output： pool-1-thread-3 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-5 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-2 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-1 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-4 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-3 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-4 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-1 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-5 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-1 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-2 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-5 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-4 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-3 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-2 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-1 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-4 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-5 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-3 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-2 End. Time = Sun Apr 12 11:14:47 CST 2020 4.2 线程池原理分析 承接 4.1 节，我们通过代码输出结果可以看出：线程首先会先执行 5 个任务，然后这些任务有任务被执行完的话，就会去拿新的任务执行。 大家可以先通过上面讲解的内容，分析一下到底是咋回事？（自己独立思考一会） 现在，我们就分析上面的输出内容来简单分析一下线程池原理。 为了搞懂线程池的原理，我们需要首先分析一下 execute方法。在 4.1 节中的 Demo 中我们使用 executor.execute(worker)来提交一个任务到线程池中去，这个方法非常重要，下面我们来看看它的源码： // 存放线程池的运行状态 (runState) 和线程池内有效线程的数量 (workerCount) private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); private static int workerCountOf(int c) { return c & CAPACITY; } //任务队列 private final BlockingQueue workQueue; public void execute(Runnable command) { // 如果任务为null，则抛出异常。 if (command == null) throw new NullPointerException(); // ctl 中保存的线程池当前的一些状态信息 int c = ctl.get(); // 下面会涉及到 3 步 操作 // 1.首先判断当前线程池中之行的任务数量是否小于 corePoolSize // 如果小于的话，通过addWorker(command, true)新建一个线程，并将任务(command)添加到该线程中；然后，启动该线程从而执行任务。 if (workerCountOf(c) 通过下图可以更好的对上面这 3 步做一个展示，下图是我为了省事直接从网上找到，原地址不明。 addWorker 这个方法主要用来创建新的工作线程，如果返回true说明创建和启动工作线程成功，否则的话返回的就是false。 // 全局锁，并发操作必备 private final ReentrantLock mainLock = new ReentrantLock(); // 跟踪线程池的最大大小，只有在持有全局锁mainLock的前提下才能访问此集合 private int largestPoolSize; // 工作线程集合，存放线程池中所有的（活跃的）工作线程，只有在持有全局锁mainLock的前提下才能访问此集合 private final HashSet workers = new HashSet<>(); //获取线程池状态 private static int runStateOf(int c) { return c & ~CAPACITY; } //判断线程池的状态是否为 Running private static boolean isRunning(int c) { return c = SHUTDOWN && ! (rs == SHUTDOWN && firstTask == null && ! workQueue.isEmpty())) return false; for (;;) { //获取线程池中线程的数量 int wc = workerCountOf(c); // core参数为true的话表明队列也满了，线程池大小变为 maximumPoolSize if (wc >= CAPACITY || wc >= (core ? corePoolSize : maximumPoolSize)) return false; //原子操作将workcount的数量加1 if (compareAndIncrementWorkerCount(c)) break retry; // 如果线程的状态改变了就再次执行上述操作 c = ctl.get(); if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop } } // 标记工作线程是否启动成功 boolean workerStarted = false; // 标记工作线程是否创建成功 boolean workerAdded = false; Worker w = null; try { w = new Worker(firstTask); final Thread t = w.thread; if (t != null) { // 加锁 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { //获取线程池状态 int rs = runStateOf(ctl.get()); //rs largestPoolSize) largestPoolSize = s; // 工作线程是否启动成功 workerAdded = true; } } finally { // 释放锁 mainLock.unlock(); } //// 如果成功添加工作线程，则调用Worker内部的线程实例t的Thread#start()方法启动真实的线程实例 if (workerAdded) { t.start(); /// 标记线程启动成功 workerStarted = true; } } } finally { // 线程启动失败，需要从工作线程中移除对应的Worker if (! workerStarted) addWorkerFailed(w); } return workerStarted; } 更多关于线程池源码分析的内容推荐这篇文章：《JUC线程池ThreadPoolExecutor源码分析》 现在，让我们在回到 4.1 节我们写的 Demo， 现在应该是不是很容易就可以搞懂它的原理了呢？ 没搞懂的话，也没关系，可以看看我的分析： 我们在代码中模拟了 10 个任务，我们配置的核心线程数为 5 、等待队列容量为 100 ，所以每次只可能存在 5 个任务同时执行，剩下的 5 个任务会被放到等待队列中去。当前的5个任务中如果有任务被执行完了，线程池就会去拿新的任务执行。 4.3 几个常见的对比 4.3.1 Runnable vs Callable Runnable自 Java 1.0 以来一直存在，但Callable仅在 Java 1.5 中引入,目的就是为了来处理Runnable不支持的用例。Runnable 接口不会返回结果或抛出检查异常，但是Callable 接口可以。所以，如果任务不需要返回结果或抛出异常推荐使用 Runnable 接口，这样代码看起来会更加简洁。 工具类 Executors 可以实现 Runnable 对象和 Callable 对象之间的相互转换。（Executors.callable（Runnable task）或 Executors.callable（Runnable task，Object resule））。 Runnable.java @FunctionalInterface public interface Runnable { /** * 被线程执行，没有返回值也无法抛出异常 */ public abstract void run(); } Callable.java @FunctionalInterface public interface Callable { /** * 计算结果，或在无法这样做时抛出异常。 * @return 计算得出的结果 * @throws 如果无法计算结果，则抛出异常 */ V call() throws Exception; } 4.3.2 execute() vs submit() execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功与否； submit()方法用于提交需要返回值的任务。线程池会返回一个 Future 类型的对象，通过这个 Future 对象可以判断任务是否执行成功，并且可以通过 Future 的 get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用 get（long timeout，TimeUnit unit）方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。 我们以AbstractExecutorService接口中的一个 submit 方法为例子来看看源代码： public Future submit(Runnable task) { if (task == null) throw new NullPointerException(); RunnableFuture ftask = newTaskFor(task, null); execute(ftask); return ftask; } 上面方法调用的 newTaskFor 方法返回了一个 FutureTask 对象。 protected RunnableFuture newTaskFor(Runnable runnable, T value) { return new FutureTask(runnable, value); } 我们再来看看execute()方法： public void execute(Runnable command) { ... } 4.3.3 shutdown()VSshutdownNow() shutdown（） :关闭线程池，线程池的状态变为 SHUTDOWN。线程池不再接受新任务了，但是队列里的任务得执行完毕。 shutdownNow（） :关闭线程池，线程的状态变为 STOP。线程池会终止当前正在运行的任务，并停止处理排队的任务并返回正在等待执行的 List。 4.3.2 isTerminated() VS isShutdown() isShutDown 当调用 shutdown() 方法后返回为 true。 isTerminated 当调用 shutdown() 方法后，并且所有提交的任务完成后返回为 true 4.4 加餐:Callable+ThreadPoolExecutor示例代码 MyCallable.java import java.util.concurrent.Callable; public class MyCallable implements Callable { @Override public String call() throws Exception { Thread.sleep(1000); //返回执行当前 Callable 的线程名字 return Thread.currentThread().getName(); } } CallableDemo.java import java.util.ArrayList; import java.util.Date; import java.util.List; import java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.Callable; import java.util.concurrent.ExecutionException; import java.util.concurrent.Future; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; public class CallableDemo { private static final int CORE_POOL_SIZE = 5; private static final int MAX_POOL_SIZE = 10; private static final int QUEUE_CAPACITY = 100; private static final Long KEEP_ALIVE_TIME = 1L; public static void main(String[] args) { //使用阿里巴巴推荐的创建线程池的方式 //通过ThreadPoolExecutor构造函数自定义参数创建 ThreadPoolExecutor executor = new ThreadPoolExecutor( CORE_POOL_SIZE, MAX_POOL_SIZE, KEEP_ALIVE_TIME, TimeUnit.SECONDS, new ArrayBlockingQueue<>(QUEUE_CAPACITY), new ThreadPoolExecutor.CallerRunsPolicy()); List> futureList = new ArrayList<>(); Callable callable = new MyCallable(); for (int i = 0; i future = executor.submit(callable); //将返回值 future 添加到 list，我们可以通过 future 获得 执行 Callable 得到的返回值 futureList.add(future); } for (Future fut : futureList) { try { System.out.println(new Date() + \"::\" + fut.get()); } catch (InterruptedException | ExecutionException e) { e.printStackTrace(); } } //关闭线程池 executor.shutdown(); } } Output: Wed Nov 13 13:40:41 CST 2019::pool-1-thread-1 Wed Nov 13 13:40:42 CST 2019::pool-1-thread-2 Wed Nov 13 13:40:42 CST 2019::pool-1-thread-3 Wed Nov 13 13:40:42 CST 2019::pool-1-thread-4 Wed Nov 13 13:40:42 CST 2019::pool-1-thread-5 Wed Nov 13 13:40:42 CST 2019::pool-1-thread-3 Wed Nov 13 13:40:43 CST 2019::pool-1-thread-2 Wed Nov 13 13:40:43 CST 2019::pool-1-thread-1 Wed Nov 13 13:40:43 CST 2019::pool-1-thread-4 Wed Nov 13 13:40:43 CST 2019::pool-1-thread-5 五 几种常见的线程池详解 5.1 FixedThreadPool 5.1.1 介绍 FixedThreadPool 被称为可重用固定线程数的线程池。通过 Executors 类中的相关源代码来看一下相关实现： /** * 创建一个可重用固定数量线程的线程池 */ public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue(), threadFactory); } 另外还有一个 FixedThreadPool 的实现方法，和上面的类似，所以这里不多做阐述： public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue()); } 从上面源代码可以看出新创建的 FixedThreadPool 的 corePoolSize 和 maximumPoolSize 都被设置为 nThreads，这个 nThreads 参数是我们使用的时候自己传递的。 5.1.2 执行任务过程介绍 FixedThreadPool 的 execute() 方法运行示意图（该图片来源：《Java 并发编程的艺术》）： 上图说明： 如果当前运行的线程数小于 corePoolSize， 如果再来新任务的话，就创建新的线程来执行任务； 当前运行的线程数等于 corePoolSize 后， 如果再来新任务的话，会将任务加入 LinkedBlockingQueue； 线程池中的线程执行完 手头的任务后，会在循环中反复从 LinkedBlockingQueue 中获取任务来执行； 5.1.3 为什么不推荐使用FixedThreadPool？ FixedThreadPool 使用无界队列 LinkedBlockingQueue（队列的容量为 Intger.MAX_VALUE）作为线程池的工作队列会对线程池带来如下影响 ： 当线程池中的线程数达到 corePoolSize 后，新任务将在无界队列中等待，因此线程池中的线程数不会超过 corePoolSize； 由于使用无界队列时 maximumPoolSize 将是一个无效参数，因为不可能存在任务队列满的情况。所以，通过创建 FixedThreadPool的源码可以看出创建的 FixedThreadPool 的 corePoolSize 和 maximumPoolSize 被设置为同一个值。 由于 1 和 2，使用无界队列时 keepAliveTime 将是一个无效参数； 运行中的 FixedThreadPool（未执行 shutdown()或 shutdownNow()）不会拒绝任务，在任务比较多的时候会导致 OOM（内存溢出）。 5.2 SingleThreadExecutor 详解 5.2.1 介绍 SingleThreadExecutor 是只有一个线程的线程池。下面看看SingleThreadExecutor 的实现： /** *返回只有一个线程的线程池 */ public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue(), threadFactory)); } public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue())); } 从上面源代码可以看出新创建的 SingleThreadExecutor 的 corePoolSize 和 maximumPoolSize 都被设置为 1.其他参数和 FixedThreadPool 相同。 5.2.2 执行任务过程介绍 SingleThreadExecutor 的运行示意图（该图片来源：《Java 并发编程的艺术》）： 上图说明; 如果当前运行的线程数少于 corePoolSize，则创建一个新的线程执行任务； 当前线程池中有一个运行的线程后，将任务加入 LinkedBlockingQueue 线程执行完当前的任务后，会在循环中反复从LinkedBlockingQueue 中获取任务来执行； 5.2.3 为什么不推荐使用SingleThreadExecutor？ SingleThreadExecutor 使用无界队列 LinkedBlockingQueue 作为线程池的工作队列（队列的容量为 Intger.MAX_VALUE）。SingleThreadExecutor 使用无界队列作为线程池的工作队列会对线程池带来的影响与 FixedThreadPool 相同。说简单点就是可能会导致 OOM， 5.3 CachedThreadPool 详解 5.3.1 介绍 CachedThreadPool 是一个会根据需要创建新线程的线程池。下面通过源码来看看 CachedThreadPool 的实现： /** * 创建一个线程池，根据需要创建新线程，但会在先前构建的线程可用时重用它。 */ public static ExecutorService newCachedThreadPool(ThreadFactory threadFactory) { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue(), threadFactory); } public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue()); } CachedThreadPool 的corePoolSize 被设置为空（0），maximumPoolSize被设置为 Integer.MAX.VALUE，即它是无界的，这也就意味着如果主线程提交任务的速度高于 maximumPool 中线程处理任务的速度时，CachedThreadPool 会不断创建新的线程。极端情况下，这样会导致耗尽 cpu 和内存资源。 5.3.2 执行任务过程介绍 CachedThreadPool 的 execute()方法的执行示意图（该图片来源：《Java 并发编程的艺术》）： 上图说明： 首先执行 SynchronousQueue.offer(Runnable task) 提交任务到任务队列。如果当前 maximumPool 中有闲线程正在执行 SynchronousQueue.poll(keepAliveTime,TimeUnit.NANOSECONDS)，那么主线程执行 offer 操作与空闲线程执行的 poll 操作配对成功，主线程把任务交给空闲线程执行，execute()方法执行完成，否则执行下面的步骤 2； 当初始 maximumPool 为空，或者 maximumPool 中没有空闲线程时，将没有线程执行 SynchronousQueue.poll(keepAliveTime,TimeUnit.NANOSECONDS)。这种情况下，步骤 1 将失败，此时 CachedThreadPool 会创建新线程执行任务，execute 方法执行完成； 5.3.3 为什么不推荐使用CachedThreadPool？ CachedThreadPool允许创建的线程数量为 Integer.MAX_VALUE ，可能会创建大量线程，从而导致 OOM。 六 ScheduledThreadPoolExecutor 详解 ScheduledThreadPoolExecutor 主要用来在给定的延迟后运行任务，或者定期执行任务。 这个在实际项目中基本不会被用到，因为有其他方案选择比如quartz。大家只需要简单了解一下它的思想。关于如何在 Spring Boot 中 实现定时任务，可以查看这篇文章《5 分钟搞懂如何在 Spring Boot 中 Schedule Tasks》。 6.1 简介 ScheduledThreadPoolExecutor 使用的任务队列 DelayQueue 封装了一个 PriorityQueue，PriorityQueue 会对队列中的任务进行排序，执行所需时间短的放在前面先被执行(ScheduledFutureTask 的 time 变量小的先执行)，如果执行所需时间相同则先提交的任务将被先执行(ScheduledFutureTask 的 squenceNumber 变量小的先执行)。 ScheduledThreadPoolExecutor 和 Timer 的比较： Timer 对系统时钟的变化敏感，ScheduledThreadPoolExecutor不是； Timer 只有一个执行线程，因此长时间运行的任务可以延迟其他任务。 ScheduledThreadPoolExecutor 可以配置任意数量的线程。 此外，如果你想（通过提供 ThreadFactory），你可以完全控制创建的线程; 在TimerTask 中抛出的运行时异常会杀死一个线程，从而导致 Timer 死机:-( ...即计划任务将不再运行。ScheduledThreadExecutor 不仅捕获运行时异常，还允许您在需要时处理它们（通过重写 afterExecute 方法ThreadPoolExecutor）。抛出异常的任务将被取消，但其他任务将继续运行。 综上，在 JDK1.5 之后，你没有理由再使用 Timer 进行任务调度了。 备注： Quartz 是一个由 java 编写的任务调度库，由 OpenSymphony 组织开源出来。在实际项目开发中使用 Quartz 的还是居多，比较推荐使用 Quartz。因为 Quartz 理论上能够同时对上万个任务进行调度，拥有丰富的功能特性，包括任务调度、任务持久化、可集群化、插件等等。 6.2 运行机制 ScheduledThreadPoolExecutor 的执行主要分为两大部分： 当调用 ScheduledThreadPoolExecutor 的 scheduleAtFixedRate() 方法或者scheduleWirhFixedDelay() 方法时，会向 ScheduledThreadPoolExecutor 的 DelayQueue 添加一个实现了 RunnableScheduledFuture 接口的 ScheduledFutureTask 。 线程池中的线程从 DelayQueue 中获取 ScheduledFutureTask，然后执行任务。 ScheduledThreadPoolExecutor 为了实现周期性的执行任务，对 ThreadPoolExecutor做了如下修改： 使用 DelayQueue 作为任务队列； 获取任务的方不同 执行周期任务后，增加了额外的处理 6.3 ScheduledThreadPoolExecutor 执行周期任务的步骤 线程 1 从 DelayQueue 中获取已到期的 ScheduledFutureTask（DelayQueue.take()）。到期任务是指 ScheduledFutureTask的 time 大于等于当前系统的时间； 线程 1 执行这个 ScheduledFutureTask； 线程 1 修改 ScheduledFutureTask 的 time 变量为下次将要被执行的时间； 线程 1 把这个修改 time 之后的 ScheduledFutureTask 放回 DelayQueue 中（DelayQueue.add())。 七 线程池大小确定 线程池数量的确定一直是困扰着程序员的一个难题，大部分程序员在设定线程池大小的时候就是随心而定。 很多人甚至可能都会觉得把线程池配置过大一点比较好！我觉得这明显是有问题的。就拿我们生活中非常常见的一例子来说：并不是人多就能把事情做好，增加了沟通交流成本。你本来一件事情只需要 3 个人做，你硬是拉来了 6 个人，会提升做事效率嘛？我想并不会。 线程数量过多的影响也是和我们分配多少人做事情一样，对于多线程这个场景来说主要是增加了上下文切换成本。不清楚什么是上下文切换的话，可以看我下面的介绍。 上下文切换： 多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。概括来说就是：当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换。 上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。 Linux 相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。 类比于实现世界中的人类通过合作做某件事情，我们可以肯定的一点是线程池大小设置过大或者过小都会有问题，合适的才是最好。 如果我们设置的线程池数量太小的话，如果同一时间有大量任务/请求需要处理，可能会导致大量的请求/任务在任务队列中排队等待执行，甚至会出现任务队列满了之后任务/请求无法处理的情况，或者大量任务堆积在任务队列导致 OOM。这样很明显是有问题的！ CPU 根本没有得到充分利用。 但是，如果我们设置线程数量太大，大量线程可能会同时在争取 CPU 资源，这样会导致大量的上下文切换，从而增加线程的执行时间，影响了整体执行效率。 有一个简单并且适用面比较广的公式： CPU 密集型任务(N+1)： 这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N（CPU 核心数）+1，比 CPU 核心数多出来的一个线程是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用 CPU 的空闲时间。 I/O 密集型任务(2N)： 这种任务应用起来，系统会用大部分的时间来处理 I/O 交互，而线程在处理 I/O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用。因此在 I/O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N。 如何判断是 CPU 密集任务还是 IO 密集任务？ CPU 密集型简单理解就是利用 CPU 计算能力的任务比如你在内存中对大量数据进行排序。单凡涉及到网络读取，文件读取这类都是 IO 密集型，这类任务的特点是 CPU 计算耗费时间相比于等待 IO 操作完成的时间来说很少，大部分时间都花在了等待 IO 操作完成上。 八 参考 《Java 并发编程的艺术》 Java Scheduler ScheduledExecutorService ScheduledThreadPoolExecutor Example java.util.concurrent.ScheduledThreadPoolExecutor Example ThreadPoolExecutor – Java Thread Pool Example 九 其他推荐阅读 Java 并发（三）线程池原理 如何优雅的使用和理解线程池 "},"zother6-JavaGuide/java/Multithread/synchronized.html":{"url":"zother6-JavaGuide/java/Multithread/synchronized.html","title":"Synchronized","keywords":"","body":" synchronized关键字最主要的三种使用方式的总结 修饰实例方法，作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁 修饰静态方法，作用于当前类对象加锁，进入同步代码前要获得当前类对象的锁 。也就是给当前类加锁，会作用于类的所有对象实例，因为静态成员不属于任何一个实例对象，是类成员（ static 表明这是该类的一个静态资源，不管new了多少个对象，只有一份，所以对该类的所有对象都加了锁）。所以如果一个线程A调用一个实例对象的非静态 synchronized 方法，而线程B需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。 修饰代码块，指定加锁对象，对给定对象加锁，进入同步代码块前要获得给定对象的锁。 和 synchronized 方法一样，synchronized(this)代码块也是锁定当前对象的。synchronized 关键字加到 static 静态方法和 synchronized(class)代码块上都是是给 Class 类上锁。这里再提一下：synchronized关键字加到非 static 静态方法上是给对象实例上锁。另外需要注意的是：尽量不要使用 synchronized(String a) 因为JVM中，字符串常量池具有缓冲功能！ 下面我已一个常见的面试题为例讲解一下 synchronized 关键字的具体使用。 面试中面试官经常会说：“单例模式了解吗？来给我手写一下！给我解释一下双重检验锁方式实现单例模式的原理呗！” 双重校验锁实现对象单例（线程安全） public class Singleton { private volatile static Singleton uniqueInstance; private Singleton() { } public static Singleton getUniqueInstance() { //先判断对象是否已经实例过，没有实例化过才进入加锁代码 if (uniqueInstance == null) { //类对象加锁 synchronized (Singleton.class) { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } } } return uniqueInstance; } } 另外，需要注意 uniqueInstance 采用 volatile 关键字修饰也是很有必要。 uniqueInstance 采用 volatile 关键字修饰也是很有必要的， uniqueInstance = new Singleton(); 这段代码其实是分为三步执行： 为 uniqueInstance 分配内存空间 初始化 uniqueInstance 将 uniqueInstance 指向分配的内存地址 但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1->3->2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化。 使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。 synchronized 关键字底层原理总结 synchronized 关键字底层原理属于 JVM 层面。 ① synchronized 同步语句块的情况 public class SynchronizedDemo { public void method() { synchronized (this) { System.out.println(\"synchronized 代码块\"); } } } 通过 JDK 自带的 javap 命令查看 SynchronizedDemo 类的相关字节码信息：首先切换到类的对应目录执行 javac SynchronizedDemo.java 命令生成编译后的 .class 文件，然后执行javap -c -s -v -l SynchronizedDemo.class。 从上面我们可以看出： synchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指明同步代码块的结束位置。 当执行 monitorenter 指令时，线程试图获取锁也就是获取 monitor(monitor对象存在于每个Java对象的对象头中，synchronized 锁便是通过这种方式获取锁的，也是为什么Java中任意对象可以作为锁的原因) 的持有权.当计数器为0则可以成功获取，获取后将锁计数器设为1也就是加1。相应的在执行 monitorexit 指令后，将锁计数器设为0，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。 ② synchronized 修饰方法的的情况 public class SynchronizedDemo2 { public synchronized void method() { System.out.println(\"synchronized 方法\"); } } synchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法，JVM 通过该 ACC_SYNCHRONIZED 访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。 在 Java 早期版本中，synchronized 属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的 synchronized 效率低的原因。庆幸的是在 Java 6 之后 Java 官方对从 JVM 层面对synchronized 较大优化，所以现在的 synchronized 锁效率也优化得很不错了。JDK1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。 JDK1.6 之后的底层优化 JDK1.6 对锁的实现引入了大量的优化，如偏向锁、轻量级锁、自旋锁、适应性自旋锁、锁消除、锁粗化等技术来减少锁操作的开销。 锁主要存在四中状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈而逐渐升级。注意锁可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率。 ①偏向锁 引入偏向锁的目的和引入轻量级锁的目的很像，他们都是为了没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗。但是不同是：轻量级锁在无竞争的情况下使用 CAS 操作去代替使用互斥量。而偏向锁在无竞争的情况下会把整个同步都消除掉。 偏向锁的“偏”就是偏心的偏，它的意思是会偏向于第一个获得它的线程，如果在接下来的执行中，该锁没有被其他线程获取，那么持有偏向锁的线程就不需要进行同步！关于偏向锁的原理可以查看《深入理解Java虚拟机：JVM高级特性与最佳实践》第二版的13章第三节锁优化。 但是对于锁竞争比较激烈的场合，偏向锁就失效了，因为这样场合极有可能每次申请锁的线程都是不相同的，因此这种场合下不应该使用偏向锁，否则会得不偿失，需要注意的是，偏向锁失败后，并不会立即膨胀为重量级锁，而是先升级为轻量级锁。 ② 轻量级锁 倘若偏向锁失败，虚拟机并不会立即升级为重量级锁，它还会尝试使用一种称为轻量级锁的优化手段(1.6之后加入的)。轻量级锁不是为了代替重量级锁，它的本意是在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗，因为使用轻量级锁时，不需要申请互斥量。另外，轻量级锁的加锁和解锁都用到了CAS操作。 关于轻量级锁的加锁和解锁的原理可以查看《深入理解Java虚拟机：JVM高级特性与最佳实践》第二版的13章第三节锁优化。 轻量级锁能够提升程序同步性能的依据是“对于绝大部分锁，在整个同步周期内都是不存在竞争的”，这是一个经验数据。如果没有竞争，轻量级锁使用 CAS 操作避免了使用互斥操作的开销。但如果存在锁竞争，除了互斥量开销外，还会额外发生CAS操作，因此在有锁竞争的情况下，轻量级锁比传统的重量级锁更慢！如果锁竞争激烈，那么轻量级将很快膨胀为重量级锁！ ③ 自旋锁和自适应自旋 轻量级锁失败后，虚拟机为了避免线程真实地在操作系统层面挂起，还会进行一项称为自旋锁的优化手段。 互斥同步对性能最大的影响就是阻塞的实现，因为挂起线程/恢复线程的操作都需要转入内核态中完成（用户态转换到内核态会耗费时间）。 一般线程持有锁的时间都不是太长，所以仅仅为了这一点时间去挂起线程/恢复线程是得不偿失的。 所以，虚拟机的开发团队就这样去考虑：“我们能不能让后面来的请求获取锁的线程等待一会而不被挂起呢？看看持有锁的线程是否很快就会释放锁”。为了让一个线程等待，我们只需要让线程执行一个忙循环（自旋），这项技术就叫做自旋。 百度百科对自旋锁的解释： 何谓自旋锁？它是为实现保护共享资源而提出一种锁机制。其实，自旋锁与互斥锁比较类似，它们都是为了解决对某项资源的互斥使用。无论是互斥锁，还是自旋锁，在任何时刻，最多只能有一个保持者，也就说，在任何时刻最多只能有一个执行单元获得锁。但是两者在调度机制上略有不同。对于互斥锁，如果资源已经被占用，资源申请者只能进入睡眠状态。但是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁，\"自旋\"一词就是因此而得名。 自旋锁在 JDK1.6 之前其实就已经引入了，不过是默认关闭的，需要通过--XX:+UseSpinning参数来开启。JDK1.6及1.6之后，就改为默认开启的了。需要注意的是：自旋等待不能完全替代阻塞，因为它还是要占用处理器时间。如果锁被占用的时间短，那么效果当然就很好了！反之，相反！自旋等待的时间必须要有限度。如果自旋超过了限定次数任然没有获得锁，就应该挂起线程。自旋次数的默认值是10次，用户可以修改--XX:PreBlockSpin来更改。 另外,在 JDK1.6 中引入了自适应的自旋锁。自适应的自旋锁带来的改进就是：自旋的时间不在固定了，而是和前一次同一个锁上的自旋时间以及锁的拥有者的状态来决定，虚拟机变得越来越“聪明”了。 ④ 锁消除 锁消除理解起来很简单，它指的就是虚拟机即使编译器在运行时，如果检测到那些共享数据不可能存在竞争，那么就执行锁消除。锁消除可以节省毫无意义的请求锁的时间。 ⑤ 锁粗化 原则上，我们在编写代码的时候，总是推荐将同步块的作用范围限制得尽量小，——直在共享数据的实际作用域才进行同步，这样是为了使得需要同步的操作数量尽可能变小，如果存在锁竞争，那等待线程也能尽快拿到锁。 大部分情况下，上面的原则都是没有问题的，但是如果一系列的连续操作都对同一个对象反复加锁和解锁，那么会带来很多不必要的性能消耗。 Synchronized 和 ReenTrantLock 的对比 ① 两者都是可重入锁 两者都是可重入锁。“可重入锁”概念是：自己可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果不可锁重入的话，就会造成死锁。同一个线程每次获取锁，锁的计数器都自增1，所以要等到锁的计数器下降为0时才能释放锁。 ② synchronized 依赖于 JVM 而 ReenTrantLock 依赖于 API synchronized 是依赖于 JVM 实现的，前面我们也讲到了 虚拟机团队在 JDK1.6 为 synchronized 关键字进行了很多优化，但是这些优化都是在虚拟机层面实现的，并没有直接暴露给我们。ReenTrantLock 是 JDK 层面实现的（也就是 API 层面，需要 lock() 和 unlock 方法配合 try/finally 语句块来完成），所以我们可以通过查看它的源代码，来看它是如何实现的。 ③ ReenTrantLock 比 synchronized 增加了一些高级功能 相比synchronized，ReenTrantLock增加了一些高级功能。主要来说主要有三点：①等待可中断；②可实现公平锁；③可实现选择性通知（锁可以绑定多个条件） ReenTrantLock提供了一种能够中断等待锁的线程的机制，通过lock.lockInterruptibly()来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。 ReenTrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。 ReenTrantLock默认情况是非公平的，可以通过 ReenTrantLock类的ReentrantLock(boolean fair)构造方法来制定是否是公平的。 synchronized关键字与wait()和notify/notifyAll()方法相结合可以实现等待/通知机制，ReentrantLock类当然也可以实现，但是需要借助于Condition接口与newCondition() 方法。Condition是JDK1.5之后才有的，它具有很好的灵活性，比如可以实现多路通知功能也就是在一个Lock对象中可以创建多个Condition实例（即对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 在使用notify/notifyAll()方法进行通知时，被通知的线程是由 JVM 选择的，用ReentrantLock类结合Condition实例可以实现“选择性通知” ，这个功能非常重要，而且是Condition接口默认提供的。而synchronized关键字就相当于整个Lock对象中只有一个Condition实例，所有的线程都注册在它一个身上。如果执行notifyAll()方法的话就会通知所有处于等待状态的线程这样会造成很大的效率问题，而Condition实例的signalAll()方法 只会唤醒注册在该Condition实例中的所有等待线程。 如果你想使用上述功能，那么选择ReenTrantLock是一个不错的选择。 ④ 性能已不是选择标准 在JDK1.6之前，synchronized 的性能是比 ReenTrantLock 差很多。具体表示为：synchronized 关键字吞吐量随线程数的增加，下降得非常严重。而ReenTrantLock 基本保持一个比较稳定的水平。我觉得这也侧面反映了， synchronized 关键字还有非常大的优化余地。后续的技术发展也证明了这一点，我们上面也讲了在 JDK1.6 之后 JVM 团队对 synchronized 关键字做了很多优化。JDK1.6 之后，synchronized 和 ReenTrantLock 的性能基本是持平了。所以网上那些说因为性能才选择 ReenTrantLock 的文章都是错的！JDK1.6之后，性能已经不是选择synchronized和ReenTrantLock的影响因素了！而且虚拟机在未来的性能改进中会更偏向于原生的synchronized，所以还是提倡在synchronized能满足你的需求的情况下，优先考虑使用synchronized关键字来进行同步！优化后的synchronized和ReenTrantLock一样，在很多地方都是用到了CAS操作。 "},"zother6-JavaGuide/java/Multithread/ThreadLocal.html":{"url":"zother6-JavaGuide/java/Multithread/ThreadLocal.html","title":"Thread Local","keywords":"","body":"ThreadLocal造成OOM内存溢出案例演示与原理分析 深入理解 Java 之 ThreadLocal 工作原理 ThreadLocal ThreadLocal简介 通常情况下，我们创建的变量是可以被任何一个线程访问并修改的。如果想实现每一个线程都有自己的专属本地变量该如何解决呢？ JDK中提供的ThreadLocal类正是为了解决这样的问题。 ThreadLocal类主要解决的就是让每个线程绑定自己的值，可以将ThreadLocal类形象的比喻成存放数据的盒子，盒子中可以存储每个线程的私有数据。 如果你创建了一个ThreadLocal变量，那么访问这个变量的每个线程都会有这个变量的本地副本，这也是ThreadLocal变量名的由来。他们可以使用 get（） 和 set（） 方法来获取默认值或将其值更改为当前线程所存的副本的值，从而避免了线程安全问题。 再举个简单的例子： 比如有两个人去宝屋收集宝物，这两个共用一个袋子的话肯定会产生争执，但是给他们两个人每个人分配一个袋子的话就不会出现这样的问题。如果把这两个人比作线程的话，那么ThreadLocal就是用来这两个线程竞争的。 ThreadLocal示例 相信看了上面的解释，大家已经搞懂 ThreadLocal 类是个什么东西了。 import java.text.SimpleDateFormat; import java.util.Random; public class ThreadLocalExample implements Runnable{ // SimpleDateFormat 不是线程安全的，所以每个线程都要有自己独立的副本 private static final ThreadLocal formatter = ThreadLocal.withInitial(() -> new SimpleDateFormat(\"yyyyMMdd HHmm\")); public static void main(String[] args) throws InterruptedException { ThreadLocalExample obj = new ThreadLocalExample(); for(int i=0 ; i Output: Thread Name= 0 default Formatter = yyyyMMdd HHmm Thread Name= 0 formatter = yy-M-d ah:mm Thread Name= 1 default Formatter = yyyyMMdd HHmm Thread Name= 2 default Formatter = yyyyMMdd HHmm Thread Name= 1 formatter = yy-M-d ah:mm Thread Name= 3 default Formatter = yyyyMMdd HHmm Thread Name= 2 formatter = yy-M-d ah:mm Thread Name= 4 default Formatter = yyyyMMdd HHmm Thread Name= 3 formatter = yy-M-d ah:mm Thread Name= 4 formatter = yy-M-d ah:mm Thread Name= 5 default Formatter = yyyyMMdd HHmm Thread Name= 5 formatter = yy-M-d ah:mm Thread Name= 6 default Formatter = yyyyMMdd HHmm Thread Name= 6 formatter = yy-M-d ah:mm Thread Name= 7 default Formatter = yyyyMMdd HHmm Thread Name= 7 formatter = yy-M-d ah:mm Thread Name= 8 default Formatter = yyyyMMdd HHmm Thread Name= 9 default Formatter = yyyyMMdd HHmm Thread Name= 8 formatter = yy-M-d ah:mm Thread Name= 9 formatter = yy-M-d ah:mm 从输出中可以看出，Thread-0已经改变了formatter的值，但仍然是thread-2默认格式化程序与初始化值相同，其他线程也一样。 上面有一段代码用到了创建 ThreadLocal 变量的那段代码用到了 Java8 的知识，它等于下面这段代码，如果你写了下面这段代码的话，IDEA会提示你转换为Java8的格式(IDEA真的不错！)。因为ThreadLocal类在Java 8中扩展，使用一个新的方法withInitial()，将Supplier功能接口作为参数。 private static final ThreadLocal formatter = new ThreadLocal(){ @Override protected SimpleDateFormat initialValue() { return new SimpleDateFormat(\"yyyyMMdd HHmm\"); } }; ThreadLocal原理 从 Thread类源代码入手。 public class Thread implements Runnable { ...... //与此线程有关的ThreadLocal值。由ThreadLocal类维护 ThreadLocal.ThreadLocalMap threadLocals = null; //与此线程有关的InheritableThreadLocal值。由InheritableThreadLocal类维护 ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; ...... } 从上面Thread类 源代码可以看出Thread 类中有一个 threadLocals 和 一个 inheritableThreadLocals 变量，它们都是 ThreadLocalMap 类型的变量,我们可以把 ThreadLocalMap 理解为ThreadLocal 类实现的定制化的 HashMap。默认情况下这两个变量都是null，只有当前线程调用 ThreadLocal 类的 set或get方法时才创建它们，实际上调用这两个方法的时候，我们调用的是ThreadLocalMap类对应的 get()、set()方法。 ThreadLocal类的set()方法 public void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); } ThreadLocalMap getMap(Thread t) { return t.threadLocals; } 通过上面这些内容，我们足以通过猜测得出结论：最终的变量是放在了当前线程的 ThreadLocalMap 中，并不是存在 ThreadLocal 上，ThreadLocal 可以理解为只是ThreadLocalMap的封装，传递了变量值。 每个Thread中都具备一个ThreadLocalMap，而ThreadLocalMap可以存储以ThreadLocal为key的键值对。 比如我们在同一个线程中声明了两个 ThreadLocal 对象的话，会使用 Thread内部都是使用仅有那个ThreadLocalMap 存放数据的，ThreadLocalMap的 key 就是 ThreadLocal对象，value 就是 ThreadLocal 对象调用set方法设置的值。ThreadLocal 是 map结构是为了让每个线程可以关联多个 ThreadLocal变量。这也就解释了ThreadLocal声明的变量为什么在每一个线程都有自己的专属本地变量。 public class Thread implements Runnable { ...... //与此线程有关的ThreadLocal值。由ThreadLocal类维护 ThreadLocal.ThreadLocalMap threadLocals = null; //与此线程有关的InheritableThreadLocal值。由InheritableThreadLocal类维护 ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; ...... } ThreadLocalMap是ThreadLocal的静态内部类。 ThreadLocal 内存泄露问题 ThreadLocalMap 中使用的 key 为 ThreadLocal 的弱引用,而 value 是强引用。所以，如果 ThreadLocal 没有被外部强引用的情况下，在垃圾回收的时候会 key 会被清理掉，而 value 不会被清理掉。这样一来，ThreadLocalMap 中就会出现key为null的Entry。假如我们不做任何措施的话，value 永远无法被GC 回收，这个时候就可能会产生内存泄露。ThreadLocalMap实现中已经考虑了这种情况，在调用 set()、get()、remove() 方法的时候，会清理掉 key 为 null 的记录。使用完 ThreadLocal方法后 最好手动调用remove()方法 static class Entry extends WeakReference> { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal k, Object v) { super(k); value = v; } } 弱引用介绍： 如果一个对象只具有弱引用，那就类似于可有可无的生活用品。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它 所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。 弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。 "},"zother6-JavaGuide/java/Multithread/创建线程的几种方式总结.html":{"url":"zother6-JavaGuide/java/Multithread/创建线程的几种方式总结.html","title":"创建线程的几种方式总结","keywords":"","body":"面试官：“创建线程有哪几种常见的方式？” 继承 Thread 类 实现 Runnable 接口 使用 Executor 框架 使用 FutureTask 最简单的两种方式 1.继承 Thread 类 2.实现 Runnable 接口 比较实用的两种方式 3.使用 Executor 框架 Executor 框架是 Java5 之后引进的，在 Java 5 之后，通过 Executor 来启动线程比使用 Thread 的 start 方法更好，除了更易管理，效率更好（用线程池实现，节约开销）外，还有关键的一点：有助于避免 this 逃逸问题。 补充：this 逃逸是指在构造函数返回之前其他线程就持有该对象的引用. 调用尚未构造完全的对象的方法可能引发令人疑惑的错误。 Executor 框架不仅包括了线程池的管理，还提供了线程工厂、队列以及拒绝策略等，Executor 框架让并发编程变得更加简单。 为了能搞懂如何使用 Executor 框架创建 Executor 框架结构(主要由三大部分组成) 1) 任务(Runnable /Callable) 执行任务需要实现的 Runnable 接口 或 Callable接口。Runnable 接口或 Callable 接口 实现类都可以被 ThreadPoolExecutor 或 ScheduledThreadPoolExecutor 执行。 2) 任务的执行(Executor) 如下图所示，包括任务执行机制的核心接口 Executor ，以及继承自 Executor 接口的 ExecutorService 接口。ThreadPoolExecutor 和 ScheduledThreadPoolExecutor 这两个关键类实现了 ExecutorService 接口。 4.使用 FutureTask "},"zother6-JavaGuide/java/Multithread/多线程学习指南.html":{"url":"zother6-JavaGuide/java/Multithread/多线程学习指南.html","title":"多线程学习指南","keywords":"","body":"前言 这是我的第二篇专门介绍如何去学习某个知识点的文章，在上一篇《写给 Java 程序员看的算法学习指南！》 的文章中，我推荐了一些关于 算法学习的书籍以及资源 。 相比于写技术文章来说，写这种这种类型的文章实际花费的时间可能会稍微少一点。但是，这种学习指南形式的文章，我想对于 Java 初学者甚至是工作几年的 Java 工程师来说应该还是非常有帮助的！ 我们都知道多线程应该是大部分 Java 程序员最难啃的一块骨头之一，这部分内容的难度跨度大，难实践，并且市面上的参考资料的质量也层次不齐。 在这篇文章中，我会首先介绍一下 Java 多线程学习 中比较重要的一些问题，然后还会推荐一些比较不错的学习资源供大家参考。希望对你们学习多线程相关的知识能有帮助。以下介绍的很多知识点你都可以在这里找到：https://snailclimb.gitee.io/javaguide/#/?id=并发 另外，我还将本文的内容同步到了 Github 上，点击阅读原文即可直达。如果你觉得有任何需要完善和修改的地方，都可以去 Github 给我提交 Issue 或者 PR（推荐）。 一.Java 多线程知识点总结 1.1.多线程基础 什么是线程和进程? 线程与进程的关系,区别及优缺点？ 说说并发与并行的区别? 为什么要使用多线程呢? 使用多线程可能带来什么问题?（内存泄漏、死锁、线程不安全等等） 创建线程有哪几种方式？（a.继承 Thread 类;b.实现 Runnable 接口;c. 使用 Executor 框架;d.使用 FutureTask） 说说线程的生命周期和状态? 什么是上下文切换? 什么是线程死锁?如何避免死锁? 说说 sleep() 方法和 wait() 方法区别和共同点? 为什么我们调用 start() 方法时会执行 run() 方法，为什么我们不能直接调用 run() 方法？ ...... 1.2.多线程知识进阶 volatile 关键字 Java 内存模型（JMM）; 重排序与 happens-before 原则了解吗? volatile 关键字的作用; 说说 synchronized 关键字和 volatile 关键字的区别; ...... ThreadLocal 有啥用（解决了什么问题）？怎么用？ 原理了解吗？ 内存泄露问题了解吗？ 线程池 为什么要用线程池？ 你会使用线程池吗？ 如何创建线程池比较好？ （推荐使用 ThreadPoolExecutor 构造函数创建线程池） ThreadPoolExecutor 类的重要参数了解吗？ThreadPoolExecutor 饱和策略了解吗？ 线程池原理了解吗？ 几种常见的线程池了解吗？为什么不推荐使用FixedThreadPool？ 如何设置线程池的大小？ ...... AQS 简介 原理 AQS 常用组件。 Semaphore(信号量)-允许多个线程同时访问 CountDownLatch （倒计时器）-CountDownLatch 允许 count 个线程阻塞在一个地方，直至所有线程的任务都执行完毕。 CyclicBarrier(循环栅栏)-CyclicBarrier 和 CountDownLatch 非常类似，它也可以实现线程间的技术等待，但是它的功能比 CountDownLatch 更加复杂和强大。主要应用场景和 CountDownLatch 类似。 ReentrantLock 和 ReentrantReadWriteLock ...... 锁 锁的常见分类 可重入锁和非可重入锁 公平锁与非公平锁 读写锁和排它锁 synchronized 关键字 说一说自己对于 synchronized 关键字的了解； 说说自己是怎么使用 synchronized 关键字，在项目中用到了吗; 讲一下 synchronized 关键字的底层原理； 说说 JDK1.6 之后的 synchronized 关键字底层做了哪些优化，可以详细介绍一下这些优化吗； 谈谈 synchronized 和 ReentrantLock 的区别； ...... ReentrantLock 和 ReentrantReadWriteLock ReadWriteLock StampedLock（JDK8） Atomic 与 CAS CAS: 介绍 原理 Atomic 原子类： 介绍一下 Atomic 原子类； JUC 包中的原子类是哪 4 类?； 讲讲 AtomicInteger 的使用； 能不能给我简单介绍一下 AtomicInteger 类的原理。 ...... 并发容器 JDK 提供的这些容器大部分在 java.util.concurrent 包中。 ConcurrentHashMap: 线程安全的 HashMap CopyOnWriteArrayList: 线程安全的 List，在读多写少的场合性能非常好，远远好于 Vector. ConcurrentLinkedQueue: 高效的并发队列，使用链表实现。可以看做一个线程安全的 LinkedList，这是一个非阻塞队列。 BlockingQueue: 这是一个接口，JDK 内部通过链表、数组等方式实现了这个接口。表示阻塞队列，非常适合用于作为数据共享的通道。 ConcurrentSkipListMap: 跳表的实现。这是一个 Map，使用跳表的数据结构进行快速查找。 ...... Future 和 CompletableFuture 二.书籍推荐 《Java 并发编程之美》 我觉得这本书还是非常适合我们用来学习 Java 多线程的。这本书的讲解非常通俗易懂，作者从并发编程基础到实战都是信手拈来。 另外，这本书的作者加多自身也会经常在网上发布各种技术文章。我觉得这本书也是加多大佬这么多年在多线程领域的沉淀所得的结果吧！他书中的内容基本都是结合代码讲解，非常有说服力！ 《实战 Java 高并发程序设计》 这个是我第二本要推荐的书籍，比较适合作为多线程入门/进阶书籍来看。这本书内容同样是理论结合实战，对于每个知识点的讲解也比较通俗易懂，整体结构也比较清。 《深入浅出 Java 多线程》 这本书是几位大厂（如阿里）的大佬开源的，Github 地址：https://github.com/RedSpider1/concurrent 几位作者为了写好《深入浅出 Java 多线程》这本书阅读了大量的 Java 多线程方面的书籍和博客，然后再加上他们的经验总结、Demo 实例、源码解析，最终才形成了这本书。 这本书的质量也是非常过硬！给作者们点个赞！这本书有统一的排版规则和语言风格、清晰的表达方式和逻辑。并且每篇文章初稿写完后，作者们就会互相审校，合并到主分支时所有成员会再次审校，最后再通篇修订了三遍。 《Java 并发编程的艺术》 这本书不是很适合作为 Java 多线程入门书籍，需要具备一定的 JVM 基础，有些东西讲的还是挺深入的。另外，就我自己阅读这本书的感觉来说，我觉得这本书的章节规划有点杂乱，但是，具体到某个知识点又很棒！这可能也和这本书由三名作者共同编写完成有关系吧！ 综上：这本书并不是和 Java 多线程入门，你也不需要把这本书的每一章节都看一遍，建议挑选自己想要详细了解的知识点来看。 三.总结 在这篇文章中我主要总结了 Java 多线程方面的知识点，并且推荐了相关的书籍。并发这部分东西实战的话比较难，你可以尝试学会了某个知识点之后然后在自己写过的一些项目上实践。另外，leetcode 有一个练习多线程的类别： https://leetcode-cn.com/problemset/concurrency 可以作为参考。 为了这篇文章的内容更加完善，我还将本文的内容同步到了 Github 上，点击阅读原文即可直达。如果你觉得有任何需要完善和修改的地方，都可以去 Github 给我提交 Issue 或者 PR（推荐）。 "},"zother6-JavaGuide/java/Multithread/并发容器总结.html":{"url":"zother6-JavaGuide/java/Multithread/并发容器总结.html","title":"并发容器总结","keywords":"","body":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java 面试突击》以及 Java 工程师必备学习资源。 一 JDK 提供的并发容器总结 二 ConcurrentHashMap 三 CopyOnWriteArrayList 3.1 CopyOnWriteArrayList 简介 3.2 CopyOnWriteArrayList 是如何做到的？ 3.3 CopyOnWriteArrayList 读取和写入源码简单分析 3.3.1 CopyOnWriteArrayList 读取操作的实现 3.3.2 CopyOnWriteArrayList 写入操作的实现 四 ConcurrentLinkedQueue 五 BlockingQueue 5.1 BlockingQueue 简单介绍 5.2 ArrayBlockingQueue 5.3 LinkedBlockingQueue 5.4 PriorityBlockingQueue 六 ConcurrentSkipListMap 七 参考 一 JDK 提供的并发容器总结 JDK 提供的这些容器大部分在 java.util.concurrent 包中。 ConcurrentHashMap: 线程安全的 HashMap CopyOnWriteArrayList: 线程安全的 List，在读多写少的场合性能非常好，远远好于 Vector. ConcurrentLinkedQueue: 高效的并发队列，使用链表实现。可以看做一个线程安全的 LinkedList，这是一个非阻塞队列。 BlockingQueue: 这是一个接口，JDK 内部通过链表、数组等方式实现了这个接口。表示阻塞队列，非常适合用于作为数据共享的通道。 ConcurrentSkipListMap: 跳表的实现。这是一个 Map，使用跳表的数据结构进行快速查找。 二 ConcurrentHashMap 我们知道 HashMap 不是线程安全的，在并发场景下如果要保证一种可行的方式是使用 Collections.synchronizedMap() 方法来包装我们的 HashMap。但这是通过使用一个全局的锁来同步不同线程间的并发访问，因此会带来不可忽视的性能问题。 所以就有了 HashMap 的线程安全版本—— ConcurrentHashMap 的诞生。在 ConcurrentHashMap 中，无论是读操作还是写操作都能保证很高的性能：在进行读操作时(几乎)不需要加锁，而在写操作时通过锁分段技术只对所操作的段加锁而不影响客户端对其它段的访问。 关于 ConcurrentHashMap 相关问题，我在 Java 集合框架常见面试题 这篇文章中已经提到过。下面梳理一下关于 ConcurrentHashMap 比较重要的问题： ConcurrentHashMap 和 Hashtable 的区别 ConcurrentHashMap 线程安全的具体实现方式/底层具体实现 三 CopyOnWriteArrayList 3.1 CopyOnWriteArrayList 简介 public class CopyOnWriteArrayList extends Object implements List, RandomAccess, Cloneable, Serializable 在很多应用场景中，读操作可能会远远大于写操作。由于读操作根本不会修改原有的数据，因此对于每次读取都进行加锁其实是一种资源浪费。我们应该允许多个线程同时访问 List 的内部数据，毕竟读取操作是安全的。 这和我们之前在多线程章节讲过 ReentrantReadWriteLock 读写锁的思想非常类似，也就是读读共享、写写互斥、读写互斥、写读互斥。JDK 中提供了 CopyOnWriteArrayList 类比相比于在读写锁的思想又更进一步。为了将读取的性能发挥到极致，CopyOnWriteArrayList 读取是完全不用加锁的，并且更厉害的是：写入也不会阻塞读取操作。只有写入和写入之间需要进行同步等待。这样一来，读操作的性能就会大幅度提升。那它是怎么做的呢？ 3.2 CopyOnWriteArrayList 是如何做到的？ CopyOnWriteArrayList 类的所有可变操作（add，set 等等）都是通过创建底层数组的新副本来实现的。当 List 需要被修改的时候，我并不修改原有内容，而是对原有数据进行一次复制，将修改的内容写入副本。写完之后，再将修改完的副本替换原来的数据，这样就可以保证写操作不会影响读操作了。 从 CopyOnWriteArrayList 的名字就能看出CopyOnWriteArrayList 是满足CopyOnWrite 的 ArrayList，所谓CopyOnWrite 也就是说：在计算机，如果你想要对一块内存进行修改时，我们不在原有内存块中进行写操作，而是将内存拷贝一份，在新的内存中进行写操作，写完之后呢，就将指向原来内存指针指向新的内存，原来的内存就可以被回收掉了。 3.3 CopyOnWriteArrayList 读取和写入源码简单分析 3.3.1 CopyOnWriteArrayList 读取操作的实现 读取操作没有任何同步控制和锁操作，理由就是内部数组 array 不会发生修改，只会被另外一个 array 替换，因此可以保证数据安全。 /** The array, accessed only via getArray/setArray. */ private transient volatile Object[] array; public E get(int index) { return get(getArray(), index); } @SuppressWarnings(\"unchecked\") private E get(Object[] a, int index) { return (E) a[index]; } final Object[] getArray() { return array; } 3.3.2 CopyOnWriteArrayList 写入操作的实现 CopyOnWriteArrayList 写入操作 add() 方法在添加集合的时候加了锁，保证了同步，避免了多线程写的时候会 copy 出多个副本出来。 /** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return {@code true} (as specified by {@link Collection#add}) */ public boolean add(E e) { final ReentrantLock lock = this.lock; lock.lock();//加锁 try { Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1);//拷贝新数组 newElements[len] = e; setArray(newElements); return true; } finally { lock.unlock();//释放锁 } } 四 ConcurrentLinkedQueue Java 提供的线程安全的 Queue 可以分为阻塞队列和非阻塞队列，其中阻塞队列的典型例子是 BlockingQueue，非阻塞队列的典型例子是 ConcurrentLinkedQueue，在实际应用中要根据实际需要选用阻塞队列或者非阻塞队列。 阻塞队列可以通过加锁来实现，非阻塞队列可以通过 CAS 操作实现。 从名字可以看出，ConcurrentLinkedQueue这个队列使用链表作为其数据结构．ConcurrentLinkedQueue 应该算是在高并发环境中性能最好的队列了。它之所有能有很好的性能，是因为其内部复杂的实现。 ConcurrentLinkedQueue 内部代码我们就不分析了，大家知道 ConcurrentLinkedQueue 主要使用 CAS 非阻塞算法来实现线程安全就好了。 ConcurrentLinkedQueue 适合在对性能要求相对较高，同时对队列的读写存在多个线程同时进行的场景，即如果对队列加锁的成本较高则适合使用无锁的 ConcurrentLinkedQueue 来替代。 五 BlockingQueue 5.1 BlockingQueue 简单介绍 上面我们己经提到了 ConcurrentLinkedQueue 作为高性能的非阻塞队列。下面我们要讲到的是阻塞队列——BlockingQueue。阻塞队列（BlockingQueue）被广泛使用在“生产者-消费者”问题中，其原因是 BlockingQueue 提供了可阻塞的插入和移除的方法。当队列容器已满，生产者线程会被阻塞，直到队列未满；当队列容器为空时，消费者线程会被阻塞，直至队列非空时为止。 BlockingQueue 是一个接口，继承自 Queue，所以其实现类也可以作为 Queue 的实现来使用，而 Queue 又继承自 Collection 接口。下面是 BlockingQueue 的相关实现类： 下面主要介绍一下:ArrayBlockingQueue、LinkedBlockingQueue、PriorityBlockingQueue，这三个 BlockingQueue 的实现类。 5.2 ArrayBlockingQueue ArrayBlockingQueue 是 BlockingQueue 接口的有界队列实现类，底层采用数组来实现。ArrayBlockingQueue 一旦创建，容量不能改变。其并发控制采用可重入锁来控制，不管是插入操作还是读取操作，都需要获取到锁才能进行操作。当队列容量满时，尝试将元素放入队列将导致操作阻塞;尝试从一个空队列中取一个元素也会同样阻塞。 ArrayBlockingQueue 默认情况下不能保证线程访问队列的公平性，所谓公平性是指严格按照线程等待的绝对时间顺序，即最先等待的线程能够最先访问到 ArrayBlockingQueue。而非公平性则是指访问 ArrayBlockingQueue 的顺序不是遵守严格的时间顺序，有可能存在，当 ArrayBlockingQueue 可以被访问时，长时间阻塞的线程依然无法访问到 ArrayBlockingQueue。如果保证公平性，通常会降低吞吐量。如果需要获得公平性的 ArrayBlockingQueue，可采用如下代码： private static ArrayBlockingQueue blockingQueue = new ArrayBlockingQueue(10,true); 5.3 LinkedBlockingQueue LinkedBlockingQueue 底层基于单向链表实现的阻塞队列，可以当做无界队列也可以当做有界队列来使用，同样满足 FIFO 的特性，与 ArrayBlockingQueue 相比起来具有更高的吞吐量，为了防止 LinkedBlockingQueue 容量迅速增，损耗大量内存。通常在创建 LinkedBlockingQueue 对象时，会指定其大小，如果未指定，容量等于 Integer.MAX_VALUE。 相关构造方法: /** *某种意义上的无界队列 * Creates a {@code LinkedBlockingQueue} with a capacity of * {@link Integer#MAX_VALUE}. */ public LinkedBlockingQueue() { this(Integer.MAX_VALUE); } /** *有界队列 * Creates a {@code LinkedBlockingQueue} with the given (fixed) capacity. * * @param capacity the capacity of this queue * @throws IllegalArgumentException if {@code capacity} is not greater * than zero */ public LinkedBlockingQueue(int capacity) { if (capacity (null); } 5.4 PriorityBlockingQueue PriorityBlockingQueue 是一个支持优先级的无界阻塞队列。默认情况下元素采用自然顺序进行排序，也可以通过自定义类实现 compareTo() 方法来指定元素排序规则，或者初始化时通过构造器参数 Comparator 来指定排序规则。 PriorityBlockingQueue 并发控制采用的是 ReentrantLock，队列为无界队列（ArrayBlockingQueue 是有界队列，LinkedBlockingQueue 也可以通过在构造函数中传入 capacity 指定队列最大的容量，但是 PriorityBlockingQueue 只能指定初始的队列大小，后面插入元素的时候，如果空间不够的话会自动扩容）。 简单地说，它就是 PriorityQueue 的线程安全版本。不可以插入 null 值，同时，插入队列的对象必须是可比较大小的（comparable），否则报 ClassCastException 异常。它的插入操作 put 方法不会 block，因为它是无界队列（take 方法在队列为空的时候会阻塞）。 推荐文章： 《解读 Java 并发队列 BlockingQueue》 https://javadoop.com/post/java-concurrent-queue 六 ConcurrentSkipListMap 下面这部分内容参考了极客时间专栏《数据结构与算法之美》以及《实战 Java 高并发程序设计》。 为了引出 ConcurrentSkipListMap，先带着大家简单理解一下跳表。 对于一个单链表，即使链表是有序的，如果我们想要在其中查找某个数据，也只能从头到尾遍历链表，这样效率自然就会很低，跳表就不一样了。跳表是一种可以用来快速查找的数据结构，有点类似于平衡树。它们都可以对元素进行快速的查找。但一个重要的区别是：对平衡树的插入和删除往往很可能导致平衡树进行一次全局的调整。而对跳表的插入和删除只需要对整个数据结构的局部进行操作即可。这样带来的好处是：在高并发的情况下，你会需要一个全局锁来保证整个平衡树的线程安全。而对于跳表，你只需要部分锁即可。这样，在高并发环境下，你就可以拥有更好的性能。而就查询的性能而言，跳表的时间复杂度也是 O(logn) 所以在并发数据结构中，JDK 使用跳表来实现一个 Map。 跳表的本质是同时维护了多个链表，并且链表是分层的， 最低层的链表维护了跳表内所有的元素，每上面一层链表都是下面一层的子集。 跳表内的所有链表的元素都是排序的。查找时，可以从顶级链表开始找。一旦发现被查找的元素大于当前链表中的取值，就会转入下一层链表继续找。这也就是说在查找过程中，搜索是跳跃式的。如上图所示，在跳表中查找元素 18。 查找 18 的时候原来需要遍历 18 次，现在只需要 7 次即可。针对链表长度比较大的时候，构建索引查找效率的提升就会非常明显。 从上面很容易看出，跳表是一种利用空间换时间的算法。 使用跳表实现 Map 和使用哈希算法实现 Map 的另外一个不同之处是：哈希并不会保存元素的顺序，而跳表内所有的元素都是排序的。因此在对跳表进行遍历时，你会得到一个有序的结果。所以，如果你的应用需要有序性，那么跳表就是你不二的选择。JDK 中实现这一数据结构的类是 ConcurrentSkipListMap。 七 参考 《实战 Java 高并发程序设计》 https://javadoop.com/post/java-concurrent-queue https://juejin.im/post/5aeebd02518825672f19c546 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java 面试突击》: 由本文档衍生的专为面试而生的《Java 面试突击》V2.0 PDF 版本公众号后台回复 \"面试突击\" 即可免费领取！ Java 工程师必备学习资源: 一些 Java 工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/java/What's New in JDK8/Java8foreach指南.html":{"url":"zother6-JavaGuide/java/What's New in JDK8/Java8foreach指南.html","title":"Java8foreach指南","keywords":"","body":" 本文由 JavaGuide 翻译，原文地址：https://www.baeldung.com/foreach-java 1 概述 在Java 8中引入的forEach循环为程序员提供了一种新的，简洁而有趣的迭代集合的方式。 在本文中，我们将看到如何将forEach与集合一起使用，它采用何种参数以及此循环与增强的for循环的不同之处。 2 基础知识 public interface Collection extends Iterable Collection 接口实现了 Iterable 接口，而 Iterable 接口在 Java 8开始具有一个新的 API： void forEach(Consumer action)//对 Iterable的每个元素执行给定的操作，直到所有元素都被处理或动作引发异常。 使用forEach，我们可以迭代一个集合并对每个元素执行给定的操作，就像任何其他迭代器一样。 例如，迭代和打印字符串集合的for循环版本： for (String name : names) { System.out.println(name); } 我们可以使用forEach写这个 ： names.forEach(name -> { System.out.println(name); }); 3.使用forEach方法 3.1 匿名类 我们使用 forEach迭代集合并对每个元素执行特定操作。要执行的操作包含在实现Consumer接口的类中，并作为参数传递给forEach 。 所述消费者接口是一个功能接口(具有单个抽象方法的接口）。它接受输入并且不返回任何结果。 Consumer 接口定义如下： @FunctionalInterface public interface Consumer { void accept(T t); } 任何实现，例如，只是打印字符串的消费者： Consumer printConsumer = new Consumer() { public void accept(String name) { System.out.println(name); }; }; 可以作为参数传递给forEach： names.forEach(printConsumer); 但这不是通过消费者和使用forEach API 创建操作的唯一方法。让我们看看我们将使用forEach方法的另外2种最流行的方式： 3.2 Lambda表达式 Java 8功能接口的主要优点是我们可以使用Lambda表达式来实例化它们，并避免使用庞大的匿名类实现。 由于 Consumer 接口属于函数式接口，我们可以通过以下形式在Lambda中表达它： (argument) -> { body } name -> System.out.println(name) names.forEach(name -> System.out.println(name)); 3.3 方法参考 我们可以使用方法引用语法而不是普通的Lambda语法，其中已存在一个方法来对类执行操作： names.forEach(System.out::println); 4.forEach在集合中的使用 4.1.迭代集合 任何类型Collection的可迭代 - 列表，集合，队列 等都具有使用forEach的相同语法。 因此，正如我们已经看到的，迭代列表的元素： List names = Arrays.asList(\"Larry\", \"Steve\", \"James\"); names.forEach(System.out::println); 同样对于一组： Set uniqueNames = new HashSet<>(Arrays.asList(\"Larry\", \"Steve\", \"James\")); uniqueNames.forEach(System.out::println); 或者让我们说一个队列也是一个集合： Queue namesQueue = new ArrayDeque<>(Arrays.asList(\"Larry\", \"Steve\", \"James\")); namesQueue.forEach(System.out::println); 4.2.迭代Map - 使用Map的forEach Map没有实现Iterable接口，但它提供了自己的forEach 变体，它接受BiConsumer。* Map namesMap = new HashMap<>(); namesMap.put(1, \"Larry\"); namesMap.put(2, \"Steve\"); namesMap.put(3, \"James\"); namesMap.forEach((key, value) -> System.out.println(key + \" \" + value)); 4.3.迭代一个Map - 通过迭代entrySet namesMap.entrySet().forEach(entry -> System.out.println(entry.getKey() + \" \" + entry.getValue())); "},"zother6-JavaGuide/java/What's New in JDK8/Java8Tutorial.html":{"url":"zother6-JavaGuide/java/What's New in JDK8/Java8Tutorial.html","title":"Java 8 Tutorial","keywords":"","body":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。 随着 Java 8 的普及度越来越高，很多人都提到面试中关于Java 8 也是非常常问的知识点。应各位要求和需要，我打算对这部分知识做一个总结。本来准备自己总结的，后面看到Github 上有一个相关的仓库，地址： https://github.com/winterbe/java8-tutorial。这个仓库是英文的，我对其进行了翻译并添加和修改了部分内容，下面是正文了。 Java 8 Tutorial 接口的默认方法(Default Methods for Interfaces) Lambda表达式(Lambda expressions) 函数式接口(Functional Interfaces) 方法和构造函数引用(Method and Constructor References) Lamda 表达式作用域(Lambda Scopes) 访问局部变量 访问字段和静态变量 访问默认接口方法 内置函数式接口(Built-in Functional Interfaces) Predicates Functions Suppliers Consumers Comparators Optionals Streams(流) Filter(过滤) Sorted(排序) Map(映射) Match(匹配) Count(计数) Reduce(规约) Parallel Streams(并行流) Sequential Sort(串行排序) Parallel Sort(并行排序) Maps Date API(日期相关API) Clock Timezones(时区) LocalTime(本地时间) LocalDate(本地日期) LocalDateTime(本地日期时间) Annotations(注解) Where to go from here? Java 8 Tutorial 欢迎阅读我对Java 8的介绍。本教程将逐步指导您完成所有新语言功能。 在简短的代码示例的基础上，您将学习如何使用默认接口方法，lambda表达式，方法引用和可重复注释。 在本文的最后，您将熟悉最新的 API 更改，如流，函数式接口(Functional Interfaces)，Map 类的扩展和新的 Date API。 没有大段枯燥的文字，只有一堆注释的代码片段。 接口的默认方法(Default Methods for Interfaces) Java 8使我们能够通过使用 default 关键字向接口添加非抽象方法实现。 此功能也称为虚拟扩展方法。 第一个例子： interface Formula{ double calculate(int a); default double sqrt(int a) { return Math.sqrt(a); } } Formula 接口中除了抽象方法计算接口公式还定义了默认方法 sqrt。 实现该接口的类只需要实现抽象方法 calculate。 默认方法sqrt 可以直接使用。当然你也可以直接通过接口创建对象，然后实现接口中的默认方法就可以了，我们通过代码演示一下这种方式。 public class Main { public static void main(String[] args) { // 通过匿名内部类方式访问接口 Formula formula = new Formula() { @Override public double calculate(int a) { return sqrt(a * 100); } }; System.out.println(formula.calculate(100)); // 100.0 System.out.println(formula.sqrt(16)); // 4.0 } } formula 是作为匿名对象实现的。该代码非常容易理解，6行代码实现了计算 sqrt(a * 100)。在下一节中，我们将会看到在 Java 8 中实现单个方法对象有一种更好更方便的方法。 译者注： 不管是抽象类还是接口，都可以通过匿名内部类的方式访问。不能通过抽象类或者接口直接创建对象。对于上面通过匿名内部类方式访问接口，我们可以这样理解：一个内部类实现了接口里的抽象方法并且返回一个内部类对象，之后我们让接口的引用来指向这个对象。 Lambda表达式(Lambda expressions) 首先看看在老版本的Java中是如何排列字符串的： List names = Arrays.asList(\"peter\", \"anna\", \"mike\", \"xenia\"); Collections.sort(names, new Comparator() { @Override public int compare(String a, String b) { return b.compareTo(a); } }); 只需要给静态方法Collections.sort 传入一个 List 对象以及一个比较器来按指定顺序排列。通常做法都是创建一个匿名的比较器对象然后将其传递给 sort 方法。 在Java 8 中你就没必要使用这种传统的匿名对象的方式了，Java 8提供了更简洁的语法，lambda表达式： Collections.sort(names, (String a, String b) -> { return b.compareTo(a); }); 可以看出，代码变得更段且更具有可读性，但是实际上还可以写得更短： Collections.sort(names, (String a, String b) -> b.compareTo(a)); 对于函数体只有一行代码的，你可以去掉大括号{}以及return关键字，但是你还可以写得更短点： names.sort((a, b) -> b.compareTo(a)); List 类本身就有一个 sort 方法。并且Java编译器可以自动推导出参数类型，所以你可以不用再写一次类型。接下来我们看看lambda表达式还有什么其他用法。 函数式接口(Functional Interfaces) 译者注： 原文对这部分解释不太清楚，故做了修改！ Java 语言设计者们投入了大量精力来思考如何使现有的函数友好地支持Lambda。最终采取的方法是：增加函数式接口的概念。“函数式接口”是指仅仅只包含一个抽象方法,但是可以有多个非抽象方法(也就是上面提到的默认方法)的接口。 像这样的接口，可以被隐式转换为lambda表达式。java.lang.Runnable 与 java.util.concurrent.Callable 是函数式接口最典型的两个例子。Java 8增加了一种特殊的注解@FunctionalInterface,但是这个注解通常不是必须的(某些情况建议使用)，只要接口只包含一个抽象方法，虚拟机会自动判断该接口为函数式接口。一般建议在接口上使用@FunctionalInterface 注解进行声明，这样的话，编译器如果发现你标注了这个注解的接口有多于一个抽象方法的时候会报错的，如下图所示 示例： @FunctionalInterface public interface Converter { T convert(F from); } // TODO 将数字字符串转换为整数类型 Converter converter = (from) -> Integer.valueOf(from); Integer converted = converter.convert(\"123\"); System.out.println(converted.getClass()); //class java.lang.Integer 译者注： 大部分函数式接口都不用我们自己写，Java8都给我们实现好了，这些接口都在java.util.function包里。 方法和构造函数引用(Method and Constructor References) 前一节中的代码还可以通过静态方法引用来表示： Converter converter = Integer::valueOf; Integer converted = converter.convert(\"123\"); System.out.println(converted.getClass()); //class java.lang.Integer Java 8允许您通过::关键字传递方法或构造函数的引用。 上面的示例显示了如何引用静态方法。 但我们也可以引用对象方法： class Something { String startsWith(String s) { return String.valueOf(s.charAt(0)); } } Something something = new Something(); Converter converter = something::startsWith; String converted = converter.convert(\"Java\"); System.out.println(converted); // \"J\" 接下来看看构造函数是如何使用::关键字来引用的，首先我们定义一个包含多个构造函数的简单类： class Person { String firstName; String lastName; Person() {} Person(String firstName, String lastName) { this.firstName = firstName; this.lastName = lastName; } } 接下来我们指定一个用来创建Person对象的对象工厂接口： interface PersonFactory { P create(String firstName, String lastName); } 这里我们使用构造函数引用来将他们关联起来，而不是手动实现一个完整的工厂： PersonFactory personFactory = Person::new; Person person = personFactory.create(\"Peter\", \"Parker\"); 我们只需要使用 Person::new 来获取Person类构造函数的引用，Java编译器会自动根据PersonFactory.create方法的参数类型来选择合适的构造函数。 Lamda 表达式作用域(Lambda Scopes) 访问局部变量 我们可以直接在 lambda 表达式中访问外部的局部变量： final int num = 1; Converter stringConverter = (from) -> String.valueOf(from + num); stringConverter.convert(2); // 3 但是和匿名对象不同的是，这里的变量num可以不用声明为final，该代码同样正确： int num = 1; Converter stringConverter = (from) -> String.valueOf(from + num); stringConverter.convert(2); // 3 不过这里的 num 必须不可被后面的代码修改（即隐性的具有final的语义），例如下面的就无法编译： int num = 1; Converter stringConverter = (from) -> String.valueOf(from + num); num = 3;//在lambda表达式中试图修改num同样是不允许的。 访问字段和静态变量 与局部变量相比，我们对lambda表达式中的实例字段和静态变量都有读写访问权限。 该行为和匿名对象是一致的。 class Lambda4 { static int outerStaticNum; int outerNum; void testScopes() { Converter stringConverter1 = (from) -> { outerNum = 23; return String.valueOf(from); }; Converter stringConverter2 = (from) -> { outerStaticNum = 72; return String.valueOf(from); }; } } 访问默认接口方法 还记得第一节中的 formula 示例吗？ Formula 接口定义了一个默认方法sqrt，可以从包含匿名对象的每个 formula 实例访问该方法。 这不适用于lambda表达式。 无法从 lambda 表达式中访问默认方法,故以下代码无法编译： Formula formula = (a) -> sqrt(a * 100); 内置函数式接口(Built-in Functional Interfaces) JDK 1.8 API包含许多内置函数式接口。 其中一些借口在老版本的 Java 中是比较常见的比如： Comparator 或Runnable，这些接口都增加了@FunctionalInterface注解以便能用在 lambda 表达式上。 但是 Java 8 API 同样还提供了很多全新的函数式接口来让你的编程工作更加方便，有一些接口是来自 Google Guava 库里的，即便你对这些很熟悉了，还是有必要看看这些是如何扩展到lambda上使用的。 Predicates Predicate 接口是只有一个参数的返回布尔类型值的 断言型 接口。该接口包含多种默认方法来将 Predicate 组合成其他复杂的逻辑（比如：与，或，非）： 译者注： Predicate 接口源码如下 package java.util.function; import java.util.Objects; @FunctionalInterface public interface Predicate { // 该方法是接受一个传入类型,返回一个布尔值.此方法应用于判断. boolean test(T t); //and方法与关系型运算符\"&&\"相似，两边都成立才返回true default Predicate and(Predicate other) { Objects.requireNonNull(other); return (t) -> test(t) && other.test(t); } // 与关系运算符\"!\"相似，对判断进行取反 default Predicate negate() { return (t) -> !test(t); } //or方法与关系型运算符\"||\"相似，两边只要有一个成立就返回true default Predicate or(Predicate other) { Objects.requireNonNull(other); return (t) -> test(t) || other.test(t); } // 该方法接收一个Object对象,返回一个Predicate类型.此方法用于判断第一个test的方法与第二个test方法相同(equal). static Predicate isEqual(Object targetRef) { return (null == targetRef) ? Objects::isNull : object -> targetRef.equals(object); } 示例： Predicate predicate = (s) -> s.length() > 0; predicate.test(\"foo\"); // true predicate.negate().test(\"foo\"); // false Predicate nonNull = Objects::nonNull; Predicate isNull = Objects::isNull; Predicate isEmpty = String::isEmpty; Predicate isNotEmpty = isEmpty.negate(); Functions Function 接口接受一个参数并生成结果。默认方法可用于将多个函数链接在一起（compose, andThen）： 译者注： Function 接口源码如下 package java.util.function; import java.util.Objects; @FunctionalInterface public interface Function { //将Function对象应用到输入的参数上，然后返回计算结果。 R apply(T t); //将两个Function整合，并返回一个能够执行两个Function对象功能的Function对象。 default Function compose(Function before) { Objects.requireNonNull(before); return (V v) -> apply(before.apply(v)); } // default Function andThen(Function after) { Objects.requireNonNull(after); return (T t) -> after.apply(apply(t)); } static Function identity() { return t -> t; } } Function toInteger = Integer::valueOf; Function backToString = toInteger.andThen(String::valueOf); backToString.apply(\"123\"); // \"123\" Suppliers Supplier 接口产生给定泛型类型的结果。 与 Function 接口不同，Supplier 接口不接受参数。 Supplier personSupplier = Person::new; personSupplier.get(); // new Person Consumers Consumer 接口表示要对单个输入参数执行的操作。 Consumer greeter = (p) -> System.out.println(\"Hello, \" + p.firstName); greeter.accept(new Person(\"Luke\", \"Skywalker\")); Comparators Comparator 是老Java中的经典接口， Java 8在此之上添加了多种默认方法： Comparator comparator = (p1, p2) -> p1.firstName.compareTo(p2.firstName); Person p1 = new Person(\"John\", \"Doe\"); Person p2 = new Person(\"Alice\", \"Wonderland\"); comparator.compare(p1, p2); // > 0 comparator.reversed().compare(p1, p2); // Optionals Optionals不是函数式接口，而是用于防止 NullPointerException 的漂亮工具。这是下一节的一个重要概念，让我们快速了解一下Optionals的工作原理。 Optional 是一个简单的容器，其值可能是null或者不是null。在Java 8之前一般某个函数应该返回非空对象但是有时却什么也没有返回，而在Java 8中，你应该返回 Optional 而不是 null。 译者注：示例中每个方法的作用已经添加。 //of（）：为非null的值创建一个Optional Optional optional = Optional.of(\"bam\"); // isPresent（）： 如果值存在返回true，否则返回false optional.isPresent(); // true //get()：如果Optional有值则将其返回，否则抛出NoSuchElementException optional.get(); // \"bam\" //orElse（）：如果有值则将其返回，否则返回指定的其它值 optional.orElse(\"fallback\"); // \"bam\" //ifPresent（）：如果Optional实例有值则为其调用consumer，否则不做处理 optional.ifPresent((s) -> System.out.println(s.charAt(0))); // \"b\" 推荐阅读：[Java8]如何正确使用Optional Streams(流) java.util.Stream 表示能应用在一组元素上一次执行的操作序列。Stream 操作分为中间操作或者最终操作两种，最终操作返回一特定类型的计算结果，而中间操作返回Stream本身，这样你就可以将多个操作依次串起来。Stream 的创建需要指定一个数据源，比如java.util.Collection 的子类，List 或者 Set， Map 不支持。Stream 的操作可以串行执行或者并行执行。 首先看看Stream是怎么用，首先创建实例代码的用到的数据List： List stringList = new ArrayList<>(); stringList.add(\"ddd2\"); stringList.add(\"aaa2\"); stringList.add(\"bbb1\"); stringList.add(\"aaa1\"); stringList.add(\"bbb3\"); stringList.add(\"ccc\"); stringList.add(\"bbb2\"); stringList.add(\"ddd1\"); Java 8扩展了集合类，可以通过 Collection.stream() 或者 Collection.parallelStream() 来创建一个Stream。下面几节将详细解释常用的Stream操作： Filter(过滤) 过滤通过一个predicate接口来过滤并只保留符合条件的元素，该操作属于中间操作，所以我们可以在过滤后的结果来应用其他Stream操作（比如forEach）。forEach需要一个函数来对过滤后的元素依次执行。forEach是一个最终操作，所以我们不能在forEach之后来执行其他Stream操作。 // 测试 Filter(过滤) stringList .stream() .filter((s) -> s.startsWith(\"a\")) .forEach(System.out::println);//aaa2 aaa1 forEach 是为 Lambda 而设计的，保持了最紧凑的风格。而且 Lambda 表达式本身是可以重用的，非常方便。 Sorted(排序) 排序是一个 中间操作，返回的是排序好后的 Stream。如果你不指定一个自定义的 Comparator 则会使用默认排序。 // 测试 Sort (排序) stringList .stream() .sorted() .filter((s) -> s.startsWith(\"a\")) .forEach(System.out::println);// aaa1 aaa2 需要注意的是，排序只创建了一个排列好后的Stream，而不会影响原有的数据源，排序之后原数据stringCollection是不会被修改的： System.out.println(stringList);// ddd2, aaa2, bbb1, aaa1, bbb3, ccc, bbb2, ddd1 Map(映射) 中间操作 map 会将元素根据指定的 Function 接口来依次将元素转成另外的对象。 下面的示例展示了将字符串转换为大写字符串。你也可以通过map来将对象转换成其他类型，map返回的Stream类型是根据你map传递进去的函数的返回值决定的。 // 测试 Map 操作 stringList .stream() .map(String::toUpperCase) .sorted((a, b) -> b.compareTo(a)) .forEach(System.out::println);// \"DDD2\", \"DDD1\", \"CCC\", \"BBB3\", \"BBB2\", \"AAA2\", \"AAA1\" Match(匹配) Stream提供了多种匹配操作，允许检测指定的Predicate是否匹配整个Stream。所有的匹配操作都是 最终操作 ，并返回一个 boolean 类型的值。 // 测试 Match (匹配)操作 boolean anyStartsWithA = stringList .stream() .anyMatch((s) -> s.startsWith(\"a\")); System.out.println(anyStartsWithA); // true boolean allStartsWithA = stringList .stream() .allMatch((s) -> s.startsWith(\"a\")); System.out.println(allStartsWithA); // false boolean noneStartsWithZ = stringList .stream() .noneMatch((s) -> s.startsWith(\"z\")); System.out.println(noneStartsWithZ); // true Count(计数) 计数是一个 最终操作，返回Stream中元素的个数，返回值类型是 long。 //测试 Count (计数)操作 long startsWithB = stringList .stream() .filter((s) -> s.startsWith(\"b\")) .count(); System.out.println(startsWithB); // 3 Reduce(规约) 这是一个 最终操作 ，允许通过指定的函数来讲stream中的多个元素规约为一个元素，规约后的结果是通过Optional 接口表示的： //测试 Reduce (规约)操作 Optional reduced = stringList .stream() .sorted() .reduce((s1, s2) -> s1 + \"#\" + s2); reduced.ifPresent(System.out::println);//aaa1#aaa2#bbb1#bbb2#bbb3#ccc#ddd1#ddd2 译者注： 这个方法的主要作用是把 Stream 元素组合起来。它提供一个起始值（种子），然后依照运算规则（BinaryOperator），和前面 Stream 的第一个、第二个、第 n 个元素组合。从这个意义上说，字符串拼接、数值的 sum、min、max、average 都是特殊的 reduce。例如 Stream 的 sum 就相当于Integer sum = integers.reduce(0, (a, b) -> a+b);也有没有起始值的情况，这时会把 Stream 的前面两个元素组合起来，返回的是 Optional。 // 字符串连接，concat = \"ABCD\" String concat = Stream.of(\"A\", \"B\", \"C\", \"D\").reduce(\"\", String::concat); // 求最小值，minValue = -3.0 double minValue = Stream.of(-1.5, 1.0, -3.0, -2.0).reduce(Double.MAX_VALUE, Double::min); // 求和，sumValue = 10, 有起始值 int sumValue = Stream.of(1, 2, 3, 4).reduce(0, Integer::sum); // 求和，sumValue = 10, 无起始值 sumValue = Stream.of(1, 2, 3, 4).reduce(Integer::sum).get(); // 过滤，字符串连接，concat = \"ace\" concat = Stream.of(\"a\", \"B\", \"c\", \"D\", \"e\", \"F\"). filter(x -> x.compareTo(\"Z\") > 0). reduce(\"\", String::concat); 上面代码例如第一个示例的 reduce()，第一个参数（空白字符）即为起始值，第二个参数（String::concat）为 BinaryOperator。这类有起始值的 reduce() 都返回具体的对象。而对于第四个示例没有起始值的 reduce()，由于可能没有足够的元素，返回的是 Optional，请留意这个区别。更多内容查看： IBM：Java 8 中的 Streams API 详解 Parallel Streams(并行流) 前面提到过Stream有串行和并行两种，串行Stream上的操作是在一个线程中依次完成，而并行Stream则是在多个线程上同时执行。 下面的例子展示了是如何通过并行Stream来提升性能： 首先我们创建一个没有重复元素的大表： int max = 1000000; List values = new ArrayList<>(max); for (int i = 0; i 我们分别用串行和并行两种方式对其进行排序，最后看看所用时间的对比。 Sequential Sort(串行排序) //串行排序 long t0 = System.nanoTime(); long count = values.stream().sorted().count(); System.out.println(count); long t1 = System.nanoTime(); long millis = TimeUnit.NANOSECONDS.toMillis(t1 - t0); System.out.println(String.format(\"sequential sort took: %d ms\", millis)); 1000000 sequential sort took: 709 ms//串行排序所用的时间 Parallel Sort(并行排序) //并行排序 long t0 = System.nanoTime(); long count = values.parallelStream().sorted().count(); System.out.println(count); long t1 = System.nanoTime(); long millis = TimeUnit.NANOSECONDS.toMillis(t1 - t0); System.out.println(String.format(\"parallel sort took: %d ms\", millis)); 1000000 parallel sort took: 475 ms//串行排序所用的时间 上面两个代码几乎是一样的，但是并行版的快了 50% 左右，唯一需要做的改动就是将 stream() 改为parallelStream()。 Maps 前面提到过，Map 类型不支持 streams，不过Map提供了一些新的有用的方法来处理一些日常任务。Map接口本身没有可用的 stream（）方法，但是你可以在键，值上创建专门的流或者通过 map.keySet().stream(),map.values().stream()和map.entrySet().stream()。 此外,Maps 支持各种新的和有用的方法来执行常见任务。 Map map = new HashMap<>(); for (int i = 0; i System.out.println(val));//val0 val1 val2 val3 val4 val5 val6 val7 val8 val9 putIfAbsent 阻止我们在null检查时写入额外的代码;forEach接受一个 consumer 来对 map 中的每个元素操作。 此示例显示如何使用函数在 map 上计算代码： map.computeIfPresent(3, (num, val) -> val + num); map.get(3); // val33 map.computeIfPresent(9, (num, val) -> null); map.containsKey(9); // false map.computeIfAbsent(23, num -> \"val\" + num); map.containsKey(23); // true map.computeIfAbsent(3, num -> \"bam\"); map.get(3); // val33 接下来展示如何在Map里删除一个键值全都匹配的项： map.remove(3, \"val3\"); map.get(3); // val33 map.remove(3, \"val33\"); map.get(3); // null 另外一个有用的方法： map.getOrDefault(42, \"not found\"); // not found 对Map的元素做合并也变得很容易了： map.merge(9, \"val9\", (value, newValue) -> value.concat(newValue)); map.get(9); // val9 map.merge(9, \"concat\", (value, newValue) -> value.concat(newValue)); map.get(9); // val9concat Merge 做的事情是如果键名不存在则插入，否则则对原键对应的值做合并操作并重新插入到map中。 Date API(日期相关API) Java 8在 java.time 包下包含一个全新的日期和时间API。新的Date API与Joda-Time库相似，但它们不一样。以下示例涵盖了此新 API 的最重要部分。译者对这部分内容参考相关书籍做了大部分修改。 译者注(总结)： Clock 类提供了访问当前日期和时间的方法，Clock 是时区敏感的，可以用来取代 System.currentTimeMillis() 来获取当前的微秒数。某一个特定的时间点也可以使用 Instant 类来表示，Instant 类也可以用来创建旧版本的java.util.Date 对象。 在新API中时区使用 ZoneId 来表示。时区可以很方便的使用静态方法of来获取到。 抽象类ZoneId（在java.time包中）表示一个区域标识符。 它有一个名为getAvailableZoneIds的静态方法，它返回所有区域标识符。 jdk1.8中新增了 LocalDate 与 LocalDateTime等类来解决日期处理方法，同时引入了一个新的类DateTimeFormatter 来解决日期格式化问题。可以使用Instant代替 Date，LocalDateTime代替 Calendar，DateTimeFormatter 代替 SimpleDateFormat。 Clock Clock 类提供了访问当前日期和时间的方法，Clock 是时区敏感的，可以用来取代 System.currentTimeMillis() 来获取当前的微秒数。某一个特定的时间点也可以使用 Instant 类来表示，Instant 类也可以用来创建旧版本的java.util.Date 对象。 Clock clock = Clock.systemDefaultZone(); long millis = clock.millis(); System.out.println(millis);//1552379579043 Instant instant = clock.instant(); System.out.println(instant); Date legacyDate = Date.from(instant); //2019-03-12T08:46:42.588Z System.out.println(legacyDate);//Tue Mar 12 16:32:59 CST 2019 Timezones(时区) 在新API中时区使用 ZoneId 来表示。时区可以很方便的使用静态方法of来获取到。 抽象类ZoneId（在java.time包中）表示一个区域标识符。 它有一个名为getAvailableZoneIds的静态方法，它返回所有区域标识符。 //输出所有区域标识符 System.out.println(ZoneId.getAvailableZoneIds()); ZoneId zone1 = ZoneId.of(\"Europe/Berlin\"); ZoneId zone2 = ZoneId.of(\"Brazil/East\"); System.out.println(zone1.getRules());// ZoneRules[currentStandardOffset=+01:00] System.out.println(zone2.getRules());// ZoneRules[currentStandardOffset=-03:00] LocalTime(本地时间) LocalTime 定义了一个没有时区信息的时间，例如 晚上10点或者 17:30:15。下面的例子使用前面代码创建的时区创建了两个本地时间。之后比较时间并以小时和分钟为单位计算两个时间的时间差： LocalTime now1 = LocalTime.now(zone1); LocalTime now2 = LocalTime.now(zone2); System.out.println(now1.isBefore(now2)); // false long hoursBetween = ChronoUnit.HOURS.between(now1, now2); long minutesBetween = ChronoUnit.MINUTES.between(now1, now2); System.out.println(hoursBetween); // -3 System.out.println(minutesBetween); // -239 LocalTime 提供了多种工厂方法来简化对象的创建，包括解析时间字符串. LocalTime late = LocalTime.of(23, 59, 59); System.out.println(late); // 23:59:59 DateTimeFormatter germanFormatter = DateTimeFormatter .ofLocalizedTime(FormatStyle.SHORT) .withLocale(Locale.GERMAN); LocalTime leetTime = LocalTime.parse(\"13:37\", germanFormatter); System.out.println(leetTime); // 13:37 LocalDate(本地日期) LocalDate 表示了一个确切的日期，比如 2014-03-11。该对象值是不可变的，用起来和LocalTime基本一致。下面的例子展示了如何给Date对象加减天/月/年。另外要注意的是这些对象是不可变的，操作返回的总是一个新实例。 LocalDate today = LocalDate.now();//获取现在的日期 System.out.println(\"今天的日期: \"+today);//2019-03-12 LocalDate tomorrow = today.plus(1, ChronoUnit.DAYS); System.out.println(\"明天的日期: \"+tomorrow);//2019-03-13 LocalDate yesterday = tomorrow.minusDays(2); System.out.println(\"昨天的日期: \"+yesterday);//2019-03-11 LocalDate independenceDay = LocalDate.of(2019, Month.MARCH, 12); DayOfWeek dayOfWeek = independenceDay.getDayOfWeek(); System.out.println(\"今天是周几:\"+dayOfWeek);//TUESDAY 从字符串解析一个 LocalDate 类型和解析 LocalTime 一样简单,下面是使用 DateTimeFormatter 解析字符串的例子： String str1 = \"2014==04==12 01时06分09秒\"; // 根据需要解析的日期、时间字符串定义解析所用的格式器 DateTimeFormatter fomatter1 = DateTimeFormatter .ofPattern(\"yyyy==MM==dd HH时mm分ss秒\"); LocalDateTime dt1 = LocalDateTime.parse(str1, fomatter1); System.out.println(dt1); // 输出 2014-04-12T01:06:09 String str2 = \"2014$$$四月$$$13 20小时\"; DateTimeFormatter fomatter2 = DateTimeFormatter .ofPattern(\"yyy$$$MMM$$$dd HH小时\"); LocalDateTime dt2 = LocalDateTime.parse(str2, fomatter2); System.out.println(dt2); // 输出 2014-04-13T20:00 再来看一个使用 DateTimeFormatter 格式化日期的示例 LocalDateTime rightNow=LocalDateTime.now(); String date=DateTimeFormatter.ISO_LOCAL_DATE_TIME.format(rightNow); System.out.println(date);//2019-03-12T16:26:48.29 DateTimeFormatter formatter=DateTimeFormatter.ofPattern(\"YYYY-MM-dd HH:mm:ss\"); System.out.println(formatter.format(rightNow));//2019-03-12 16:26:48 LocalDateTime(本地日期时间) LocalDateTime 同时表示了时间和日期，相当于前两节内容合并到一个对象上了。LocalDateTime 和 LocalTime还有 LocalDate 一样，都是不可变的。LocalDateTime 提供了一些能访问具体字段的方法。 LocalDateTime sylvester = LocalDateTime.of(2014, Month.DECEMBER, 31, 23, 59, 59); DayOfWeek dayOfWeek = sylvester.getDayOfWeek(); System.out.println(dayOfWeek); // WEDNESDAY Month month = sylvester.getMonth(); System.out.println(month); // DECEMBER long minuteOfDay = sylvester.getLong(ChronoField.MINUTE_OF_DAY); System.out.println(minuteOfDay); // 1439 只要附加上时区信息，就可以将其转换为一个时间点Instant对象，Instant时间点对象可以很容易的转换为老式的java.util.Date。 Instant instant = sylvester .atZone(ZoneId.systemDefault()) .toInstant(); Date legacyDate = Date.from(instant); System.out.println(legacyDate); // Wed Dec 31 23:59:59 CET 2014 格式化LocalDateTime和格式化时间和日期一样的，除了使用预定义好的格式外，我们也可以自己定义格式： DateTimeFormatter formatter = DateTimeFormatter .ofPattern(\"MMM dd, yyyy - HH:mm\"); LocalDateTime parsed = LocalDateTime.parse(\"Nov 03, 2014 - 07:13\", formatter); String string = formatter.format(parsed); System.out.println(string); // Nov 03, 2014 - 07:13 和java.text.NumberFormat不一样的是新版的DateTimeFormatter是不可变的，所以它是线程安全的。 关于时间日期格式的详细信息在这里。 Annotations(注解) 在Java 8中支持多重注解了，先看个例子来理解一下是什么意思。 首先定义一个包装类Hints注解用来放置一组具体的Hint注解： @interface Hints { Hint[] value(); } @Repeatable(Hints.class) @interface Hint { String value(); } Java 8允许我们把同一个类型的注解使用多次，只需要给该注解标注一下@Repeatable即可。 例 1: 使用包装类当容器来存多个注解（老方法） @Hints({@Hint(\"hint1\"), @Hint(\"hint2\")}) class Person {} 例 2：使用多重注解（新方法） @Hint(\"hint1\") @Hint(\"hint2\") class Person {} 第二个例子里java编译器会隐性的帮你定义好@Hints注解，了解这一点有助于你用反射来获取这些信息： Hint hint = Person.class.getAnnotation(Hint.class); System.out.println(hint); // null Hints hints1 = Person.class.getAnnotation(Hints.class); System.out.println(hints1.value().length); // 2 Hint[] hints2 = Person.class.getAnnotationsByType(Hint.class); System.out.println(hints2.length); // 2 即便我们没有在 Person类上定义 @Hints注解，我们还是可以通过 getAnnotation(Hints.class)来获取 @Hints注解，更加方便的方法是使用 getAnnotationsByType 可以直接获取到所有的@Hint注解。 另外Java 8的注解还增加到两种新的target上了： @Target({ElementType.TYPE_PARAMETER, ElementType.TYPE_USE}) @interface MyAnnotation {} Where to go from here? 关于Java 8的新特性就写到这了，肯定还有更多的特性等待发掘。JDK 1.8里还有很多很有用的东西，比如Arrays.parallelSort, StampedLock和CompletableFuture等等。 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \"Java面试突击\" 即可免费领取！ Java工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/java/What's New in JDK8/Java8教程推荐.html":{"url":"zother6-JavaGuide/java/What's New in JDK8/Java8教程推荐.html","title":"Java8教程推荐","keywords":"","body":"书籍 《Java8 In Action》 《写给大忙人看的Java SE 8》 上述书籍的PDF版本见 https://shimo.im/docs/CPB0PK05rP4CFmI2/ 中的 “Java 书籍推荐”。 开源文档 【译】Java 8 简明教程：https://github.com/wizardforcel/modern-java-zh 30 seconds of java8: https://github.com/biezhi/30-seconds-of-java8 视频 尚硅谷 Java 8 新特性 视频资源见： https://shimo.im/docs/CPB0PK05rP4CFmI2/ 。 "},"zother6-JavaGuide/java/BIO-NIO-AIO.html":{"url":"zother6-JavaGuide/java/BIO-NIO-AIO.html","title":"BIO NIO AIO","keywords":"","body":"熟练掌握 BIO,NIO,AIO 的基本概念以及一些常见问题是你准备面试的过程中不可或缺的一部分，另外这些知识点也是你学习 Netty 的基础。 BIO,NIO,AIO 总结 1. BIO (Blocking I/O) 1.1 传统 BIO 1.2 伪异步 IO 1.3 代码示例 1.4 总结 2. NIO (New I/O) 2.1 NIO 简介 2.2 NIO的特性/NIO与IO区别 1)Non-blocking IO（非阻塞IO） 2)Buffer(缓冲区) 3)Channel (通道) 4)Selectors(选择器) 2.3 NIO 读数据和写数据方式 2.4 NIO核心组件简单介绍 2.5 代码示例 3. AIO (Asynchronous I/O) 参考 BIO,NIO,AIO 总结 Java 中的 BIO、NIO和 AIO 理解为是 Java 语言对操作系统的各种 IO 模型的封装。程序员在使用这些 API 的时候，不需要关心操作系统层面的知识，也不需要根据不同操作系统编写不同的代码。只需要使用Java的API就可以了。 在讲 BIO,NIO,AIO 之前先来回顾一下这样几个概念：同步与异步，阻塞与非阻塞。 关于同步和异步的概念解读困扰着很多程序员，大部分的解读都会带有自己的一点偏见。参考了 Stackoverflow相关问题后对原有答案进行了进一步完善： When you execute something synchronously, you wait for it to finish before moving on to another task. When you execute something asynchronously, you can move on to another task before it finishes. 当你同步执行某项任务时，你需要等待其完成才能继续执行其他任务。当你异步执行某些操作时，你可以在完成另一个任务之前继续进行。 同步 ：两个同步任务相互依赖，并且一个任务必须以依赖于另一任务的某种方式执行。 比如在A->B事件模型中，你需要先完成 A 才能执行B。 再换句话说，同步调用种被调用者未处理完请求之前，调用不返回，调用者会一直等待结果的返回。 异步： 两个异步的任务完全独立的，一方的执行不需要等待另外一方的执行。再换句话说，异步调用种一调用就返回结果不需要等待结果返回，当结果返回的时候通过回调函数或者其他方式拿着结果再做相关事情， 阻塞和非阻塞 阻塞： 阻塞就是发起一个请求，调用者一直等待请求结果返回，也就是当前线程会被挂起，无法从事其他任务，只有当条件就绪才能继续。 非阻塞： 非阻塞就是发起一个请求，调用者不用一直等着结果返回，可以先去干其他事情。 如何区分 “同步/异步 ”和 “阻塞/非阻塞” 呢？ 同步/异步是从行为角度描述事物的，而阻塞和非阻塞描述的当前事物的状态（等待调用结果时的状态）。 1. BIO (Blocking I/O) 同步阻塞I/O模式，数据的读取写入必须阻塞在一个线程内等待其完成。 1.1 传统 BIO BIO通信（一请求一应答）模型图如下(图源网络，原出处不明)： 采用 BIO 通信模型 的服务端，通常由一个独立的 Acceptor 线程负责监听客户端的连接。我们一般通过在while(true) 循环中服务端会调用 accept() 方法等待接收客户端的连接的方式监听请求，请求一旦接收到一个连接请求，就可以建立通信套接字在这个通信套接字上进行读写操作，此时不能再接收其他客户端连接请求，只能等待同当前连接的客户端的操作执行完成， 不过可以通过多线程来支持多个客户端的连接，如上图所示。 如果要让 BIO 通信模型 能够同时处理多个客户端请求，就必须使用多线程（主要原因是socket.accept()、socket.read()、socket.write() 涉及的三个主要函数都是同步阻塞的），也就是说它在接收到客户端连接请求之后为每个客户端创建一个新的线程进行链路处理，处理完成之后，通过输出流返回应答给客户端，线程销毁。这就是典型的 一请求一应答通信模型 。我们可以设想一下如果这个连接不做任何事情的话就会造成不必要的线程开销，不过可以通过 线程池机制 改善，线程池还可以让线程的创建和回收成本相对较低。使用FixedThreadPool 可以有效的控制了线程的最大数量，保证了系统有限的资源的控制，实现了N(客户端请求数量):M(处理客户端请求的线程数量)的伪异步I/O模型（N 可以远远大于 M），下面一节\"伪异步 BIO\"中会详细介绍到。 我们再设想一下当客户端并发访问量增加后这种模型会出现什么问题？ 在 Java 虚拟机中，线程是宝贵的资源，线程的创建和销毁成本很高，除此之外，线程的切换成本也是很高的。尤其在 Linux 这样的操作系统中，线程本质上就是一个进程，创建和销毁线程都是重量级的系统函数。如果并发访问量增加会导致线程数急剧膨胀可能会导致线程堆栈溢出、创建新线程失败等问题，最终导致进程宕机或者僵死，不能对外提供服务。 1.2 伪异步 IO 为了解决同步阻塞I/O面临的一个链路需要一个线程处理的问题，后来有人对它的线程模型进行了优化一一一后端通过一个线程池来处理多个客户端的请求接入，形成客户端个数M：线程池最大线程数N的比例关系，其中M可以远远大于N.通过线程池可以灵活地调配线程资源，设置线程的最大值，防止由于海量并发接入导致线程耗尽。 伪异步IO模型图(图源网络，原出处不明)： 采用线程池和任务队列可以实现一种叫做伪异步的 I/O 通信框架，它的模型图如上图所示。当有新的客户端接入时，将客户端的 Socket 封装成一个Task（该任务实现java.lang.Runnable接口）投递到后端的线程池中进行处理，JDK 的线程池维护一个消息队列和 N 个活跃线程，对消息队列中的任务进行处理。由于线程池可以设置消息队列的大小和最大线程数，因此，它的资源占用是可控的，无论多少个客户端并发访问，都不会导致资源的耗尽和宕机。 伪异步I/O通信框架采用了线程池实现，因此避免了为每个请求都创建一个独立线程造成的线程资源耗尽问题。不过因为它的底层仍然是同步阻塞的BIO模型，因此无法从根本上解决问题。 1.3 代码示例 下面代码中演示了BIO通信（一请求一应答）模型。我们会在客户端创建多个线程依次连接服务端并向其发送\"当前时间+:hello world\"，服务端会为每个客户端线程创建一个线程来处理。代码示例出自闪电侠的博客，原地址如下： https://www.jianshu.com/p/a4e03835921a 客户端 /** * * @author 闪电侠 * @date 2018年10月14日 * @Description:客户端 */ public class IOClient { public static void main(String[] args) { // TODO 创建多个线程，模拟多个客户端连接服务端 new Thread(() -> { try { Socket socket = new Socket(\"127.0.0.1\", 3333); while (true) { try { socket.getOutputStream().write((new Date() + \": hello world\").getBytes()); Thread.sleep(2000); } catch (Exception e) { } } } catch (IOException e) { } }).start(); } } 服务端 /** * @author 闪电侠 * @date 2018年10月14日 * @Description: 服务端 */ public class IOServer { public static void main(String[] args) throws IOException { // TODO 服务端处理客户端连接请求 ServerSocket serverSocket = new ServerSocket(3333); // 接收到客户端连接请求之后为每个客户端创建一个新的线程进行链路处理 new Thread(() -> { while (true) { try { // 阻塞方法获取新的连接 Socket socket = serverSocket.accept(); // 每一个新的连接都创建一个线程，负责读取数据 new Thread(() -> { try { int len; byte[] data = new byte[1024]; InputStream inputStream = socket.getInputStream(); // 按字节流方式读取数据 while ((len = inputStream.read(data)) != -1) { System.out.println(new String(data, 0, len)); } } catch (IOException e) { } }).start(); } catch (IOException e) { } } }).start(); } } 1.4 总结 在活动连接数不是特别高（小于单机1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的 I/O 并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。但是，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。因此，我们需要一种更高效的 I/O 处理模型来应对更高的并发量。 2. NIO (New I/O) 2.1 NIO 简介 NIO是一种同步非阻塞的I/O模型，在Java 1.4 中引入了 NIO 框架，对应 java.nio 包，提供了 Channel , Selector，Buffer等抽象。 NIO中的N可以理解为Non-blocking，不单纯是New。它支持面向缓冲的，基于通道的I/O操作方法。 NIO提供了与传统BIO模型中的 Socket 和 ServerSocket 相对应的 SocketChannel 和 ServerSocketChannel 两种不同的套接字通道实现,两种通道都支持阻塞和非阻塞两种模式。阻塞模式使用就像传统中的支持一样，比较简单，但是性能和可靠性都不好；非阻塞模式正好与之相反。对于低负载、低并发的应用程序，可以使用同步阻塞I/O来提升开发速率和更好的维护性；对于高负载、高并发的（网络）应用，应使用 NIO 的非阻塞模式来开发。 2.2 NIO的特性/NIO与IO区别 如果是在面试中回答这个问题，我觉得首先肯定要从 NIO 流是非阻塞 IO 而 IO 流是阻塞 IO 说起。然后，可以从 NIO 的3个核心组件/特性为 NIO 带来的一些改进来分析。如果，你把这些都回答上了我觉得你对于 NIO 就有了更为深入一点的认识，面试官问到你这个问题，你也能很轻松的回答上来了。 1)Non-blocking IO（非阻塞IO） IO流是阻塞的，NIO流是不阻塞的。 Java NIO使我们可以进行非阻塞IO操作。比如说，单线程中从通道读取数据到buffer，同时可以继续做别的事情，当数据读取到buffer中后，线程再继续处理数据。写数据也是一样的。另外，非阻塞写也是如此。一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。 Java IO的各种流是阻塞的。这意味着，当一个线程调用 read() 或 write() 时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了 2)Buffer(缓冲区) IO 面向流(Stream oriented)，而 NIO 面向缓冲区(Buffer oriented)。 Buffer是一个对象，它包含一些要写入或者要读出的数据。在NIO类库中加入Buffer对象，体现了新库与原I/O的一个重要区别。在面向流的I/O中·可以将数据直接写入或者将数据直接读到 Stream 对象中。虽然 Stream 中也有 Buffer 开头的扩展类，但只是流的包装类，还是从流读到缓冲区，而 NIO 却是直接读到 Buffer 中进行操作。 在NIO厍中，所有数据都是用缓冲区处理的。在读取数据时，它是直接读到缓冲区中的; 在写入数据时，写入到缓冲区中。任何时候访问NIO中的数据，都是通过缓冲区进行操作。 最常用的缓冲区是 ByteBuffer,一个 ByteBuffer 提供了一组功能用于操作 byte 数组。除了ByteBuffer,还有其他的一些缓冲区，事实上，每一种Java基本类型（除了Boolean类型）都对应有一种缓冲区。 3)Channel (通道) NIO 通过Channel（通道） 进行读写。 通道是双向的，可读也可写，而流的读写是单向的。无论读写，通道只能和Buffer交互。因为 Buffer，通道可以异步地读写。 4)Selector (选择器) NIO有选择器，而IO没有。 选择器用于使用单个线程处理多个通道。因此，它需要较少的线程来处理这些通道。线程之间的切换对于操作系统来说是昂贵的。 因此，为了提高系统效率选择器是有用的。 2.3 NIO 读数据和写数据方式 通常来说NIO中的所有IO都是从 Channel（通道） 开始的。 从通道进行数据读取 ：创建一个缓冲区，然后请求通道读取数据。 从通道进行数据写入 ：创建一个缓冲区，填充数据，并要求通道写入数据。 数据读取和写入操作图示： 2.4 NIO核心组件简单介绍 NIO 包含下面几个核心的组件： Channel(通道) Buffer(缓冲区) Selector(选择器) 整个NIO体系包含的类远远不止这三个，只能说这三个是NIO体系的“核心API”。我们上面已经对这三个概念进行了基本的阐述，这里就不多做解释了。 2.5 代码示例 代码示例出自闪电侠的博客，原地址如下： https://www.jianshu.com/p/a4e03835921a 客户端 IOClient.java 的代码不变，我们对服务端使用 NIO 进行改造。以下代码较多而且逻辑比较复杂，大家看看就好。 /** * * @author 闪电侠 * @date 2019年2月21日 * @Description: NIO 改造后的服务端 */ public class NIOServer { public static void main(String[] args) throws IOException { // 1. serverSelector负责轮询是否有新的连接，服务端监测到新的连接之后，不再创建一个新的线程， // 而是直接将新连接绑定到clientSelector上，这样就不用 IO 模型中 1w 个 while 循环在死等 Selector serverSelector = Selector.open(); // 2. clientSelector负责轮询连接是否有数据可读 Selector clientSelector = Selector.open(); new Thread(() -> { try { // 对应IO编程中服务端启动 ServerSocketChannel listenerChannel = ServerSocketChannel.open(); listenerChannel.socket().bind(new InetSocketAddress(3333)); listenerChannel.configureBlocking(false); listenerChannel.register(serverSelector, SelectionKey.OP_ACCEPT); while (true) { // 监测是否有新的连接，这里的1指的是阻塞的时间为 1ms if (serverSelector.select(1) > 0) { Set set = serverSelector.selectedKeys(); Iterator keyIterator = set.iterator(); while (keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if (key.isAcceptable()) { try { // (1) 每来一个新连接，不需要创建一个线程，而是直接注册到clientSelector SocketChannel clientChannel = ((ServerSocketChannel) key.channel()).accept(); clientChannel.configureBlocking(false); clientChannel.register(clientSelector, SelectionKey.OP_READ); } finally { keyIterator.remove(); } } } } } } catch (IOException ignored) { } }).start(); new Thread(() -> { try { while (true) { // (2) 批量轮询是否有哪些连接有数据可读，这里的1指的是阻塞的时间为 1ms if (clientSelector.select(1) > 0) { Set set = clientSelector.selectedKeys(); Iterator keyIterator = set.iterator(); while (keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if (key.isReadable()) { try { SocketChannel clientChannel = (SocketChannel) key.channel(); ByteBuffer byteBuffer = ByteBuffer.allocate(1024); // (3) 面向 Buffer clientChannel.read(byteBuffer); byteBuffer.flip(); System.out.println( Charset.defaultCharset().newDecoder().decode(byteBuffer).toString()); } finally { keyIterator.remove(); key.interestOps(SelectionKey.OP_READ); } } } } } } catch (IOException ignored) { } }).start(); } } 为什么大家都不愿意用 JDK 原生 NIO 进行开发呢？从上面的代码中大家都可以看出来，是真的难用！除了编程复杂、编程模型难之外，它还有以下让人诟病的问题： JDK 的 NIO 底层由 epoll 实现，该实现饱受诟病的空轮询 bug 会导致 cpu 飙升 100% 项目庞大之后，自行实现的 NIO 很容易出现各类 bug，维护成本较高，上面这一坨代码我都不能保证没有 bug Netty 的出现很大程度上改善了 JDK 原生 NIO 所存在的一些让人难以忍受的问题。 3. AIO (Asynchronous I/O) AIO 也就是 NIO 2。在 Java 7 中引入了 NIO 的改进版 NIO 2,它是异步非阻塞的IO模型。异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。 AIO 是异步IO的缩写，虽然 NIO 在网络操作中，提供了非阻塞的方法，但是 NIO 的 IO 行为还是同步的。对于 NIO 来说，我们的业务线程是在 IO 操作准备好时，得到通知，接着就由这个线程自行进行 IO 操作，IO操作本身是同步的。（除了 AIO 其他的 IO 类型都是同步的，这一点可以从底层IO线程模型解释，推荐一篇文章：《漫话：如何给女朋友解释什么是Linux的五种IO模型？》 ） 查阅网上相关资料，我发现就目前来说 AIO 的应用还不是很广泛，Netty 之前也尝试使用过 AIO，不过又放弃了。 参考 《Netty 权威指南》第二版 https://zhuanlan.zhihu.com/p/23488863 (美团技术团队) "},"zother6-JavaGuide/java/J2EE基础知识.html":{"url":"zother6-JavaGuide/java/J2EE基础知识.html","title":"J2EE基础知识","keywords":"","body":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。 Servlet总结 阐述Servlet和CGI的区别? CGI的不足之处: Servlet的优点： Servlet接口中有哪些方法及Servlet生命周期探秘 get和post请求的区别 什么情况下调用doGet()和doPost() 转发（Forward）和重定向（Redirect）的区别 自动刷新(Refresh) Servlet与线程安全 JSP和Servlet是什么关系 JSP工作原理 JSP有哪些内置对象、作用分别是什么 Request对象的主要方法有哪些 request.getAttribute()和 request.getParameter()有何区别 include指令include的行为的区别 JSP九大内置对象，七大动作，三大指令 讲解JSP中的四种作用域 如何实现JSP或Servlet的单线程模式 实现会话跟踪的技术有哪些 Cookie和Session的的区别 Servlet总结 在Java Web程序中，Servlet主要负责接收用户请求 HttpServletRequest,在doGet(),doPost()中做相应的处理，并将回应HttpServletResponse反馈给用户。Servlet 可以设置初始化参数，供Servlet内部使用。一个Servlet类只会有一个实例，在它初始化时调用init()方法，销毁时调用destroy()方法。Servlet需要在web.xml中配置（MyEclipse中创建Servlet会自动配置），一个Servlet可以设置多个URL访问。Servlet不是线程安全，因此要谨慎使用类变量。 阐述Servlet和CGI的区别? CGI的不足之处: 1，需要为每个请求启动一个操作CGI程序的系统进程。如果请求频繁，这将会带来很大的开销。 2，需要为每个请求加载和运行一个CGI程序，这将带来很大的开销 3，需要重复编写处理网络协议的代码以及编码，这些工作都是非常耗时的。 Servlet的优点: 1，只需要启动一个操作系统进程以及加载一个JVM，大大降低了系统的开销 2，如果多个请求需要做同样处理的时候，这时候只需要加载一个类，这也大大降低了开销 3，所有动态加载的类可以实现对网络协议以及请求解码的共享，大大降低了工作量。 4，Servlet能直接和Web服务器交互，而普通的CGI程序不能。Servlet还能在各个程序之间共享数据，使数据库连接池之类的功能很容易实现。 补充：Sun Microsystems公司在1996年发布Servlet技术就是为了和CGI进行竞争，Servlet是一个特殊的Java程序，一个基于Java的Web应用通常包含一个或多个Servlet类。Servlet不能够自行创建并执行，它是在Servlet容器中运行的，容器将用户的请求传递给Servlet程序，并将Servlet的响应回传给用户。通常一个Servlet会关联一个或多个JSP页面。以前CGI经常因为性能开销上的问题被诟病，然而Fast CGI早就已经解决了CGI效率上的问题，所以面试的时候大可不必信口开河的诟病CGI，事实上有很多你熟悉的网站都使用了CGI技术。 参考：《javaweb整合开发王者归来》P7 Servlet接口中有哪些方法及Servlet生命周期探秘 Servlet接口定义了5个方法，其中前三个方法与Servlet生命周期相关： void init(ServletConfig config) throws ServletException void service(ServletRequest req, ServletResponse resp) throws ServletException, java.io.IOException void destroy() java.lang.String getServletInfo() ServletConfig getServletConfig() 生命周期： Web容器加载Servlet并将其实例化后，Servlet生命周期开始，容器运行其init()方法进行Servlet的初始化；请求到达时调用Servlet的service()方法，service()方法会根据需要调用与请求对应的doGet或doPost等方法；当服务器关闭或项目被卸载时服务器会将Servlet实例销毁，此时会调用Servlet的destroy()方法。init方法和destroy方法只会执行一次，service方法客户端每次请求Servlet都会执行。Servlet中有时会用到一些需要初始化与销毁的资源，因此可以把初始化资源的代码放入init方法中，销毁资源的代码放入destroy方法中，这样就不需要每次处理客户端的请求都要初始化与销毁资源。 参考：《javaweb整合开发王者归来》P81 get和post请求的区别 get和post请求实际上是没有区别，大家可以自行查询相关文章（参考文章：https://www.cnblogs.com/logsharing/p/8448446.html，知乎对应的问题链接：get和post区别？）！ 可以把 get 和 post 当作两个不同的行为，两者并没有什么本质区别，底层都是 TCP 连接。 get请求用来从服务器上获得资源，而post是用来向服务器提交数据。比如你要获取人员列表可以用 get 请求，你需要创建一个人员可以用 post 。这也是 Restful API 最基本的一个要求。 推荐阅读： https://www.zhihu.com/question/28586791 https://mp.weixin.qq.com/s?__biz=MzI3NzIzMzg3Mw==&mid=100000054&idx=1&sn=71f6c214f3833d9ca20b9f7dcd9d33e4#rd 什么情况下调用doGet()和doPost() Form标签里的method的属性为get时调用doGet()，为post时调用doPost()。 转发(Forward)和重定向(Redirect)的区别 转发是服务器行为，重定向是客户端行为。 转发（Forward） 通过RequestDispatcher对象的forward（HttpServletRequest request,HttpServletResponse response）方法实现的。RequestDispatcher可以通过HttpServletRequest 的getRequestDispatcher()方法获得。例如下面的代码就是跳转到login_success.jsp页面。 request.getRequestDispatcher(\"login_success.jsp\").forward(request, response); 重定向（Redirect） 是利用服务器返回的状态码来实现的。客户端浏览器请求服务器的时候，服务器会返回一个状态码。服务器通过 HttpServletResponse 的 setStatus(int status) 方法设置状态码。如果服务器返回301或者302，则浏览器会到新的网址重新请求该资源。 从地址栏显示来说 forward是服务器请求资源,服务器直接访问目标地址的URL,把那个URL的响应内容读取过来,然后把这些内容再发给浏览器.浏览器根本不知道服务器发送的内容从哪里来的,所以它的地址栏还是原来的地址. redirect是服务端根据逻辑,发送一个状态码,告诉浏览器重新去请求那个地址.所以地址栏显示的是新的URL. 从数据共享来说 forward:转发页面和转发到的页面可以共享request里面的数据. redirect:不能共享数据. 从运用地方来说 forward:一般用于用户登陆的时候,根据角色转发到相应的模块. redirect:一般用于用户注销登陆时返回主页面和跳转到其它的网站等 从效率来说 forward:高. redirect:低. 自动刷新(Refresh) 自动刷新不仅可以实现一段时间之后自动跳转到另一个页面，还可以实现一段时间之后自动刷新本页面。Servlet中通过HttpServletResponse对象设置Header属性实现自动刷新例如： Response.setHeader(\"Refresh\",\"5;URL=http://localhost:8080/servlet/example.htm\"); 其中5为时间，单位为秒。URL指定就是要跳转的页面（如果设置自己的路径，就会实现每过5秒自动刷新本页面一次） Servlet与线程安全 Servlet不是线程安全的，多线程并发的读写会导致数据不同步的问题。 解决的办法是尽量不要定义name属性，而是要把name变量分别定义在doGet()和doPost()方法内。虽然使用synchronized(name){}语句块可以解决问题，但是会造成线程的等待，不是很科学的办法。 注意：多线程的并发的读写Servlet类属性会导致数据不同步。但是如果只是并发地读取属性而不写入，则不存在数据不同步的问题。因此Servlet里的只读属性最好定义为final类型的。 参考：《javaweb整合开发王者归来》P92 JSP和Servlet是什么关系 其实这个问题在上面已经阐述过了，Servlet是一个特殊的Java程序，它运行于服务器的JVM中，能够依靠服务器的支持向浏览器提供显示内容。JSP本质上是Servlet的一种简易形式，JSP会被服务器处理成一个类似于Servlet的Java程序，可以简化页面内容的生成。Servlet和JSP最主要的不同点在于，Servlet的应用逻辑是在Java文件中，并且完全从表示层中的HTML分离开来。而JSP的情况是Java和HTML可以组合成一个扩展名为.jsp的文件。有人说，Servlet就是在Java中写HTML，而JSP就是在HTML中写Java代码，当然这个说法是很片面且不够准确的。JSP侧重于视图，Servlet更侧重于控制逻辑，在MVC架构模式中，JSP适合充当视图（view）而Servlet适合充当控制器（controller）。 JSP工作原理 JSP是一种Servlet，但是与HttpServlet的工作方式不太一样。HttpServlet是先由源代码编译为class文件后部署到服务器下，为先编译后部署。而JSP则是先部署后编译。JSP会在客户端第一次请求JSP文件时被编译为HttpJspPage类（接口Servlet的一个子类）。该类会被服务器临时存放在服务器工作目录里面。下面通过实例给大家介绍。 工程JspLoginDemo下有一个名为login.jsp的Jsp文件，把工程第一次部署到服务器上后访问这个Jsp文件，我们发现这个目录下多了下图这两个东东。 .class文件便是JSP对应的Servlet。编译完毕后再运行class文件来响应客户端请求。以后客户端访问login.jsp的时候，Tomcat将不再重新编译JSP文件，而是直接调用class文件来响应客户端请求。 由于JSP只会在客户端第一次请求的时候被编译 ，因此第一次请求JSP时会感觉比较慢，之后就会感觉快很多。如果把服务器保存的class文件删除，服务器也会重新编译JSP。 开发Web程序时经常需要修改JSP。Tomcat能够自动检测到JSP程序的改动。如果检测到JSP源代码发生了改动。Tomcat会在下次客户端请求JSP时重新编译JSP，而不需要重启Tomcat。这种自动检测功能是默认开启的，检测改动会消耗少量的时间，在部署Web应用的时候可以在web.xml中将它关掉。 参考：《javaweb整合开发王者归来》P97 JSP有哪些内置对象、作用分别是什么 JSP内置对象 - CSDN博客 JSP有9个内置对象： request：封装客户端的请求，其中包含来自GET或POST请求的参数； response：封装服务器对客户端的响应； pageContext：通过该对象可以获取其他对象； session：封装用户会话的对象； application：封装服务器运行环境的对象； out：输出服务器响应的输出流对象； config：Web应用的配置对象； page：JSP页面本身（相当于Java程序中的this）； exception：封装页面抛出异常的对象。 Request对象的主要方法有哪些 setAttribute(String name,Object)：设置名字为name的request 的参数值 getAttribute(String name)：返回由name指定的属性值 getAttributeNames()：返回request 对象所有属性的名字集合，结果是一个枚举的实例 getCookies()：返回客户端的所有 Cookie 对象，结果是一个Cookie 数组 getCharacterEncoding() ：返回请求中的字符编码方式 = getContentLength() ：返回请求的 Body的长度 getHeader(String name) ：获得HTTP协议定义的文件头信息 getHeaders(String name) ：返回指定名字的request Header 的所有值，结果是一个枚举的实例 getHeaderNames() ：返回所以request Header 的名字，结果是一个枚举的实例 getInputStream() ：返回请求的输入流，用于获得请求中的数据 getMethod() ：获得客户端向服务器端传送数据的方法 getParameter(String name) ：获得客户端传送给服务器端的有 name指定的参数值 getParameterNames() ：获得客户端传送给服务器端的所有参数的名字，结果是一个枚举的实例 getParameterValues(String name)：获得有name指定的参数的所有值 getProtocol()：获取客户端向服务器端传送数据所依据的协议名称 getQueryString() ：获得查询字符串 getRequestURI() ：获取发出请求字符串的客户端地址 getRemoteAddr()：获取客户端的 IP 地址 getRemoteHost() ：获取客户端的名字 getSession([Boolean create]) ：返回和请求相关 Session getServerName() ：获取服务器的名字 getServletPath()：获取客户端所请求的脚本文件的路径 getServerPort()：获取服务器的端口号 removeAttribute(String name)：删除请求中的一个属性 request.getAttribute()和 request.getParameter()有何区别 从获取方向来看： getParameter()是获取 POST/GET 传递的参数值； getAttribute()是获取对象容器中的数据值； 从用途来看： getParameter()用于客户端重定向时，即点击了链接或提交按扭时传值用，即用于在用表单或url重定向传值时接收数据用。 getAttribute() 用于服务器端重定向时，即在 sevlet 中使用了 forward 函数,或 struts 中使用了 mapping.findForward。 getAttribute 只能收到程序用 setAttribute 传过来的值。 另外，可以用 setAttribute(),getAttribute() 发送接收对象.而 getParameter() 显然只能传字符串。 setAttribute() 是应用服务器把这个对象放在该页面所对应的一块内存中去，当你的页面服务器重定向到另一个页面时，应用服务器会把这块内存拷贝另一个页面所对应的内存中。这样getAttribute()就能取得你所设下的值，当然这种方法可以传对象。session也一样，只是对象在内存中的生命周期不一样而已。getParameter()只是应用服务器在分析你送上来的 request页面的文本时，取得你设在表单或 url 重定向时的值。 总结： getParameter()返回的是String,用于读取提交的表单中的值;（获取之后会根据实际需要转换为自己需要的相应类型，比如整型，日期类型啊等等） getAttribute()返回的是Object，需进行转换,可用setAttribute()设置成任意对象，使用很灵活，可随时用 include指令include的行为的区别 include指令： JSP可以通过include指令来包含其他文件。被包含的文件可以是JSP文件、HTML文件或文本文件。包含的文件就好像是该JSP文件的一部分，会被同时编译执行。 语法格式如下： include动作： 动作元素用来包含静态和动态的文件。该动作把指定文件插入正在生成的页面。语法格式如下： JSP九大内置对象，七大动作，三大指令 JSP九大内置对象，七大动作，三大指令总结 讲解JSP中的四种作用域 JSP中的四种作用域包括page、request、session和application，具体来说： page代表与一个页面相关的对象和属性。 request代表与Web客户机发出的一个请求相关的对象和属性。一个请求可能跨越多个页面，涉及多个Web组件；需要在页面显示的临时数据可以置于此作用域。 session代表与某个用户与服务器建立的一次会话相关的对象和属性。跟某个用户相关的数据应该放在用户自己的session中。 application代表与整个Web应用程序相关的对象和属性，它实质上是跨越整个Web应用程序，包括多个页面、请求和会话的一个全局作用域。 如何实现JSP或Servlet的单线程模式 对于JSP页面，可以通过page指令进行设置。 对于Servlet，可以让自定义的Servlet实现SingleThreadModel标识接口。 说明：如果将JSP或Servlet设置成单线程工作模式，会导致每个请求创建一个Servlet实例，这种实践将导致严重的性能问题（服务器的内存压力很大，还会导致频繁的垃圾回收），所以通常情况下并不会这么做。 实现会话跟踪的技术有哪些 使用Cookie 向客户端发送Cookie Cookie c =new Cookie(\"name\",\"value\"); //创建Cookie c.setMaxAge(60*60*24); //设置最大时效，此处设置的最大时效为一天 response.addCookie(c); //把Cookie放入到HTTP响应中 从客户端读取Cookie String name =\"name\"; Cookie[]cookies =request.getCookies(); if(cookies !=null){ for(int i= 0;i 优点: 数据可以持久保存，不需要服务器资源，简单，基于文本的Key-Value 缺点: 大小受到限制，用户可以禁用Cookie功能，由于保存在本地，有一定的安全风险。 URL 重写 在URL中添加用户会话的信息作为请求的参数，或者将唯一的会话ID添加到URL结尾以标识一个会话。 优点： 在Cookie被禁用的时候依然可以使用 缺点： 必须对网站的URL进行编码，所有页面必须动态生成，不能用预先记录下来的URL进行访问。 3.隐藏的表单域 优点： Cookie被禁时可以使用 缺点： 所有页面必须是表单提交之后的结果。 HttpSession 在所有会话跟踪技术中，HttpSession对象是最强大也是功能最多的。当一个用户第一次访问某个网站时会自动创建 HttpSession，每个用户可以访问他自己的HttpSession。可以通过HttpServletRequest对象的getSession方 法获得HttpSession，通过HttpSession的setAttribute方法可以将一个值放在HttpSession中，通过调用 HttpSession对象的getAttribute方法，同时传入属性名就可以获取保存在HttpSession中的对象。与上面三种方式不同的 是，HttpSession放在服务器的内存中，因此不要将过大的对象放在里面，即使目前的Servlet容器可以在内存将满时将HttpSession 中的对象移到其他存储设备中，但是这样势必影响性能。添加到HttpSession中的值可以是任意Java对象，这个对象最好实现了 Serializable接口，这样Servlet容器在必要的时候可以将其序列化到文件中，否则在序列化时就会出现异常。 Cookie和Session的的区别 Cookie 和 Session都是用来跟踪浏览器用户身份的会话方式，但是两者的应用场景不太一样。 Cookie 一般用来保存用户信息 比如①我们在 Cookie 中保存已经登录过得用户信息，下次访问网站的时候页面可以自动帮你登录的一些基本信息给填了；②一般的网站都会有保持登录也就是说下次你再访问网站的时候就不需要重新登录了，这是因为用户登录的时候我们可以存放了一个 Token 在 Cookie 中，下次登录的时候只需要根据 Token 值来查找用户即可(为了安全考虑，重新登录一般要将 Token 重写)；③登录一次网站后访问网站其他页面不需要重新登录。Session 的主要作用就是通过服务端记录用户的状态。 典型的场景是购物车，当你要添加商品到购物车的时候，系统不知道是哪个用户操作的，因为 HTTP 协议是无状态的。服务端给特定的用户创建特定的 Session 之后就可以标识这个用户并且跟踪这个用户了。 Cookie 数据保存在客户端(浏览器端)，Session 数据保存在服务器端。 Cookie 存储在客户端中，而Session存储在服务器上，相对来说 Session 安全性更高。如果使用 Cookie 的一些敏感信息不要写入 Cookie 中，最好能将 Cookie 信息加密然后使用到的时候再去服务器端解密。 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \"Java面试突击\" 即可免费领取！ Java工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/java/JAD反编译tricks.html":{"url":"zother6-JavaGuide/java/JAD反编译tricks.html","title":"JAD反编译tricks","keywords":"","body":"jad反编译工具，已经不再更新，且只支持JDK1.4，但并不影响其强大的功能。 基本用法：jad xxx.class，会生成直接可读的xxx.jad文件。 自动拆装箱 对于基本类型和包装类型之间的转换，通过xxxValue()和valueOf()两个方法完成自动拆装箱，使用jad进行反编译可以看到该过程： public class Demo { public static void main(String[] args) { int x = new Integer(10); // 自动拆箱 Integer y = x; // 自动装箱 } } 反编译后结果： public class Demo { public Demo(){} public static void main(String args[]) { int i = (new Integer(10)).intValue(); // intValue()拆箱 Integer integer = Integer.valueOf(i); // valueOf()装箱 } } foreach语法糖 在遍历迭代时可以foreach语法糖，对于数组类型直接转换成for循环： // 原始代码 int[] arr = {1, 2, 3, 4, 5}; for(int item: arr) { System.out.println(item); } } // 反编译后代码 int ai[] = { 1, 2, 3, 4, 5 }; int ai1[] = ai; int i = ai1.length; // 转换成for循环 for(int j = 0; j 对于容器类的遍历会使用iterator进行迭代： import java.io.PrintStream; import java.util.*; public class Demo { public Demo() {} public static void main(String args[]) { ArrayList arraylist = new ArrayList(); arraylist.add(Integer.valueOf(1)); arraylist.add(Integer.valueOf(2)); arraylist.add(Integer.valueOf(3)); Integer integer; // 使用的for循环+Iterator，类似于链表迭代： // for (ListNode cur = head; cur != null; System.out.println(cur.val)){ // cur = cur.next; // } for(Iterator iterator = arraylist.iterator(); iterator.hasNext(); System.out.println(integer)) integer = (Integer)iterator.next(); } } Arrays.asList(T...) 熟悉Arrays.asList(T...)用法的小伙伴都应该知道，asList()方法传入的参数不能是基本类型的数组，必须包装成包装类型再使用，否则对应生成的列表的大小永远是1： import java.util.*; public class Demo { public static void main(String[] args) { int[] arr1 = {1, 2, 3}; Integer[] arr2 = {1, 2, 3}; List lists1 = Arrays.asList(arr1); List lists2 = Arrays.asList(arr2); System.out.println(lists1.size()); // 1 System.out.println(lists2.size()); // 3 } } 从反编译结果来解释，为什么传入基本类型的数组后，返回的List大小是1： // 反编译后文件 import java.io.PrintStream; import java.util.Arrays; import java.util.List; public class Demo { public Demo() {} public static void main(String args[]) { int ai[] = { 1, 2, 3 }; // 使用包装类型，全部元素由int包装为Integer Integer ainteger[] = { Integer.valueOf(1), Integer.valueOf(2), Integer.valueOf(3) }; // 注意这里被反编译成二维数组，而且是一个1行三列的二维数组 // list.size()当然返回1 List list = Arrays.asList(new int[][] { ai }); List list1 = Arrays.asList(ainteger); System.out.println(list.size()); System.out.println(list1.size()); } } 从上面结果可以看到，传入基本类型的数组后，会被转换成一个二维数组，而且是new int[1][arr.length]这样的数组，调用list.size()当然返回1。 注解 Java中的类、接口、枚举、注解都可以看做是类类型。使用jad来看一下@interface被转换成什么： import java.lang.annotation.Retention; import java.lang.annotation.RetentionPolicy; @Retention(RetentionPolicy.RUNTIME) public @interface Foo{ String[] value(); boolean bar(); } 查看反编译代码可以看出： 自定义的注解类Foo被转换成接口Foo，并且继承Annotation接口 原来自定义接口中的value()和bar()被转换成抽象方法 import java.lang.annotation.Annotation; public interface Foo extends Annotation { public abstract String[] value(); public abstract boolean bar(); } 注解通常和反射配合使用，而且既然自定义的注解最终被转换成接口，注解中的属性被转换成接口中的抽象方法，那么通过反射之后拿到接口实例，在通过接口实例自然能够调用对应的抽象方法： import java.util.Arrays; @Foo(value={\"sherman\", \"decompiler\"}, bar=true) public class Demo{ public static void main(String[] args) { Foo foo = Demo.class.getAnnotation(Foo.class); System.out.println(Arrays.toString(foo.value())); // [sherman, decompiler] System.out.println(foo.bar()); // true } } 枚举 通过jad反编译可以很好地理解枚举类。 空枚举 先定义一个空的枚举类： public enum DummyEnum { } 使用jad反编译查看结果： 自定义枚举类被转换成final类，并且继承Enum 提供了两个参数（name，odinal）的私有构造器，并且调用了父类的构造器。注意即使没有提供任何参数，也会有该该构造器，其中name就是枚举实例的名称，odinal是枚举实例的索引号 初始化了一个private static final自定义类型的空数组 $VALUES 提供了两个public static方法： values()方法通过clone()方法返回内部$VALUES的浅拷贝。这个方法结合私有构造器可以完美实现单例模式，想一想values()方法是不是和单例模式中getInstance()方法功能类似 valueOf(String s)：调用父类Enum的valueOf方法并强转返回 public final class DummyEnum extends Enum { // 功能和单例模式的getInstance()方法相同 public static DummyEnum[] values() { return (DummyEnum[])$VALUES.clone(); } // 调用父类的valueOf方法，并墙砖返回 public static DummyEnum valueOf(String s) { return (DummyEnum)Enum.valueOf(DummyEnum, s); } // 默认提供一个私有的私有两个参数的构造器，并调用父类Enum的构造器 private DummyEnum(String s, int i) { super(s, i); } // 初始化一个private static final的本类空数组 private static final DummyEnum $VALUES[] = new DummyEnum[0]; } 包含抽象方法的枚举 枚举类中也可以包含抽象方法，但是必须定义枚举实例并且立即重写抽象方法，就像下面这样： public enum DummyEnum { DUMMY1 { public void dummyMethod() { System.out.println(\"[1]: implements abstract method in enum class\"); } }, DUMMY2 { public void dummyMethod() { System.out.println(\"[2]: implements abstract method in enum class\"); } }; abstract void dummyMethod(); } 再来反编译看看有哪些变化： 原来final class变成了abstract class：这很好理解，有抽象方法的类自然是抽象类 多了两个public static final的成员DUMMY1、DUMMY2，这两个实例的初始化过程被放到了static代码块中，并且实例过程中直接重写了抽象方法，类似于匿名内部类的形式。 数组$VALUES[]初始化时放入枚举实例 还有其它变化么？ 在反编译后的DummyEnum类中，是存在抽象方法的，而枚举实例在静态代码块中初始化过程中重写了抽象方法。在Java中，抽象方法和抽象方法重写同时放在一个类中，只能通过内部类形式完成。因此上面第二点应该说成就是以内部类形式初始化。 可以看一下DummyEnum.class存放的位置，应该多了两个文件： DummyEnum$1.class DummyEnum$2.class Java中.class文件出现$符号表示有内部类存在，就像OutClass$InnerClass，这两个文件出现也应证了上面的匿名内部类初始化的说法。 import java.io.PrintStream; public abstract class DummyEnum extends Enum { public static DummyEnum[] values() { return (DummyEnum[])$VALUES.clone(); } public static DummyEnum valueOf(String s) { return (DummyEnum)Enum.valueOf(DummyEnum, s); } private DummyEnum(String s, int i) { super(s, i); } // 抽象方法 abstract void dummyMethod(); // 两个pubic static final实例 public static final DummyEnum DUMMY1; public static final DummyEnum DUMMY2; private static final DummyEnum $VALUES[]; // static代码块进行初始化 static { DUMMY1 = new DummyEnum(\"DUMMY1\", 0) { public void dummyMethod() { System.out.println(\"[1]: implements abstract method in enum class\"); } } ; DUMMY2 = new DummyEnum(\"DUMMY2\", 1) { public void dummyMethod() { System.out.println(\"[2]: implements abstract method in enum class\"); } } ; // 对本类数组进行初始化 $VALUES = (new DummyEnum[] { DUMMY1, DUMMY2 }); } } 正常的枚举类 实际开发中，枚举类通常的形式是有两个参数（int code，Sring msg）的构造器，可以作为状态码进行返回。Enum类实际上也是提供了包含两个参数且是protected的构造器，这里为了避免歧义，将枚举类的构造器设置为三个，使用jad反编译： 最大的变化是：现在的private构造器从2个参数变成5个，而且在内部仍然将前两个参数通过super传递给父类，剩余的三个参数才是真正自己提供的参数。可以想象，如果自定义的枚举类只提供了一个参数，最终生成底层代码中private构造器应该有三个参数，前两个依然通过super传递给父类。 public final class CustomEnum extends Enum { public static CustomEnum[] values() { return (CustomEnum[])$VALUES.clone(); } public static CustomEnum valueOf(String s) { return (CustomEnum)Enum.valueOf(CustomEnum, s); } private CustomEnum(String s, int i, int j, String s1, Object obj) { super(s, i); code = j; msg = s1; data = obj; } public static final CustomEnum FIRST; public static final CustomEnum SECOND; public static final CustomEnum THIRD; private int code; private String msg; private Object data; private static final CustomEnum $VALUES[]; static { FIRST = new CustomEnum(\"FIRST\", 0, 10010, \"first\", Long.valueOf(100L)); SECOND = new CustomEnum(\"SECOND\", 1, 10020, \"second\", \"Foo\"); THIRD = new CustomEnum(\"THIRD\", 2, 10030, \"third\", new Object()); $VALUES = (new CustomEnum[] { FIRST, SECOND, THIRD }); } } "},"zother6-JavaGuide/java/Java IO与NIO.html":{"url":"zother6-JavaGuide/java/Java IO与NIO.html","title":"Java IO与NIO","keywords":"","body":" IO流学习总结 一　Java IO，硬骨头也能变软 二　java IO体系的学习总结 三　Java IO面试题 NIO与AIO学习总结 一 Java NIO 概览 二 Java NIO 之 Buffer(缓冲区) 三 Java NIO 之 Channel（通道） 四 Java NIO之Selector（选择器） 五 Java NIO之拥抱Path和Files 六 NIO学习总结以及NIO新特性介绍 七 Java NIO AsynchronousFileChannel异步文件通 八 高并发Java（8）：NIO和AIO 推荐阅读 在 Java 7 中体会 NIO.2 异步执行的快乐 Java AIO总结与示例 IO流学习总结 一　Java IO，硬骨头也能变软 （1） 按操作方式分类结构图： （2）按操作对象分类结构图 二　java IO体系的学习总结 IO流的分类： 按照流的流向分，可以分为输入流和输出流； 按照操作单元划分，可以划分为字节流和字符流； 按照流的角色划分为节点流和处理流。 流的原理浅析: java Io流共涉及40多个类，这些类看上去很杂乱，但实际上很有规则，而且彼此之间存在非常紧密的联系， Java Io流的40多个类都是从如下4个抽象类基类中派生出来的。 InputStream/Reader: 所有的输入流的基类，前者是字节输入流，后者是字符输入流。 OutputStream/Writer: 所有输出流的基类，前者是字节输出流，后者是字符输出流。 常用的io流的用法 三　Java IO面试题 NIO与AIO学习总结 一 Java NIO 概览 NIO简介: Java NIO 是 java 1.4, 之后新出的一套IO接口NIO中的N可以理解为Non-blocking，不单纯是New。 NIO的特性/NIO与IO区别: 1)IO是面向流的，NIO是面向缓冲区的； 2)IO流是阻塞的，NIO流是不阻塞的; 3)NIO有选择器，而IO没有。 读数据和写数据方式: 从通道进行数据读取 ：创建一个缓冲区，然后请求通道读取数据。 从通道进行数据写入 ：创建一个缓冲区，填充数据，并要求通道写入数据。 NIO核心组件简单介绍 Channels Buffers Selectors 二 Java NIO 之 Buffer(缓冲区) Buffer(缓冲区)介绍: Java NIO Buffers用于和NIO Channel交互。 我们从Channel中读取数据到buffers里，从Buffer把数据写入到Channels； Buffer本质上就是一块内存区； 一个Buffer有三个属性是必须掌握的，分别是：capacity容量、position位置、limit限制。 Buffer的常见方法 Buffer clear() Buffer flip() Buffer rewind() Buffer position(int newPosition) Buffer的使用方式/方法介绍: 分配缓冲区（Allocating a Buffer）:ByteBuffer buf = ByteBuffer.allocate(28);//以ByteBuffer为例子 写入数据到缓冲区（Writing Data to a Buffer） 写数据到Buffer有两种方法： 1.从Channel中写数据到Buffer int bytesRead = inChannel.read(buf); //read into buffer. 2.通过put写数据： buf.put(127); Buffer常用方法测试 说实话，NIO编程真的难，通过后面这个测试例子，你可能才能勉强理解前面说的Buffer方法的作用。 三 Java NIO 之 Channel（通道） Channel（通道）介绍 通常来说NIO中的所有IO都是从 Channel（通道） 开始的。 NIO Channel通道和流的区别： FileChannel的使用 SocketChannel和ServerSocketChannel的使用 ️DatagramChannel的使用 Scatter / Gather Scatter: 从一个Channel读取的信息分散到N个缓冲区中(Buufer). Gather: 将N个Buffer里面内容按照顺序发送到一个Channel. 通道之间的数据传输 在Java NIO中如果一个channel是FileChannel类型的，那么他可以直接把数据传输到另一个channel。 transferFrom() :transferFrom方法把数据从通道源传输到FileChannel transferTo() :transferTo方法把FileChannel数据传输到另一个channel 四 Java NIO之Selector（选择器） Selector（选择器）介绍 Selector 一般称 为选择器 ，当然你也可以翻译为 多路复用器 。它是Java NIO核心组件中的一个，用于检查一个或多个NIO Channel（通道）的状态是否处于可读、可写。如此可以实现单线程管理多个channels,也就是可以管理多个网络链接。 使用Selector的好处在于： 使用更少的线程来就可以来处理通道了， 相比使用多个线程，避免了线程上下文切换带来的开销。 Selector（选择器）的使用方法介绍 Selector的创建Selector selector = Selector.open(); 注册Channel到Selector(Channel必须是非阻塞的)channel.configureBlocking(false); SelectionKey key = channel.register(selector, Selectionkey.OP_READ); SelectionKey介绍 一个SelectionKey键表示了一个特定的通道对象和一个特定的选择器对象之间的注册关系。 从Selector中选择channel(Selecting Channels via a Selector) 选择器维护注册过的通道的集合，并且这种注册关系都被封装在SelectionKey当中. 停止选择的方法 wakeup()方法 和close()方法。 模板代码 有了模板代码我们在编写程序时，大多数时间都是在模板代码中添加相应的业务代码。 客户端与服务端简单交互实例 五 Java NIO之拥抱Path和Files 一 文件I/O基石：Path： 创建一个Path File和Path之间的转换，File和URI之间的转换 获取Path的相关信息 移除Path中的冗余项 二 拥抱Files类： Files.exists() 检测文件路径是否存在 Files.createFile() 创建文件 Files.createDirectories()和Files.createDirectory()创建文件夹 Files.delete()方法 可以删除一个文件或目录 Files.copy()方法可以吧一个文件从一个地址复制到另一个位置 获取文件属性 遍历一个文件夹 Files.walkFileTree()遍历整个目录 六 NIO学习总结以及NIO新特性介绍 内存映射： 这个功能主要是为了提高大文件的读写速度而设计的。内存映射文件(memory-mappedfile)能让你创建和修改那些大到无法读入内存的文件。有了内存映射文件，你就可以认为文件已经全部读进了内存，然后把它当成一个非常大的数组来访问了。将文件的一段区域映射到内存中，比传统的文件处理速度要快很多。内存映射文件它虽然最终也是要从磁盘读取数据，但是它并不需要将数据读取到OS内核缓冲区，而是直接将进程的用户私有地址空间中的一部分区域与文件对象建立起映射关系，就好像直接从内存中读、写文件一样，速度当然快了。 七 Java NIO AsynchronousFileChannel异步文件通 Java7中新增了AsynchronousFileChannel作为nio的一部分。AsynchronousFileChannel使得数据可以进行异步读写。 八 高并发Java（8）：NIO和AIO 推荐阅读 在 Java 7 中体会 NIO.2 异步执行的快乐 Java AIO总结与示例 AIO是异步IO的缩写，虽然NIO在网络操作中，提供了非阻塞的方法，但是NIO的IO行为还是同步的。对于NIO来说，我们的业务线程是在IO操作准备好时，得到通知，接着就由这个线程自行进行IO操作，IO操作本身是同步的。 欢迎关注我的微信公众号:\"Java面试通关手册\"（一个有温度的微信公众号，期待与你共同进步~~~坚持原创，分享美文，分享各种Java学习资源）： "},"zother6-JavaGuide/java/java-naming-conventions.html":{"url":"zother6-JavaGuide/java/java-naming-conventions.html","title":"Java Naming Conventions","keywords":"","body":" 原文链接：https://www.cnblogs.com/liqiangchn/p/12000361.html 简洁清爽的代码风格应该是大多数工程师所期待的。在工作中笔者常常因为起名字而纠结，夸张点可以说是编程5分钟，命名两小时！究竟为什么命名成为了工作中的拦路虎。 每个公司都有不同的标准，目的是为了保持统一，减少沟通成本，提升团队研发效能。所以本文中是笔者结合阿里巴巴开发规范，以及工作中的见闻针对Java领域相关命名进行整理和总结，仅供参考。 一，Java中的命名规范 好的命名能体现出代码的特征，含义或者是用途，让阅读者可以根据名称的含义快速厘清程序的脉络。不同语言中采用的命名形式大相径庭，Java中常用到的命名形式共有三种，既首字母大写的UpperCamelCase，首字母小写的lowerCamelCase以及全部大写的并用下划线分割单词的UPPER_CAMEL_UNSER_SCORE。通常约定，类一般采用大驼峰命名，方法和局部变量使用小驼峰命名，而大写下划线命名通常是常量和枚举中使用。 类型 约束 例 项目名 全部小写，多个单词用中划线分隔‘-’ spring-cloud 包名 全部小写 com.alibaba.fastjson 类名 单词首字母大写 Feature, ParserConfig,DefaultFieldDeserializer 变量名 首字母小写，多个单词组成时，除首个单词，其他单词首字母都要大写 password, userName 常量名 全部大写，多个单词，用'_'分隔 CACHE_EXPIRED_TIME 方法 同变量 read(), readObject(), getById() 二，包命名 包名统一使用小写，点分隔符之间有且仅有一个自然语义的英文单词或者多个单词自然连接到一块（如 springframework，deepspace不需要使用任何分割）。包名统一使用单数形式，如果类命有复数含义，则可以使用复数形式。 包名的构成可以分为以下几四部分【前缀】 【发起者名】【项目名】【模块名】。常见的前缀可以分为以下几种： 前缀名 例 含义 indi（或onem ） indi.发起者名.项目名.模块名.…… 个体项目，指个人发起，但非自己独自完成的项目，可公开或私有项目，copyright主要属于发起者。 pers pers.个人名.项目名.模块名.…… 个人项目，指个人发起，独自完成，可分享的项目，copyright主要属于个人 priv priv.个人名.项目名.模块名.…… 私有项目，指个人发起，独自完成，非公开的私人使用的项目，copyright属于个人。 team team.团队名.项目名.模块名.…… 团队项目，指由团队发起，并由该团队开发的项目，copyright属于该团队所有 顶级域名 com.公司名.项目名.模块名.…… 公司项目，copyright由项目发起的公司所有 三，类命名 类名使用大驼峰命名形式，类命通常时名词或名词短语，接口名除了用名词和名词短语以外，还可以使用形容词或形容词短语，如Cloneable，Callable等，表示实现该接口的类有某种功能或能力。对于测试类则以它要测试的类开头，以Test结尾，如HashMapTest。 对于一些特殊特有名词缩写也可以使用全大写命名，比如XMLHttpRequest，不过笔者认为缩写三个字母以内都大写，超过三个字母则按照要给单词算。这个没有标准如阿里巴巴中fastjson用JSONObject作为类命，而google则使用JsonObjectRequest命名，对于这种特殊的缩写，原则是统一就好。 属性 约束 例 抽象类 Abstract 或者 Base 开头 BaseUserService 枚举类 Enum 作为后缀 GenderEnum 工具类 Utils作为后缀 StringUtils 异常类 Exception结尾 RuntimeException 接口实现类 接口名+ Impl UserServiceImpl 领域模型相关 /DO/DTO/VO/DAO 正例：UserDAO 反例： UserDo， UserDao 设计模式相关类 Builder，Factory等 当使用到设计模式时，需要使用对应的设计模式作为后缀，如ThreadFactory 处理特定功能的 Handler，Predicate, Validator 表示处理器，校验器，断言，这些类工厂还有配套的方法名如handle，predicate，validate 测试类 Test结尾 UserServiceTest， 表示用来测试UserService类的 MVC分层 Controller，Service，ServiceImpl，DAO后缀 UserManageController，UserManageDAO 四，方法 方法命名采用小驼峰的形式，首字小写，往后的每个单词首字母都要大写。 和类名不同的是，方法命名一般为动词或动词短语，与参数或参数名共同组成动宾短语，即动词 + 名词。一个好的函数名一般能通过名字直接获知该函数实现什么样的功能。 4.1 返回真伪值的方法 注：Prefix-前缀，Suffix-后缀，Alone-单独使用 位置 单词 意义 例 Prefix is 对象是否符合期待的状态 isValid Prefix can 对象能否执行所期待的动作 canRemove Prefix should 调用方执行某个命令或方法是好还是不好,应不应该，或者说推荐还是不推荐 shouldMigrate Prefix has 对象是否持有所期待的数据和属性 hasObservers Prefix needs 调用方是否需要执行某个命令或方法 needsMigrate 4.2 用来检查的方法 单词 意义 例 ensure 检查是否为期待的状态，不是则抛出异常或返回error code ensureCapacity validate 检查是否为正确的状态，不是则抛出异常或返回error code validateInputs 4.3 按需求才执行的方法 位置 单词 意义 例 Suffix IfNeeded 需要的时候执行，不需要的时候什么都不做 drawIfNeeded Prefix might 同上 mightCreate Prefix try 尝试执行，失败时抛出异常或是返回errorcode tryCreate Suffix OrDefault 尝试执行，失败时返回默认值 getOrDefault Suffix OrElse 尝试执行、失败时返回实际参数中指定的值 getOrElse Prefix force 强制尝试执行。error抛出异常或是返回值 forceCreate, forceStop 4.4 异步相关方法 位置 单词 意义 例 Prefix blocking 线程阻塞方法 blockingGetUser Suffix InBackground 执行在后台的线程 doInBackground Suffix Async 异步方法 sendAsync Suffix Sync 对应已有异步方法的同步方法 sendSync Prefix or Alone schedule Job和Task放入队列 schedule, scheduleJob Prefix or Alone post 同上 postJob Prefix or Alone execute 执行异步方法（注：我一般拿这个做同步方法名） execute, executeTask Prefix or Alone start 同上 start, startJob Prefix or Alone cancel 停止异步方法 cancel, cancelJob Prefix or Alone stop 同上 stop, stopJob 4.5 回调方法 位置 单词 意义 例 Prefix on 事件发生时执行 onCompleted Prefix before 事件发生前执行 beforeUpdate Prefix pre 同上 preUpdate Prefix will 同上 willUpdate Prefix after 事件发生后执行 afterUpdate Prefix post 同上 postUpdate Prefix did 同上 didUpdate Prefix should 确认事件是否可以发生时执行 shouldUpdate 4.6 操作对象生命周期的方法 单词 意义 例 initialize 初始化。也可作为延迟初始化使用 initialize pause 暂停 onPause ，pause stop 停止 onStop，stop abandon 销毁的替代 abandon destroy 同上 destroy dispose 同上 dispose 4.7 与集合操作相关的方法 单词 意义 例 contains 是否持有与指定对象相同的对象 contains add 添加 addJob append 添加 appendJob insert 插入到下标n insertJob put 添加与key对应的元素 putJob remove 移除元素 removeJob enqueue 添加到队列的最末位 enqueueJob dequeue 从队列中头部取出并移除 dequeueJob push 添加到栈头 pushJob pop 从栈头取出并移除 popJob peek 从栈头取出但不移除 peekJob find 寻找符合条件的某物 findById 4.8 与数据相关的方法 单词 意义 例 create 新创建 createAccount new 新创建 newAccount from 从既有的某物新建，或是从其他的数据新建 fromConfig to 转换 toString update 更新既有某物 updateAccount load 读取 loadAccount fetch 远程读取 fetchAccount delete 删除 deleteAccount remove 删除 removeAccount save 保存 saveAccount store 保存 storeAccount commit 保存 commitChange apply 保存或应用 applyChange clear 清除数据或是恢复到初始状态 clearAll reset 清除数据或是恢复到初始状态 resetAll 4.9 成对出现的动词 单词 意义 get获取 set 设置 add 增加 remove 删除 create 创建 destory 移除 start 启动 stop 停止 open 打开 close 关闭 read 读取 write 写入 load 载入 save 保存 create 创建 destroy 销毁 begin 开始 end 结束 backup 备份 restore 恢复 import 导入 export 导出 split 分割 merge 合并 inject 注入 extract 提取 attach 附着 detach 脱离 bind 绑定 separate 分离 view 查看 browse 浏览 edit 编辑 modify 修改 select 选取 mark 标记 copy 复制 paste 粘贴 undo 撤销 redo 重做 insert 插入 delete 移除 add 加入 append 添加 clean 清理 clear 清除 index 索引 sort 排序 find 查找 search 搜索 increase 增加 decrease 减少 play 播放 pause 暂停 launch 启动 run 运行 compile 编译 execute 执行 debug 调试 trace 跟踪 observe 观察 listen 监听 build 构建 publish 发布 input 输入 output 输出 encode 编码 decode 解码 encrypt 加密 decrypt 解密 compress 压缩 decompress 解压缩 pack 打包 unpack 解包 parse 解析 emit 生成 connect 连接 disconnect 断开 send 发送 receive 接收 download 下载 upload 上传 refresh 刷新 synchronize 同步 update 更新 revert 复原 lock 锁定 unlock 解锁 check out 签出 check in 签入 submit 提交 commit 交付 push 推 pull 拉 expand 展开 collapse 折叠 begin 起始 end 结束 start 开始 finish 完成 enter 进入 exit 退出 abort 放弃 quit 离开 obsolete 废弃 depreciate 废旧 collect 收集 aggregate 聚集 五，变量&常量命名 5.1 变量命名 变量是指在程序运行中可以改变其值的量，包括成员变量和局部变量。变量名由多单词组成时，第一个单词的首字母小写，其后单词的首字母大写，俗称骆驼式命名法（也称驼峰命名法），如 computedValues，index、变量命名时，尽量简短且能清楚的表达变量的作用，命名体现具体的业务含义即可。 变量名不应以下划线或美元符号开头，尽管这在语法上是允许的。变量名应简短且富于描述。变量名的选用应该易于记忆，即，能够指出其用途。尽量避免单个字符的变量名，除非是一次性的临时变量。pojo中的布尔变量，都不要加is(数据库中的布尔字段全都要加 is_ 前缀)。 5.2 常量命名 常量命名CONSTANT_CASE，一般采用全部大写（作为方法参数时除外），单词间用下划线分割。那么什么是常量呢？ 常量是在作用域内保持不变的值，一般使用final进行修饰。一般分为三种，全局常量（public static final修饰），类内常量（private static final 修饰）以及局部常量（方法内，或者参数中的常量），局部常量比较特殊，通常采用小驼峰命名即可。 /** * 一个demo * * @author Jann Lee * @date 2019-12-07 00:25 **/ public class HelloWorld { /** * 局部常量(正例) */ public static final long USER_MESSAGE_CACHE_EXPIRE_TIME = 3600; /** * 局部常量(反例，命名不清晰） */ public static final long MESSAGE_CACHE_TIME = 3600; /** * 全局常量 */ private static final String ERROR_MESSAGE = \" error message\"; /** * 成员变量 */ private int currentUserId; /** * 控制台打印 {@code message} 信息 * * @param message 消息体，局部常量 */ public void sayHello(final String message){ System.out.println(\"Hello world!\"); } } 常量一般都有自己的业务含义,不要害怕长度过长而进行省略或者缩写。如，用户消息缓存过期时间的表示，那种方式更佳清晰，交给你来评判。 通用命名规则# 尽量不要使用拼音；杜绝拼音和英文混用。对于一些通用的表示或者难以用英文描述的可以采用拼音，一旦采用拼音就坚决不能和英文混用。 正例： BeiJing， HangZhou 反例： validateCanShu 命名过程中尽量不要出现特殊的字符，常量除外。 尽量不要和jdk或者框架中已存在的类重名，也不能使用java中的关键字命名。 妙用介词，如for(可以用同音的4代替), to(可用同音的2代替), from, with，of等。 如类名采用User4RedisDO，方法名getUserInfoFromRedis，convertJson2Map等。 六，代码注解 6.1 注解的原则 好的命名增加代码阅读性，代码的命名往往有严格的限制。而注解不同，程序员往往可以自由发挥，单并不意味着可以为所欲为之胡作非为。优雅的注解通常要满足三要素。 Nothing is strange 没有注解的代码对于阅读者非常不友好，哪怕代码写的在清除，阅读者至少从心理上会有抵触，更何况代码中往往有许多复杂的逻辑，所以一定要写注解，不仅要记录代码的逻辑，还有说清楚修改的逻辑。 Less is more 从代码维护角度来讲，代码中的注解一定是精华中的精华。合理清晰的命名能让代码易于理解，对于逻辑简单且命名规范，能够清楚表达代码功能的代码不需要注解。滥用注解会增加额外的负担，更何况大部分都是废话。 // 根据id获取信息【废话注解】 getMessageById(id) Advance with the time 注解应该随着代码的变动而改变，注解表达的信息要与代码中完全一致。通常情况下修改代码后一定要修改注解。 6.2 注解格式 注解大体上可以分为两种，一种是javadoc注解，另一种是简单注解。javadoc注解可以生成JavaAPI为外部用户提供有效的支持javadoc注解通常在使用IDEA，或者Eclipse等开发工具时都可以自动生成，也支持自定义的注解模板，仅需要对对应的字段进行解释。参与同一项目开发的同学，尽量设置成相同的注解模板。 a. 包注解 包注解在工作中往往比较特殊，通过包注解可以快速知悉当前包下代码是用来实现哪些功能，强烈建议工作中加上，尤其是对于一些比较复杂的包，包注解一般在包的根目录下，名称统一为package-info.java。 /** * 落地也质量检测 * 1. 用来解决什么问题 * 对广告主投放的广告落地页进行性能检测，模拟不同的系统，如Android，IOS等; 模拟不同的网络：2G，3G，4G，wifi等 * * 2. 如何实现 * 基于chrome浏览器，用chromedriver驱动浏览器，设置对应的网络，OS参数，获取到浏览器返回结果。 * * 注意： 网络环境配置信息{@link cn.mycookies.landingpagecheck.meta.NetWorkSpeedEnum}目前使用是常规速度，可以根据实际情况进行调整 * * @author cruder * @time 2019/12/7 20:3 下午 */ package cn.mycookies.landingpagecheck; b. 类注接 javadoc注解中，每个类都必须有注解。 /** * Copyright (C), 2019-2020, Jann balabala... * * 类的介绍：这是一个用来做什么事情的类，有哪些功能，用到的技术..... * * @author 类创建者姓名 保持对齐 * @date 创建日期 保持对齐 * @version 版本号 保持对齐 */ c. 属性注解 在每个属性前面必须加上属性注释，通常有以下两种形式，至于怎么选择，你高兴就好，不过一个项目中要保持统一。 /** 提示信息 */ private String userName; /** * 密码 */ private String password; d. 方法注释 在每个方法前面必须加上方法注释，对于方法中的每个参数，以及返回值都要有说明。 /** * 方法的详细说明，能干嘛，怎么实现的，注意事项... * * @param xxx 参数1的使用说明， 能否为null * @return 返回结果的说明， 不同情况下会返回怎样的结果 * @throws 异常类型 注明从此类方法中抛出异常的说明 */ e. 构造方法注释 在每个构造方法前面必须加上注释，注释模板如下： /** * 构造方法的详细说明 * * @param xxx 参数1的使用说明， 能否为null * @throws 异常类型 注明从此类方法中抛出异常的说明 */ 而简单注解往往是需要工程师字节定义，在使用注解时应该注意以下几点： 枚举类的各个属性值都要使用注解，枚举可以理解为是常量，通常不会发生改变，通常会被在多个地方引用，对枚举的修改和添加属性通常会带来很大的影响。 保持排版整洁，不要使用行尾注释；双斜杠和星号之后要用1个空格分隔。 id = 1;// 反例：不要使用行尾注释 //反例：换行符与注释之间没有缩进 int age = 18; // 正例：姓名 String name; /** * 1. 多行注释 * * 2. 对于不同的逻辑说明，可以用空行分隔 */ 总结 无论是命名和注解，他们的目的都是为了让代码和工程师进行对话，增强代码的可读性，可维护性。优秀的代码往往能够见名知意，注解往往是对命名的补充和完善。命名太南了！ 参考文献： 《码出高效》 https://www.cnblogs.com/wangcp-2014/p/10215620.html https://qiita.com/KeithYokoma/items/2193cf79ba76563e3db6 https://google.github.io/styleguide/javaguide.html#s2.1-file-name "},"zother6-JavaGuide/java/Java基础知识.html":{"url":"zother6-JavaGuide/java/Java基础知识.html","title":"Java基础知识","keywords":"","body":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java 面试突击》以及 Java 工程师必备学习资源。 1. 面向对象和面向过程的区别 2. Java 语言有哪些特点? 3. 关于 JVM JDK 和 JRE 最详细通俗的解答 JVM JDK 和 JRE 4. Oracle JDK 和 OpenJDK 的对比 5. Java 和 C++的区别? 6. 什么是 Java 程序的主类 应用程序和小程序的主类有何不同? 7. Java 应用程序与小程序之间有哪些差别? 8. 字符型常量和字符串常量的区别? 9. 构造器 Constructor 是否可被 override? 10. 重载和重写的区别 - [重载](#重载) - [重写](#重写) 11. Java 面向对象编程三大特性: 封装 继承 多态 封装 继承 多态 12. String StringBuffer 和 StringBuilder 的区别是什么? String 为什么是不可变的? 13. 自动装箱与拆箱 14. 在一个静态方法内调用一个非静态成员为什么是非法的? 15. 在 Java 中定义一个不做事且没有参数的构造方法的作用 16. import java 和 javax 有什么区别？ 17. 接口和抽象类的区别是什么？ 18. 成员变量与局部变量的区别有哪些？ 19. 创建一个对象用什么运算符?对象实体与对象引用有何不同? 20. 什么是方法的返回值?返回值在类的方法里的作用是什么? 21. 一个类的构造方法的作用是什么? 若一个类没有声明构造方法，该程序能正确执行吗? 为什么? 22. 构造方法有哪些特性？ 23. 静态方法和实例方法有何不同 24. 对象的相等与指向他们的引用相等,两者有什么不同? 25. 在调用子类构造方法之前会先调用父类没有参数的构造方法,其目的是? 26. == 与 equals(重要) 27. hashCode 与 equals (重要) hashCode（）介绍 为什么要有 hashCode hashCode（）与 equals（）的相关规定 28. 为什么 Java 中只有值传递？ 29. 简述线程、程序、进程的基本概念。以及他们之间关系是什么? 30. 线程有哪些基本状态? 31 关于 final 关键字的一些总结 32 Java 中的异常处理 Java 异常类层次结构图 Throwable 类常用方法 异常处理总结 33 Java 序列化中如果有些字段不想进行序列化，怎么办？ 34 获取用键盘输入常用的两种方法 35 Java 中 IO 流 Java 中 IO 流分为几种? 既然有了字节流,为什么还要有字符流? BIO,NIO,AIO 有什么区别? 36. 常见关键字总结:static,final,this,super 37. Collections 工具类和 Arrays 工具类常见方法总结 38. 深拷贝 vs 浅拷贝 参考 公众号 1. 面向对象和面向过程的区别 面向过程 ：面向过程性能比面向对象高。 因为类调用时需要实例化，开销比较大，比较消耗资源，所以当性能是最重要的考量因素的时候，比如单片机、嵌入式开发、Linux/Unix 等一般采用面向过程开发。但是，面向过程没有面向对象易维护、易复用、易扩展。 面向对象 ：面向对象易维护、易复用、易扩展。 因为面向对象有封装、继承、多态性的特性，所以可以设计出低耦合的系统，使系统更加灵活、更加易于维护。但是，面向对象性能比面向过程低。 参见 issue : 面向过程 ：面向过程性能比面向对象高？？ 这个并不是根本原因，面向过程也需要分配内存，计算内存偏移量，Java 性能差的主要原因并不是因为它是面向对象语言，而是 Java 是半编译语言，最终的执行代码并不是可以直接被 CPU 执行的二进制机械码。 而面向过程语言大多都是直接编译成机械码在电脑上执行，并且其它一些面向过程的脚本语言性能也并不一定比 Java 好。 2. Java 语言有哪些特点? 简单易学； 面向对象（封装，继承，多态）； 平台无关性（ Java 虚拟机实现平台无关性）； 可靠性； 安全性； 支持多线程（ C++ 语言没有内置的多线程机制，因此必须调用操作系统的多线程功能来进行多线程程序设计，而 Java 语言却提供了多线程支持）； 支持网络编程并且很方便（ Java 语言诞生本身就是为简化网络编程设计的，因此 Java 语言不仅支持网络编程而且很方便）； 编译与解释并存； 修正（参见： issue#544）：C++11 开始（2011 年的时候）,C++就引入了多线程库，在 windows、linux、macos 都可以使用std::thread和std::async来创建线程。参考链接：http://www.cplusplus.com/reference/thread/thread/?kw=thread 3. 关于 JVM JDK 和 JRE 最详细通俗的解答 JVM Java 虚拟机（JVM）是运行 Java 字节码的虚拟机。JVM 有针对不同系统的特定实现（Windows，Linux，macOS），目的是使用相同的字节码，它们都会给出相同的结果。 什么是字节码?采用字节码的好处是什么? 在 Java 中，JVM 可以理解的代码就叫做字节码（即扩展名为 .class 的文件），它不面向任何特定的处理器，只面向虚拟机。Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。所以 Java 程序运行时比较高效，而且，由于字节码并不针对一种特定的机器，因此，Java 程序无须重新编译便可在多种不同操作系统的计算机上运行。 Java 程序从源代码到运行一般有下面 3 步： 我们需要格外注意的是 .class->机器码 这一步。在这一步 JVM 类加载器首先加载字节码文件，然后通过解释器逐行解释执行，这种方式的执行速度会相对比较慢。而且，有些方法和代码块是经常需要被调用的(也就是所谓的热点代码)，所以后面引进了 JIT 编译器，而 JIT 属于运行时编译。当 JIT 编译器完成第一次编译后，其会将字节码对应的机器码保存下来，下次可以直接使用。而我们知道，机器码的运行效率肯定是高于 Java 解释器的。这也解释了我们为什么经常会说 Java 是编译与解释共存的语言。 HotSpot 采用了惰性评估(Lazy Evaluation)的做法，根据二八定律，消耗大部分系统资源的只有那一小部分的代码（热点代码），而这也就是 JIT 所需要编译的部分。JVM 会根据代码每次被执行的情况收集信息并相应地做出一些优化，因此执行的次数越多，它的速度就越快。JDK 9 引入了一种新的编译模式 AOT(Ahead of Time Compilation)，它是直接将字节码编译成机器码，这样就避免了 JIT 预热等各方面的开销。JDK 支持分层编译和 AOT 协作使用。但是 ，AOT 编译器的编译质量是肯定比不上 JIT 编译器的。 总结： Java 虚拟机（JVM）是运行 Java 字节码的虚拟机。JVM 有针对不同系统的特定实现（Windows，Linux，macOS），目的是使用相同的字节码，它们都会给出相同的结果。字节码和不同系统的 JVM 实现是 Java 语言“一次编译，随处可以运行”的关键所在。 JDK 和 JRE JDK 是 Java Development Kit，它是功能齐全的 Java SDK。它拥有 JRE 所拥有的一切，还有编译器（javac）和工具（如 javadoc 和 jdb）。它能够创建和编译程序。 JRE 是 Java 运行时环境。它是运行已编译 Java 程序所需的所有内容的集合，包括 Java 虚拟机（JVM），Java 类库，java 命令和其他的一些基础构件。但是，它不能用于创建新程序。 如果你只是为了运行一下 Java 程序的话，那么你只需要安装 JRE 就可以了。如果你需要进行一些 Java 编程方面的工作，那么你就需要安装 JDK 了。但是，这不是绝对的。有时，即使您不打算在计算机上进行任何 Java 开发，仍然需要安装 JDK。例如，如果要使用 JSP 部署 Web 应用程序，那么从技术上讲，您只是在应用程序服务器中运行 Java 程序。那你为什么需要 JDK 呢？因为应用程序服务器会将 JSP 转换为 Java servlet，并且需要使用 JDK 来编译 servlet。 4. Oracle JDK 和 OpenJDK 的对比 可能在看这个问题之前很多人和我一样并没有接触和使用过 OpenJDK 。那么 Oracle 和 OpenJDK 之间是否存在重大差异？下面我通过收集到的一些资料，为你解答这个被很多人忽视的问题。 对于 Java 7，没什么关键的地方。OpenJDK 项目主要基于 Sun 捐赠的 HotSpot 源代码。此外，OpenJDK 被选为 Java 7 的参考实现，由 Oracle 工程师维护。关于 JVM，JDK，JRE 和 OpenJDK 之间的区别，Oracle 博客帖子在 2012 年有一个更详细的答案： 问：OpenJDK 存储库中的源代码与用于构建 Oracle JDK 的代码之间有什么区别？ 答：非常接近 - 我们的 Oracle JDK 版本构建过程基于 OpenJDK 7 构建，只添加了几个部分，例如部署代码，其中包括 Oracle 的 Java 插件和 Java WebStart 的实现，以及一些封闭的源代码派对组件，如图形光栅化器，一些开源的第三方组件，如 Rhino，以及一些零碎的东西，如附加文档或第三方字体。展望未来，我们的目的是开源 Oracle JDK 的所有部分，除了我们考虑商业功能的部分。 总结： Oracle JDK 大概每 6 个月发一次主要版本，而 OpenJDK 版本大概每三个月发布一次。但这不是固定的，我觉得了解这个没啥用处。详情参见：https://blogs.oracle.com/java-platform-group/update-and-faq-on-the-java-se-release-cadence 。 OpenJDK 是一个参考模型并且是完全开源的，而 Oracle JDK 是 OpenJDK 的一个实现，并不是完全开源的； Oracle JDK 比 OpenJDK 更稳定。OpenJDK 和 Oracle JDK 的代码几乎相同，但 Oracle JDK 有更多的类和一些错误修复。因此，如果您想开发企业/商业软件，我建议您选择 Oracle JDK，因为它经过了彻底的测试和稳定。某些情况下，有些人提到在使用 OpenJDK 可能会遇到了许多应用程序崩溃的问题，但是，只需切换到 Oracle JDK 就可以解决问题； 在响应性和 JVM 性能方面，Oracle JDK 与 OpenJDK 相比提供了更好的性能； Oracle JDK 不会为即将发布的版本提供长期支持，用户每次都必须通过更新到最新版本获得支持来获取最新版本； Oracle JDK 根据二进制代码许可协议获得许可，而 OpenJDK 根据 GPL v2 许可获得许可。 5. Java 和 C++的区别? 我知道很多人没学过 C++，但是面试官就是没事喜欢拿咱们 Java 和 C++ 比呀！没办法！！！就算没学过 C++，也要记下来！ 都是面向对象的语言，都支持封装、继承和多态 Java 不提供指针来直接访问内存，程序内存更加安全 Java 的类是单继承的，C++ 支持多重继承；虽然 Java 的类不可以多继承，但是接口可以多继承。 Java 有自动内存管理机制，不需要程序员手动释放无用内存 在 C 语言中，字符串或字符数组最后都会有一个额外的字符‘\\0’来表示结束。但是，Java 语言中没有结束符这一概念。 这是一个值得深度思考的问题，具体原因推荐看这篇文章： https://blog.csdn.net/sszgg2006/article/details/49148189 6. 什么是 Java 程序的主类 应用程序和小程序的主类有何不同? 一个程序中可以有多个类，但只能有一个类是主类。在 Java 应用程序中，这个主类是指包含 main（）方法的类。而在 Java 小程序中，这个主类是一个继承自系统类 JApplet 或 Applet 的子类。应用程序的主类不一定要求是 public 类，但小程序的主类要求必须是 public 类。主类是 Java 程序执行的入口点。 7. Java 应用程序与小程序之间有哪些差别? 简单说应用程序是从主线程启动(也就是 main() 方法)。applet 小程序没有 main() 方法，主要是嵌在浏览器页面上运行(调用init()或者run()来启动)，嵌入浏览器这点跟 flash 的小游戏类似。 8. 字符型常量和字符串常量的区别? 形式上: 字符常量是单引号引起的一个字符; 字符串常量是双引号引起的若干个字符 含义上: 字符常量相当于一个整型值( ASCII 值),可以参加表达式运算; 字符串常量代表一个地址值(该字符串在内存中存放位置) 占内存大小 字符常量只占 2 个字节; 字符串常量占若干个字节 (注意： char 在 Java 中占两个字节) java 编程思想第四版：2.2.2 节 9. 构造器 Constructor 是否可被 override? Constructor 不能被 override（重写）,但是可以 overload（重载）,所以你可以看到一个类中有多个构造函数的情况。 10. 重载和重写的区别 重载就是同样的一个方法能够根据输入数据的不同，做出不同的处理 重写就是当子类继承自父类的相同方法，输入数据一样，但要做出有别于父类的响应时，你就要覆盖父类方法 重载 发生在同一个类中，方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同。 下面是《Java 核心技术》对重载这个概念的介绍： 综上：重载就是同一个类中多个同名方法根据不同的传参来执行不同的逻辑处理。 重写 重写发生在运行期，是子类对父类的允许访问的方法的实现过程进行重新编写。 返回值类型、方法名、参数列表必须相同，抛出的异常范围小于等于父类，访问修饰符范围大于等于父类。 如果父类方法访问修饰符为 private/final/static 则子类就不能重写该方法，但是被 static 修饰的方法能够被再次声明。 构造方法无法被重写 综上：重写就是子类对父类方法的重新改造，外部样子不能改变，内部逻辑可以改变 暖心的 Guide 哥最后再来个图标总结一下！ 区别点 重载方法 重写方法 发生范围 同一个类 子类 中 参数列表 必须修改 一定不能修改 返回类型 可修改 一定不能修改 异常 可修改 可以减少或删除，一定不能抛出新的或者更广的异常 访问修饰符 可修改 一定不能做更严格的限制（可以降低限制） 发生阶段 编译期 运行期 11. Java 面向对象编程三大特性: 封装 继承 多态 封装 封装把一个对象的属性私有化，同时提供一些可以被外界访问的属性的方法，如果属性不想被外界访问，我们大可不必提供方法给外界访问。但是如果一个类没有提供给外界访问的方法，那么这个类也没有什么意义了。 继承 继承是使用已存在的类的定义作为基础建立新类的技术，新类的定义可以增加新的数据或新的功能，也可以用父类的功能，但不能选择性地继承父类。通过使用继承我们能够非常方便地复用以前的代码。 关于继承如下 3 点请记住： 子类拥有父类对象所有的属性和方法（包括私有属性和私有方法），但是父类中的私有属性和方法子类是无法访问，只是拥有。 子类可以拥有自己属性和方法，即子类可以对父类进行扩展。 子类可以用自己的方式实现父类的方法。（以后介绍）。 多态 所谓多态就是指程序中定义的引用变量所指向的具体类型和通过该引用变量发出的方法调用在编程时并不确定，而是在程序运行期间才确定，即一个引用变量到底会指向哪个类的实例对象，该引用变量发出的方法调用到底是哪个类中实现的方法，必须在由程序运行期间才能决定。 在 Java 中有两种形式可以实现多态：继承（多个子类对同一方法的重写）和接口（实现接口并覆盖接口中同一方法）。 12. String StringBuffer 和 StringBuilder 的区别是什么? String 为什么是不可变的? 可变性 简单的来说：String 类中使用 final 关键字修饰字符数组来保存字符串，private final char value[]，所以 String 对象是不可变的。 补充（来自issue 675）：在 Java 9 之后，String 类的实现改用 byte 数组存储字符串 private final byte[] value; 而 StringBuilder 与 StringBuffer 都继承自 AbstractStringBuilder 类，在 AbstractStringBuilder 中也是使用字符数组保存字符串char[]value 但是没有用 final 关键字修饰，所以这两种对象都是可变的。 StringBuilder 与 StringBuffer 的构造方法都是调用父类构造方法也就是 AbstractStringBuilder 实现的，大家可以自行查阅源码。 AbstractStringBuilder.java abstract class AbstractStringBuilder implements Appendable, CharSequence { /** * The value is used for character storage. */ char[] value; /** * The count is the number of characters used. */ int count; AbstractStringBuilder(int capacity) { value = new char[capacity]; } 线程安全性 String 中的对象是不可变的，也就可以理解为常量，线程安全。AbstractStringBuilder 是 StringBuilder 与 StringBuffer 的公共父类，定义了一些字符串的基本操作，如 expandCapacity、append、insert、indexOf 等公共方法。StringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。StringBuilder 并没有对方法进行加同步锁，所以是非线程安全的。 性能 每次对 String 类型进行改变的时候，都会生成一个新的 String 对象，然后将指针指向新的 String 对象。StringBuffer 每次都会对 StringBuffer 对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用 StringBuilder 相比使用 StringBuffer 仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险。 对于三者使用的总结： 操作少量的数据: 适用 String 单线程操作字符串缓冲区下操作大量数据: 适用 StringBuilder 多线程操作字符串缓冲区下操作大量数据: 适用 StringBuffer 13. 自动装箱与拆箱 装箱：将基本类型用它们对应的引用类型包装起来； 拆箱：将包装类型转换为基本数据类型； 更多内容见：深入剖析 Java 中的装箱和拆箱 14. 在一个静态方法内调用一个非静态成员为什么是非法的? 由于静态方法可以不通过对象进行调用，因此在静态方法里，不能调用其他非静态变量，也不可以访问非静态变量成员。 15. 在 Java 中定义一个不做事且没有参数的构造方法的作用 Java 程序在执行子类的构造方法之前，如果没有用 super()来调用父类特定的构造方法，则会调用父类中“没有参数的构造方法”。因此，如果父类中只定义了有参数的构造方法，而在子类的构造方法中又没有用 super()来调用父类中特定的构造方法，则编译时将发生错误，因为 Java 程序在父类中找不到没有参数的构造方法可供执行。解决办法是在父类里加上一个不做事且没有参数的构造方法。 16. import java 和 javax 有什么区别？ 刚开始的时候 JavaAPI 所必需的包是 java 开头的包，javax 当时只是扩展 API 包来使用。然而随着时间的推移，javax 逐渐地扩展成为 Java API 的组成部分。但是，将扩展从 javax 包移动到 java 包确实太麻烦了，最终会破坏一堆现有的代码。因此，最终决定 javax 包将成为标准 API 的一部分。 所以，实际上 java 和 javax 没有区别。这都是一个名字。 17. 接口和抽象类的区别是什么？ 接口的方法默认是 public，所有方法在接口中不能有实现(Java 8 开始接口方法可以有默认实现），而抽象类可以有非抽象的方法。 接口中除了 static、final 变量，不能有其他变量，而抽象类中则不一定。 一个类可以实现多个接口，但只能实现一个抽象类。接口自己本身可以通过 extends 关键字扩展多个接口。 接口方法默认修饰符是 public，抽象方法可以有 public、protected 和 default 这些修饰符（抽象方法就是为了被重写所以不能使用 private 关键字修饰！）。 从设计层面来说，抽象是对类的抽象，是一种模板设计，而接口是对行为的抽象，是一种行为的规范。 备注： 在 JDK8 中，接口也可以定义静态方法，可以直接用接口名调用。实现类和实现是不可以调用的。如果同时实现两个接口，接口中定义了一样的默认方法，则必须重写，不然会报错。(详见 issue:https://github.com/Snailclimb/JavaGuide/issues/146。 jdk9 的接口被允许定义私有方法 。 总结一下 jdk7~jdk9 Java 中接口概念的变化（相关阅读）： 在 jdk 7 或更早版本中，接口里面只能有常量变量和抽象方法。这些接口方法必须由选择实现接口的类实现。 jdk8 的时候接口可以有默认方法和静态方法功能。 Jdk 9 在接口中引入了私有方法和私有静态方法。 18. 成员变量与局部变量的区别有哪些？ 从语法形式上看:成员变量是属于类的，而局部变量是在方法中定义的变量或是方法的参数；成员变量可以被 public,private,static 等修饰符所修饰，而局部变量不能被访问控制修饰符及 static 所修饰；但是，成员变量和局部变量都能被 final 所修饰。 从变量在内存中的存储方式来看:如果成员变量是使用static修饰的，那么这个成员变量是属于类的，如果没有使用static修饰，这个成员变量是属于实例的。而对象存在于堆内存，局部变量则存在于栈内存。 从变量在内存中的生存时间上看:成员变量是对象的一部分，它随着对象的创建而存在，而局部变量随着方法的调用而自动消失。 成员变量如果没有被赋初值:则会自动以类型的默认值而赋值（一种情况例外:被 final 修饰的成员变量也必须显式地赋值），而局部变量则不会自动赋值。 19. 创建一个对象用什么运算符?对象实体与对象引用有何不同? new 运算符，new 创建对象实例（对象实例在堆内存中），对象引用指向对象实例（对象引用存放在栈内存中）。一个对象引用可以指向 0 个或 1 个对象（一根绳子可以不系气球，也可以系一个气球）;一个对象可以有 n 个引用指向它（可以用 n 条绳子系住一个气球）。 20. 什么是方法的返回值?返回值在类的方法里的作用是什么? 方法的返回值是指我们获取到的某个方法体中的代码执行后产生的结果！（前提是该方法可能产生结果）。返回值的作用:接收出结果，使得它可以用于其他的操作！ 21. 一个类的构造方法的作用是什么? 若一个类没有声明构造方法，该程序能正确执行吗? 为什么? 主要作用是完成对类对象的初始化工作。可以执行。因为一个类即使没有声明构造方法也会有默认的不带参数的构造方法。 22. 构造方法有哪些特性？ 名字与类名相同。 没有返回值，但不能用 void 声明构造函数。 生成类的对象时自动执行，无需调用。 23. 静态方法和实例方法有何不同 在外部调用静态方法时，可以使用\"类名.方法名\"的方式，也可以使用\"对象名.方法名\"的方式。而实例方法只有后面这种方式。也就是说，调用静态方法可以无需创建对象。 静态方法在访问本类的成员时，只允许访问静态成员（即静态成员变量和静态方法），而不允许访问实例成员变量和实例方法；实例方法则无此限制。 24. 对象的相等与指向他们的引用相等,两者有什么不同? 对象的相等，比的是内存中存放的内容是否相等。而引用相等，比较的是他们指向的内存地址是否相等。 25. 在调用子类构造方法之前会先调用父类没有参数的构造方法,其目的是? 帮助子类做初始化工作。 26. == 与 equals(重要) == : 它的作用是判断两个对象的地址是不是相等。即，判断两个对象是不是同一个对象(基本数据类型==比较的是值，引用数据类型==比较的是内存地址)。 equals() : 它的作用也是判断两个对象是否相等。但它一般有两种使用情况： 情况 1：类没有覆盖 equals() 方法。则通过 equals() 比较该类的两个对象时，等价于通过“==”比较这两个对象。 情况 2：类覆盖了 equals() 方法。一般，我们都覆盖 equals() 方法来比较两个对象的内容是否相等；若它们的内容相等，则返回 true (即，认为这两个对象相等)。 举个例子： public class test1 { public static void main(String[] args) { String a = new String(\"ab\"); // a 为一个引用 String b = new String(\"ab\"); // b为另一个引用,对象的内容一样 String aa = \"ab\"; // 放在常量池中 String bb = \"ab\"; // 从常量池中查找 if (aa == bb) // true System.out.println(\"aa==bb\"); if (a == b) // false，非同一对象 System.out.println(\"a==b\"); if (a.equals(b)) // true System.out.println(\"aEQb\"); if (42 == 42.0) { // true System.out.println(\"true\"); } } } 说明： String 中的 equals 方法是被重写过的，因为 object 的 equals 方法是比较的对象的内存地址，而 String 的 equals 方法比较的是对象的值。 当创建 String 类型的对象时，虚拟机会在常量池中查找有没有已经存在的值和要创建的值相同的对象，如果有就把它赋给当前引用。如果没有就在常量池中重新创建一个 String 对象。 27. hashCode 与 equals (重要) 面试官可能会问你：“你重写过 hashcode 和 equals 么，为什么重写 equals 时必须重写 hashCode 方法？” hashCode（）介绍 hashCode() 的作用是获取哈希码，也称为散列码；它实际上是返回一个 int 整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode() 定义在 JDK 的 Object.java 中，这就意味着 Java 中的任何类都包含有 hashCode() 函数。 散列表存储的是键值对(key-value)，它的特点是：能根据“键”快速的检索出对应的“值”。这其中就利用到了散列码！（可以快速找到所需要的对象） 为什么要有 hashCode 我们先以“HashSet 如何检查重复”为例子来说明为什么要有 hashCode： 当你把对象加入 HashSet 时，HashSet 会先计算对象的 hashcode 值来判断对象加入的位置，同时也会与该位置其他已经加入的对象的 hashcode 值作比较，如果没有相符的 hashcode，HashSet 会假设对象没有重复出现。但是如果发现有相同 hashcode 值的对象，这时会调用 equals()方法来检查 hashcode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让其加入操作成功。如果不同的话，就会重新散列到其他位置。（摘自我的 Java 启蒙书《Head first java》第二版）。这样我们就大大减少了 equals 的次数，相应就大大提高了执行速度。 通过我们可以看出：hashCode() 的作用就是获取哈希码，也称为散列码；它实际上是返回一个 int 整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode()在散列表中才有用，在其它情况下没用。在散列表中 hashCode() 的作用是获取对象的散列码，进而确定该对象在散列表中的位置。 hashCode（）与 equals（）的相关规定 如果两个对象相等，则 hashcode 一定也是相同的 两个对象相等,对两个对象分别调用 equals 方法都返回 true 两个对象有相同的 hashcode 值，它们也不一定是相等的 因此，equals 方法被覆盖过，则 hashCode 方法也必须被覆盖 hashCode() 的默认行为是对堆上的对象产生独特值。如果没有重写 hashCode()，则该 class 的两个对象无论如何都不会相等（即使这两个对象指向相同的数据） 推荐阅读：Java hashCode() 和 equals()的若干问题解答 28. 为什么 Java 中只有值传递？ 为什么 Java 中只有值传递？ 29. 简述线程、程序、进程的基本概念。以及他们之间关系是什么? 线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享同一块内存空间和一组系统资源，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。 程序是含有指令和数据的文件，被存储在磁盘或其他的数据存储设备中，也就是说程序是静态的代码。 进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。简单来说，一个进程就是一个执行中的程序，它在计算机中一个指令接着一个指令地执行着，同时，每个进程还占有某些系统资源如 CPU 时间，内存空间，文件，输入输出设备的使用权等等。换句话说，当程序在执行时，将会被操作系统载入内存中。 线程是进程划分成的更小的运行单位。线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。从另一角度来说，进程属于操作系统的范畴，主要是同一段时间内，可以同时执行一个以上的程序，而线程则是在同一程序内几乎同时执行一个以上的程序段。 30. 线程有哪些基本状态? Java 线程在运行的生命周期中的指定时刻只可能处于下面 6 种不同状态的其中一个状态（图源《Java 并发编程艺术》4.1.4 节）。 线程在生命周期中并不是固定处于某一个状态而是随着代码的执行在不同状态之间切换。Java 线程状态变迁如下图所示（图源《Java 并发编程艺术》4.1.4 节）： 由上图可以看出： 线程创建之后它将处于 NEW（新建） 状态，调用 start() 方法后开始运行，线程这时候处于 READY（可运行） 状态。可运行状态的线程获得了 cpu 时间片（timeslice）后就处于 RUNNING（运行） 状态。 操作系统隐藏 Java 虚拟机（JVM）中的 READY 和 RUNNING 状态，它只能看到 RUNNABLE 状态（图源：HowToDoInJava：Java Thread Life Cycle and Thread States），所以 Java 系统一般将这两个状态统称为 RUNNABLE（运行中） 状态 。 当线程执行 wait()方法之后，线程进入 WAITING（等待）状态。进入等待状态的线程需要依靠其他线程的通知才能够返回到运行状态，而 TIME_WAITING(超时等待) 状态相当于在等待状态的基础上增加了超时限制，比如通过 sleep（long millis）方法或 wait（long millis）方法可以将 Java 线程置于 TIMED WAITING 状态。当超时时间到达后 Java 线程将会返回到 RUNNABLE 状态。当线程调用同步方法时，在没有获取到锁的情况下，线程将会进入到 BLOCKED（阻塞） 状态。线程在执行 Runnable 的run()方法之后将会进入到 TERMINATED（终止） 状态。 31 关于 final 关键字的一些总结 final 关键字主要用在三个地方：变量、方法、类。 对于一个 final 变量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改；如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象。 当用 final 修饰一个类时，表明这个类不能被继承。final 类中的所有成员方法都会被隐式地指定为 final 方法。 使用 final 方法的原因有两个。第一个原因是把方法锁定，以防任何继承类修改它的含义；第二个原因是效率。在早期的 Java 实现版本中，会将 final 方法转为内嵌调用。但是如果方法过于庞大，可能看不到内嵌调用带来的任何性能提升（现在的 Java 版本已经不需要使用 final 方法进行这些优化了）。类中所有的 private 方法都隐式地指定为 final。 32 Java 中的异常处理 Java 异常类层次结构图 图片来自：https://simplesnippets.tech/exception-handling-in-java-part-1/ 图片来自：https://chercher.tech/java-programming/exceptions-java 在 Java 中，所有的异常都有一个共同的祖先 java.lang 包中的 Throwable 类。Throwable： 有两个重要的子类：Exception（异常） 和 Error（错误） ，二者都是 Java 异常处理的重要子类，各自都包含大量子类。 Error（错误）:是程序无法处理的错误，表示运行应用程序中较严重问题。大多数错误与代码编写者执行的操作无关，而表示代码运行时 JVM（Java 虚拟机）出现的问题。例如，Java 虚拟机运行错误（Virtual MachineError），当 JVM 不再有继续执行操作所需的内存资源时，将出现 OutOfMemoryError。这些异常发生时，Java 虚拟机（JVM）一般会选择线程终止。 这些错误表示故障发生于虚拟机自身、或者发生在虚拟机试图执行应用时，如 Java 虚拟机运行错误（Virtual MachineError）、类定义错误（NoClassDefFoundError）等。这些错误是不可查的，因为它们在应用程序的控制和处理能力之 外，而且绝大多数是程序运行时不允许出现的状况。对于设计合理的应用程序来说，即使确实发生了错误，本质上也不应该试图去处理它所引起的异常状况。在 Java 中，错误通过 Error 的子类描述。 Exception（异常）:是程序本身可以处理的异常。Exception 类有一个重要的子类 RuntimeException。RuntimeException 异常由 Java 虚拟机抛出。NullPointerException（要访问的变量没有引用任何对象时，抛出该异常）、ArithmeticException（算术运算异常，一个整数除以 0 时，抛出该异常）和 ArrayIndexOutOfBoundsException （下标越界异常）。 注意：异常和错误的区别：异常能被程序本身处理，错误是无法处理。 Throwable 类常用方法 public string getMessage():返回异常发生时的简要描述 public string toString():返回异常发生时的详细信息 public string getLocalizedMessage():返回异常对象的本地化信息。使用 Throwable 的子类覆盖这个方法，可以生成本地化信息。如果子类没有覆盖该方法，则该方法返回的信息与 getMessage（）返回的结果相同 public void printStackTrace():在控制台上打印 Throwable 对象封装的异常信息 异常处理总结 try 块： 用于捕获异常。其后可接零个或多个 catch 块，如果没有 catch 块，则必须跟一个 finally 块。 catch 块： 用于处理 try 捕获到的异常。 finally 块： 无论是否捕获或处理异常，finally 块里的语句都会被执行。当在 try 块或 catch 块中遇到 return 语句时，finally 语句块将在方法返回之前被执行。 在以下 4 种特殊情况下，finally 块不会被执行： 在 finally 语句块第一行发生了异常。 因为在其他行，finally 块还是会得到执行 在前面的代码中用了 System.exit(int)已退出程序。 exit 是带参函数 ；若该语句在异常语句之后，finally 会执行 程序所在的线程死亡。 关闭 CPU。 下面这部分内容来自 issue:https://github.com/Snailclimb/JavaGuide/issues/190。 注意： 当 try 语句和 finally 语句中都有 return 语句时，在方法返回之前，finally 语句的内容将被执行，并且 finally 语句的返回值将会覆盖原始的返回值。如下： public static int f(int value) { try { return value * value; } finally { if (value == 2) { return 0; } } } 如果调用 f(2)，返回值将是 0，因为 finally 语句的返回值覆盖了 try 语句块的返回值。 33 Java 序列化中如果有些字段不想进行序列化，怎么办？ 对于不想进行序列化的变量，使用 transient 关键字修饰。 transient 关键字的作用是：阻止实例中那些用此关键字修饰的的变量序列化；当对象被反序列化时，被 transient 修饰的变量值不会被持久化和恢复。transient 只能修饰变量，不能修饰类和方法。 34 获取用键盘输入常用的两种方法 方法 1：通过 Scanner Scanner input = new Scanner(System.in); String s = input.nextLine(); input.close(); 方法 2：通过 BufferedReader BufferedReader input = new BufferedReader(new InputStreamReader(System.in)); String s = input.readLine(); 35 Java 中 IO 流 Java 中 IO 流分为几种? 按照流的流向分，可以分为输入流和输出流； 按照操作单元划分，可以划分为字节流和字符流； 按照流的角色划分为节点流和处理流。 Java Io 流共涉及 40 多个类，这些类看上去很杂乱，但实际上很有规则，而且彼此之间存在非常紧密的联系， Java I0 流的 40 多个类都是从如下 4 个抽象类基类中派生出来的。 InputStream/Reader: 所有的输入流的基类，前者是字节输入流，后者是字符输入流。 OutputStream/Writer: 所有输出流的基类，前者是字节输出流，后者是字符输出流。 按操作方式分类结构图： 按操作对象分类结构图： 既然有了字节流,为什么还要有字符流? 问题本质想问：不管是文件读写还是网络发送接收，信息的最小存储单元都是字节，那为什么 I/O 流操作要分为字节流操作和字符流操作呢？ 回答：字符流是由 Java 虚拟机将字节转换得到的，问题就出在这个过程还算是非常耗时，并且，如果我们不知道编码类型就很容易出现乱码问题。所以， I/O 流就干脆提供了一个直接操作字符的接口，方便我们平时对字符进行流操作。如果音频文件、图片等媒体文件用字节流比较好，如果涉及到字符的话使用字符流比较好。 BIO,NIO,AIO 有什么区别? BIO (Blocking I/O): 同步阻塞 I/O 模式，数据的读取写入必须阻塞在一个线程内等待其完成。在活动连接数不是特别高（小于单机 1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的 I/O 并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。但是，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。因此，我们需要一种更高效的 I/O 处理模型来应对更高的并发量。 NIO (Non-blocking/New I/O): NIO 是一种同步非阻塞的 I/O 模型，在 Java 1.4 中引入了 NIO 框架，对应 java.nio 包，提供了 Channel , Selector，Buffer 等抽象。NIO 中的 N 可以理解为 Non-blocking，不单纯是 New。它支持面向缓冲的，基于通道的 I/O 操作方法。 NIO 提供了与传统 BIO 模型中的 Socket 和 ServerSocket 相对应的 SocketChannel 和 ServerSocketChannel 两种不同的套接字通道实现,两种通道都支持阻塞和非阻塞两种模式。阻塞模式使用就像传统中的支持一样，比较简单，但是性能和可靠性都不好；非阻塞模式正好与之相反。对于低负载、低并发的应用程序，可以使用同步阻塞 I/O 来提升开发速率和更好的维护性；对于高负载、高并发的（网络）应用，应使用 NIO 的非阻塞模式来开发 AIO (Asynchronous I/O): AIO 也就是 NIO 2。在 Java 7 中引入了 NIO 的改进版 NIO 2,它是异步非阻塞的 IO 模型。异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。AIO 是异步 IO 的缩写，虽然 NIO 在网络操作中，提供了非阻塞的方法，但是 NIO 的 IO 行为还是同步的。对于 NIO 来说，我们的业务线程是在 IO 操作准备好时，得到通知，接着就由这个线程自行进行 IO 操作，IO 操作本身是同步的。查阅网上相关资料，我发现就目前来说 AIO 的应用还不是很广泛，Netty 之前也尝试使用过 AIO，不过又放弃了。 36. 常见关键字总结:static,final,this,super 详见笔主的这篇文章: https://snailclimb.gitee.io/javaguide/#/docs/java/basic/final,static,this,super 37. Collections 工具类和 Arrays 工具类常见方法总结 详见笔主的这篇文章: https://gitee.com/SnailClimb/JavaGuide/blob/master/docs/java/basic/Arrays,CollectionsCommonMethods.md 38. 深拷贝 vs 浅拷贝 浅拷贝：对基本数据类型进行值传递，对引用数据类型进行引用传递般的拷贝，此为浅拷贝。 深拷贝：对基本数据类型进行值传递，对引用数据类型，创建一个新的对象，并复制其内容，此为深拷贝。 参考 https://stackoverflow.com/questions/1906445/what-is-the-difference-between-jdk-and-jre https://www.educba.com/oracle-vs-openjdk/ https://stackoverflow.com/questions/22358071/differences-between-oracle-jdk-and-openjdk?answertab=active#tab-top 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java 面试突击》: 由本文档衍生的专为面试而生的《Java 面试突击》V2.0 PDF 版本公众号后台回复 \"Java 面试突击\" 即可免费领取！ Java 工程师必备学习资源: 一些 Java 工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/java/Java疑难点.html":{"url":"zother6-JavaGuide/java/Java疑难点.html","title":"Java疑难点","keywords":"","body":" 1. 基础 1.1. 正确使用 equals 方法 1.2. 整型包装类值的比较 1.3. BigDecimal 1.3.1. BigDecimal 的用处 1.3.2. BigDecimal 的大小比较 1.3.3. BigDecimal 保留几位小数 1.3.4. BigDecimal 的使用注意事项 1.3.5. 总结 1.4. 基本数据类型与包装数据类型的使用标准 2. 集合 2.1. Arrays.asList()使用指南 2.1.1. 简介 2.1.2. 《阿里巴巴Java 开发手册》对其的描述 2.1.3. 使用时的注意事项总结 2.1.4. 如何正确的将数组转换为ArrayList? 2.2. Collection.toArray()方法使用的坑&如何反转数组 2.3. 不要在 foreach 循环里进行元素的 remove/add 操作 1. 基础 1.1. 正确使用 equals 方法 Object的equals方法容易抛空指针异常，应使用常量或确定有值的对象来调用 equals。 举个例子： // 不能使用一个值为null的引用类型变量来调用非静态方法，否则会抛出异常 String str = null; if (str.equals(\"SnailClimb\")) { ... } else { .. } 运行上面的程序会抛出空指针异常，但是我们把第二行的条件判断语句改为下面这样的话，就不会抛出空指针异常，else 语句块得到执行。： \"SnailClimb\".equals(str);// false 不过更推荐使用 java.util.Objects#equals(JDK7 引入的工具类)。 Objects.equals(null,\"SnailClimb\");// false 我们看一下java.util.Objects#equals的源码就知道原因了。 public static boolean equals(Object a, Object b) { // 可以避免空指针异常。如果a==null的话此时a.equals(b)就不会得到执行，避免出现空指针异常。 return (a == b) || (a != null && a.equals(b)); } 注意： Reference:Java中equals方法造成空指针异常的原因及解决方案 每种原始类型都有默认值一样，如int默认值为 0，boolean 的默认值为 false，null 是任何引用类型的默认值，不严格的说是所有 Object 类型的默认值。 可以使用 == 或者 != 操作来比较null值，但是不能使用其他算法或者逻辑操作。在Java中null == null将返回true。 不能使用一个值为null的引用类型变量来调用非静态方法，否则会抛出异常 1.2. 整型包装类值的比较 所有整型包装类对象值的比较必须使用equals方法。 先看下面这个例子： Integer x = 3; Integer y = 3; System.out.println(x == y);// true Integer a = new Integer(3); Integer b = new Integer(3); System.out.println(a == b);//false System.out.println(a.equals(b));//true 当使用自动装箱方式创建一个Integer对象时，当数值在-128 ~127时，会将创建的 Integer 对象缓存起来，当下次再出现该数值时，直接从缓存中取出对应的Integer对象。所以上述代码中，x和y引用的是相同的Integer对象。 注意：如果你的IDE(IDEA/Eclipse)上安装了阿里巴巴的p3c插件，这个插件如果检测到你用 ==的话会报错提示，推荐安装一个这个插件，很不错。 1.3. BigDecimal 1.3.1. BigDecimal 的用处 《阿里巴巴Java开发手册》中提到：浮点数之间的等值判断，基本数据类型不能用==来比较，包装数据类型不能用 equals 来判断。 具体原理和浮点数的编码方式有关，这里就不多提了，我们下面直接上实例： float a = 1.0f - 0.9f; float b = 0.9f - 0.8f; System.out.println(a);// 0.100000024 System.out.println(b);// 0.099999964 System.out.println(a == b);// false 具有基本数学知识的我们很清楚的知道输出并不是我们想要的结果（精度丢失），我们如何解决这个问题呢？一种很常用的方法是：使用使用 BigDecimal 来定义浮点数的值，再进行浮点数的运算操作。 BigDecimal a = new BigDecimal(\"1.0\"); BigDecimal b = new BigDecimal(\"0.9\"); BigDecimal c = new BigDecimal(\"0.8\"); BigDecimal x = a.subtract(b);// 0.1 BigDecimal y = b.subtract(c);// 0.1 System.out.println(x.equals(y));// true 1.3.2. BigDecimal 的大小比较 a.compareTo(b) : 返回 -1 表示小于，0 表示 等于， 1表示 大于。 BigDecimal a = new BigDecimal(\"1.0\"); BigDecimal b = new BigDecimal(\"0.9\"); System.out.println(a.compareTo(b));// 1 1.3.3. BigDecimal 保留几位小数 通过 setScale方法设置保留几位小数以及保留规则。保留规则有挺多种，不需要记，IDEA会提示。 BigDecimal m = new BigDecimal(\"1.255433\"); BigDecimal n = m.setScale(3,BigDecimal.ROUND_HALF_DOWN); System.out.println(n);// 1.255 1.3.4. BigDecimal 的使用注意事项 注意：我们在使用BigDecimal时，为了防止精度丢失，推荐使用它的 BigDecimal(String) 构造方法来创建对象。《阿里巴巴Java开发手册》对这部分内容也有提到如下图所示。 1.3.5. 总结 BigDecimal 主要用来操作（大）浮点数，BigInteger 主要用来操作大整数（超过 long 类型）。 BigDecimal 的实现利用到了 BigInteger, 所不同的是 BigDecimal 加入了小数位的概念 1.4. 基本数据类型与包装数据类型的使用标准 Reference:《阿里巴巴Java开发手册》 【强制】所有的 POJO 类属性必须使用包装数据类型。 【强制】RPC 方法的返回值和参数必须使用包装数据类型。 【推荐】所有的局部变量使用基本数据类型。 比如我们如果自定义了一个Student类,其中有一个属性是成绩score,如果用Integer而不用int定义,一次考试,学生可能没考,值是null,也可能考了,但考了0分,值是0,这两个表达的状态明显不一样. 说明 :POJO 类属性没有初值是提醒使用者在需要使用时，必须自己显式地进行赋值，任何 NPE 问题，或者入库检查，都由使用者来保证。 正例 : 数据库的查询结果可能是 null，因为自动拆箱，用基本数据类型接收有 NPE 风险。 反例 : 比如显示成交总额涨跌情况，即正负 x%，x 为基本数据类型，调用的 RPC 服务，调用不成功时，返回的是默认值，页面显示为 0%，这是不合理的，应该显示成中划线。所以包装数据类型的 null 值，能够表示额外的信息，如:远程调用失败，异常退出。 2. 集合 2.1. Arrays.asList()使用指南 最近使用Arrays.asList()遇到了一些坑，然后在网上看到这篇文章：Java Array to List Examples 感觉挺不错的，但是还不是特别全面。所以，自己对于这块小知识点进行了简单的总结。 2.1.1. 简介 Arrays.asList()在平时开发中还是比较常见的，我们可以使用它将一个数组转换为一个List集合。 String[] myArray = { \"Apple\", \"Banana\", \"Orange\" }； List myList = Arrays.asList(myArray); //上面两个语句等价于下面一条语句 List myList = Arrays.asList(\"Apple\",\"Banana\", \"Orange\"); JDK 源码对于这个方法的说明： /** *返回由指定数组支持的固定大小的列表。此方法作为基于数组和基于集合的API之间的桥梁，与 Collection.toArray()结合使用。返回的List是可序列化并实现RandomAccess接口。 */ public static List asList(T... a) { return new ArrayList<>(a); } 2.1.2. 《阿里巴巴Java 开发手册》对其的描述 Arrays.asList()将数组转换为集合后,底层其实还是数组，《阿里巴巴Java 开发手册》对于这个方法有如下描述： 方法.png) 2.1.3. 使用时的注意事项总结 传递的数组必须是对象数组，而不是基本类型。 Arrays.asList()是泛型方法，传入的对象必须是对象数组。 int[] myArray = { 1, 2, 3 }; List myList = Arrays.asList(myArray); System.out.println(myList.size());//1 System.out.println(myList.get(0));//数组地址值 System.out.println(myList.get(1));//报错：ArrayIndexOutOfBoundsException int [] array=(int[]) myList.get(0); System.out.println(array[0]);//1 当传入一个原生数据类型数组时，Arrays.asList() 的真正得到的参数就不是数组中的元素，而是数组对象本身！此时List 的唯一元素就是这个数组，这也就解释了上面的代码。 我们使用包装类型数组就可以解决这个问题。 Integer[] myArray = { 1, 2, 3 }; 使用集合的修改方法:add()、remove()、clear()会抛出异常。 List myList = Arrays.asList(1, 2, 3); myList.add(4);//运行时报错：UnsupportedOperationException myList.remove(1);//运行时报错：UnsupportedOperationException myList.clear();//运行时报错：UnsupportedOperationException Arrays.asList() 方法返回的并不是 java.util.ArrayList ，而是 java.util.Arrays 的一个内部类,这个内部类并没有实现集合的修改方法或者说并没有重写这些方法。 List myList = Arrays.asList(1, 2, 3); System.out.println(myList.getClass());//class java.util.Arrays$ArrayList 下图是java.util.Arrays$ArrayList的简易源码，我们可以看到这个类重写的方法有哪些。 private static class ArrayList extends AbstractList implements RandomAccess, java.io.Serializable { ... @Override public E get(int index) { ... } @Override public E set(int index, E element) { ... } @Override public int indexOf(Object o) { ... } @Override public boolean contains(Object o) { ... } @Override public void forEach(Consumer action) { ... } @Override public void replaceAll(UnaryOperator operator) { ... } @Override public void sort(Comparator c) { ... } } 我们再看一下java.util.AbstractList的remove()方法，这样我们就明白为啥会抛出UnsupportedOperationException。 public E remove(int index) { throw new UnsupportedOperationException(); } 2.1.4. 如何正确的将数组转换为ArrayList? stackoverflow：https://dwz.cn/vcBkTiTW 1. 自己动手实现（教育目的） //JDK1.5+ static List arrayToList(final T[] array) { final List l = new ArrayList(array.length); for (final T s : array) { l.add(s); } return (l); } Integer [] myArray = { 1, 2, 3 }; System.out.println(arrayToList(myArray).getClass());//class java.util.ArrayList 2. 最简便的方法(推荐) List list = new ArrayList<>(Arrays.asList(\"a\", \"b\", \"c\")) 3. 使用 Java8 的Stream(推荐) Integer [] myArray = { 1, 2, 3 }; List myList = Arrays.stream(myArray).collect(Collectors.toList()); //基本类型也可以实现转换（依赖boxed的装箱操作） int [] myArray2 = { 1, 2, 3 }; List myList = Arrays.stream(myArray2).boxed().collect(Collectors.toList()); 4. 使用 Guava(推荐) 对于不可变集合，你可以使用ImmutableList类及其of()与copyOf()工厂方法：（参数不能为空） List il = ImmutableList.of(\"string\", \"elements\"); // from varargs List il = ImmutableList.copyOf(aStringArray); // from array 对于可变集合，你可以使用Lists类及其newArrayList()工厂方法： List l1 = Lists.newArrayList(anotherListOrCollection); // from collection List l2 = Lists.newArrayList(aStringArray); // from array List l3 = Lists.newArrayList(\"or\", \"string\", \"elements\"); // from varargs 5. 使用 Apache Commons Collections List list = new ArrayList(); CollectionUtils.addAll(list, str); 2.2. Collection.toArray()方法使用的坑&如何反转数组 该方法是一个泛型方法： T[] toArray(T[] a); 如果toArray方法中没有传递任何参数的话返回的是Object类型数组。 String [] s= new String[]{ \"dog\", \"lazy\", \"a\", \"over\", \"jumps\", \"fox\", \"brown\", \"quick\", \"A\" }; List list = Arrays.asList(s); Collections.reverse(list); s=list.toArray(new String[0]);//没有指定类型的话会报错 由于JVM优化，new String[0]作为Collection.toArray()方法的参数现在使用更好，new String[0]就是起一个模板的作用，指定了返回数组的类型，0是为了节省空间，因为它只是为了说明返回的类型。详见：https://shipilev.net/blog/2016/arrays-wisdom-ancients/ 2.3. 不要在 foreach 循环里进行元素的 remove/add 操作 如果要进行remove操作，可以调用迭代器的 remove方法而不是集合类的 remove 方法。因为如果列表在任何时间从结构上修改创建迭代器之后，以任何方式除非通过迭代器自身remove/add方法，迭代器都将抛出一个ConcurrentModificationException,这就是单线程状态下产生的 fail-fast 机制。 fail-fast 机制 ：多个线程对 fail-fast 集合进行修改的时，可能会抛出ConcurrentModificationException，单线程下也会出现这种情况，上面已经提到过。 java.util包下面的所有的集合类都是fail-fast的，而java.util.concurrent包下面的所有的类都是fail-safe的。 "},"zother6-JavaGuide/java/Java编程规范.html":{"url":"zother6-JavaGuide/java/Java编程规范.html","title":"Java编程规范","keywords":"","body":"讲真的，下面推荐的文章或者资源建议阅读 3 遍以上。 团队 阿里巴巴Java开发手册（详尽版） https://github.com/alibaba/p3c/blob/master/阿里巴巴Java开发手册（华山版）.pdf Google Java编程风格指南： http://hawstein.com/2014/01/20/google-java-style/ 个人 程序员你为什么这么累: https://xwjie.github.io/rule/ 如何写出优雅的 Java 代码 使用 IntelliJ IDEA 作为您的集成开发环境 (IDE) 使用 JDK 8 或更高版本 使用 Maven/Gradle 使用 Lombok 编写单元测试 重构:常见,但也很慢 注意代码规范 定期联络客户，以获取他们的反馈 上述建议的详细内容：八点建议助您写出优雅的Java代码。 更多代码优化相关内容推荐： 业务复杂=if else？刚来的大神竟然用策略+工厂彻底干掉了他们！ 一些不错的 Java 实践！推荐阅读3遍以上！ [解锁新姿势] 兄dei，你代码需要优化了 消灭 Java 代码的“坏味道” "},"zother6-JavaGuide/java/多线程系列.html":{"url":"zother6-JavaGuide/java/多线程系列.html","title":"多线程系列","keywords":"","body":" 多线程系列文章 下列文章，我都更新在了我的博客专栏：Java并发编程指南。 Java多线程学习（一）Java多线程入门 Java多线程学习（二）synchronized关键字（1） Java多线程学习（二）synchronized关键字（2） Java多线程学习（三）volatile关键字 Java多线程学习（四）等待/通知（wait/notify）机制 Java多线程学习（五）线程间通信知识点补充 Java多线程学习（六）Lock锁的使用 Java多线程学习（七）并发编程中一些问题 Java多线程学习（八）线程池与Executor 框架 多线程系列文章重要知识点与思维导图 Java多线程学习（一）Java多线程入门 Java多线程学习（二）synchronized关键字（1） 注意：可重入锁的概念。 另外要注意：synchronized取得的锁都是对象锁，而不是把一段代码或方法当做锁。 如果多个线程访问的是同一个对象，哪个线程先执行带synchronized关键字的方法，则哪个线程就持有该方法，那么其他线程只能呈等待状态。如果多个线程访问的是多个对象则不一定，因为多个对象会产生多个锁。 Java多线程学习（二）synchronized关键字（2） 注意： 其他线程执行对象中synchronized同步方法（上一节我们介绍过，需要回顾的可以看上一节的文章）和synchronized(this)代码块时呈现同步效果; 如果两个线程使用了同一个“对象监视器”（synchronized(object)）,运行结果同步，否则不同步. synchronized关键字加到static静态方法和synchronized(class)代码块上都是是给Class类上锁，而synchronized关键字加到非static静态方法上是给对象上锁。 数据类型String的常量池属性:在Jvm中具有String常量池缓存的功能 Java多线程学习（三）volatile关键字 注意： synchronized关键字和volatile关键字比较 Java多线程学习（四）等待/通知（wait/notify）机制 Java多线程学习（五）线程间通信知识点补充 注意： ThreadLocal类主要解决的就是让每个线程绑定自己的值，可以将ThreadLocal类形象的比喻成存放数据的盒子，盒子中可以存储每个线程的私有数据。 Java多线程学习（六）Lock锁的使用 Java多线程学习（七）并发编程中一些问题 Java多线程学习（八）线程池与Executor 框架 "},"zother6-JavaGuide/java/手把手教你定位常见Java性能问题.html":{"url":"zother6-JavaGuide/java/手把手教你定位常见Java性能问题.html","title":"手把手教你定位常见Java性能问题","keywords":"","body":"手把手教你定位常见Java性能问题 概述 性能优化一向是后端服务优化的重点，但是线上性能故障问题不是经常出现，或者受限于业务产品，根本就没办法出现性能问题，包括笔者自己遇到的性能问题也不多，所以为了提前储备知识，当出现问题的时候不会手忙脚乱，我们本篇文章来模拟下常见的几个Java性能故障，来学习怎么去分析和定位。 预备知识 既然是定位问题，肯定是需要借助工具，我们先了解下需要哪些工具可以帮忙定位问题。 top命令 top命令使我们最常用的Linux命令之一，它可以实时的显示当前正在执行的进程的CPU使用率，内存使用率等系统信息。top -Hp pid 可以查看线程的系统资源使用情况。 vmstat命令 vmstat是一个指定周期和采集次数的虚拟内存检测工具，可以统计内存，CPU，swap的使用情况，它还有一个重要的常用功能，用来观察进程的上下文切换。字段说明如下: r: 运行队列中进程数量（当数量大于CPU核数表示有阻塞的线程） b: 等待IO的进程数量 swpd: 使用虚拟内存大小 free: 空闲物理内存大小 buff: 用作缓冲的内存大小(内存和硬盘的缓冲区) cache: 用作缓存的内存大小（CPU和内存之间的缓冲区） si: 每秒从交换区写到内存的大小，由磁盘调入内存 so: 每秒写入交换区的内存大小，由内存调入磁盘 bi: 每秒读取的块数 bo: 每秒写入的块数 in: 每秒中断数，包括时钟中断。 cs: 每秒上下文切换数。 us: 用户进程执行时间百分比(user time) sy: 内核系统进程执行时间百分比(system time) wa: IO等待时间百分比 id: 空闲时间百分比 pidstat命令 pidstat 是 Sysstat 中的一个组件，也是一款功能强大的性能监测工具，top 和 vmstat 两个命令都是监测进程的内存、CPU 以及 I/O 使用情况，而 pidstat 命令可以检测到线程级别的。pidstat命令线程切换字段说明如下： UID ：被监控任务的真实用户ID。 TGID ：线程组ID。 TID：线程ID。 cswch/s：主动切换上下文次数，这里是因为资源阻塞而切换线程，比如锁等待等情况。 nvcswch/s：被动切换上下文次数，这里指CPU调度切换了线程。 jstack命令 jstack是JDK工具命令，它是一种线程堆栈分析工具，最常用的功能就是使用 jstack pid 命令查看线程的堆栈信息，也经常用来排除死锁情况。 jstat 命令 它可以检测Java程序运行的实时情况，包括堆内存信息和垃圾回收信息，我们常常用来查看程序垃圾回收情况。常用的命令是jstat -gc pid。信息字段说明如下： S0C：年轻代中 To Survivor 的容量（单位 KB）； S1C：年轻代中 From Survivor 的容量（单位 KB）； S0U：年轻代中 To Survivor 目前已使用空间（单位 KB）； S1U：年轻代中 From Survivor 目前已使用空间（单位 KB）； EC：年轻代中 Eden 的容量（单位 KB）； EU：年轻代中 Eden 目前已使用空间（单位 KB）； OC：老年代的容量（单位 KB）； OU：老年代目前已使用空间（单位 KB）； MC：元空间的容量（单位 KB）； MU：元空间目前已使用空间（单位 KB）； YGC：从应用程序启动到采样时年轻代中 gc 次数； YGCT：从应用程序启动到采样时年轻代中 gc 所用时间 (s)； FGC：从应用程序启动到采样时 老年代（Full Gc）gc 次数； FGCT：从应用程序启动到采样时 老年代代（Full Gc）gc 所用时间 (s)； GCT：从应用程序启动到采样时 gc 用的总时间 (s)。 jmap命令 jmap也是JDK工具命令，他可以查看堆内存的初始化信息以及堆内存的使用情况，还可以生成dump文件来进行详细分析。查看堆内存情况命令jmap -heap pid。 mat内存工具 MAT(Memory Analyzer Tool)工具是eclipse的一个插件(MAT也可以单独使用)，它分析大内存的dump文件时，可以非常直观的看到各个对象在堆空间中所占用的内存大小、类实例数量、对象引用关系、利用OQL对象查询，以及可以很方便的找出对象GC Roots的相关信息。 idea中也有这么一个插件，就是JProfiler。 相关阅读： 《性能诊断利器 JProfiler 快速入门和最佳实践》：https://segmentfault.com/a/1190000017795841 模拟环境准备 基础环境jdk1.8，采用SpringBoot框架来写几个接口来触发模拟场景，首先是模拟CPU占满情况 CPU占满 模拟CPU占满还是比较简单，直接写一个死循环计算消耗CPU即可。 /** * 模拟CPU占满 */ @GetMapping(\"/cpu/loop\") public void testCPULoop() throws InterruptedException { System.out.println(\"请求cpu死循环\"); Thread.currentThread().setName(\"loop-thread-cpu\"); int num = 0; while (true) { num++; if (num == Integer.MAX_VALUE) { System.out.println(\"reset\"); } num = 0; } } 请求接口地址测试curl localhost:8080/cpu/loop,发现CPU立马飙升到100% 通过执行top -Hp 32805 查看Java线程情况 执行 printf '%x' 32826 获取16进制的线程id，用于dump信息查询，结果为 803a。最后我们执行jstack 32805 |grep -A 20 803a来查看下详细的dump信息。 这里dump信息直接定位出了问题方法以及代码行，这就定位出了CPU占满的问题。 内存泄露 模拟内存泄漏借助了ThreadLocal对象来完成，ThreadLocal是一个线程私有变量，可以绑定到线程上，在整个线程的生命周期都会存在，但是由于ThreadLocal的特殊性，ThreadLocal是基于ThreadLocalMap实现的，ThreadLocalMap的Entry继承WeakReference，而Entry的Key是WeakReference的封装，换句话说Key就是弱引用，弱引用在下次GC之后就会被回收，如果ThreadLocal在set之后不进行后续的操作，因为GC会把Key清除掉，但是Value由于线程还在存活，所以Value一直不会被回收，最后就会发生内存泄漏。 /** * 模拟内存泄漏 */ @GetMapping(value = \"/memory/leak\") public String leak() { System.out.println(\"模拟内存泄漏\"); ThreadLocal localVariable = new ThreadLocal(); localVariable.set(new Byte[4096 * 1024]);// 为线程添加变量 return \"ok\"; } 我们给启动加上堆内存大小限制，同时设置内存溢出的时候输出堆栈快照并输出日志。 java -jar -Xms500m -Xmx500m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/heapdump.hprof -XX:+PrintGCTimeStamps -XX:+PrintGCDetails -Xloggc:/tmp/heaplog.log analysis-demo-0.0.1-SNAPSHOT.jar 启动成功后我们循环执行100次,for i in {1..500}; do curl localhost:8080/memory/leak;done,还没执行完毕，系统已经返回500错误了。查看系统日志出现了如下异常： java.lang.OutOfMemoryError: Java heap space 我们用jstat -gc pid 命令来看看程序的GC情况。 很明显，内存溢出了，堆内存经过45次 Full Gc 之后都没释放出可用内存，这说明当前堆内存中的对象都是存活的，有GC Roots引用，无法回收。那是什么原因导致内存溢出呢？是不是我只要加大内存就行了呢？如果是普通的内存溢出也许扩大内存就行了，但是如果是内存泄漏的话，扩大的内存不一会就会被占满，所以我们还需要确定是不是内存泄漏。我们之前保存了堆 Dump 文件，这个时候借助我们的MAT工具来分析下。导入工具选择Leak Suspects Report，工具直接就会给你列出问题报告。 这里已经列出了可疑的4个内存泄漏问题，我们点击其中一个查看详情。 这里已经指出了内存被线程占用了接近50M的内存，占用的对象就是ThreadLocal。如果想详细的通过手动去分析的话，可以点击Histogram,查看最大的对象占用是谁，然后再分析它的引用关系，即可确定是谁导致的内存溢出。 上图发现占用内存最大的对象是一个Byte数组，我们看看它到底被那个GC Root引用导致没有被回收。按照上图红框操作指引，结果如下图： 我们发现Byte数组是被线程对象引用的，图中也标明，Byte数组对像的GC Root是线程，所以它是不会被回收的，展开详细信息查看，我们发现最终的内存占用对象是被ThreadLocal对象占据了。这也和MAT工具自动帮我们分析的结果一致。 死锁 死锁会导致耗尽线程资源，占用内存，表现就是内存占用升高，CPU不一定会飙升(看场景决定)，如果是直接new线程，会导致JVM内存被耗尽，报无法创建线程的错误，这也是体现了使用线程池的好处。 ExecutorService service = new ThreadPoolExecutor(4, 10, 0, TimeUnit.SECONDS, new LinkedBlockingQueue(1024), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy()); /** * 模拟死锁 */ @GetMapping(\"/cpu/test\") public String testCPU() throws InterruptedException { System.out.println(\"请求cpu\"); Object lock1 = new Object(); Object lock2 = new Object(); service.submit(new DeadLockThread(lock1, lock2), \"deadLookThread-\" + new Random().nextInt()); service.submit(new DeadLockThread(lock2, lock1), \"deadLookThread-\" + new Random().nextInt()); return \"ok\"; } public class DeadLockThread implements Runnable { private Object lock1; private Object lock2; public DeadLockThread1(Object lock1, Object lock2) { this.lock1 = lock1; this.lock2 = lock2; } @Override public void run() { synchronized (lock2) { System.out.println(Thread.currentThread().getName()+\"get lock2 and wait lock1\"); try { TimeUnit.MILLISECONDS.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } synchronized (lock1) { System.out.println(Thread.currentThread().getName()+\"get lock1 and lock2 \"); } } } } 我们循环请求接口2000次，发现不一会系统就出现了日志错误，线程池和队列都满了,由于我选择的当队列满了就拒绝的策略，所以系统直接抛出异常。 java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.FutureTask@2760298 rejected from java.util.concurrent.ThreadPoolExecutor@7ea7cd51[Running, pool size = 10, active threads = 10, queued tasks = 1024, completed tasks = 846] 通过ps -ef|grep java命令找出 Java 进程 pid，执行jstack pid 即可出现java线程堆栈信息，这里发现了5个死锁，我们只列出其中一个，很明显线程pool-1-thread-2锁住了0x00000000f8387d88等待0x00000000f8387d98锁，线程pool-1-thread-1锁住了0x00000000f8387d98等待锁0x00000000f8387d88,这就产生了死锁。 Java stack information for the threads listed above: =================================================== \"pool-1-thread-2\": at top.luozhou.analysisdemo.controller.DeadLockThread2.run(DeadLockThread.java:30) - waiting to lock (a java.lang.Object) - locked (a java.lang.Object) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) \"pool-1-thread-1\": at top.luozhou.analysisdemo.controller.DeadLockThread1.run(DeadLockThread.java:30) - waiting to lock (a java.lang.Object) - locked (a java.lang.Object) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Found 5 deadlocks. 线程频繁切换 上下文切换会导致将大量CPU时间浪费在寄存器、内核栈以及虚拟内存的保存和恢复上，导致系统整体性能下降。当你发现系统的性能出现明显的下降时候，需要考虑是否发生了大量的线程上下文切换。 @GetMapping(value = \"/thread/swap\") public String theadSwap(int num) { System.out.println(\"模拟线程切换\"); for (int i = 0; i 这里我创建多个线程去执行基础的原子+1操作，然后让出 CPU 资源，理论上 CPU 就会去调度别的线程，我们请求接口创建100个线程看看效果如何，curl localhost:8080/thread/swap?num=100。接口请求成功后，我们执行`vmstat 1 10，表示每1秒打印一次，打印10次，线程切换采集结果如下： procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 101 0 128000 878384 908 468684 0 0 0 0 4071 8110498 14 86 0 0 0 100 0 128000 878384 908 468684 0 0 0 0 4065 8312463 15 85 0 0 0 100 0 128000 878384 908 468684 0 0 0 0 4107 8207718 14 87 0 0 0 100 0 128000 878384 908 468684 0 0 0 0 4083 8410174 14 86 0 0 0 100 0 128000 878384 908 468684 0 0 0 0 4083 8264377 14 86 0 0 0 100 0 128000 878384 908 468688 0 0 0 108 4182 8346826 14 86 0 0 0 这里我们关注4个指标，r,cs,us,sy。 r=100,说明等待的进程数量是100，线程有阻塞。 cs=800多万，说明每秒上下文切换了800多万次，这个数字相当大了。 us=14，说明用户态占用了14%的CPU时间片去处理逻辑。 sy=86，说明内核态占用了86%的CPU，这里明显就是做上下文切换工作了。 我们通过top命令以及top -Hp pid查看进程和线程CPU情况，发现Java线程CPU占满了，但是线程CPU使用情况很平均，没有某一个线程把CPU吃满的情况。 PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 87093 root 20 0 4194788 299056 13252 S 399.7 16.1 65:34.67 java PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 87189 root 20 0 4194788 299056 13252 R 4.7 16.1 0:41.11 java 87129 root 20 0 4194788 299056 13252 R 4.3 16.1 0:41.14 java 87130 root 20 0 4194788 299056 13252 R 4.3 16.1 0:40.51 java 87133 root 20 0 4194788 299056 13252 R 4.3 16.1 0:40.59 java 87134 root 20 0 4194788 299056 13252 R 4.3 16.1 0:40.95 java 结合上面用户态CPU只使用了14%，内核态CPU占用了86%，可以基本判断是Java程序线程上下文切换导致性能问题。 我们使用pidstat命令来看看Java进程内部的线程切换数据，执行pidstat -p 87093 -w 1 10,采集数据如下： 11:04:30 PM UID TGID TID cswch/s nvcswch/s Command 11:04:30 PM 0 - 87128 0.00 16.07 |__java 11:04:30 PM 0 - 87129 0.00 15.60 |__java 11:04:30 PM 0 - 87130 0.00 15.54 |__java 11:04:30 PM 0 - 87131 0.00 15.60 |__java 11:04:30 PM 0 - 87132 0.00 15.43 |__java 11:04:30 PM 0 - 87133 0.00 16.02 |__java 11:04:30 PM 0 - 87134 0.00 15.66 |__java 11:04:30 PM 0 - 87135 0.00 15.23 |__java 11:04:30 PM 0 - 87136 0.00 15.33 |__java 11:04:30 PM 0 - 87137 0.00 16.04 |__java 根据上面采集的信息，我们知道Java的线程每秒切换15次左右，正常情况下，应该是个位数或者小数。结合这些信息我们可以断定Java线程开启过多，导致频繁上下文切换，从而影响了整体性能。 为什么系统的上下文切换是每秒800多万，而 Java 进程中的某一个线程切换才15次左右？ 系统上下文切换分为三种情况: 1、多任务：在多任务环境中，一个进程被切换出CPU，运行另外一个进程，这里会发生上下文切换。 2、中断处理：发生中断时，硬件会切换上下文。在vmstat命令中是in 3、用户和内核模式切换：当操作系统中需要在用户模式和内核模式之间进行转换时，需要进行上下文切换,比如进行系统函数调用。 Linux 为每个 CPU 维护了一个就绪队列，将活跃进程按照优先级和等待 CPU 的时间排序，然后选择最需要 CPU 的进程，也就是优先级最高和等待 CPU 时间最长的进程来运行。也就是vmstat命令中的r。 那么，进程在什么时候才会被调度到 CPU 上运行呢？ 进程执行完终止了，它之前使用的 CPU 会释放出来，这时再从就绪队列中拿一个新的进程来运行 为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片被轮流分配给各个进程。当某个进程时间片耗尽了就会被系统挂起，切换到其它等待 CPU 的进程运行。 进程在系统资源不足时，要等待资源满足后才可以运行，这时进程也会被挂起，并由系统调度其它进程运行。 当进程通过睡眠函数 sleep 主动挂起时，也会重新调度。 当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行。 发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序。 结合我们之前的内容分析，阻塞的就绪队列是100左右，而我们的CPU只有4核，这部分原因造成的上下文切换就可能会相当高，再加上中断次数是4000左右和系统的函数调用等，整个系统的上下文切换到800万也不足为奇了。Java内部的线程切换才15次，是因为线程使用Thread.yield()来让出CPU资源，但是CPU有可能继续调度该线程，这个时候线程之间并没有切换，这也是为什么内部的某个线程切换次数并不是非常大的原因。 总结 本文模拟了常见的性能问题场景，分析了如何定位CPU100%、内存泄漏、死锁、线程频繁切换问题。分析问题我们需要做好两件事，第一，掌握基本的原理，第二，借助好工具。本文也列举了分析问题的常用工具和命令，希望对你解决问题有所帮助。当然真正的线上环境可能十分复杂，并没有模拟的环境那么简单，但是原理是一样的，问题的表现也是类似的，我们重点抓住原理，活学活用，相信复杂的线上问题也可以顺利解决。 参考 1、https://linux.die.net/man/1/pidstat 2、https://linux.die.net/man/8/vmstat 3、https://help.eclipse.org/2020-03/index.jsp?topic=/org.eclipse.mat.ui.help/welcome.html 4、https://www.linuxblogs.cn/articles/18120200.html 5、https://www.tutorialspoint.com/what-is-context-switching-in-operating-system "},"zother6-JavaGuide/network/HTTPS中的TLS.html":{"url":"zother6-JavaGuide/network/HTTPS中的TLS.html","title":"HTTPS中的TLS","keywords":"","body":" 1. SSL 与 TLS 2. 从网络协议的角度理解 HTTPS 3. 从密码学的角度理解 HTTPS 3.1. TLS 工作流程 3.2. 密码基础 3.2.1. 伪随机数生成器 3.2.2. 消息认证码 3.2.3. 数字签名 3.2.4. 公钥密码 3.2.5. 证书 3.2.6. 密码小结 3.3. TLS 使用的密码技术 3.4. TLS 总结 4. RSA 简单示例 5. 参考 1. SSL 与 TLS SSL：（Secure Socket Layer） 安全套接层，于 1994 年由网景公司设计，并于 1995 年发布了 3.0 版本TLS：（Transport Layer Security）传输层安全性协议，是 IETF 在 SSL3.0 的基础上设计的协议以下全部使用 TLS 来表示 2. 从网络协议的角度理解 HTTPS HTTP：HyperText Transfer Protocol 超文本传输协议HTTPS：Hypertext Transfer Protocol Secure 超文本传输安全协议TLS：位于 HTTP 和 TCP 之间的协议，其内部有 TLS握手协议、TLS记录协议HTTPS 经由 HTTP 进行通信，但利用 TLS 来保证安全，即 HTTPS = HTTP + TLS 3. 从密码学的角度理解 HTTPS HTTPS 使用 TLS 保证安全，这里的“安全”分两部分，一是传输内容加密、二是服务端的身份认证 3.1. TLS 工作流程 此为服务端单向认证，还有客户端/服务端双向认证，流程类似，只不过客户端也有自己的证书，并发送给服务器进行验证 3.2. 密码基础 3.2.1. 伪随机数生成器 为什么叫伪随机数，因为没有真正意义上的随机数，具体可以参考 Random/TheadLocalRandom它的主要作用在于生成对称密码的秘钥、用于公钥密码生成秘钥对 3.2.2. 消息认证码 消息认证码主要用于验证消息的完整性与消息的认证，其中消息的认证指“消息来自正确的发送者” 消息认证码用于验证和认证，而不是加密 发送者与接收者事先共享秘钥 发送者根据发送消息计算 MAC 值 发送者发送消息和 MAC 值 接收者根据接收到的消息计算 MAC 值 接收者根据自己计算的 MAC 值与收到的 MAC 对比 如果对比成功，说明消息完整，并来自与正确的发送者 3.2.3. 数字签名 消息认证码的缺点在于无法防止否认，因为共享秘钥被 client、server 两端拥有，server 可以伪造 client 发送给自己的消息（自己给自己发送消息），为了解决这个问题，我们需要它们有各自的秘钥不被第二个知晓（这样也解决了共享秘钥的配送问题） 数字签名和消息认证码都不是为了加密可以将单向散列函数获取散列值的过程理解为使用 md5 摘要算法获取摘要的过程 使用自己的私钥对自己所认可的消息生成一个该消息专属的签名，这就是数字签名，表明我承认该消息来自自己注意：私钥用于加签，公钥用于解签，每个人都可以解签，查看消息的归属人 3.2.4. 公钥密码 公钥密码也叫非对称密码，由公钥和私钥组成，它是最开始是为了解决秘钥的配送传输安全问题，即，我们不配送私钥，只配送公钥，私钥由本人保管它与数字签名相反，公钥密码的私钥用于解密、公钥用于加密，每个人都可以用别人的公钥加密，但只有对应的私钥才能解开密文client：明文 + 公钥 = 密文server：密文 + 私钥 = 明文注意：公钥用于加密，私钥用于解密，只有私钥的归属者，才能查看消息的真正内容 3.2.5. 证书 证书：全称公钥证书（Public-Key Certificate, PKC）,里面保存着归属者的基本信息，以及证书过期时间、归属者的公钥，并由认证机构（Certification Authority, CA）施加数字签名，表明，某个认证机构认定该公钥的确属于此人 想象这个场景：你想在支付宝页面交易，你需要支付宝的公钥进行加密通信，于是你从百度上搜索关键字“支付宝公钥”，你获得了支什宝的公钥，这个时候，支什宝通过中间人攻击，让你访问到了他们支什宝的页面，最后你在这个支什宝页面完美的使用了支什宝的公钥完成了与支什宝的交易 在上面的场景中，你可以理解支付宝证书就是由支付宝的公钥、和给支付宝颁发证书的企业的数字签名组成任何人都可以给自己或别人的公钥添加自己的数字签名，表明：我拿我的尊严担保，我的公钥/别人的公钥是真的，至于信不信那是另一回事了 3.2.6. 密码小结 密码 作用 组成 消息认证码 确认消息的完整、并对消息的来源认证 共享秘钥+消息的散列值 数字签名 对消息的散列值签名 公钥+私钥+消息的散列值 公钥密码 解决秘钥的配送问题 公钥+私钥+消息 证书 解决公钥的归属问题 公钥密码中的公钥+数字签名 3.3. TLS 使用的密码技术 伪随机数生成器：秘钥生成随机性，更难被猜测 对称密码：对称密码使用的秘钥就是由伪随机数生成，相较于非对称密码，效率更高 消息认证码：保证消息信息的完整性、以及验证消息信息的来源 公钥密码：证书技术使用的就是公钥密码 数字签名：验证证书的签名，确定由真实的某个 CA 颁发 证书：解决公钥的真实归属问题，降低中间人攻击概率 3.4. TLS 总结 TLS 是一系列密码工具的框架，作为框架，它也是非常的灵活，体现在每个工具套件它都可以替换，即：客户端与服务端之间协商密码套件，从而更难的被攻破，例如使用不同方式的对称密码，或者公钥密码、数字签名生成方式、单向散列函数技术的替换等 4. RSA 简单示例 RSA 是一种公钥密码算法，我们简单的走一遍它的加密解密过程加密算法：密文 = (明文^E) mod N，其中公钥为{E,N}，即”求明文的E次方的对 N 的余数“解密算法：明文 = (密文^D) mod N，其中秘钥为{D,N}，即”求密文的D次方的对 N 的余数“例：我们已知公钥为{5,323}，私钥为{29,323}，明文为300，请写出加密和解密的过程： 加密：密文 = 123 ^ 5 mod 323 = 225解密：明文 = 225 ^ 29 mod 323 = [[(225 ^ 5) mod 323] [(225 ^ 5) mod 323] [(225 ^ 5) mod 323] [(225 ^ 5) mod 323] [(225 ^ 5) mod 323] [(225 ^ 4) mod 323]] mod 323 = (4 4 4 4 4 290) mod 323 = 123 5. 参考 SSL加密发生在哪里：https://security.stackexchange.com/questions/19681/where-does-ssl-encryption-take-place TLS工作流程：https://blog.csdn.net/ustccw/article/details/76691248 《图解密码技术》：https://book.douban.com/subject/26822106/ 豆瓣评分 9.5 "},"zother6-JavaGuide/network/干货：计算机网络知识总结.html":{"url":"zother6-JavaGuide/network/干货：计算机网络知识总结.html","title":"干货：计算机网络知识总结","keywords":"","body":"1. 计算机概述 2. 物理层 3. 数据链路层 4. 网络层 5. 运输层 6. 应用层 一计算机概述 （1），基本术语 结点 （node）： 网络中的结点可以是计算机，集线器，交换机或路由器等。 链路（link ）： 从一个结点到另一个结点的一段物理线路。中间没有任何其他交点。 主机（host）： 连接在因特网上的计算机. ISP（Internet Service Provider）： 因特网服务提供者（提供商）. IXP（Internet eXchange Point）： 互联网交换点IXP的主要作用就是允许两个网络直接相连并交换分组，而不需要再通过第三个网络来转发分组。. RFC(Request For Comments) 意思是“请求评议”，包含了关于Internet几乎所有的重要的文字资料。 广域网WAN（Wide Area Network） 任务是通过长距离运送主机发送的数据 城域网MAN（Metropolitan Area Network） 用来将多个局域网进行互连 局域网LAN（Local Area Network） 学校或企业大多拥有多个互连的局域网 个人区域网PAN（Personal Area Network） 在个人工作的地方把属于个人使用的电子设备用无线技术连接起来的网络 端系统（end system）： 处在因特网边缘的部分即是连接在因特网上的所有的主机. 分组（packet ）： 因特网中传送的数据单元。由首部header和数据段组成。分组又称为包，首部可称为包头。 存储转发（store and forward ）: 路由器收到一个分组，先存储下来，再检查其首部，查找转发表，按照首部中的目的地址，找到合适的接口转发出去。 带宽（bandwidth）： 在计算机网络中，表示在单位时间内从网络中的某一点到另一点所能通过的“最高数据率”。常用来表示网络的通信线路所能传送数据的能力。单位是“比特每秒”，记为b/s。 吞吐量（throughput ）： 表示在单位时间内通过某个网络（或信道、接口）的数据量。吞吐量更经常地用于对现实世界中的网络的一种测量，以便知道实际上到底有多少数据量能够通过网络。吞吐量受网络的带宽或网络的额定速率的限制。 （2），重要知识点总结 1，计算机网络（简称网络）把许多计算机连接在一起，而互联网把许多网络连接在一起，是网络的网络。 2，小写字母i开头的internet（互联网）是通用名词，它泛指由多个计算机网络相互连接而成的网络。在这些网络之间的通信协议（即通信规则）可以是任意的。 大写字母I开头的Internet（互联网）是专用名词，它指全球最大的，开放的，由众多网络相互连接而成的特定的互联网，并采用TCP/IP协议作为通信规则，其前身为ARPANET。Internet的推荐译名为因特网，现在一般流行称为互联网。 3，路由器是实现分组交换的关键构件，其任务是转发收到的分组，这是网络核心部分最重要的功能。分组交换采用存储转发技术，表示把一个报文（要发送的整块数据）分为几个分组后再进行传送。在发送报文之前，先把较长的报文划分成为一个个更小的等长数据段。在每个数据端的前面加上一些由必要的控制信息组成的首部后，就构成了一个分组。分组又称为包。分组是在互联网中传送的数据单元，正是由于分组的头部包含了诸如目的地址和源地址等重要控制信息，每一个分组才能在互联网中独立的选择传输路径，并正确地交付到分组传输的终点。 4，互联网按工作方式可划分为边缘部分和核心部分。主机在网络的边缘部分，其作用是进行信息处理。由大量网络和连接这些网络的路由器组成核心部分，其作用是提供连通性和交换。 5，计算机通信是计算机中进程（即运行着的程序）之间的通信。计算机网络采用的通信方式是客户-服务器方式（C/S方式）和对等连接方式（P2P方式）。 6，客户和服务器都是指通信中所涉及的应用进程。客户是服务请求方，服务器是服务提供方。 7，按照作用范围的不同，计算机网络分为广域网WAN，城域网MAN，局域网LAN，个人区域网PAN。 8，计算机网络最常用的性能指标是：速率，带宽，吞吐量，时延（发送时延，处理时延，排队时延），时延带宽积，往返时间和信道利用率。 9，网络协议即协议，是为进行网络中的数据交换而建立的规则。计算机网络的各层以及其协议集合，称为网络的体系结构。 10，五层体系结构由应用层，运输层，网络层（网际层），数据链路层，物理层组成。运输层最主要的协议是TCP和UDP协议，网络层最重要的协议是IP协议。 ## 二物理层 ### （1），基本术语 #### 数据（data）： 运送消息的实体。 #### 信号（signal）： 数据的电气的或电磁的表现。或者说信号是适合在传输介质上传输的对象。 #### 码元（ code）： 在使用时间域（或简称为时域）的波形来表示数字信号时，代表不同离散数值的基本波形。 #### 单工（simplex ）： 只能有一个方向的通信而没有反方向的交互。 #### 半双工（half duplex ）： 通信的双方都可以发送信息，但不能双方同时发送(当然也就不能同时接收)。 #### 全双工（full duplex）： 通信的双方可以同时发送和接收信息。 #### 奈氏准则： 在任何信道中，码元的传输的效率是有上限的，传输速率超过此上限，就会出现严重的码间串扰问题，使接收端对码元的判决（即识别）成为不可能。 #### 基带信号（baseband signal）： 来自信源的信号。指没有经过调制的数字信号或模拟信号。 #### 带通（频带）信号（bandpass signal）： 把基带信号经过载波调制后，把信号的频率范围搬移到较高的频段以便在信道中传输（即仅在一段频率范围内能够通过信道），这里调制过后的信号就是带通信号。 #### 调制（modulation ）： 对信号源的信息进行处理后加到载波信号上，使其变为适合在信道传输的形式的过程。 #### 信噪比（signal-to-noise ratio ）： 指信号的平均功率和噪声的平均功率之比，记为S/N。信噪比（dB）=10*log10（S/N） #### 信道复用（channel multiplexing ）： 指多个用户共享同一个信道。（并不一定是同时） #### 比特率（bit rate ）： 单位时间（每秒）内传送的比特数。 #### 波特率（baud rate）： 单位时间载波调制状态改变的次数。针对数据信号对载波的调制速率。 #### 复用（multiplexing）： 共享信道的方法 #### ADSL（Asymmetric Digital Subscriber Line ）： 非对称数字用户线。 #### 光纤同轴混合网（HFC网）: 在目前覆盖范围很广的有线电视网的基础上开发的一种居民宽带接入网 ### （2），重要知识点总结 1，物理层的主要任务就是确定与传输媒体接口有关的一些特性，如机械特性，电气特性，功能特性，过程特性。 2，一个数据通信系统可划分为三大部分，即源系统，传输系统，目的系统。源系统包括源点（或源站，信源）和发送器，目的系统包括接收器和终点。 3，通信的目的是传送消息。如话音，文字，图像等都是消息，数据是运送消息的实体。信号则是数据的电器或电磁的表现。 4，根据信号中代表消息的参数的取值方式不同，信号可分为模拟信号（或连续信号）和数字信号（或离散信号）。在使用时间域（简称时域）的波形表示数字信号时，代表不同离散数值的基本波形称为码元。 5，根据双方信息交互的方式，通信可划分为单向通信（或单工通信），双向交替通信（或半双工通信），双向同时通信（全双工通信）。 6，来自信源的信号称为基带信号。信号要在信道上传输就要经过调制。调制有基带调制和带通调制之分。最基本的带通调制方法有调幅，调频和调相。还有更复杂的调制方法，如正交振幅调制。 7，要提高数据在信道上的传递速率，可以使用更好的传输媒体，或使用先进的调制技术。但数据传输速率不可能任意被提高。 8，传输媒体可分为两大类，即导引型传输媒体（双绞线，同轴电缆，光纤）和非导引型传输媒体（无线，红外，大气激光）。 9，为了有效利用光纤资源，在光纤干线和用户之间广泛使用无源光网络PON。无源光网络无需配备电源，其长期运营成本和管理成本都很低。最流行的无源光网络是以太网无源光网络EPON和吉比特无源光网络GPON。 （3），最重要的知识点 ①，物理层的任务 透明地传送比特流。也可以将物理层的主要任务描述为确定与传输媒体的接口的一些特性，即：机械特性（接口所用接线器的一些物理属性如形状尺寸），电气特性（接口电缆的各条线上出现的电压的范围），功能特性（某条线上出现的某一电平的电压的意义），过程特性（对于不同功能能的各种可能事件的出现顺序）。 拓展： 物理层考虑的是怎样才能在连接各种计算机的传输媒体上传输数据比特流，而不是指具体的传输媒体。现有的计算机网络中的硬件设备和传输媒体的种类非常繁多，而且通信手段也有许多不同的方式。物理层的作用正是尽可能地屏蔽掉这些传输媒体和通信手段的差异，使物理层上面的数据链路层感觉不到这些差异，这样就可以使数据链路层只考虑完成本层的协议和服务，而不必考虑网络的具体传输媒体和通信手段是什么。 ②，几种常用的信道复用技术 ③，几种常用的宽带接入技术，主要是ADSL和FTTx 用户到互联网的宽带接入方法有非对称数字用户线ADSL（用数字技术对现有的模拟电话线进行改造，而不需要重新布线。ASDL的快速版本是甚高速数字用户线VDSL。），光纤同轴混合网HFC（是在目前覆盖范围很广的有线电视网的基础上开发的一种居民宽带接入网）和FTTx（即光纤到······）。 三数据链路层 （1），基本术语 链路（link）： 一个结点到相邻结点的一段物理链路 数据链路（data link）： 把实现控制数据运输的协议的硬件和软件加到链路上就构成了数据链路 循环冗余检验CRC（Cyclic Redundancy Check）： 为了保证数据传输的可靠性，CRC是数据链路层广泛使用的一种检错技术 帧（frame）： 一个数据链路层的传输单元，由一个数据链路层首部和其携带的封包所组成协议数据单元。 MTU（Maximum Transfer Uint ）： 最大传送单元。帧的数据部分的的长度上限。 误码率BER（Bit Error Rate ）： 在一段时间内，传输错误的比特占所传输比特总数的比率。 PPP（Point-to-Point Protocol ）： 点对点协议。即用户计算机和ISP进行通信时所使用的数据链路层协议。以下是PPP帧的示意图: MAC地址（Media Access Control或者Medium Access Control）： 意译为媒体访问控制，或称为物理地址、硬件地址，用来定义网络设备的位置。 在OSI模型中，第三层网络层负责 IP地址，第二层数据链路层则负责 MAC地址。 因此一个主机会有一个MAC地址，而每个网络位置会有一个专属于它的IP地址 。 地址是识别某个系统的重要标识符，“名字指出我们所要寻找的资源，地址指出资源所在的地方，路由告诉我们如何到达该处” 网桥（bridge）： 一种用于数据链路层实现中继，连接两个或多个局域网的网络互连设备。 交换机（switch ）： 广义的来说，交换机指的是一种通信系统中完成信息交换的设备。这里工作在数据链路层的交换机指的是交换式集线器，其实质是一个多接口的网桥 （2），重要知识点总结 1，链路是从一个结点到相邻节点的一段物理链路，数据链路则在链路的基础上增加了一些必要的硬件（如网络适配器）和软件（如协议的实现） 2，数据链路层使用的主要是**点对点信道**和**广播信道**两种。 3，数据链路层传输的协议数据单元是帧。数据链路层的三个基本问题是：**封装成帧**，**透明传输**和**差错检测** 4，**循环冗余检验CRC**是一种检错方法，而帧检验序列FCS是添加在数据后面的冗余码 5，**点对点协议PPP**是数据链路层使用最多的一种协议，它的特点是：简单，只检测差错而不去纠正差错，不使用序号，也不进行流量控制，可同时支持多种网络层协议 6，PPPoE是为宽带上网的主机使用的链路层协议 7，局域网的优点是：具有广播功能，从一个站点可方便地访问全网；便于系统的扩展和逐渐演变；提高了系统的可靠性，可用性和生存性。 8，共向媒体通信资源的方法有二：一是静态划分信道(各种复用技术)，而是动态媒体接入控制，又称为多点接入（随即接入或受控接入） 9，计算机与外接局域网通信需要通过通信适配器（或网络适配器），它又称为网络接口卡或网卡。**计算器的硬件地址就在适配器的ROM中**。 10，以太网采用的无连接的工作方式，对发送的数据帧不进行编号，也不要求对方发回确认。目的站收到有差错帧就把它丢掉，其他什么也不做 11，以太网采用的协议是具有冲突检测的**载波监听多点接入CSMA/CD**。协议的特点是：**发送前先监听，边发送边监听，一旦发现总线上出现了碰撞，就立即停止发送。然后按照退避算法等待一段随机时间后再次发送。** 因此，每一个站点在自己发送数据之后的一小段时间内，存在这遭遇碰撞的可能性。以太网上的各站点平等的争用以太网信道 12，以太网的适配器具有过滤功能，它只接收单播帧，广播帧和多播帧。 13，使用集线器可以在物理层扩展以太网（扩展后的以太网仍然是一个网络） ### （3），最重要的知识点 #### ① 数据链路层的点对点信道和广播信道的特点，以及这两种信道所使用的协议（PPP协议以及CSMA/CD协议）的特点 #### ② 数据链路层的三个基本问题：**封装成帧**，**透明传输**，**差错检测** #### ③ 以太网的MAC层硬件地址 #### ④ 适配器，转发器，集线器，网桥，以太网交换机的作用以及适用场合 ## 四网络层 ### （1），基本术语 #### 虚电路（Virtual Circuit）： 在两个终端设备的逻辑或物理端口之间，通过建立的双向的透明传输通道。虚电路表示这只是一条逻辑上的连接，分组都沿着这条逻辑连接按照存储转发方式传送，而并不是真正建立了一条物理连接。 #### IP（Internet Protocol ）： 网际协议 IP 是 TCP/IP体系中两个最主要的协议之一，是TCP/IP体系结构网际层的核心。配套的有ARP，RARP，ICMP，IGMP。 ![这里写图片描述](https://user-gold-cdn.xitu.io/2018/4/1/1627f92f98436286?w=453&h=331&f=jpeg&s=27535) #### ARP（Address Resolution Protocol）： 地址解析协议 #### ICMP（Internet Control Message Protocol ）： 网际控制报文协议 （ICMP 允许主机或路由器报告差错情况和提供有关异常情况的报告。） #### 子网掩码（subnet mask ）： 它是一种用来指明一个IP地址的哪些位标识的是主机所在的子网以及哪些位标识的是主机的位掩码。子网掩码不能单独存在，它必须结合IP地址一起使用。 #### CIDR（ Classless Inter-Domain Routing ）： 无分类域间路由选择 （特点是消除了传统的 A 类、B 类和 C 类地址以及划分子网的概念，并使用各种长度的“网络前缀”(network-prefix)来代替分类地址中的网络号和子网号） #### 默认路由（default route）： 当在路由表中查不到能到达目的地址的路由时，路由器选择的路由。默认路由还可以减小路由表所占用的空间和搜索路由表所用的时间。 #### 路由选择算法（Virtual Circuit）： 路由选择协议的核心部分。因特网采用自适应的，分层次的路由选择协议。 ### （2），重要知识点总结 1，TCP/IP协议中的网络层向上只提供简单灵活的，无连接的，尽最大努力交付的数据报服务。网络层不提供服务质量的承诺，不保证分组交付的时限所传送的分组可能出错，丢失，重复和失序。进程之间通信的可靠性由运输层负责 2，在互联网的交付有两种，一是在本网络直接交付不用经过路由器，另一种是和其他网络的间接交付，至少经过一个路由器，但最后一次一定是直接交付 3，分类的IP地址由网络号字段（指明网络）和主机号字段（指明主机）组成。网络号字段最前面的类别指明IP地址的类别。IP地址是一种分等级的地址结构。IP地址管理机构分配IP地址时只分配网络号，主机号由得到该网络号的单位自行分配。路由器根据目的主机所连接的网络号来转发分组。一个路由器至少连接到两个网络，所以一个路由器至少应当有两个不同的IP地址 4，IP数据报分为首部和数据两部分。首部的前一部分是固定长度，共20字节，是所有IP数据包必须具有的（源地址，目的地址，总长度等重要地段都固定在首部）。一些长度可变的可选字段固定在首部的后面。IP首部中的生存时间给出了IP数据报在互联网中所能经过的最大路由器数。可防止IP数据报在互联网中无限制的兜圈子。 5，地址解析协议ARP把IP地址解析为硬件地址。ARP的高速缓存可以大大减少网络上的通信量。因为这样可以使主机下次再与同样地址的主机通信时，可以直接从高速缓存中找到所需要的硬件地址而不需要再去广播方式发送ARP请求分组 6，无分类域间路由选择CIDR是解决目前IP地址紧缺的一个好办法。CIDR记法把IP地址后面加上斜线“/”，然后写上前缀所所占的位数。前缀（或网络前缀用来指明网络），前缀后面的部分是后缀，用来指明主机。CIDR把前缀都相同的连续的IP地址组成一个“CIDR地址块”，IP地址分配都以CIDR地址块为单位。 7， 网际控制报文协议是IP层的协议.ICMP报文作为IP数据报的数据，加上首部后组成IP数据报发送出去。使用ICMP数据报并不是为了实现可靠传输。ICMP允许主机或路由器报告差错情况和提供有关异常情况的报告。ICMP报文的种类有两种 ICMP差错报告报文和ICMP询问报文。 8，要解决IP地址耗尽的问题，最根本的办法是采用具有更大地址空间的新版本IP协议-IPv6。IPv6所带来的变化有①更大的地址空间（采用128位地址）②灵活的首部格式③改进的选项④支持即插即用⑤支持资源的预分配⑥IPv6的首部改为8字节对齐。另外IP数据报的目的地址可以是以下三种基本类型地址之一：单播，多播和任播 9，虚拟专用网络VPN利用公用的互联网作为本机构专用网之间的通信载体。VPN内使用互联网的专用地址。一个VPN至少要有一个路由器具有合法的全球IP地址，这样才能和本系统的另一个VPN通过互联网进行通信。所有通过互联网传送的数据都需要加密 10， MPLS的特点是：①支持面向连接的服务质量②支持流量工程，平衡网络负载③有效的支持虚拟专用网VPN。MPLS在入口节点给每一个IP数据报打上固定长度的“标记”，然后根据标记在第二层（链路层）用硬件进行转发（在标记交换路由器中进行标记交换），因而转发速率大大加快。 （3），最重要知识点 ① 虚拟互联网络的概念 ② IP地址和物理地址的关系 ③ 传统的分类的IP地址（包括子网掩码）和无分类域间路由选择CIDR ④ 路由选择协议的工作原理 五运输层 （1），基本术语 进程（process）： 指计算机中正在运行的程序实体 应用进程互相通信： 一台主机的进程和另一台主机中的一个进程交换数据的过程（另外注意通信真正的端点不是主机而是主机中的进程，也就是说端到端的通信是应用进程之间的通信） 传输层的复用与分用： 复用指发送方不同的进程都可以通过统一个运输层协议传送数据。分用指接收方的运输层在剥去报文的首部后能把这些数据正确的交付到目的应用进程。 TCP（Transmission Control Protocol）： 传输控制协议 UDP（User Datagram Protocol）： 用户数据报协议 端口（port）（link）： 端口的目的是为了确认对方机器是那个进程在于自己进行交互，比如MSN和QQ的端口不同，如果没有端口就可能出现QQ进程和MSN交互错误。端口又称协议端口号。 停止等待协议（link）： 指发送方每发送完一个分组就停止发送，等待对方确认，在收到确认之后在发送下一个分组。 流量控制（link）： 就是让发送方的发送速率不要太快，既要让接收方来得及接收，也不要使网络发生拥塞。 拥塞控制（link）： 防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提，就是网络能够承受现有的网络负荷。 （2），重要知识点总结 1，运输层提供应用进程之间的逻辑通信，也就是说，运输层之间的通信并不是真正在两个运输层之间直接传输数据。运输层向应用层屏蔽了下面网络的细节（如网络拓补，所采用的路由选择协议等），它使应用进程之间看起来好像两个运输层实体之间有一条端到端的逻辑通信信道。 2，网络层为主机提供逻辑通信，而运输层为应用进程之间提供端到端的逻辑通信。 3，运输层的两个重要协议是用户数据报协议UDP和传输控制协议TCP。按照OSI的术语，两个对等运输实体在通信时传送的数据单位叫做运输协议数据单元TPDU（Transport Protocol Data Unit）。但在TCP/IP体系中，则根据所使用的协议是TCP或UDP，分别称之为TCP报文段或UDP用户数据报。 4，UDP在传送数据之前不需要先建立连接，远地主机在收到UDP报文后，不需要给出任何确认。虽然UDP不提供可靠交付，但在某些情况下UDP确是一种最有效的工作方式。 TCP提供面向连接的服务。在传送数据之前必须先建立连接，数据传送结束后要释放连接。TCP不提供广播或多播服务。由于TCP要提供可靠的，面向连接的传输服务，这一难以避免增加了许多开销，如确认，流量控制，计时器以及连接管理等。这不仅使协议数据单元的首部增大很多，还要占用许多处理机资源。 5，硬件端口是不同硬件设备进行交互的接口，而软件端口是应用层各种协议进程与运输实体进行层间交互的一种地址。UDP和TCP的首部格式中都有源端口和目的端口这两个重要字段。当运输层收到IP层交上来的运输层报文时，就能够 根据其首部中的目的端口号把数据交付应用层的目的应用层。（两个进程之间进行通信不光要知道对方IP地址而且要知道对方的端口号(为了找到对方计算机中的应用进程)） 6，运输层用一个16位端口号标志一个端口。端口号只有本地意义，它只是为了标志计算机应用层中的各个进程在和运输层交互时的层间接口。在互联网的不同计算机中，相同的端口号是没有关联的。协议端口号简称端口。虽然通信的终点是应用进程，但只要把所发送的报文交到目的主机的某个合适端口，剩下的工作（最后交付目的进程）就由TCP和UDP来完成。 7，运输层的端口号分为服务器端使用的端口号（0~1023指派给熟知端口，1024~49151是登记端口号）和客户端暂时使用的端口号（49152~65535） 8，UDP的主要特点是①无连接②尽最大努力交付③面向报文④无拥塞控制⑤支持一对一，一对多，多对一和多对多的交互通信⑥首部开销小（只有四个字段：源端口，目的端口，长度和检验和） 9，TCP的主要特点是①面向连接②每一条TCP连接只能是一对一的③提供可靠交付④提供全双工通信⑤面向字节流 10，TCP用主机的IP地址加上主机上的端口号作为TCP连接的端点。这样的端点就叫做套接字（socket）或插口。套接字用（IP地址：端口号）来表示。每一条TCP连接唯一被通信两端的两个端点所确定。 11，停止等待协议是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组。 12，为了提高传输效率，发送方可以不使用低效率的停止等待协议，而是采用流水线传输。流水线传输就是发送方可连续发送多个分组，不必每发完一个分组就停下来等待对方确认。这样可使信道上一直有数据不间断的在传送。这种传输方式可以明显提高信道利用率。 13，停止等待协议中超时重传是指只要超过一段时间仍然没有收到确认，就重传前面发送过的分组（认为刚才发送过的分组丢失了）。因此每发送完一个分组需要设置一个超时计时器，其重转时间应比数据在分组传输的平均往返时间更长一些。这种自动重传方式常称为自动重传请求ARQ。另外在停止等待协议中若收到重复分组，就丢弃该分组，但同时还要发送确认。连续ARQ协议可提高信道利用率。发送维持一个发送窗口，凡位于发送窗口内的分组可连续发送出去，而不需要等待对方确认。接收方一般采用累积确认，对按序到达的最后一个分组发送确认，表明到这个分组位置的所有分组都已经正确收到了。 14，TCP报文段的前20个字节是固定的，后面有4n字节是根据需要增加的选项。因此，TCP首部的最小长度是20字节。 15，TCP使用滑动窗口机制。发送窗口里面的序号表示允许发送的序号。发送窗口后沿的后面部分表示已发送且已收到确认，而发送窗口前沿的前面部分表示不允许发送。发送窗口后沿的变化情况有两种可能，即不动（没有收到新的确认）和前移（收到了新的确认）。发送窗口的前沿通常是不断向前移动的。一般来说，我们总是希望数据传输更快一些。但如果发送方把数据发送的过快，接收方就可能来不及接收，这就会造成数据的丢失。所谓流量控制就是让发送方的发送速率不要太快，要让接收方来得及接收。 16，在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏。这种情况就叫拥塞。拥塞控制就是为了防止过多的数据注入到网络中，这样就可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提，就是网络能够承受现有的网络负荷。拥塞控制是一个全局性的过程，涉及到所有的主机，所有的路由器，以及与降低网络传输性能有关的所有因素。相反，流量控制往往是点对点通信量的控制，是个端到端的问题。流量控制所要做到的就是抑制发送端发送数据的速率，以便使接收端来得及接收。 17，为了进行拥塞控制，TCP发送方要维持一个拥塞窗口cwnd的状态变量。拥塞控制窗口的大小取决于网络的拥塞程度，并且动态变化。发送方让自己的发送窗口取为拥塞窗口和接收方的接受窗口中较小的一个。 18，TCP的拥塞控制采用了四种算法，即慢开始，拥塞避免，快重传和快恢复。在网络层也可以使路由器采用适当的分组丢弃策略（如主动队列管理AQM），以减少网络拥塞的发生。 19，运输连接的三个阶段，即：连接建立，数据传送和连接释放。 20，主动发起TCP连接建立的应用进程叫做客户，而被动等待连接建立的应用进程叫做服务器。TCP连接采用三报文握手机制。服务器要确认用户的连接请求，然后客户要对服务器的确认进行确认。 21，TCP的连接释放采用四报文握手机制。任何一方都可以在数据传送结束后发出连接释放的通知，待对方确认后进入半关闭状态。当另一方也没有数据再发送时，则发送连接释放通知，对方确认后就完全关闭了TCP连接 （3），最重要的知识点 ① 端口和套接字的意义 ② 无连接UDP的特点 ③ 面向连接TCP的特点 ④ 在不可靠的网络上实现可靠传输的工作原理，停止等待协议和ARQ协议 ① TCP的滑动窗口，流量控制，拥塞控制和连接管理 六应用层 （1），基本术语 域名系统（DNS）： DNS（Domain Name System，域名系统），万维网上作为域名和IP地址相互映射的一个分布式数据库，能够使用户更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。 通过域名，最终得到该域名对应的IP地址的过程叫做域名解析（或主机名解析）。DNS协议运行在UDP协议之上，使用端口号53。在RFC文档中RFC 2181对DNS有规范说明，RFC 2136对DNS的动态更新进行说明，RFC 2308对DNS查询的反向缓存进行说明。 文件传输协议（FTP）： FTP 是File TransferProtocol（文件传输协议）的英文简称，而中文简称为“文传协议”。用于Internet上的控制文件的双向传输。同时，它也是一个应用程序（Application）。 基于不同的操作系统有不同的FTP应用程序，而所有这些应用程序都遵守同一种协议以传输文件。在FTP的使用当中，用户经常遇到两个概念：\"下载\"（Download）和\"上传\"（Upload）。 \"下载\"文件就是从远程主机拷贝文件至自己的计算机上；\"上传\"文件就是将文件从自己的计算机中拷贝至远程主机上。用Internet语言来说，用户可通过客户机程序向（从）远程主机上传（下载）文件。 简单文件传输协议（TFTP）： TFTP（Trivial File Transfer Protocol,简单文件传输协议）是TCP/IP协议族中的一个用来在客户机与服务器之间进行简单文件传输的协议，提供不复杂、开销不大的文件传输服务。端口号为69。 远程终端协议（TELENET）： Telnet协议是TCP/IP协议族中的一员，是Internet远程登陆服务的标准协议和主要方式。它为用户提供了在本地计算机上完成远程主机工作的能力。 在终端使用者的电脑上使用telnet程序，用它连接到服务器。终端使用者可以在telnet程序中输入命令，这些命令会在服务器上运行，就像直接在服务器的控制台上输入一样。 可以在本地就能控制服务器。要开始一个telnet会话，必须输入用户名和密码来登录服务器。Telnet是常用的远程控制Web服务器的方法。 万维网（WWW）： WWW是环球信息网的缩写，（亦作“Web”、“WWW”、“'W3'”，英文全称为“World Wide Web”），中文名字为“万维网”，\"环球网\"等，常简称为Web。分为Web客户端和Web服务器程序。 WWW可以让Web客户端（常用浏览器）访问浏览Web服务器上的页面。是一个由许多互相链接的超文本组成的系统，通过互联网访问。在这个系统中，每个有用的事物，称为一样“资源”；并且由一个全局“统一资源标识符”（URI）标识；这些资源通过超文本传输协议（Hypertext Transfer Protocol）传送给用户，而后者通过点击链接来获得资源。 万维网联盟（英语：World Wide Web Consortium，简称W3C），又称W3C理事会。1994年10月在麻省理工学院（MIT）计算机科学实验室成立。万维网联盟的创建者是万维网的发明者蒂姆·伯纳斯-李。 万维网并不等同互联网，万维网只是互联网所能提供的服务其中之一，是靠着互联网运行的一项服务。 万维网的大致工作工程： 统一资源定位符（URL）： 统一资源定位符是对可以从互联网上得到的资源的位置和访问方法的一种简洁的表示，是互联网上标准资源的地址。互联网上的每个文件都有一个唯一的URL，它包含的信息指出文件的位置以及浏览器应该怎么处理它。 超文本传输协议（HTTP）： 超文本传输协议（HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议。所有的WWW文件都必须遵守这个标准。 设计HTTP最初的目的是为了提供一种发布和接收HTML页面的方法。1960年美国人Ted Nelson构思了一种通过计算机处理文本信息的方法，并称之为超文本（hypertext）,这成为了HTTP超文本传输协议标准架构的发展根基。 代理服务器（Proxy Server）： 代理服务器（Proxy Server）是一种网络实体，它又称为万维网高速缓存。 代理服务器把最近的一些请求和响应暂存在本地磁盘中。当新请求到达时，若代理服务器发现这个请求与暂时存放的的请求相同，就返回暂存的响应，而不需要按URL的地址再次去互联网访问该资源。 代理服务器可在客户端或服务器工作，也可以在中间系统工作。 http请求头： http请求头，HTTP客户程序（例如浏览器），向服务器发送请求的时候必须指明请求类型（一般是GET或者POST）。如有必要，客户程序还可以选择发送其他的请求头。 - Accept：浏览器可接受的MIME类型。 - Accept-Charset：浏览器可接受的字符集。 - Accept-Encoding：浏览器能够进行解码的数据编码方式，比如gzip。Servlet能够向支持gzip的浏览器返回经gzip编码的HTML页面。许多情形下这可以减少5到10倍的下载时间。 - Accept-Language：浏览器所希望的语言种类，当服务器能够提供一种以上的语言版本时要用到。 - Authorization：授权信息，通常出现在对服务器发送的WWW-Authenticate头的应答中。 - Connection：表示是否需要持久连接。如果Servlet看到这里的值为“Keep-Alive”，或者看到请求使用的是HTTP 1.1（HTTP 1.1默认进行持久连接），它就可以利用持久连接的优点，当页面包含多个元素时（例如Applet，图片），显著地减少下载所需要的时间。要实现这一点，Servlet需要在应答中发送一个Content-Length头，最简单的实现方法是：先把内容写入ByteArrayOutputStream，然后在正式写出内容之前计算它的大小。 - Content-Length：表示请求消息正文的长度。 - Cookie：这是最重要的请求头信息之一 - From：请求发送者的email地址，由一些特殊的Web客户程序使用，浏览器不会用到它。 - Host：初始URL中的主机和端口。 - If-Modified-Since：只有当所请求的内容在指定的日期之后又经过修改才返回它，否则返回304“Not Modified”应答。 - Pragma：指定“no-cache”值表示服务器必须返回一个刷新后的文档，即使它是代理服务器而且已经有了页面的本地拷贝。 - Referer：包含一个URL，用户从该URL代表的页面出发访问当前请求的页面。 - User-Agent：浏览器类型，如果Servlet返回的内容与浏览器类型有关则该值非常有用。 简单邮件传输协议(SMTP)： SMTP（Simple Mail Transfer Protocol）即简单邮件传输协议,它是一组用于由源地址到目的地址传送邮件的规则，由它来控制信件的中转方式。 SMTP协议属于TCP/IP协议簇，它帮助每台计算机在发送或中转信件时找到下一个目的地。 通过SMTP协议所指定的服务器,就可以把E-mail寄到收信人的服务器上了，整个过程只要几分钟。SMTP服务器则是遵循SMTP协议的发送邮件服务器，用来发送或中转发出的电子邮件。 搜索引擎： 搜索引擎（Search Engine）是指根据一定的策略、运用特定的计算机程序从互联网上搜集信息，在对信息进行组织和处理后，为用户提供检索服务，将用户检索相关的信息展示给用户的系统。 搜索引擎包括全文索引、目录索引、元搜索引擎、垂直搜索引擎、集合式搜索引擎、门户搜索引擎与免费链接列表等。 全文索引： 全文索引技术是目前搜索引擎的关键技术。 试想在1M大小的文件中搜索一个词，可能需要几秒，在100M的文件中可能需要几十秒，如果在更大的文件中搜索那么就需要更大的系统开销，这样的开销是不现实的。 所以在这样的矛盾下出现了全文索引技术，有时候有人叫倒排文档技术。 目录索引： 目录索引（ search index/directory)，顾名思义就是将网站分门别类地存放在相应的目录中，因此用户在查询信息时，可选择关键词搜索，也可按分类目录逐层查找。 垂直搜索引擎： 垂直搜索引擎是针对某一个行业的专业搜索引擎，是搜索引擎的细分和延伸，是对网页库中的某类专门的信息进行一次整合，定向分字段抽取出需要的数据进行处理后再以某种形式返回给用户。 垂直搜索是相对通用搜索引擎的信息量大、查询不准确、深度不够等提出来的新的搜索引擎服务模式，通过针对某一特定领域、某一特定人群或某一特定需求提供的有一定价值的信息和相关服务。 其特点就是“专、精、深”，且具有行业色彩，相比较通用搜索引擎的海量信息无序化，垂直搜索引擎则显得更加专注、具体和深入。 （2），重要知识点总结 1，文件传输协议（FTP）使用TCP可靠的运输服务。FTP使用客户服务器方式。一个FTP服务器进程可以同时为多个用户提供服务。在进进行文件传输时，FTP的客户和服务器之间要先建立两个并行的TCP连接:控制连接和数据连接。实际用于传输文件的是数据连接。 2，万维网客户程序与服务器之间进行交互使用的协议是超文本传输协议HTTP。HTTP使用TCP连接进行可靠传输。但HTTP本身是无连接、无状态的。HTTP/1.1协议使用了持续连接（分为非流水线方式和流水线方式） 3，电子邮件把邮件发送到收件人使用的邮件服务器，并放在其中的收件人邮箱中，收件人可随时上网到自己使用的邮件服务器读取，相当于电子邮箱。 4，一个电子邮件系统有三个重要组成构件：用户代理、邮件服务器、邮件协议（包括邮件发送协议，如SMTP，和邮件读取协议，如POP3和IMAP）。用户代理和邮件服务器都要运行这些协议。 （3），最重要知识点总结 ① 域名系统-从域名解析出IP地址 ② 访问一个网站大致的过程 ③ 系统调用和应用编程接口概念 "},"zother6-JavaGuide/network/计算机网络.html":{"url":"zother6-JavaGuide/network/计算机网络.html","title":"计算机网络","keywords":"","body":"一 OSI与TCP/IP各层的结构与功能,都有哪些协议? 学习计算机网络时我们一般采用折中的办法，也就是中和 OSI 和 TCP/IP 的优点，采用一种只有五层协议的体系结构，这样既简洁又能将概念阐述清楚。 结合互联网的情况，自上而下地，非常简要的介绍一下各层的作用。 1.1 应用层 应用层(application-layer）的任务是通过应用进程间的交互来完成特定网络应用。应用层协议定义的是应用进程（进程：主机中正在运行的程序）间的通信和交互的规则。对于不同的网络应用需要不同的应用层协议。在互联网中应用层协议很多，如域名系统DNS，支持万维网应用的 HTTP协议，支持电子邮件的 SMTP协议等等。我们把应用层交互的数据单元称为报文。 域名系统 域名系统(Domain Name System缩写 DNS，Domain Name被译为域名)是因特网的一项核心服务，它作为可以将域名和IP地址相互映射的一个分布式数据库，能够使人更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。（百度百科）例如：一个公司的 Web 网站可看作是它在网上的门户，而域名就相当于其门牌地址，通常域名都使用该公司的名称或简称。例如上面提到的微软公司的域名，类似的还有：IBM 公司的域名是 www.ibm.com、Oracle 公司的域名是 www.oracle.com、Cisco公司的域名是 www.cisco.com 等。 HTTP协议 超文本传输协议（HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议。所有的 WWW（万维网） 文件都必须遵守这个标准。设计 HTTP 最初的目的是为了提供一种发布和接收 HTML 页面的方法。（百度百科） 1.2 运输层 运输层(transport layer)的主要任务就是负责向两台主机进程之间的通信提供通用的数据传输服务。应用进程利用该服务传送应用层报文。“通用的”是指并不针对某一个特定的网络应用，而是多种应用可以使用同一个运输层服务。由于一台主机可同时运行多个线程，因此运输层有复用和分用的功能。所谓复用就是指多个应用层进程可同时使用下面运输层的服务，分用和复用相反，是运输层把收到的信息分别交付上面应用层中的相应进程。 运输层主要使用以下两种协议: 传输控制协议 TCP（Transmission Control Protocol）--提供面向连接的，可靠的数据传输服务。 用户数据协议 UDP（User Datagram Protocol）--提供无连接的，尽最大努力的数据传输服务（不保证数据传输的可靠性）。 TCP 与 UDP 的对比见问题三。 1.3 网络层 在 计算机网络中进行通信的两个计算机之间可能会经过很多个数据链路，也可能还要经过很多通信子网。网络层的任务就是选择合适的网间路由和交换结点， 确保数据及时传送。 在发送数据时，网络层把运输层产生的报文段或用户数据报封装成分组和包进行传送。在 TCP/IP 体系结构中，由于网络层使用 IP 协议，因此分组也叫 IP 数据报 ，简称 数据报。 这里要注意：不要把运输层的“用户数据报 UDP ”和网络层的“ IP 数据报”弄混。另外，无论是哪一层的数据单元，都可笼统地用“分组”来表示。 这里强调指出，网络层中的“网络”二字已经不是我们通常谈到的具体网络，而是指计算机网络体系结构模型中第三层的名称. 互联网是由大量的异构（heterogeneous）网络通过路由器（router）相互连接起来的。互联网使用的网络层协议是无连接的网际协议（Intert Protocol）和许多路由选择协议，因此互联网的网络层也叫做网际层或IP层。 1.4 数据链路层 数据链路层(data link layer)通常简称为链路层。两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层的协议。 在两个相邻节点之间传送数据时，数据链路层将网络层交下来的 IP 数据报组装成帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息（如同步信息，地址信息，差错控制等）。 在接收数据时，控制信息使接收端能够知道一个帧从哪个比特开始和到哪个比特结束。这样，数据链路层在收到一个帧后，就可从中提出数据部分，上交给网络层。 控制信息还使接收端能够检测到所收到的帧中有误差错。如果发现差错，数据链路层就简单地丢弃这个出了差错的帧，以避免继续在网络中传送下去白白浪费网络资源。如果需要改正数据在链路层传输时出现差错（这就是说，数据链路层不仅要检错，而且还要纠错），那么就要采用可靠性传输协议来纠正出现的差错。这种方法会使链路层的协议复杂些。 1.5 物理层 在物理层上所传送的数据单位是比特。 物理层(physical layer)的作用是实现相邻计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。 使其上面的数据链路层不必考虑网络的具体传输介质是什么。“透明传送比特流”表示经实际电路传送后的比特流没有发生变化，对传送的比特流来说，这个电路好像是看不见的。 在互联网使用的各种协中最重要和最著名的就是 TCP/IP 两个协议。现在人们经常提到的TCP/IP并不一定单指TCP和IP这两个具体的协议，而往往表示互联网所使用的整个TCP/IP协议族。 1.6 总结一下 上面我们对计算机网络的五层体系结构有了初步的了解，下面附送一张七层体系结构图总结一下（图片来源于网络）。 二 TCP 三次握手和四次挥手(面试常客) 为了准确无误地把数据送达目标处，TCP协议采用了三次握手策略。 2.1 TCP 三次握手漫画图解 如下图所示，下面的两个机器人通过3次握手确定了对方能正确接收和发送消息(图片来源：《图解HTTP》)。 简单示意图： 客户端–发送带有 SYN 标志的数据包–一次握手–服务端 服务端–发送带有 SYN/ACK 标志的数据包–二次握手–客户端 客户端–发送带有带有 ACK 标志的数据包–三次握手–服务端 2.2 为什么要三次握手 三次握手的目的是建立可靠的通信信道，说到通讯，简单来说就是数据的发送与接收，而三次握手最主要的目的就是双方确认自己与对方的发送与接收是正常的。 第一次握手：Client 什么都不能确认；Server 确认了对方发送正常，自己接收正常 第二次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：对方发送正常，自己接收正常 第三次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己发送、接收正常，对方发送、接收正常 所以三次握手就能确认双发收发功能都正常，缺一不可。 2.3 第2次握手传回了ACK，为什么还要传回SYN？ 接收端传回发送端所发送的ACK是为了告诉客户端，我接收到的信息确实就是你所发送的信号了，这表明从客户端到服务端的通信是正常的。而回传SYN则是为了建立并确认从服务端到客户端的通信。” SYN 同步序列编号(Synchronize Sequence Numbers) 是 TCP/IP 建立连接时使用的握手信号。在客户机和服务器之间建立正常的 TCP 网络连接时，客户机首先发出一个 SYN 消息，服务器使用 SYN-ACK 应答表示接收到了这个消息，最后客户机再以 ACK(Acknowledgement）消息响应。这样在客户机和服务器之间才能建立起可靠的 TCP 连接，数据才可以在客户机和服务器之间传递。 2.5 为什么要四次挥手 断开一个 TCP 连接则需要“四次挥手”： 客户端-发送一个 FIN，用来关闭客户端到服务器的数据传送 服务器-收到这个 FIN，它发回一 个 ACK，确认序号为收到的序号加1 。和 SYN 一样，一个 FIN 将占用一个序号 服务器-关闭与客户端的连接，发送一个FIN给客户端 客户端-发回 ACK 报文确认，并将确认序号设置为收到序号加1 任何一方都可以在数据传送结束后发出连接释放的通知，待对方确认后进入半关闭状态。当另一方也没有数据再发送的时候，则发出连接释放通知，对方确认后就完全关闭了TCP连接。 举个例子：A 和 B 打电话，通话即将结束后，A 说“我没啥要说的了”，B回答“我知道了”，但是 B 可能还会有要说的话，A 不能要求 B 跟着自己的节奏结束通话，于是 B 可能又巴拉巴拉说了一通，最后 B 说“我说完了”，A 回答“知道了”，这样通话才算结束。 上面讲的比较概括，推荐一篇讲的比较细致的文章：https://blog.csdn.net/qzcsu/article/details/72861891 三 TCP,UDP 协议的区别 UDP 在传送数据之前不需要先建立连接，远地主机在收到 UDP 报文后，不需要给出任何确认。虽然 UDP 不提供可靠交付，但在某些情况下 UDP 确是一种最有效的工作方式（一般用于即时通信），比如： QQ 语音、 QQ 视频 、直播等等 TCP 提供面向连接的服务。在传送数据之前必须先建立连接，数据传送结束后要释放连接。 TCP 不提供广播或多播服务。由于 TCP 要提供可靠的，面向连接的传输服务（TCP的可靠体现在TCP在传递数据之前，会有三次握手来建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制，在数据传完后，还会断开连接用来节约系统资源），这一难以避免增加了许多开销，如确认，流量控制，计时器以及连接管理等。这不仅使协议数据单元的首部增大很多，还要占用许多处理机资源。TCP 一般用于文件传输、发送和接收邮件、远程登录等场景。 四 TCP 协议如何保证可靠传输 应用数据被分割成 TCP 认为最适合发送的数据块。 TCP 给发送的每一个包进行编号，接收方对数据包进行排序，把有序数据传送给应用层。 校验和： TCP 将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP 将丢弃这个报文段和不确认收到此报文段。 TCP 的接收端会丢弃重复的数据。 流量控制： TCP 连接的每一方都有固定大小的缓冲空间，TCP的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP 使用的流量控制协议是可变大小的滑动窗口协议。 （TCP 利用滑动窗口实现流量控制） 拥塞控制： 当网络拥塞时，减少数据的发送。 ARQ协议： 也是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组。 超时重传： 当 TCP 发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。 4.1 ARQ协议 自动重传请求（Automatic Repeat-reQuest，ARQ）是OSI模型中数据链路层和传输层的错误纠正协议之一。它通过使用确认和超时这两个机制，在不可靠服务的基础上实现可靠的信息传输。如果发送方在发送后一段时间之内没有收到确认帧，它通常会重新发送。ARQ包括停止等待ARQ协议和连续ARQ协议。 停止等待ARQ协议 停止等待协议是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认（回复ACK）。如果过了一段时间（超时时间后），还是没有收到 ACK 确认，说明没有发送成功，需要重新发送，直到收到确认后再发下一个分组； 在停止等待协议中，若接收方收到重复分组，就丢弃该分组，但同时还要发送确认； 优点： 简单 缺点： 信道利用率低，等待时间长 1) 无差错情况: 发送方发送分组,接收方在规定时间内收到,并且回复确认.发送方再次发送。 2) 出现差错情况（超时重传）: 停止等待协议中超时重传是指只要超过一段时间仍然没有收到确认，就重传前面发送过的分组（认为刚才发送过的分组丢失了）。因此每发送完一个分组需要设置一个超时计时器，其重传时间应比数据在分组传输的平均往返时间更长一些。这种自动重传方式常称为 自动重传请求 ARQ 。另外在停止等待协议中若收到重复分组，就丢弃该分组，但同时还要发送确认。连续 ARQ 协议 可提高信道利用率。发送维持一个发送窗口，凡位于发送窗口内的分组可连续发送出去，而不需要等待对方确认。接收方一般采用累积确认，对按序到达的最后一个分组发送确认，表明到这个分组位置的所有分组都已经正确收到了。 3) 确认丢失和确认迟到 确认丢失 ：确认消息在传输过程丢失。当A发送M1消息，B收到后，B向A发送了一个M1确认消息，但却在传输过程中丢失。而A并不知道，在超时计时过后，A重传M1消息，B再次收到该消息后采取以下两点措施：1. 丢弃这个重复的M1消息，不向上层交付。 2. 向A发送确认消息。（不会认为已经发送过了，就不再发送。A能重传，就证明B的确认消息丢失）。 确认迟到 ：确认消息在传输过程中迟到。A发送M1消息，B收到并发送确认。在超时时间内没有收到确认消息，A重传M1消息，B仍然收到并继续发送确认消息（B收到了2份M1）。此时A收到了B第二次发送的确认消息。接着发送其他数据。过了一会，A收到了B第一次发送的对M1的确认消息（A也收到了2份确认消息）。处理如下：1. A收到重复的确认后，直接丢弃。2. B收到重复的M1后，也直接丢弃重复的M1。 连续ARQ协议 连续 ARQ 协议可提高信道利用率。发送方维持一个发送窗口，凡位于发送窗口内的分组可以连续发送出去，而不需要等待对方确认。接收方一般采用累计确认，对按序到达的最后一个分组发送确认，表明到这个分组为止的所有分组都已经正确收到了。 优点： 信道利用率高，容易实现，即使确认丢失，也不必重传。 缺点： 不能向发送方反映出接收方已经正确收到的所有分组的信息。 比如：发送方发送了 5条 消息，中间第三条丢失（3号），这时接收方只能对前两个发送确认。发送方无法知道后三个分组的下落，而只好把后三个全部重传一次。这也叫 Go-Back-N（回退 N），表示需要退回来重传已经发送过的 N 个消息。 4.2 滑动窗口和流量控制 TCP 利用滑动窗口实现流量控制。流量控制是为了控制发送方发送速率，保证接收方来得及接收。 接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。 4.3 拥塞控制 在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏。这种情况就叫拥塞。拥塞控制就是为了防止过多的数据注入到网络中，这样就可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提，就是网络能够承受现有的网络负荷。拥塞控制是一个全局性的过程，涉及到所有的主机，所有的路由器，以及与降低网络传输性能有关的所有因素。相反，流量控制往往是点对点通信量的控制，是个端到端的问题。流量控制所要做到的就是抑制发送端发送数据的速率，以便使接收端来得及接收。 为了进行拥塞控制，TCP 发送方要维持一个 拥塞窗口(cwnd) 的状态变量。拥塞控制窗口的大小取决于网络的拥塞程度，并且动态变化。发送方让自己的发送窗口取为拥塞窗口和接收方的接受窗口中较小的一个。 TCP的拥塞控制采用了四种算法，即 慢开始 、 拥塞避免 、快重传 和 快恢复。在网络层也可以使路由器采用适当的分组丢弃策略（如主动队列管理 AQM），以减少网络拥塞的发生。 慢开始： 慢开始算法的思路是当主机开始发送数据时，如果立即把大量数据字节注入到网络，那么可能会引起网络阻塞，因为现在还不知道网络的符合情况。经验表明，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是由小到大逐渐增大拥塞窗口数值。cwnd初始值为1，每经过一个传播轮次，cwnd加倍。 拥塞避免： 拥塞避免算法的思路是让拥塞窗口cwnd缓慢增大，即每经过一个往返时间RTT就把发送放的cwnd加1. 快重传与快恢复： 在 TCP/IP 中，快速重传和恢复（fast retransmit and recovery，FRR）是一种拥塞控制算法，它能快速恢复丢失的数据包。没有 FRR，如果数据包丢失了，TCP 将会使用定时器来要求传输暂停。在暂停的这段时间内，没有新的或复制的数据包被发送。有了 FRR，如果接收机接收到一个不按顺序的数据段，它会立即给发送机发送一个重复确认。如果发送机接收到三个重复确认，它会假定确认件指出的数据段丢失了，并立即重传这些丢失的数据段。有了 FRR，就不会因为重传时要求的暂停被耽误。 　当有单独的数据包丢失时，快速重传和恢复（FRR）能最有效地工作。当有多个数据信息包在某一段很短的时间内丢失时，它则不能很有效地工作。 五 在浏览器中输入url地址 ->> 显示主页的过程(面试常客) 百度好像最喜欢问这个问题。 打开一个网页，整个过程会使用哪些协议 图解（图片来源：《图解HTTP》）： 总体来说分为以下几个过程: DNS解析 TCP连接 发送HTTP请求 服务器处理请求并返回HTTP报文 浏览器解析渲染页面 连接结束 具体可以参考下面这篇文章： https://segmentfault.com/a/1190000006879700 六 状态码 七 各种协议与HTTP协议之间的关系 一般面试官会通过这样的问题来考察你对计算机网络知识体系的理解。 图片来源：《图解HTTP》 八 HTTP长连接,短连接 在HTTP/1.0中默认使用短连接。也就是说，客户端和服务器每进行一次HTTP操作，就建立一次连接，任务结束就中断连接。当客户端浏览器访问的某个HTML或其他类型的Web页中包含有其他的Web资源（如JavaScript文件、图像文件、CSS文件等），每遇到这样一个Web资源，浏览器就会重新建立一个HTTP会话。 而从HTTP/1.1起，默认使用长连接，用以保持连接特性。使用长连接的HTTP协议，会在响应头加入这行代码： Connection:keep-alive 在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，客户端再次访问这个服务器时，会继续使用这一条已经建立的连接。Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。实现长连接需要客户端和服务端都支持长连接。 HTTP协议的长连接和短连接，实质上是TCP协议的长连接和短连接。 —— 《HTTP长连接、短连接究竟是什么？》 九 HTTP是不保存状态的协议,如何保存用户状态? HTTP 是一种不保存状态，即无状态（stateless）协议。也就是说 HTTP 协议自身不对请求和响应之间的通信状态进行保存。那么我们保存用户状态呢？Session 机制的存在就是为了解决这个问题，Session 的主要作用就是通过服务端记录用户的状态。典型的场景是购物车，当你要添加商品到购物车的时候，系统不知道是哪个用户操作的，因为 HTTP 协议是无状态的。服务端给特定的用户创建特定的 Session 之后就可以标识这个用户并且跟踪这个用户了（一般情况下，服务器会在一定时间内保存这个 Session，过了时间限制，就会销毁这个Session）。 在服务端保存 Session 的方法很多，最常用的就是内存和数据库(比如是使用内存数据库redis保存)。既然 Session 存放在服务器端，那么我们如何实现 Session 跟踪呢？大部分情况下，我们都是通过在 Cookie 中附加一个 Session ID 来方式来跟踪。 Cookie 被禁用怎么办? 最常用的就是利用 URL 重写把 Session ID 直接附加在URL路径的后面。 十 Cookie的作用是什么?和Session有什么区别？ Cookie 和 Session都是用来跟踪浏览器用户身份的会话方式，但是两者的应用场景不太一样。 Cookie 一般用来保存用户信息 比如①我们在 Cookie 中保存已经登录过得用户信息，下次访问网站的时候页面可以自动帮你登录的一些基本信息给填了；②一般的网站都会有保持登录也就是说下次你再访问网站的时候就不需要重新登录了，这是因为用户登录的时候我们可以存放了一个 Token 在 Cookie 中，下次登录的时候只需要根据 Token 值来查找用户即可(为了安全考虑，重新登录一般要将 Token 重写)；③登录一次网站后访问网站其他页面不需要重新登录。Session 的主要作用就是通过服务端记录用户的状态。 典型的场景是购物车，当你要添加商品到购物车的时候，系统不知道是哪个用户操作的，因为 HTTP 协议是无状态的。服务端给特定的用户创建特定的 Session 之后就可以标识这个用户并且跟踪这个用户了。 Cookie 数据保存在客户端(浏览器端)，Session 数据保存在服务器端。 Cookie 存储在客户端中，而Session存储在服务器上，相对来说 Session 安全性更高。如果要在 Cookie 中存储一些敏感信息，不要直接写入 Cookie 中，最好能将 Cookie 信息加密然后使用到的时候再去服务器端解密。 十一 HTTP 1.0和HTTP 1.1的主要区别是什么? 这部分回答引用这篇文章 https://mp.weixin.qq.com/s/GICbiyJpINrHZ41u_4zT-A? 的一些内容。 HTTP1.0最早在网页中使用是在1996年，那个时候只是使用一些较为简单的网页上和网络请求上，而HTTP1.1则在1999年才开始广泛应用于现在的各大浏览器网络请求中，同时HTTP1.1也是当前使用最为广泛的HTTP协议。 主要区别主要体现在： 长连接 : 在HTTP/1.0中，默认使用的是短连接，也就是说每次请求都要重新建立一次连接。HTTP 是基于TCP/IP协议的,每一次建立或者断开连接都需要三次握手四次挥手的开销，如果每次请求都要这样的话，开销会比较大。因此最好能维持一个长连接，可以用个长连接来发多个请求。HTTP 1.1起，默认使用长连接 ,默认开启Connection： keep-alive。 HTTP/1.1的持续连接有非流水线方式和流水线方式 。流水线方式是客户在收到HTTP的响应报文之前就能接着发送新的请求报文。与之相对应的非流水线方式是客户在收到前一个响应后才能发送下一个请求。 错误状态响应码 :在HTTP1.1中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突；410（Gone）表示服务器上的某个资源被永久性的删除。 缓存处理 :在HTTP1.0中主要使用header里的If-Modified-Since,Expires来做为缓存判断的标准，HTTP1.1则引入了更多的缓存控制策略例如Entity tag，If-Unmodified-Since, If-Match, If-None-Match等更多可供选择的缓存头来控制缓存策略。 带宽优化及网络连接的使用 :HTTP1.0中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1则在请求头引入了range头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。 十二 URI和URL的区别是什么? URI(Uniform Resource Identifier) 是统一资源标志符，可以唯一标识一个资源。 URL(Uniform Resource Location) 是统一资源定位符，可以提供该资源的路径。它是一种具体的 URI，即 URL 可以用来标识一个资源，而且还指明了如何 locate 这个资源。 URI的作用像身份证号一样，URL的作用更像家庭住址一样。URL是一种具体的URI，它不仅唯一标识资源，而且还提供了定位该资源的信息。 十三 HTTP 和 HTTPS 的区别？ 端口 ：HTTP的URL由“http://”起始且默认使用端口80，而HTTPS的URL由“https://”起始且默认使用端口443。 安全性和资源消耗： HTTP协议运行在TCP之上，所有传输的内容都是明文，客户端和服务器端都无法验证对方的身份。HTTPS是运行在SSL/TLS之上的HTTP协议，SSL/TLS 运行在TCP之上。所有传输的内容都经过加密，加密采用对称加密，但对称加密的密钥用服务器方的证书进行了非对称加密。所以说，HTTP 安全性没有 HTTPS高，但是 HTTPS 比HTTP耗费更多服务器资源。 对称加密：密钥只有一个，加密解密为同一个密码，且加解密速度快，典型的对称加密算法有DES、AES等； 非对称加密：密钥成对出现（且根据公钥无法推知私钥，根据私钥也无法推知公钥），加密解密使用不同密钥（公钥加密需要私钥解密，私钥加密需要公钥解密），相对对称加密速度较慢，典型的非对称加密算法有RSA、DSA等。 建议 非常推荐大家看一下 《图解HTTP》 这本书，这本书页数不多，但是内容很是充实，不管是用来系统的掌握网络方面的一些知识还是说纯粹为了应付面试都有很大帮助。下面的一些文章只是参考。大二学习这门课程的时候，我们使用的教材是 《计算机网络第七版》（谢希仁编著），不推荐大家看这本教材，书非常厚而且知识偏理论，不确定大家能不能心平气和的读完。 参考 https://blog.csdn.net/qq_16209077/article/details/52718250 https://blog.csdn.net/zixiaomuwu/article/details/60965466 https://blog.csdn.net/turn__back/article/details/73743641 https://mp.weixin.qq.com/s/GICbiyJpINrHZ41u_4zT-A? "},"zother6-JavaGuide/operating-system/basis.html":{"url":"zother6-JavaGuide/operating-system/basis.html","title":"Basis","keywords":"","body":"大家好，我是 Guide 哥！很多读者抱怨计算操作系统的知识点比较繁杂，自己也没有多少耐心去看，但是面试的时候又经常会遇到。所以，我带着我整理好的操作系统的常见问题来啦！这篇文章总结了一些我觉得比较重要的操作系统相关的问题比如进程管理、内存管理、虚拟内存等等。 文章形式通过大部分比较喜欢的面试官和求职者之间的对话形式展开。另外，Guide 哥也只是在大学的时候学习过操作系统，不过基本都忘了，为了写这篇文章这段时间看了很多相关的书籍和博客。如果文中有任何需要补充和完善的地方，你都可以在评论区指出。如果觉得内容不错的话，不要忘记点个在看哦！ 我个人觉得学好操作系统还是非常有用的，具体可以看我昨天在星球分享的一段话： 这篇文章只是对一些操作系统比较重要概念的一个概览，深入学习的话，建议大家还是老老实实地去看书。另外， 这篇文章的很多内容参考了《现代操作系统》第三版这本书，非常感谢。 一 操作系统基础 面试官顶着蓬松的假发向我走来，只见他一手拿着厚重的 Thinkpad ，一手提着他那淡黄的长裙。 1.1 什么是操作系统？ 👨‍💻面试官 ： 先来个简单问题吧！什么是操作系统？ 🙋 我 ：我通过以下四点向您介绍一下什么是操作系统吧！ 操作系统（Operating System，简称 OS）是管理计算机硬件与软件资源的程序，是计算机系统的内核与基石； 操作系统本质上是运行在计算机上的软件程序 ； 操作系统为用户提供一个与系统交互的操作界面 ； 操作系统分内核与外壳（我们可以把外壳理解成围绕着内核的应用程序，而内核就是能操作硬件的程序）。 关于内核多插一嘴：内核负责管理系统的进程、内存、设备驱动程序、文件和网络系统等等，决定着系统的性能和稳定性。是连接应用程序和硬件的桥梁。 内核就是操作系统背后黑盒的核心。 1.2 系统调用 👨‍💻面试官 ：什么是系统调用呢？ 能不能详细介绍一下。 🙋 我 ：介绍系统调用之前，我们先来了解一下用户态和系统态。 根据进程访问资源的特点，我们可以把进程在系统上的运行分为两个级别： 用户态(user mode) : 用户态运行的进程或可以直接读取用户程序的数据。 系统态(kernel mode):可以简单的理解系统态运行的进程或程序几乎可以访问计算机的任何资源，不受限制。 说了用户态和系统态之后，那么什么是系统调用呢？ 我们运行的程序基本都是运行在用户态，如果我们调用操作系统提供的系统态级别的子功能咋办呢？那就需要系统调用了！ 也就是说在我们运行的用户程序中，凡是与系统态级别的资源有关的操作（如文件管理、进程控制、内存管理等)，都必须通过系统调用方式向操作系统提出服务请求，并由操作系统代为完成。 这些系统调用按功能大致可分为如下几类： 设备管理。完成设备的请求或释放，以及设备启动等功能。 文件管理。完成文件的读、写、创建及删除等功能。 进程控制。完成进程的创建、撤销、阻塞及唤醒等功能。 进程通信。完成进程之间的消息传递或信号传递等功能。 内存管理。完成内存的分配、回收以及获取作业占用内存区大小及地址等功能。 二 进程和线程 2.1 进程和线程的区别 👨‍💻面试官: 好的！我明白了！那你再说一下： 进程和线程的区别。 🙋 我： 好的！ 下图是 Java 内存区域，我们从 JVM 的角度来说一下线程和进程之间的关系吧！ 如果你对 Java 内存区域 (运行时数据区) 这部分知识不太了解的话可以阅读一下这篇文章：《可能是把 Java 内存区域讲的最清楚的一篇文章》>) 从上图可以看出：一个进程中可以有多个线程，多个线程共享进程的堆和方法区 (JDK1.8 之后的元空间)资源，但是每个线程有自己的程序计数器、虚拟机栈 和 本地方法栈。 总结： 线程是进程划分成的更小的运行单位,一个进程在其执行的过程中可以产生多个线程。线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。线程执行开销小，但不利于资源的管理和保护；而进程正相反。 2.2 进程有哪几种状态? 👨‍💻面试官 ： 那你再说说进程有哪几种状态? 🙋 我 ：我们一般把进程大致分为 5 种状态，这一点和线程很像！ 创建状态(new) ：进程正在被创建，尚未到就绪状态。 就绪状态(ready) ：进程已处于准备运行状态，即进程获得了除了处理器之外的一切所需资源，一旦得到处理器资源(处理器分配的时间片)即可运行。 运行状态(running) ：进程正在处理器上上运行(单核 CPU 下任意时刻只有一个进程处于运行状态)。 阻塞状态(waiting) ：又称为等待状态，进程正在等待某一事件而暂停运行如等待某资源为可用或等待 IO 操作完成。即使处理器空闲，该进程也不能运行。 结束状态(terminated) ：进程正在从系统中消失。可能是进程正常结束或其他原因中断退出运行。 2.3 进程间的通信方式 👨‍💻面试官 ：进程间的通信常见的的有哪几种方式呢? 🙋 我 ：大概有 7 种常见的进程间的通信方式。 下面这部分总结参考了:《进程间通信 IPC (InterProcess Communication)》 这篇文章，推荐阅读，总结的非常不错。 管道/匿名管道(Pipes) ：用于具有亲缘关系的父子进程间或者兄弟进程之间的通信。 有名管道(Names Pipes) : 匿名管道由于没有名字，只能用于亲缘关系的进程间通信。为了克服这个缺点，提出了有名管道。有名管道严格遵循先进先出(first in first out)。有名管道以磁盘文件的方式存在，可以实现本机任意两个进程通信。 信号(Signal) ：信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生； 消息队列(Message Queuing) ：消息队列是消息的链表,具有特定的格式,存放在内存中并由消息队列标识符标识。管道和消息队列的通信数据都是先进先出的原则。与管道（无名管道：只存在于内存中的文件；命名管道：存在于实际的磁盘介质或者文件系统）不同的是消息队列存放在内核中，只有在内核重启(即，操作系统重启)或者显示地删除一个消息队列时，该消息队列才会被真正的删除。消息队列可以实现消息的随机查询,消息不一定要以先进先出的次序读取,也可以按消息的类型读取.比 FIFO 更有优势。消息队列克服了信号承载信息量少，管道只能承载无格式字 节流以及缓冲区大小受限等缺。 信号量(Semaphores) ：信号量是一个计数器，用于多进程对共享数据的访问，信号量的意图在于进程间同步。这种通信方式主要用于解决与同步相关的问题并避免竞争条件。 共享内存(Shared memory) ：使得多个进程可以访问同一块内存空间，不同进程可以及时看到对方进程中对共享内存中数据的更新。这种方式需要依靠某种同步操作，如互斥锁和信号量等。可以说这是最有用的进程间通信方式。 套接字(Sockets) : 此方法主要用于在客户端和服务器之间通过网络进行通信。套接字是支持 TCP/IP 的网络通信的基本操作单元，可以看做是不同主机之间的进程进行双向通信的端点，简单的说就是通信的两方的一种约定，用套接字中的相关函数来完成通信过程。 2.4 线程间的同步的方式 👨‍💻面试官 ：那线程间的同步的方式有哪些呢? 🙋 我 ：线程同步是两个或多个共享关键资源的线程的并发执行。应该同步线程以避免关键的资源使用冲突。操作系统一般有下面三种线程同步的方式： 互斥量(Mutex)：采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限。因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问。比如 Java 中的 synchronized 关键词和各种 Lock 都是这种机制。 信号量(Semphares) ：它允许同一时刻多个线程访问同一资源，但是需要控制同一时刻访问此资源的最大线程数量 事件(Event) :Wait/Notify：通过通知操作的方式来保持多线程同步，还可以方便的实现多线程优先级的比较操 2.5 进程的调度算法 👨‍💻面试官 ：你知道操作系统中进程的调度算法有哪些吗? 🙋 我 ：嗯嗯！这个我们大学的时候学过，是一个很重要的知识点！ 为了确定首先执行哪个进程以及最后执行哪个进程以实现最大 CPU 利用率，计算机科学家已经定义了一些算法，它们是： 先到先服务(FCFS)调度算法 : 从就绪队列中选择一个最先进入该队列的进程为之分配资源，使它立即执行并一直执行到完成或发生某事件而被阻塞放弃占用 CPU 时再重新调度。 短作业优先(SJF)的调度算法 : 从就绪队列中选出一个估计运行时间最短的进程为之分配资源，使它立即执行并一直执行到完成或发生某事件而被阻塞放弃占用 CPU 时再重新调度。 时间片轮转调度算法 : 时间片轮转调度是一种最古老，最简单，最公平且使用最广的算法，又称 RR(Round robin)调度。每个进程被分配一个时间段，称作它的时间片，即该进程允许运行的时间。 多级反馈队列调度算法 ：前面介绍的几种进程调度的算法都有一定的局限性。如短进程优先的调度算法，仅照顾了短进程而忽略了长进程 。多级反馈队列调度算法既能使高优先级的作业得到响应又能使短作业（进程）迅速完成。，因而它是目前被公认的一种较好的进程调度算法，UNIX 操作系统采取的便是这种调度算法。 优先级调度 ： 为每个流程分配优先级，首先执行具有最高优先级的进程，依此类推。具有相同优先级的进程以 FCFS 方式执行。可以根据内存要求，时间要求或任何其他资源要求来确定优先级。 三 操作系统内存管理基础 3.1 内存管理介绍 👨‍💻 面试官: 操作系统的内存管理主要是做什么？ 🙋 我： 操作系统的内存管理主要负责内存的分配与回收（malloc 函数：申请内存，free 函数：释放内存），另外地址转换也就是将逻辑地址转换成相应的物理地址等功能也是操作系统内存管理做的事情。 3.2 常见的几种内存管理机制 👨‍💻 面试官: 操作系统的内存管理机制了解吗？内存管理有哪几种方式? 🙋 我： 这个在学习操作系统的时候有了解过。 简单分为连续分配管理方式和非连续分配管理方式这两种。连续分配管理方式是指为一个用户程序分配一个连续的内存空间，常见的如 块式管理 。同样地，非连续分配管理方式允许一个程序使用的内存分布在离散或者说不相邻的内存中，常见的如页式管理 和 段式管理。 块式管理 ： 远古时代的计算机操系统的内存管理方式。将内存分为几个固定大小的块，每个块中只包含一个进程。如果程序运行需要内存的话，操作系统就分配给它一块，如果程序运行只需要很小的空间的话，分配的这块内存很大一部分几乎被浪费了。这些在每个块中未被利用的空间，我们称之为碎片。 页式管理 ：把主存分为大小相等且固定的一页一页的形式，页较小，相对相比于块式管理的划分力度更大，提高了内存利用率，减少了碎片。页式管理通过页表对应逻辑地址和物理地址。 段式管理 ： 页式管理虽然提高了内存利用率，但是页式管理其中的页实际并无任何实际意义。 段式管理把主存分为一段段的，每一段的空间又要比一页的空间小很多 。但是，最重要的是段是有实际意义的，每个段定义了一组逻辑信息，例如,有主程序段 MAIN、子程序段 X、数据段 D 及栈段 S 等。 段式管理通过段表对应逻辑地址和物理地址。 👨‍💻面试官 ： 回答的还不错！不过漏掉了一个很重要的 段页式管理机制 。段页式管理机制结合了段式管理和页式管理的优点。简单来说段页式管理机制就是把主存先分成若干段，每个段又分成若干页，也就是说 段页式管理机制 中段与段之间以及段的内部的都是离散的。 🙋 我 ：谢谢面试官！刚刚把这个给忘记了～ 3.3 快表和多级页表 👨‍💻面试官 ： 页表管理机制中有两个很重要的概念：快表和多级页表，这两个东西分别解决了页表管理中很重要的两个问题。你给我简单介绍一下吧！ 🙋 我 ：在分页内存管理中，很重要的两点是： 虚拟地址到物理地址的转换要快。 解决虚拟地址空间大，页表也会很大的问题。 快表 为了解决虚拟地址到物理地址的转换速度，操作系统在 页表方案 基础之上引入了 快表 来加速虚拟地址到物理地址的转换。我们可以把块表理解为一种特殊的高速缓冲存储器（Cache），其中的内容是页表的一部分或者全部内容。作为页表的 Cache，它的作用与页表相似，但是提高了访问速率。由于采用页表做地址转换，读写内存数据时 CPU 要访问两次主存。有了快表，有时只要访问一次高速缓冲存储器，一次主存，这样可加速查找并提高指令执行速度。 使用快表之后的地址转换流程是这样的： 根据虚拟地址中的页号查快表； 如果该页在快表中，直接从快表中读取相应的物理地址； 如果该页不在快表中，就访问内存中的页表，再从页表中得到物理地址，同时将页表中的该映射表项添加到快表中； 当快表填满后，又要登记新页时，就按照一定的淘汰策略淘汰掉快表中的一个页。 看完了之后你会发现快表和我们平时经常在我们开发的系统使用的缓存（比如 Redis）很像，的确是这样的，操作系统中的很多思想、很多经典的算法，你都可以在我们日常开发使用的各种工具或者框架中找到它们的影子。 多级页表 引入多级页表的主要目的是为了避免把全部页表一直放在内存中占用过多空间，特别是那些根本就不需要的页表就不需要保留在内存中。多级页表属于时间换空间的典型场景，具体可以查看下面这篇文章 多级页表如何节约内存：https://www.polarxiong.com/archives/多级页表如何节约内存.html 总结 为了提高内存的空间性能，提出了多级页表的概念；但是提到空间性能是以浪费时间性能为基础的，因此为了补充损失的时间性能，提出了快表（即 TLB）的概念。 不论是快表还是多级页表实际上都利用到了程序的局部性原理，局部性原理在后面的虚拟内存这部分会介绍到。 3.4 分页机制和分段机制的共同点和区别 👨‍💻面试官 ： 分页机制和分段机制有哪些共同点和区别呢？ 🙋 我 ： 共同点 ： 分页机制和分段机制都是为了提高内存利用率，较少内存碎片。 页和段都是离散存储的，所以两者都是离散分配内存的方式。但是，每个页和段中的内存是连续的。 区别 ： 页的大小是固定的，由操作系统决定；而段的大小不固定，取决于我们当前运行的程序。 分页仅仅是为了满足操作系统内存管理的需求，而段是逻辑信息的单位，在程序中可以体现为代码段，数据段，能够更好满足用户的需要。 3.5 逻辑(虚拟)地址和物理地址 👨‍💻面试官 ：你刚刚还提到了逻辑地址和物理地址这两个概念，我不太清楚，你能为我解释一下不？ 🙋 我： em...好的嘛！我们编程一般只有可能和逻辑地址打交道，比如在 C 语言中，指针里面存储的数值就可以理解成为内存里的一个地址，这个地址也就是我们说的逻辑地址，逻辑地址由操作系统决定。物理地址指的是真实物理内存中地址，更具体一点来说就是内存地址寄存器中的地址。物理地址是内存单元真正的地址。 3.6 CPU 寻址了解吗?为什么需要虚拟地址空间? 👨‍💻面试官 ：CPU 寻址了解吗?为什么需要虚拟地址空间? 🙋 我 ：这部分我真不清楚！ 于是面试完之后我默默去查阅了相关文档！留下了没有技术的泪水。。。 这部分内容参考了 Microsoft 官网的介绍，地址：https://msdn.microsoft.com/zh-cn/library/windows/hardware/hh439648(v=vs.85).aspx 现代处理器使用的是一种称为 虚拟寻址(Virtual Addressing) 的寻址方式。使用虚拟寻址，CPU 需要将虚拟地址翻译成物理地址，这样才能访问到真实的物理内存。 实际上完成虚拟地址转换为物理地址转换的硬件是 CPU 中含有一个被称为 内存管理单元（Memory Management Unit, MMU） 的硬件。如下图所示： 为什么要有虚拟地址空间呢？ 先从没有虚拟地址空间的时候说起吧！没有虚拟地址空间的时候，程序都是直接访问和操作的都是物理内存 。但是这样有什么问题呢？ 用户程序可以访问任意内存，寻址内存的每个字节，这样就很容易（有意或者无意）破坏操作系统，造成操作系统崩溃。 想要同时运行多个程序特别困难，比如你想同时运行一个微信和一个 QQ 音乐都不行。为什么呢？举个简单的例子：微信在运行的时候给内存地址 1xxx 赋值后，QQ 音乐也同样给内存地址 1xxx 赋值，那么 QQ 音乐对内存的赋值就会覆盖微信之前所赋的值，这就造成了微信这个程序就会崩溃。 总结来说：如果直接把物理地址暴露出来的话会带来严重问题，比如可能对操作系统造成伤害以及给同时运行多个程序造成困难。 通过虚拟地址访问内存有以下优势： 程序可以使用一系列相邻的虚拟地址来访问物理内存中不相邻的大内存缓冲区。 程序可以使用一系列虚拟地址来访问大于可用物理内存的内存缓冲区。当物理内存的供应量变小时，内存管理器会将物理内存页（通常大小为 4 KB）保存到磁盘文件。数据或代码页会根据需要在物理内存与磁盘之间移动。 不同进程使用的虚拟地址彼此隔离。一个进程中的代码无法更改正在由另一进程或操作系统使用的物理内存。 四 虚拟内存 4.1 什么是虚拟内存(Virtual Memory)? 👨‍💻面试官 ：再问你一个常识性的问题！什么是虚拟内存(Virtual Memory)? 🙋 我 ：这个在我们平时使用电脑特别是 Windows 系统的时候太常见了。很多时候我们使用点开了很多占内存的软件，这些软件占用的内存可能已经远远超出了我们电脑本身具有的物理内存。为什么可以这样呢？ 正是因为 虚拟内存 的存在，通过 虚拟内存 可以让程序可以拥有超过系统物理内存大小的可用内存空间。另外，虚拟内存为每个进程提供了一个一致的、私有的地址空间，它让每个进程产生了一种自己在独享主存的错觉（每个进程拥有一片连续完整的内存空间）。这样会更加有效地管理内存并减少出错。 虚拟内存是计算机系统内存管理的一种技术，我们可以手动设置自己电脑的虚拟内存。不要单纯认为虚拟内存只是“使用硬盘空间来扩展内存“的技术。虚拟内存的重要意义是它定义了一个连续的虚拟地址空间，并且 把内存扩展到硬盘空间。推荐阅读：《虚拟内存的那点事儿》 维基百科中有几句话是这样介绍虚拟内存的。 虚拟内存 使得应用程序认为它拥有连续的可用的内存（一个连续完整的地址空间），而实际上，它通常是被分隔成多个物理内存碎片，还有部分暂时存储在外部磁盘存储器上，在需要时进行数据交换。与没有使用虚拟内存技术的系统相比，使用这种技术的系统使得大型程序的编写变得更容易，对真正的物理内存（例如 RAM）的使用也更有效率。目前，大多数操作系统都使用了虚拟内存，如 Windows 家族的“虚拟内存”；Linux 的“交换空间”等。From:https://zh.wikipedia.org/wiki/虚拟内存 4.2 局部性原理 👨‍💻面试官 ：要想更好地理解虚拟内存技术，必须要知道计算机中著名的局部性原理。另外，局部性原理既适用于程序结构，也适用于数据结构，是非常重要的一个概念。 🙋 我 ：局部性原理是虚拟内存技术的基础，正是因为程序运行具有局部性原理，才可以只装入部分程序到内存就开始运行。 以下内容摘自《计算机操作系统教程》 第 4 章存储器管理。 早在 1968 年的时候，就有人指出我们的程序在执行的时候往往呈现局部性规律，也就是说在某个较短的时间段内，程序执行局限于某一小部分，程序访问的存储空间也局限于某个区域。 局部性原理表现在以下两个方面： 时间局部性 ：如果程序中的某条指令一旦执行，不久以后该指令可能再次执行；如果某数据被访问过，不久以后该数据可能再次被访问。产生时间局部性的典型原因，是由于在程序中存在着大量的循环操作。 空间局部性 ：一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也将被访问，即程序在一段时间内所访问的地址，可能集中在一定的范围之内，这是因为指令通常是顺序存放、顺序执行的，数据也一般是以向量、数组、表等形式簇聚存储的。 时间局部性是通过将近来使用的指令和数据保存到高速缓存存储器中，并使用高速缓存的层次结构实现。空间局部性通常是使用较大的高速缓存，并将预取机制集成到高速缓存控制逻辑中实现。虚拟内存技术实际上就是建立了 “内存一外存”的两级存储器的结构，利用局部性原理实现髙速缓存。 4.3 虚拟存储器 👨‍💻面试官 ：都说了虚拟内存了。你再讲讲虚拟存储器把！ 🙋 我 ： 这部分内容来自：王道考研操作系统知识点整理。 基于局部性原理，在程序装入时，可以将程序的一部分装入内存，而将其他部分留在外存，就可以启动程序执行。由于外存往往比内存大很多，所以我们运行的软件的内存大小实际上是可以比计算机系统实际的内存大小大的。在程序执行过程中，当所访问的信息不在内存时，由操作系统将所需要的部分调入内存，然后继续执行程序。另一方面，操作系统将内存中暂时不使用的内容换到外存上，从而腾出空间存放将要调入内存的信息。这样，计算机好像为用户提供了一个比实际内存大的多的存储器——虚拟存储器。 实际上，我觉得虚拟内存同样是一种时间换空间的策略，你用 CPU 的计算时间，页的调入调出花费的时间，换来了一个虚拟的更大的空间来支持程序的运行。不得不感叹，程序世界几乎不是时间换空间就是空间换时间。 4.4 虚拟内存的技术实现 👨‍💻面试官 ：虚拟内存技术的实现呢？ 🙋 我 ：虚拟内存的实现需要建立在离散分配的内存管理方式的基础上。 虚拟内存的实现有以下三种方式： 请求分页存储管理 ：建立在分页管理之上，为了支持虚拟存储器功能而增加了请求调页功能和页面置换功能。请求分页是目前最常用的一种实现虚拟存储器的方法。请求分页存储管理系统中，在作业开始运行之前，仅装入当前要执行的部分段即可运行。假如在作业运行的过程中发现要访问的页面不在内存，则由处理器通知操作系统按照对应的页面置换算法将相应的页面调入到主存，同时操作系统也可以将暂时不用的页面置换到外存中。 请求分段存储管理 ：建立在分段存储管理之上，增加了请求调段功能、分段置换功能。请求分段储存管理方式就如同请求分页储存管理方式一样，在作业开始运行之前，仅装入当前要执行的部分段即可运行；在执行过程中，可使用请求调入中断动态装入要访问但又不在内存的程序段；当内存空间已满，而又需要装入新的段时，根据置换功能适当调出某个段，以便腾出空间而装入新的段。 请求段页式存储管理 这里多说一下？很多人容易搞混请求分页与分页存储管理，两者有何不同呢？ 请求分页存储管理建立在分页管理之上。他们的根本区别是是否将程序全部所需的全部地址空间都装入主存，这也是请求分页存储管理可以提供虚拟内存的原因，我们在上面已经分析过了。 它们之间的根本区别在于是否将一作业的全部地址空间同时装入主存。请求分页存储管理不要求将作业全部地址空间同时装入主存。基于这一点，请求分页存储管理可以提供虚存，而分页存储管理却不能提供虚存。 不管是上面那种实现方式，我们一般都需要： 一定容量的内存和外存：在载入程序的时候，只需要将程序的一部分装入内存，而将其他部分留在外存，然后程序就可以执行了； 缺页中断：如果需执行的指令或访问的数据尚未在内存（称为缺页或缺段），则由处理器通知操作系统将相应的页面或段调入到内存，然后继续执行程序； 虚拟地址空间 ：逻辑地址到物理地址的变换。 4.5 页面置换算法 👨‍💻面试官 ：虚拟内存管理很重要的一个概念就是页面置换算法。那你说一下 页面置换算法的作用?常见的页面置换算法有哪些? 🙋 我 ： 这个题目经常作为笔试题出现，网上已经给出了很不错的回答，我这里只是总结整理了一下。 地址映射过程中，若在页面中发现所要访问的页面不在内存中，则发生缺页中断 。 缺页中断 就是要访问的页不在主存，需要操作系统将其调入主存后再进行访问。 在这个时候，被内存映射的文件实际上成了一个分页交换文件。 当发生缺页中断时，如果当前内存中并没有空闲的页面，操作系统就必须在内存选择一个页面将其移出内存，以便为即将调入的页面让出空间。用来选择淘汰哪一页的规则叫做页面置换算法，我们可以把页面置换算法看成是淘汰页面的规则。 OPT 页面置换算法（最佳页面置换算法） ：最佳(Optimal, OPT)置换算法所选择的被淘汰页面将是以后永不使用的，或者是在最长时间内不再被访问的页面,这样可以保证获得最低的缺页率。但由于人们目前无法预知进程在内存下的若千页面中哪个是未来最长时间内不再被访问的，因而该算法无法实现。一般作为衡量其他置换算法的方法。 FIFO（First In First Out） 页面置换算法（先进先出页面置换算法） : 总是淘汰最先进入内存的页面，即选择在内存中驻留时间最久的页面进行淘汰。 LRU （Least Currently Used）页面置换算法（最近最久未使用页面置换算法） ：LRU算法赋予每个页面一个访问字段，用来记录一个页面自上次被访问以来所经历的时间 T，当须淘汰一个页面时，选择现有页面中其 T 值最大的，即最近最久未使用的页面予以淘汰。 LFU （Least Frequently Used）页面置换算法（最少使用页面置换算法） : 该置换算法选择在之前时期使用最少的页面作为淘汰页。 Reference 《计算机操作系统—汤小丹》第四版 《深入理解计算机系统》 https://zh.wikipedia.org/wiki/输入输出内存管理单元 https://baike.baidu.com/item/快表/19781679 https://www.jianshu.com/p/1d47ed0b46d5 https://www.studytonight.com/operating-system https://www.geeksforgeeks.org/interprocess-communication-methods/ https://juejin.im/post/59f8691b51882534af254317 王道考研操作系统知识点整理： https://wizardforcel.gitbooks.io/wangdaokaoyan-os/content/13.html "},"zother6-JavaGuide/operating-system/linux.html":{"url":"zother6-JavaGuide/operating-system/linux.html","title":"Linux","keywords":"","body":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。 一 从认识操作系统开始 1.1 操作系统简介 1.2 操作系统简单分类 二 初探Linux 2.1 Linux简介 2.2 Linux诞生简介 2.3 Linux的分类 三 Linux文件系统概览 3.1 Linux文件系统简介 3.2 文件类型与目录结构 四 Linux基本命令 4.1 目录切换命令 4.2 目录的操作命令（增删改查） 4.3 文件的操作命令（增删改查） 4.4 压缩文件的操作命令 4.5 Linux的权限命令 4.6 Linux 用户管理 4.7 Linux系统用户组的管理 4.8 其他常用命令 推荐一个Github开源的Linux学习指南(Java工程师向)：https://github.com/judasn/Linux-Tutorial 学习Linux之前，我们先来简单的认识一下操作系统。 一 从认识操作系统开始 1.1 操作系统简介 我通过以下四点介绍什么是操作系统： 操作系统（Operation System，简称OS）是管理计算机硬件与软件资源的程序，是计算机系统的内核与基石； 操作系统本质上是运行在计算机上的软件程序 ； 为用户提供一个与系统交互的操作界面 ； 操作系统分内核与外壳（我们可以把外壳理解成围绕着内核的应用程序，而内核就是能操作硬件的程序）。 1.2 操作系统简单分类 Windows: 目前最流行的个人桌面操作系统 ，不做多的介绍，大家都清楚。 Unix： 最早的多用户、多任务操作系统 .按照操作系统的分类，属于分时操作系统。Unix 大多被用在服务器、工作站，现在也有用在个人计算机上。它在创建互联网、计算机网络或客户端/服务器模型方面发挥着非常重要的作用。 Linux: Linux是一套免费使用和自由传播的类Unix操作系统.Linux存在着许多不同的Linux版本，但它们都使用了 Linux内核 。Linux可安装在各种计算机硬件设备中，比如手机、平板电脑、路由器、视频游戏控制台、台式计算机、大型机和超级计算机。严格来讲，Linux这个词本身只表示Linux内核，但实际上人们已经习惯了用Linux来形容整个基于Linux内核，并且使用GNU 工程各种工具和数据库的操作系统。 二 初探Linux 2.1 Linux简介 我们上面已经介绍到了Linux，我们这里只强调三点。 类Unix系统： Linux是一种自由、开放源码的类似Unix的操作系统 Linux内核： 严格来说，Linux这个词本身只表示Linux内核 Linux之父： 一个编程领域的传奇式人物。他是Linux内核的最早作者，随后发起了这个开源项目，担任Linux内核的首要架构师与项目协调者，是当今世界最著名的电脑程序员、黑客之一。他还发起了Git这个开源项目，并为主要的开发者。 2.2 Linux诞生简介 1991年，芬兰的业余计算机爱好者Linus Torvalds编写了一款类似Minix的系统（基于微内核架构的类Unix操作系统）被ftp管理员命名为Linux 加入到自由软件基金的GNU计划中; Linux以一只可爱的企鹅作为标志，象征着敢作敢为、热爱生活。 2.3 Linux的分类 Linux根据原生程度，分为两种： 内核版本： Linux不是一个操作系统，严格来讲，Linux只是一个操作系统中的内核。内核是什么？内核建立了计算机软件与硬件之间通讯的平台，内核提供系统服务，比如文件管理、虚拟内存、设备I/O等； 发行版本： 一些组织或公司在内核版基础上进行二次开发而重新发行的版本。Linux发行版本有很多种（ubuntu和CentOS用的都很多，初学建议选择CentOS），如下图所示： 三 Linux文件系统概览 3.1 Linux文件系统简介 在Linux操作系统中，所有被操作系统管理的资源，例如网络接口卡、磁盘驱动器、打印机、输入输出设备、普通文件或是目录都被看作是一个文件。 也就是说在LINUX系统中有一个重要的概念：一切都是文件。其实这是UNIX哲学的一个体现，而Linux是重写UNIX而来，所以这个概念也就传承了下来。在UNIX系统中，把一切资源都看作是文件，包括硬件设备。UNIX系统把每个硬件都看成是一个文件，通常称为设备文件，这样用户就可以用读写文件的方式实现对硬件的访问。 3.2 文件类型与目录结构 Linux支持5种文件类型 ： Linux的目录结构如下： Linux文件系统的结构层次鲜明，就像一棵倒立的树，最顶层是其根目录： 常见目录说明： /bin： 存放二进制可执行文件(ls、cat、mkdir等)，常用命令一般都在这里； /etc： 存放系统管理和配置文件； /home： 存放所有用户文件的根目录，是用户主目录的基点，比如用户user的主目录就是/home/user，可以用~user表示； /usr ： 用于存放系统应用程序； /opt： 额外安装的可选应用程序包所放置的位置。一般情况下，我们可以把tomcat等都安装到这里； /proc： 虚拟文件系统目录，是系统内存的映射。可直接访问这个目录来获取系统信息； /root： 超级用户（系统管理员）的主目录（特权阶级^o^）； /sbin: 存放二进制可执行文件，只有root才能访问。这里存放的是系统管理员使用的系统级别的管理命令和程序。如ifconfig等； /dev： 用于存放设备文件； /mnt： 系统管理员安装临时文件系统的安装点，系统提供这个目录是让用户临时挂载其他的文件系统； /boot： 存放用于系统引导时使用的各种文件； /lib ： 存放着和系统运行相关的库文件 ； /tmp： 用于存放各种临时文件，是公用的临时文件存储点； /var： 用于存放运行时需要改变数据的文件，也是某些大文件的溢出区，比方说各种服务的日志文件（系统启动日志等。）等； /lost+found： 这个目录平时是空的，系统非正常关机而留下“无家可归”的文件（windows下叫什么.chk）就在这里。 四 Linux基本命令 下面只是给出了一些比较常用的命令。推荐一个Linux命令快查网站，非常不错，大家如果遗忘某些命令或者对某些命令不理解都可以在这里得到解决。 Linux命令大全：http://man.linuxde.net/ 4.1 目录切换命令 cd usr： 切换到该目录下usr目录 cd ..（或cd../）： 切换到上一层目录 cd /： 切换到系统根目录 cd ~： 切换到用户主目录 cd -： 切换到上一个操作所在目录 4.2 目录的操作命令(增删改查) mkdir 目录名称： 增加目录 ls或者ll（ll是ls -l的别名，ll命令可以看到该目录下的所有目录和文件的详细信息）：查看目录信息 find 目录 参数： 寻找目录（查） 示例： 列出当前目录及子目录下所有文件和文件夹: find . 在/home目录下查找以.txt结尾的文件名:find /home -name \"*.txt\" 同上，但忽略大小写: find /home -iname \"*.txt\" 当前目录及子目录下查找所有以.txt和.pdf结尾的文件:find . \\( -name \"*.txt\" -o -name \"*.pdf\" \\)或find . -name \"*.txt\" -o -name \"*.pdf\" mv 目录名称 新目录名称： 修改目录的名称（改） 注意：mv的语法不仅可以对目录进行重命名而且也可以对各种文件，压缩包等进行 重命名的操作。mv命令用来对文件或目录重新命名，或者将文件从一个目录移到另一个目录中。后面会介绍到mv命令的另一个用法。 mv 目录名称 目录的新位置： 移动目录的位置---剪切（改） 注意：mv语法不仅可以对目录进行剪切操作，对文件和压缩包等都可执行剪切操作。另外mv与cp的结果不同，mv好像文件“搬家”，文件个数并未增加。而cp对文件进行复制，文件个数增加了。 cp -r 目录名称 目录拷贝的目标位置： 拷贝目录（改），-r代表递归拷贝 注意：cp命令不仅可以拷贝目录还可以拷贝文件，压缩包等，拷贝文件和压缩包时不 用写-r递归 rm [-rf] 目录: 删除目录（删） 注意：rm不仅可以删除目录，也可以删除其他文件或压缩包，为了增强大家的记忆， 无论删除任何目录或文件，都直接使用rm -rf 目录/文件/压缩包 4.3 文件的操作命令(增删改查) touch 文件名称: 文件的创建（增） cat/more/less/tail 文件名称 文件的查看（查） cat： 查看显示文件内容 more： 可以显示百分比，回车可以向下一行， 空格可以向下一页，q可以退出查看 less： 可以使用键盘上的PgUp和PgDn向上 和向下翻页，q结束查看 tail-10 ： 查看文件的后10行，Ctrl+C结束 注意：命令 tail -f 文件 可以对某个文件进行动态监控，例如tomcat的日志文件， 会随着程序的运行，日志会变化，可以使用tail -f catalina-2016-11-11.log 监控 文 件的变化 vim 文件： 修改文件的内容（改） vim编辑器是Linux中的强大组件，是vi编辑器的加强版，vim编辑器的命令和快捷方式有很多，但此处不一一阐述，大家也无需研究的很透彻，使用vim编辑修改文件的方式基本会使用就可以了。 在实际开发中，使用vim编辑器主要作用就是修改配置文件，下面是一般步骤： vim 文件------>进入文件----->命令模式------>按i进入编辑模式----->编辑文件 ------->按Esc进入底行模式----->输入：wq/q! （输入wq代表写入内容并退出，即保存；输入q!代表强制退出不保存。） rm -rf 文件： 删除文件（删） 同目录删除：熟记 rm -rf 文件 即可 4.4 压缩文件的操作命令 1）打包并压缩文件： Linux中的打包文件一般是以.tar结尾的，压缩的命令一般是以.gz结尾的。 而一般情况下打包和压缩是一起进行的，打包并压缩后的文件的后缀名一般.tar.gz。 命令：tar -zcvf 打包压缩后的文件名 要打包压缩的文件 其中： z：调用gzip压缩命令进行压缩 c：打包文件 v：显示运行过程 f：指定文件名 比如：假如test目录下有三个文件分别是：aaa.txt bbb.txt ccc.txt，如果我们要打包test目录并指定压缩后的压缩包名称为test.tar.gz可以使用命令：tar -zcvf test.tar.gz aaa.txt bbb.txt ccc.txt或：tar -zcvf test.tar.gz /test/ 2）解压压缩包： 命令：tar [-xvf] 压缩文件 其中：x：代表解压 示例： 1 将/test下的test.tar.gz解压到当前目录下可以使用命令：tar -xvf test.tar.gz 2 将/test下的test.tar.gz解压到根目录/usr下:tar -xvf test.tar.gz -C /usr（- C代表指定解压的位置） 4.5 Linux的权限命令 操作系统中每个文件都拥有特定的权限、所属用户和所属组。权限是操作系统用来限制资源访问的机制，在Linux中权限一般分为读(readable)、写(writable)和执行(excutable)，分为三组。分别对应文件的属主(owner)，属组(group)和其他用户(other)，通过这样的机制来限制哪些用户、哪些组可以对特定的文件进行什么样的操作。通过 ls -l 命令我们可以 查看某个目录下的文件或目录的权限 示例：在随意某个目录下ls -l 第一列的内容的信息解释如下： 下面将详细讲解文件的类型、Linux中权限以及文件有所有者、所在组、其它组具体是什么？ 文件的类型： d： 代表目录 -： 代表文件 l： 代表软链接（可以认为是window中的快捷方式） Linux中权限分为以下几种： r：代表权限是可读，r也可以用数字4表示 w：代表权限是可写，w也可以用数字2表示 x：代表权限是可执行，x也可以用数字1表示 文件和目录权限的区别： 对文件和目录而言，读写执行表示不同的意义。 对于文件： 权限名称 可执行操作 r 可以使用cat查看文件的内容 w 可以修改文件的内容 x 可以将其运行为二进制文件 对于目录： 权限名称 可执行操作 r 可以查看目录下列表 w 可以创建和删除目录下文件 x 可以使用cd进入目录 需要注意的是超级用户可以无视普通用户的权限，即使文件目录权限是000，依旧可以访问。 在linux中的每个用户必须属于一个组，不能独立于组外。在linux中每个文件有所有者、所在组、其它组的概念。 所有者 一般为文件的创建者，谁创建了该文件，就天然的成为该文件的所有者，用ls ‐ahl命令可以看到文件的所有者 也可以使用chown 用户名 文件名来修改文件的所有者 。 文件所在组 当某个用户创建了一个文件后，这个文件的所在组就是该用户所在的组 用ls ‐ahl命令可以看到文件的所有组 也可以使用chgrp 组名 文件名来修改文件所在的组。 其它组 除开文件的所有者和所在组的用户外，系统的其它用户都是文件的其它组 我们再来看看如何修改文件/目录的权限。 修改文件/目录的权限的命令：chmod 示例：修改/test下的aaa.txt的权限为属主有全部权限，属主所在的组有读写权限， 其他用户只有读的权限 chmod u=rwx,g=rw,o=r aaa.txt chmod -R u=rwx,g=rwx,o=rwx ./log // 递归给log目录下的所有文件授权 上述示例还可以使用数字表示： chmod 764 aaa.txt 补充一个比较常用的东西: 假如我们装了一个zookeeper，我们每次开机到要求其自动启动该怎么办？ 新建一个脚本zookeeper 为新建的脚本zookeeper添加可执行权限，命令是:chmod +x zookeeper 把zookeeper这个脚本添加到开机启动项里面，命令是：chkconfig --add zookeeper 如果想看看是否添加成功，命令是：chkconfig --list 4.6 Linux 用户管理 Linux系统是一个多用户多任务的分时操作系统，任何一个要使用系统资源的用户，都必须首先向系统管理员申请一个账号，然后以这个账号的身份进入系统。 用户的账号一方面可以帮助系统管理员对使用系统的用户进行跟踪，并控制他们对系统资源的访问；另一方面也可以帮助用户组织文件，并为用户提供安全性保护。 Linux用户管理相关命令: useradd 选项 用户名:添加用户账号 userdel 选项 用户名:删除用户帐号 usermod 选项 用户名:修改帐号 passwd 用户名:更改或创建用户的密码 passwd -S 用户名 :显示用户账号密码信息 passwd -d 用户名: 清除用户密码 useradd命令用于Linux中创建的新的系统用户。useradd可用来建立用户帐号。帐号建好之后，再用passwd设定帐号的密码．而可用userdel删除帐号。使用useradd指令所建立的帐号，实际上是保存在/etc/passwd文本文件中。 passwd命令用于设置用户的认证信息，包括用户密码、密码过期时间等。系统管理者则能用它管理系统用户的密码。只有管理者可以指定用户名称，一般用户只能变更自己的密码。 4.7 Linux系统用户组的管理 每个用户都有一个用户组，系统可以对一个用户组中的所有用户进行集中管理。不同Linux 系统对用户组的规定有所不同，如Linux下的用户属于与它同名的用户组，这个用户组在创建用户时同时创建。 用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就是对/etc/group文件的更新。 Linux系统用户组的管理相关命令: groupadd 选项 用户组 :增加一个新的用户组 groupdel 用户组:要删除一个已有的用户组 groupmod 选项 用户组 : 修改用户组的属性 4.8 其他常用命令 pwd： 显示当前所在位置 sudo + 其他命令：以系统管理者的身份执行指令，也就是说，经由 sudo 所执行的指令就好像是 root 亲自执行。 grep 要搜索的字符串 要搜索的文件 --color： 搜索命令，--color代表高亮显示 ps -ef/ps -aux： 这两个命令都是查看当前系统正在运行进程，两者的区别是展示格式不同。如果想要查看特定的进程可以使用这样的格式：ps aux|grep redis （查看包括redis字符串的进程），也可使用 pgrep redis -a。 注意：如果直接用ps（（Process Status））命令，会显示所有进程的状态，通常结合grep命令查看某进程的状态。 kill -9 进程的pid： 杀死进程（-9 表示强制终止。） 先用ps查找进程，然后用kill杀掉 网络通信命令： 查看当前系统的网卡信息：ifconfig 查看与某台机器的连接情况：ping 查看当前系统的端口使用：netstat -an net-tools 和 iproute2 ： net-tools起源于BSD的TCP/IP工具箱，后来成为老版本Linux内核中配置网络功能的工具。但自2001年起，Linux社区已经对其停止维护。同时，一些Linux发行版比如Arch Linux和CentOS/RHEL 7则已经完全抛弃了net-tools，只支持iproute2。linux ip命令类似于ifconfig，但功能更强大，旨在替代它。更多详情请阅读如何在Linux中使用IP命令和示例 shutdown： shutdown -h now： 指定现在立即关机；shutdown +5 \"System will shutdown after 5 minutes\"：指定5分钟后关机，同时送出警告信息给登入用户。 reboot： reboot： 重开机。reboot -w： 做个重开机的模拟（只有纪录并不会真的重开机）。 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \"Java面试突击\" 即可免费领取！ Java工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/operating-system/Shell.html":{"url":"zother6-JavaGuide/operating-system/Shell.html","title":"Shell","keywords":"","body":" Shell 编程入门 走进 Shell 编程的大门 为什么要学Shell？ 什么是 Shell？ Shell 编程的 Hello World Shell 变量 Shell 编程中的变量介绍 Shell 字符串入门 Shell 字符串常见操作 Shell 数组 Shell 基本运算符 算数运算符 关系运算符 逻辑运算符 布尔运算符 字符串运算符 文件相关运算符 shell流程控制 if 条件语句 for 循环语句 while 语句 shell 函数 不带参数没有返回值的函数 有返回值的函数 带参数的函数 Shell 编程入门 走进 Shell 编程的大门 为什么要学Shell？ 学一个东西，我们大部分情况都是往实用性方向着想。从工作角度来讲，学习 Shell 是为了提高我们自己工作效率，提高产出，让我们在更少的时间完成更多的事情。 很多人会说 Shell 编程属于运维方面的知识了，应该是运维人员来做，我们做后端开发的没必要学。我觉得这种说法大错特错，相比于专门做Linux运维的人员来说，我们对 Shell 编程掌握程度的要求要比他们低，但是shell编程也是我们必须要掌握的！ 目前Linux系统下最流行的运维自动化语言就是Shell和Python了。 两者之间，Shell几乎是IT企业必须使用的运维自动化编程语言，特别是在运维工作中的服务监控、业务快速部署、服务启动停止、数据备份及处理、日志分析等环节里，shell是不可缺的。Python 更适合处理复杂的业务逻辑，以及开发复杂的运维软件工具，实现通过web访问等。Shell是一个命令解释器，解释执行用户所输入的命令和程序。一输入命令，就立即回应的交互的对话方式。 另外，了解 shell 编程也是大部分互联网公司招聘后端开发人员的要求。下图是我截取的一些知名互联网公司对于 Shell 编程的要求。 什么是 Shell？ 简单来说“Shell编程就是对一堆Linux命令的逻辑化处理”。 W3Cschool 上的一篇文章是这样介绍 Shell的，如下图所示。 Shell 编程的 Hello World 学习任何一门编程语言第一件事就是输出HelloWord了！下面我会从新建文件到shell代码编写来说下Shell 编程如何输出Hello World。 (1)新建一个文件 helloworld.sh :touch helloworld.sh，扩展名为 sh（sh代表Shell）（扩展名并不影响脚本执行，见名知意就好，如果你用 php 写 shell 脚本，扩展名就用 php 好了） (2) 使脚本具有执行权限：chmod +x helloworld.sh (3) 使用 vim 命令修改helloworld.sh文件：vim helloworld.sh(vim 文件------>进入文件----->命令模式------>按i进入编辑模式----->编辑文件 ------->按Esc进入底行模式----->输入:wq/q! （输入wq代表写入内容并退出，即保存；输入q!代表强制退出不保存。）) helloworld.sh 内容如下： #!/bin/bash #第一个shell小程序,echo 是linux中的输出命令。 echo \"helloworld!\" shell中 # 符号表示注释。shell 的第一行比较特殊，一般都会以#!开始来指定使用的 shell 类型。在linux中，除了bash shell以外，还有很多版本的shell， 例如zsh、dash等等...不过bash shell还是我们使用最多的。 (4) 运行脚本:./helloworld.sh 。（注意，一定要写成 ./helloworld.sh ，而不是 helloworld.sh ，运行其它二进制的程序也一样，直接写 helloworld.sh ，linux 系统会去 PATH 里寻找有没有叫 helloworld.sh 的，而只有 /bin, /sbin, /usr/bin，/usr/sbin 等在 PATH 里，你的当前目录通常不在 PATH 里，所以写成 helloworld.sh 是会找不到命令的，要用./helloworld.sh 告诉系统说，就在当前目录找。） Shell 变量 Shell 编程中的变量介绍 Shell编程中一般分为三种变量： 我们自己定义的变量（自定义变量）: 仅在当前 Shell 实例中有效，其他 Shell 启动的程序不能访问局部变量。 Linux已定义的环境变量（环境变量， 例如：$PATH, $HOME 等..., 这类变量我们可以直接使用），使用 env 命令可以查看所有的环境变量，而set命令既可以查看环境变量也可以查看自定义变量。 Shell变量 ：Shell变量是由 Shell 程序设置的特殊变量。Shell 变量中有一部分是环境变量，有一部分是局部变量，这些变量保证了 Shell 的正常运行 常用的环境变量: PATH 决定了shell将到哪些目录中寻找命令或程序 HOME 当前用户主目录 HISTSIZE　历史记录数 LOGNAME 当前用户的登录名 HOSTNAME　指主机的名称 SHELL 当前用户Shell类型 LANGUGE 　语言相关的环境变量，多语言可以修改此环境变量 MAIL　当前用户的邮件存放目录 PS1　基本提示符，对于root用户是#，对于普通用户是$ 使用 Linux 已定义的环境变量： 比如我们要看当前用户目录可以使用：echo $HOME命令；如果我们要看当前用户Shell类型 可以使用echo $SHELL命令。可以看出，使用方法非常简单。 使用自己定义的变量： #!/bin/bash #自定义变量hello hello=\"hello world\" echo $hello echo \"helloworld!\" Shell 编程中的变量名的命名的注意事项： 命名只能使用英文字母，数字和下划线，首个字符不能以数字开头，但是可以使用下划线（_）开头。 中间不能有空格，可以使用下划线（_）。 不能使用标点符号。 不能使用bash里的关键字（可用help命令查看保留关键字）。 Shell 字符串入门 字符串是shell编程中最常用最有用的数据类型（除了数字和字符串，也没啥其它类型好用了），字符串可以用单引号，也可以用双引号。这点和Java中有所不同。 单引号字符串： #!/bin/bash name='SnailClimb' hello='Hello, I am '$name'!' echo $hello 输出内容： Hello, I am SnailClimb! 双引号字符串： #!/bin/bash name='SnailClimb' hello=\"Hello, I am \"$name\"!\" echo $hello 输出内容： Hello, I am SnailClimb! Shell 字符串常见操作 拼接字符串： #!/bin/bash name=\"SnailClimb\" # 使用双引号拼接 greeting=\"hello, \"$name\" !\" greeting_1=\"hello, ${name} !\" echo $greeting $greeting_1 # 使用单引号拼接 greeting_2='hello, '$name' !' greeting_3='hello, ${name} !' echo $greeting_2 $greeting_3 输出结果： 获取字符串长度： #!/bin/bash #获取字符串长度 name=\"SnailClimb\" # 第一种方式 echo ${#name} #输出 10 # 第二种方式 expr length \"$name\"; 输出结果: 10 10 使用 expr 命令时，表达式中的运算符左右必须包含空格，如果不包含空格，将会输出表达式本身: expr 5+6 // 直接输出 5+6 expr 5 + 6 // 输出 11 对于某些运算符，还需要我们使用符号\\进行转义，否则就会提示语法错误。 expr 5 * 6 // 输出错误 expr 5 \\* 6 // 输出30 截取子字符串: 简单的字符串截取： #从字符串第 1 个字符开始往后截取 10 个字符 str=\"SnailClimb is a great man\" echo ${str:0:10} #输出:SnailClimb 根据表达式截取： #!bin/bash #author:amau var=\"http://www.runoob.com/linux/linux-shell-variable.html\" s1=${var%%t*}#h s2=${var%t*}#http://www.runoob.com/linux/linux-shell-variable.h s3=${var%%.*}#http://www s4=${var#*/}#/www.runoob.com/linux/linux-shell-variable.html s5=${var##*/}#linux-shell-variable.html Shell 数组 bash支持一维数组（不支持多维数组），并且没有限定数组的大小。我下面给了大家一个关于数组操作的 Shell 代码示例，通过该示例大家可以知道如何创建数组、获取数组长度、获取/删除特定位置的数组元素、删除整个数组以及遍历数组。 #!/bin/bash array=(1 2 3 4 5); # 获取数组长度 length=${#array[@]} # 或者 length2=${#array[*]} #输出数组长度 echo $length #输出：5 echo $length2 #输出：5 # 输出数组第三个元素 echo ${array[2]} #输出：3 unset array[1]# 删除下标为1的元素也就是删除第二个元素 for i in ${array[@]};do echo $i ;done # 遍历数组，输出： 1 3 4 5 unset arr_number; # 删除数组中的所有元素 for i in ${array[@]};do echo $i ;done # 遍历数组，数组元素为空，没有任何输出内容 Shell 基本运算符 说明：图片来自《菜鸟教程》 Shell 编程支持下面几种运算符 算数运算符 关系运算符 布尔运算符 字符串运算符 文件测试运算符 算数运算符 我以加法运算符做一个简单的示例（注意：不是单引号，是反引号）： #!/bin/bash a=3;b=3; val=`expr $a + $b` #输出：Total value : 6 echo \"Total value : $val\" 关系运算符 关系运算符只支持数字，不支持字符串，除非字符串的值是数字。 通过一个简单的示例演示关系运算符的使用，下面shell程序的作用是当score=100的时候输出A否则输出B。 #!/bin/bash score=90; maxscore=100; if [ $score -eq $maxscore ] then echo \"A\" else echo \"B\" fi 输出结果： B 逻辑运算符 示例： #!/bin/bash a=$(( 1 && 0)) # 输出：0；逻辑与运算只有相与的两边都是1，与的结果才是1；否则与的结果是0 echo $a; 布尔运算符 这里就不做演示了，应该挺简单的。 字符串运算符 简单示例： #!/bin/bash a=\"abc\"; b=\"efg\"; if [ $a = $b ] then echo \"a 等于 b\" else echo \"a 不等于 b\" fi 输出： a 不等于 b 文件相关运算符 使用方式很简单，比如我们定义好了一个文件路径file=\"/usr/learnshell/test.sh\" 如果我们想判断这个文件是否可读，可以这样if [ -r $file ] 如果想判断这个文件是否可写，可以这样-w $file，是不是很简单。 shell流程控制 if 条件语句 简单的 if else-if else 的条件语句示例 #!/bin/bash a=3; b=9; if [ $a -eq $b ] then echo \"a 等于 b\" elif [ $a -gt $b ] then echo \"a 大于 b\" else echo \"a 小于 b\" fi 输出结果： a 小于 b 相信大家通过上面的示例就已经掌握了 shell 编程中的 if 条件语句。不过，还要提到的一点是，不同于我们常见的 Java 以及 PHP 中的 if 条件语句，shell if 条件语句中不能包含空语句也就是什么都不做的语句。 for 循环语句 通过下面三个简单的示例认识 for 循环语句最基本的使用，实际上 for 循环语句的功能比下面你看到的示例展现的要大得多。 输出当前列表中的数据： for loop in 1 2 3 4 5 do echo \"The value is: $loop\" done 产生 10 个随机数： #!/bin/bash for i in {0..9}; do echo $RANDOM; done 输出1到5: 通常情况下 shell 变量调用需要加 $,但是 for 的 (()) 中不需要,下面来看一个例子： #!/bin/bash for((i=1;i while 语句 基本的 while 循环语句： #!/bin/bash int=1 while(( $int while循环可用于读取键盘信息： echo '按下 退出' echo -n '输入你最喜欢的电影: ' while read FILM do echo \"是的！$FILM 是一个好电影\" done 输出内容: 按下 退出 输入你最喜欢的电影: 变形金刚 是的！变形金刚 是一个好电影 无限循环： while true do command done shell 函数 不带参数没有返回值的函数 #!/bin/bash hello(){ echo \"这是我的第一个 shell 函数!\" } echo \"-----函数开始执行-----\" hello echo \"-----函数执行完毕-----\" 输出结果： -----函数开始执行----- 这是我的第一个 shell 函数! -----函数执行完毕----- 有返回值的函数 输入两个数字之后相加并返回结果： #!/bin/bash funWithReturn(){ echo \"输入第一个数字: \" read aNum echo \"输入第二个数字: \" read anotherNum echo \"两个数字分别为 $aNum 和 $anotherNum !\" return $(($aNum+$anotherNum)) } funWithReturn echo \"输入的两个数字之和为 $?\" 输出结果： 输入第一个数字: 1 输入第二个数字: 2 两个数字分别为 1 和 2 ! 输入的两个数字之和为 3 带参数的函数 #!/bin/bash funWithParam(){ echo \"第一个参数为 $1 !\" echo \"第二个参数为 $2 !\" echo \"第十个参数为 $10 !\" echo \"第十个参数为 ${10} !\" echo \"第十一个参数为 ${11} !\" echo \"参数总数有 $# 个!\" echo \"作为一个字符串输出所有参数 $* !\" } funWithParam 1 2 3 4 5 6 7 8 9 34 73 输出结果： 第一个参数为 1 ! 第二个参数为 2 ! 第十个参数为 10 ! 第十个参数为 34 ! 第十一个参数为 73 ! 参数总数有 11 个! 作为一个字符串输出所有参数 1 2 3 4 5 6 7 8 9 34 73 ! "},"zother6-JavaGuide/questions/java-big-data.html":{"url":"zother6-JavaGuide/questions/java-big-data.html","title":"Java Big Data","keywords":"","body":"写这篇文章主要是为了回答球友的一个提问，提问如下： 刚好自己对这方面有一丁点的见解，所以回答一下这位老哥的问题，如果能够解决他的问题，我也会很高兴。下面仅仅代表个人一件，环境大家批评指正与完善！ 先说一下自己的经历，大学的时候我从大二开始学习 Java ，然后学了大半年多的安卓。之后就开始学习 Java 后台，学习完 Java 后台一些常用的知识比如 Java基础、Spring、MyBatis等等之后。因为感觉大数据领域发展也挺不错的，所以就接触了一些大数据方面的知识比如当时大数据领域的霸主 Hadoop 。 我当时学习了很多比较古老的技术比如现在基本不会用的 JSP、Struts2等等。另外，我 所以，我当时在找工作之间也纠结过自己到底是投大数据岗位还是Java后台开发岗位。 主要纠结点如下： 薪资： 大数据当时的薪资水平高于 Java 后台开发很多； 前景： 我个人感觉大数据岗位的发展前景很好； 个人偏见： 感觉大数据开发比 Java后台开发听着高大上点（哈哈，当时的我就是这么真实）； 不过在我分析了大部分公司的大数据岗位的要求以及自身的优势（Java后台开发的实际经验）之后还是义无反顾的只投递 Java 后台开发岗位。 先来看一下几家典型的互联网公司对大数据工程师的要求（我找的都是允许应届毕业生投递的岗位）： SHEIN 很多人可以不了解这家低调的公司，主要原因是因为 SHEIN目前的主要业务是出口跨境电商，用户基本集中在海外。SHEIN 这些年的发展非常不错，总的来说是一家值得去的公司。 SHEIN 的大数据岗位的要求写的还是比较有代代表性的！但是我觉得加上：有扎实的Java基础、熟悉多线程与JVM相关原理 这一条可能会更好！ 一家公司可能并不具有代表性，我们再来找一家公司的大数据岗位看看。 Alibaba 说明一下，阿里巴巴大的大数据开发岗位的描述其实挺友好的比如这样描述： “如果你有参与过数据处理、分析、挖掘等相关项目更好”、“如果你对Hadoop、Hive、Hbase等分布式平台有一定的理解更好”。 实际是这样吗？nonono!我信你个鬼，你个糟老头子坏的很！毕竟这么多人竞争这一个岗位，不会像描述的这么简单。 如果你对 HDFS、HBase、Hadoop 甚至是 Elasticsearch这些不了解的话，还是会很难入场。 总结一下（偏大厂）大数据岗位的对于应届生的基本要求（社招的其实也差不多，对于经验要求会更高）: 算法和数据结构是最基本的（比如手写快排、手撕红黑树）。 有扎实的Java基础、熟悉多线程与JVM相关原理。 熟练使用 Linux ，熟悉一门脚本语言 shell 或者 Python 熟悉Hadoop架构和工作原理、MapReduce编程、HDFS；熟悉Hive,最好有HQL优化经验； 熟练掌握 Spark 及 Spark Streaming开发，有实际项目研发经验更佳； 熟悉 Elasticsearch、Kafka等技术会是加分项； ...... 所以，总的来说不论是对于 Java 后台开发还是大数据开发都会要求你的数据结构和算法 Java 基础、多线程、jvm 底层这些掌握的要很好。 很多人 Java 后台的人转大数据开发很快的原因也是在这里。 正常一点的大数据面试还是比较有难度的，比如如果你写了你会 Spark 的话，他就会问题你： 什么场景下用的Spark ？解决了什么问题？ Spark 执行机制了解吗？ Spark 内存模型了解吗？ ...... 另外，如果你的简历上写了你会 Spring 这些东西的话，面试官应该也会一并提问。可以看出现在的大数据岗位没有强制性要求你有 web 开发经验,在我那一年的时候，大部分大数据开发岗位都要求你还要有 web 开发经验。 "},"zother6-JavaGuide/questions/java-learning-path-and-methods.html":{"url":"zother6-JavaGuide/questions/java-learning-path-and-methods.html","title":"Java Learning Path And Methods","keywords":"","body":"到目前为止，我觉得不管是在公众号后台、知乎还是微信上面我被问的做多的就是：“大佬，有没有 Java 学习路线和方法”。所以，这部分单独就自己的学习经历来说点自己的看法。 下面的学习路线以及方法是笔主根据个人学习经历总结改进后得出，我相信照着这条学习路线来你的学习效率会非常高。 学习某个知识点的过程中如果不知道看什么书的话，可以查看这篇文章 ：Java 学习必备书籍推荐终极版！。 另外，很重要的一点：建议使用 Intellij IDEA 进行编码，可以单独抽点时间学习 Intellij IDEA 的使用。 下面提到的一些视频，公众号后台回复关键“1”即可获取！ 先说一个初学者很容易犯的错误：上来就通过项目学习。 很多初学者上来就像通过做项目学习，特别是在公司，我觉得这个是不太可取的。 如果的 Java基础或者 Spring Boot 基础不好的话，建议自己先提前学习一下之后再开始看视频或者通过其他方式做项目。 还有点事，我不知道为什么大家都会说边跟着项目边学习做的话效果最好，我觉得这个要加一个前提是你对这门技术有基本的了解或者说你对编程有了一定的了解。 关于如何学习且听我从一个电商系统网站的创建来说起。假如我们要创建一个基于Java的分布式/微服务电商系统的话，我们可以按照下面的学习路线来做： 首选第一步我们肯定是要从 Java 基础来学习的（如果你没有计算机基础知识的话推荐看一下《计算机导论》这类入门书籍）。 step 1:Java 基础 《Java 核心技术卷 1/2》 和 《Head First Java》 这两本书在我看来都是入门 Java 的很不错的书籍 (《Java 核心技术卷 1/2》 知识点更全，我更推荐这本书)，我倒是觉得 《Java 编程思想》 有点属于新手劝退书的意思，慎看，建议有点基础后再看。你也可以边看视频边看书学习（黑马、尚硅谷、慕课网的视频都还行）。对于 Java8 新特性的东西，我建议你基础学好之后可以看一下，暂时看不太明白也没关系，后面抽时间再回过头来看。 看完之后，你可以用自己学的东西实现一个简单的 Java 程序，也可以尝试用 Java 解决一些编程问题，以此来将自己学到的东西付诸于实践。 不太建议学习 Java基础的时候通过做游戏来巩固。为什么培训班喜欢通过这种方式呢？说白点就是为了找到你的G点（不好意思开车了哈）。新手学习完Java基础后做游戏一般是不太现实的，还不如找一些简单的程序问题解决一下比如简单的算法题。 记得多总结！打好基础！把自己重要的东西都记录下来。 API 文档放在自己可以看到的地方，以备自己可以随时查阅。为了能让自己写出更优秀的代码，《Effective Java》、《重构》 这两本书没事也可以看看。 另外，学习完之后可以看一下下面这几篇文章，检查一下自己的学习情况。这几篇文章不是我吹，可能是全网最具价值的 Java 基础知识总结，毕竟是在我的 JavaGuide开源的，经过了各路大佬以及我的不断完善。 这几篇文章总结的知识点在 Java 后端面试中的出场率也非常高哦！ Java 基础知识 Java 基础知识疑难点/易错点 【加餐】一些重要的Java程序设计题 【选看】J2EE 基础知识 我们的网站需要运行在“操作系统”之上（一般是部署在Linux系统），并且我们与网站的每次交互都需要经过“网络”，需要经历三次握手和四次挥手才能简历连接，需要HTTP才能发出请求已经拿到网站后台的相应。所以第二步，我推荐可以适当花时间看一下 操作系统与计算机网络 方面的知识。 但是，不做强求！你抽时间一定要补上就行！ step 2(可选):操作系统与计算机网络 操作系统这方面我觉得掌握操作系统的基础知识和 Linux 的常用命令就行以及一些重要概念就行了。 关于操作系统的话，我没有什么操作系统方面的书籍可以推荐，因为我自己也没认真看过几本。因为操作系统比较枯燥的原因，我建议这部分看先看视频学可能会比较好一点。我推荐一个 Github 上开源的哈工大《操作系统》课程给大家吧！地址：https://github.com/hoverwinter/HIT-OSLab 。 另外，对于 Linux 我们要掌握基本的使用就需要对一些常用命令非常熟悉比如：目录切换命令、目录操作命令、文件的操作命令、压缩或者解压文件的命令等等。推荐一个 Github 上学习 Linux 的开源文档：《Java 程序员眼中的 Linux》 计算机网络方面的学习，我觉得掌握基本的知识就行了，不需要太深究，一般面试对这方面要求也不高，毕竟不是专门做网络的。推荐 《网络是怎样连接的》 、《图解 HTTP》 这两本书来看，这两本书都属于比较有趣易懂的类型，也适合没有基础的人来看。 我们写程序的都知道一个公式叫做 “程序设计 = 算法 + 数据结构”。我们想让我们的网站的地盘更加牢固的话，我觉得数据结构与算法还是很有必要学习的。所以第三步，我推荐可以适当花时间看一下 数据结构与算法 但是，同样不做强求！你抽时间一定要补上就行！ step 3(可选):数据结构与算法 如果你想进入大厂的话，我推荐你在学习完 Java基础之后，就开始每天抽出一点时间来学习算法和数据结构。为了提高自己的编程能力，你也可以坚持刷 Leetcode。就目前国内外的大厂面试来说，刷 Leetcode 可以说已经成了不得不走的一条路。 对于想要入门算法和数据结构的朋友，建议看这两本书 《算法图解》 和 《大话数据结构》，这两本书虽然算不上很经典的书籍，但是比较有趣，对于刚入门算法和数据结构的朋友非常友好。《算法导论》 非常经典，但是对于刚入门的就不那么友好了。 另外，还有一本非常赞的算法书推荐给各位，这本书的名字就叫 《算法》，书中的代码都是用 Java 语言编写。这本书的优点太多太多比如它的讲解基础而全面、对阅读者比较友好等等。我觉得这本书唯一的缺点就是太厚了 (小声 BB，可能和作者讲解某些知识点的时候有点啰嗦有关)。除了这本书之外，《剑指 offer》 、《编程珠玑》 、《编程之美》 这三本书都被很多大佬推荐过了，对于算法面试非常有帮助。《算法之美》 这本书也非常不错，非常适合闲暇的时候看。 我们网站的页面搭建需要前端的知识，我们前端也后端的交互也需要前端的知识。所以第四步，我推荐你可以了解一下前端知识，不过不需要学的太精通。自己对与前端知识有了基本的了解之后通过 step 4:前端知识 这一步主要是学习前端基础 (HTML、CSS、JavaScript),当然 BootStrap、Layui 等等比较简单的前端框架你也可以了解一下。网上有很多这方面资源，我只推荐一个大部分初学这些知识都会看的网站：http://www.w3school.com.cn/ ，这个网站用来回顾知识也很不错 。推荐先把 HTML、CSS、JS 的基础知识过一遍，然后通过一个实际的前端项目来巩固。 另外，没记错的话 Spring Boot官方推荐的是模板引擎是 thymeleaf ，这东西和HTML很像，了解了基本语法之后很容易上手。 结合layui,booystrap这些框架的话也能做成比较美观的页面。开发一些简单的页面比如一个后端项目就是为了做个简单的前端页面做某些操作的话直接用thymeleaf就好。 现在都是前后端分离，就目前来看大部分项目都优先选择 React、Angular、Vue 这些厉害的框架来开发，这些框架的上手要求要高一些。如果你想往全栈方向发展的话（笔主目前的方向，我用 React 在公司做过两个小型项目），建议先把 JS 基础打好，然后再选择 React、Angular、Vue 其中的一个来认真学习一下。国内使用 Vue 比较多一点，国外一般用的是 React 和 Angular。 如何和后端交互呢？一般使用在 React、Vue这些框架的时候使用Axios比较多。 我们网站的数据比如用户信息、订单信息都需要存储，所以，下一步我推荐你学习 MySQl这个被广泛运用于各大网站的数据库。不光要学会如何写 sql 语句，更好的是还要搞清诸如索引这类重要的概念。 step 5:MySQL 学习 MySQL 的基本使用，基本的增删改查，SQL 命令，索引、存储过程这些都学一下吧！推荐书籍 《SQL 基础教程（第 2 版）》（入门级）、《高性能 MySQL : 第 3 版》(进阶)、《MySQL 必知必会》。 下面这些 MySQL 相关的文章强烈推荐你看看： 【推荐】MySQL/数据库 知识点总结 阿里巴巴开发手册数据库部分的一些最佳实践 一千行MySQL学习笔记 MySQL高性能优化规范建议 数据库索引总结 事务隔离级别(图文详解)) 一条SQL语句在MySQL中如何执行的 正式开发之前我们还要一些准备工具比如熟悉你的ide，熟悉Maven来帮助我们引入相关jar依赖，使用 Docker来帮助我们安装常用的软件。 step 6:常用工具 非常重要！非常重要！特别是 Git和 Docker。 IDEA：熟悉基本操作以及常用快捷。 Maven ：建议学习常用框架之前可以提前花半天时间学习一下Maven的使用。（到处找 Jar 包，下载 Jar 包是真的麻烦费事，使用 Maven 可以为你省很多事情）。 Git ：基本的 Git 技能也是必备的，试着在学习的过程中将自己的代码托管在 Github 上。（Git 入门） Docker ：学着用 Docker 安装学习中需要用到的软件比如 MySQL ,这样方便很多，可以为你节省不少时间。（Docker 入门） 利用常用框架可以极大程度简化我们的开发工作。学习完了常用工具之后，我们就可以开始常用框架的学习啦！ step 7:常用框架 学习 Struts2(可不用学)、Spring、SpringMVC、Hibernate、Mybatis、shiro 等框架的使用， (可选) 熟悉 Spring 原理（大厂面试必备），然后很有必要学习一下 SpringBoot ，学好 SpringBoot 真的很重要。很多公司对于应届生都是直接上手 SpringBoot，不过如果时间允许的话，我还是推荐你把 Spring、SpringMVC 提前学一下。 关于 SpringBoot ，推荐看一下笔主开源的 Spring Boot 教程 （SpringBoot 核心知识点总结。 基于 Spring Boot 2.19+）。 Spring 真的很重要！ 一定要搞懂 AOP 和 IOC 这两个概念。Spring 中 bean 的作用域与生命周期、SpringMVC 工作原理详解等等知识点都是非常重要的，一定要搞懂。 推荐看文档+视频结合的方式，中途配合实战来学习，学习期间可以多看看 JavaGuide 对于常用框架的总结。 另外，都 2019 年了，咱千万不要再学 JSP 了好不？ 推荐阅读： Spring Spring 学习与面试 Spring 常见问题总结 Spring中 Bean 的作用域与生命周期 SpringMVC 工作原理详解 Spring中都用到了那些设计模式? SpringBoot SpringBoot 指南/常见面试题总结 MyBatis MyBatis常见面试题总结 step 8:多线程的简单使用 多线程这部分内容可能会比较难以理解和上手，前期可以先简单地了解一下基础，到了后面有精力和能力后再回来仔细看。推荐 《Java 并发编程之美》 或者 《实战 Java 高并发程序设计》 这两本书。我目前也在重构一份我之前写的多线程学习指南，后面会更新在公众号里面。 学习完多线程之后可以通过下面这些问题检测自己是否掌握。 Java 多线程知识基础: 什么是线程和进程? 请简要描述线程与进程的关系,区别及优缺点？ 说说并发与并行的区别? 为什么要使用多线程呢? 使用多线程可能带来什么问题? 说说线程的生命周期和状态? 什么是上下文切换? 什么是线程死锁?如何避免死锁? 说说 sleep() 方法和 wait() 方法区别和共同点? 为什么我们调用 start() 方法时会执行 run() 方法，为什么我们不能直接调用 run() 方法？ Java 多线程知识进阶： synchronized 关键字:① 说一说自己对于 synchronized 关键字的了解；② 说说自己是怎么使用 synchronized 关键字，在项目中用到了吗;③ 讲一下 synchronized 关键字的底层原理；④ 说说 JDK1.6 之后的 synchronized 关键字底层做了哪些优化，可以详细介绍一下这些优化吗；⑤ 谈谈 synchronized 和 ReentrantLock 的区别。 volatile 关键字： ① 讲一下 Java 内存模型；② 说说 synchronized 关键字和 volatile 关键字的区别。 ThreadLocal：① 简介；② 原理；③ 内存泄露问题。 线程池：① 为什么要用线程池？；② 实现 Runnable 接口和 Callable 接口的区别；③ 执行 execute() 方法和 submit() 方法的区别是什么呢？；④ 如何创建线程池。 Atomic 原子类: ① 介绍一下 Atomic 原子类；② JUC 包中的原子类是哪 4 类?；③ 讲讲 AtomicInteger 的使用；④ 能不能给我简单介绍一下 AtomicInteger 类的原理。 AQS ：① 简介；② 原理；③ AQS 常用组件。 step 9:分布式 学习 Dubbo、Zookeeper来实现简单的分布式服务 学习 Redis 来提高访问速度，减少对 MySQL数据库的依赖； 学习 Elasticsearch 的使用，来为我们的网站增加搜索功能 学习常见的消息队列（比如RabbitMQ、Kafka）来解耦我们的服务(ActiveMq不要学了，已经淘汰)； ...... 到了这一步你应该是有基础的一个 Java程序员了，我推荐你可以通过一个分布式项目来学习。觉得应该是掌握这些知识点比较好的一种方式了，另外，推荐边看视频边自己做，遇到不懂的知识点要及时查阅网上博客和相关书籍，这样学习效果更好。 一定要学会拓展知识，养成自主学习的意识。 黑马项目对这些知识点的介绍都比较蜻蜓点水。 继续深入学习的话，我们要了解Netty、JVM这些东西。 step 10:深入学习 可以再回来看一下多线程方面的知识，还可以利用业余时间学习一下 NIO 和 Netty ，这样简历上也可以多点东西。如果想去大厂，JVM 的一些知识也是必学的（Java 内存区域、虚拟机垃圾算法、虚拟垃圾收集器、JVM 内存管理）推荐《深入理解 Java 虚拟机：JVM 高级特性与最佳实践（最新第二版》和《实战 Java 虚拟机》，如果嫌看书麻烦的话，你也可以看我整理的文档。 另外，现在微服务特别火，很多公司在面试也明确要求需要微服务方面的知识。如果有精力的话可以去学一下 SpringCloud 生态系统微服务方面的东西。 微服务的概念庞大，技术种类也很多，但是目前大型互联网公司广泛采用的，实话实话这些东西我不在行，自己没有真实做过微服务的项目。不过下面是我自己总结的一些关于微服务比价重要的知识，选学。 step 11:微服务 这部分太多了，选择性学习。 相关技术： 网关 :kong,soul； 分布式调用链： SkyWalking、Zipkin 日志系统： Kibana ...... Spring Cloud 相关： Eureka：服务注册与发现； Ribbon：负载均衡； Hytrix ：熔断； Zuul ：网关； Spring Cloud Config：配置中心； Spring Cloud Alibaba也是很值得学习的： Sentinel ：A lightweight powerful flow control component enabling reliability and monitoring for microservices. (轻量级的流量控制、熔断降级 Java 库)。 dubbo ：Apache Dubbo 是一个基于 Java 的高性能开源 RPC 框架。 nacos ：Nacos 致力于帮助您发现、配置和管理微服务。Nacos 提供了一组简单易用的特性集，帮助您快速实现动态服务发现、服务配置、服务元数据及流量管理。Nacos 可以作为 Dubbo 的注册中心来使用。 seata : Seata 是一种易于使用，高性能，基于 Java 的开源分布式事务解决方案。 RocketMQ ：阿里巴巴开源的一款高性能、高吞吐量的分布式消息中间件。 总结 我上面主要概括一下每一步要学习的内容，对学习规划有一个建议。知道要学什么之后，如何去学呢？我觉得学习每个知识点可以考虑这样去入手： 官网（大概率是英文，不推荐初学者看）。 书籍（知识更加系统完全，推荐）。 视频（比较容易理解，推荐，特别是初学的时候。慕课网和哔哩哔哩上面有挺多学习视频可以看，只直接在上面搜索关键词就可以了）。 网上博客（解决某一知识点的问题的时候可以看看）。 这里给各位一个建议，看视频的过程中最好跟着一起练，要做笔记！！！ 最好可以边看视频边找一本书籍看，看视频没弄懂的知识点一定要尽快解决，如何解决？ 首先百度/Google，通过搜索引擎解决不了的话就找身边的朋友或者认识的一些人。另外，一定要进行项目实战！很多人这时候就会问没有实际项目让我做怎么办？我觉得可以通过下面这几种方式： 在网上找一个符合自己能力与找工作需求的实战项目视频或者博客跟着老师一起做。做的过程中，你要有自己的思考，不要浅尝辄止，对于很多知识点，别人的讲解可能只是满足项目就够了，你自己想多点知识的话，对于重要的知识点就要自己学会去往深处学。 Github 或者码云上面有很多实战类别项目，你可以选择一个来研究，为了让自己对这个项目更加理解，在理解原有代码的基础上，你可以对原有项目进行改进或者增加功能。 自己动手去做一个自己想完成的东西，遇到不会的东西就临时去学，现学现卖(这种方式比较难，初学不推荐用这种方式，因为你脑海中没有基本的概念，写出来的代码一般会很难或者根本就做不出来一个像样的东西)。 ...... 做项目不光要做，还要改进，改善。另外，如果你的老师有相关 Java 后台项目的话，你也可以主动申请参与进来。 一定要学会分配自己时间，要学的东西很多，真的很多，搞清楚哪些东西是重点，哪些东西仅仅了解就够了。一定不要把精力都花在了学各种框架上，算法和数据结构真的很重要！ 另外，学习的过程中有一个可以参考的文档很重要，非常有助于自己的学习。我当初弄 JavaGuide： https://github.com/Snailclimb/JavaGuide 的很大一部分目的就是因为这个。客观来说，相比于博客，JavaGuide 里面的内容因为更多人的参与变得更加准确和完善。 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java 面试突击》: 由本文档衍生的专为面试而生的《Java 面试突击》V2.0 PDF 版本公众号后台回复 \"Java 面试突击\" 即可免费领取！ Java 工程师必备学习资源: 一些 Java 工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/questions/java-learning-website-blog.html":{"url":"zother6-JavaGuide/questions/java-learning-website-blog.html","title":"Java Learning Website Blog","keywords":"","body":"推荐两个视频学习网站 慕课网 第一个推荐的学习网站应该是慕课网（慕课网私聊我打钱哈！），在我初学的时候，这个网站对我的帮助挺大的，里面有很多免费的课程，也有很多付费的课程。如果你没有特殊的需求，一般免费课程就够自己学的了。 哔哩哔哩 想不到弹幕追番/原创视频小站也被推荐了吧！不得不说哔哩哔哩上面的学习资源还是很多的，现在有很多年轻人都在上面学习呢！哈哈哈 大部分年轻人最爱的小破站可是受到过央视表扬的。被誉为年轻人学习的首要阵地，哔哩哔哩干杯！ 不过在哔哩哔哩上面越靠前的视频就是最好的视频或者说最适合你的视频，也是要筛选一下的。 极客时间 主打付费学习的一个付费学习社区（极客时间私聊我打钱哈！）。不过课程的质量大部分都挺高的，我自己也看了里面很多的课程，并且很多课程都是 Java 领域大佬级别的人物将的。 推荐一些文字类型学习网站/博客 Github 最牛逼的程序员交流网站！！！没有之一。一定要多逛逛！上面有很多好东西，比如我搜索 Java（它竟然给我返回贼多 javascript 的项目，啥意思？？？） 比如我搜索女装，emm....然后就出来了这些东西，捂住眼睛，不敢看！ 菜鸟教程 对于新手入门来说很不错的网站，大部分教程都是针对的入门级别。优点是网站教程内容比较完善并且内容质量也是有保障的。 w3cschool 和菜鸟教程类似的一个网站，里面的教程也很齐全。 Stackoverflow Stack Overflow是一个程序设计领域的问答网站，网站允许注册用户提出或回答问题。和知乎很像，重大的一点不同是 Stack Overflow 可以对问题进行打分。 leetcode 网站地址：https://leetcode-cn.com/ 工作之余没事去刷个算法题，岂不是美滋滋。 一些不错的技术交流社区推荐 掘金：https://juejin.im/ 。 segmentfault ： https://segmentfault.com/ 博客园 ： https://www.cnblogs.com/ 慕课网手记 ：https://www.imooc.com/article 知乎 ：https://www.zhihu.com/ 一些不错的博客/Github 推荐 SnailClimb 的 Github ：https://github.com/Snailclimb 。（自荐一波哈！主要专注在 Java 基础和进阶、Spring、Spiring Boot、Java 面试这方面。） 徐靖峰个人博客 ：https://www.cnkirito.moe/（探讨 Java 生态的知识点，内容覆盖分布式服务治理、微服务、性能调优、各类源码分析） 田小波：http://www.tianxiaobo.com/ （Java 、Spring 、MyBatis 、Dubbo） 周立的博客： http://www.itmuch.com/（Spring Cloud、Docker、Kubernetes，及其相关生态的技术） Hollis: https://www.hollischuang.com/ (Java 后端) 方志朋的专栏 ： https://www.fangzhipeng.com/ （Java 面试 Java 并发 openresty kubernetes Docker 故事 ) 纯洁的微笑 : http://www.ityouknow.com/ （Java、SpringBoot、Spring Cloud） 芋道源码： http://www.iocoder.cn/ (专注源码)。 欢迎自荐 ...... "},"zother6-JavaGuide/questions/java-training-4-month.html":{"url":"zother6-JavaGuide/questions/java-training-4-month.html","title":"Java Training 4 Month","keywords":"","body":"问题描述： 最近在北京华软科技公司看到一个招聘，去咨询了人事部，他说培训四个月就能上岗，并且不要学费，上岗后再每还1000元，还一年，这个可靠吗？本人高中毕业，四个月能学会吗？谢谢了！！！ 下面是正文： 一般说不要学费，上岗后每月再还1000元这种十有八九都不靠谱，就算你把合同看的再仔细，别人也总有各种办法去刁难你。 另外，目前的互联网行业已经完全不是它刚开始盛行的样子了。在互联网爆火🔥的初期，你可能会简单用一下语言就能找到一个不错的工作。那时候，即使是没有学历支撑直接从培训班出来的基本也都找到了还算是不错的工作。但是，现在已经完全不一样了。我觉得主要可以从以下几个方面讲： 没有学历支撑，直接从培训班出来的找工作会很难，甚至找不到； 面试的难度可以说一年比一年难，学的人越来越多，和你竞争的也越来越多，特别是像面试阿里、腾讯、字节跳动这样的大厂，你可能要和更多人去竞争。“面试造火箭，入职拎螺丝”想想也是正常，毕竟这么多人去竞争那少数的 offer，如果不难点的话，区分度就没那么明显了； 学习计算机专业的越来越多，和你竞争的也越来越多，需求就那么一些，人多了之后，平均工资水平以后应该不会和其他行业差别这么大。但是，我个人感觉技术厉害的还是会很吃香。只是，普通的程序员的工资可能比不上前几年了。 养成一个学习习惯和编程习惯真的太重要了，一个好习惯的养成真的对后面的学习有很大帮助。 说实话我自己当初在这方面吃了不少亏，很多比较好的习惯我也是后面自己才慢慢发现，所以这里想着重给大家说一下有哪些好的学习和编程习惯。另外，不要在意自己会多少框架，真的没有一点用！ 下面是一些我觉得还不错的编程好习惯，希望对大家有帮助。 编程好习惯推荐 下面这些我都总结在了 Github 上，更多内容可以通过这个链接查看： https://github.com/Snailclimb/programmer-advancement 。 正确提问 我们平时任何时候都离不开提问特别是初学的时候，但是真正知道如何正确的提问的人很少。问别人问题前不要来一句“在吗”，你说你问了在吗我是回复好还是不回复好呢 ？不要让别人给你发 32 位的JDK，除非你是喜欢那个人。 更多关于如何提问的内容，详见 github 上开源版『提问的智慧』 https://github.com/ryanhanwu/How-To-Ask-Questions-The-Smart-Way/blob/master/README-zh_CN.md，抽时间看一下，我想看完之后应该会有很多收获。 更多内容可以查看我的这篇原创文章：如何提问 健康生活 我一直觉得这一方面是最重要的，我想很多人和我一样会无意识间忽略它，等到真的身体不舒服了，你才开始意识到健康生活的重要性。 除非万不得已，不要熬夜了。熬夜的危害就不用多说了，秃头加内分泌失调，你懂得！ 看电脑45分钟之后，起来走5分钟，看看远方放松一下。不要觉得这5分钟浪费时间，相反，这5分钟可能为你带来更大的效率提升。 可以考虑买一个电脑架子，保护好自己脊椎的同时，办公体验也会提升很多。 可以下载一个护眼宝，感觉可以护眼模式挺棒的，非常适合我们这种需要经常盯着电脑的人使用，强烈安利。 高效搜索 尽量用 google 查找技术资料以及自己在学习中遇到的一些问题。 解决 bug 程序遇到问题先在 stackoverflow 找找，大部分别人已经遇到过了。如果上面没有的话，再考虑其他解决办法。实在解决不了的话，再去问你觉得有能力帮你解决的人（注意描述好自己的问题，不要随便截一个Bug 图）。 善于总结 学习完任何一门知识后，你可能当时看视频感觉老师讲的挺容易懂的。但是，过几天后你发现你忘的一干二净，别人问你一个类似的问题，你一点思路都没有。所以，我推荐你学完一门知识后不光要及时复习，还要做好总结，让知识形成一个体系。另外，你可以假想自己要给别人讲这个知识点，你能不能把这个知识点讲清楚呢？如果不能，说明你对这个知识点还没有彻底了解。这也就是人们经常说的费曼学习技巧。 总结的方式： 有道云笔记、OneNote......这类专门用来记录笔记的软件上； 思维导图； 通过写博客输出。可以考虑自己搭建一个博客(hexo+GithubPages非常简单)，你也可以在简书、掘金......等等技术交流社区写博客。Markdown 格式参考：https://github.com/sparanoid/chinese-copywriting-guidelines 中文文案排版指北：https://github.com/sparanoid/chinese-copywriting-guidelines 写博客 写博客有哪些好处： 对知识有更加深的认识，让自己的知识体系更加完整; 督促自己学习; 可能会带来不错的经济收入; 提升个人影响力; 拥有更多机会; ...... 总的来说，写博客是一件利己利彼的事情。你可能会从中收获到很多东西，你写的东西也可能对别人也有很大的帮助。但是，写博客还是比较耗费自己时间的，你需要和工作做好权衡。 分享是一种美德，任何行业都不是靠单打独斗的，写博客、写好博客是一个程序员很好的习惯。我为人人，人人为我！ 更多内容可以查看我的这篇原创文章：我为什么推荐你写博客? 多用 Github 没事多去Github转转，如果有能力可以参与到一些开源项目中。多看看别人开源的优秀项目，看看别人的代码和设计思路，看的多了，你的编程思想也会慢慢得到提升。除了这些优秀的开源项目之外，Github上面还有很多不错的开源文档、开源资料什么的，我觉得对我们平时学习都挺有帮助。Github用得好还能装一下，毕竟人家还是一个全英文网站，咳咳咳。 实践 多去实践，将学到的东西运用到实际项目中去。很多人都找我抱怨过没有实际项目让自己去做，怎么能有项目经验呢？如果实在没有实际项目让你去做，我觉得你可以通过下面几种方式： 在网上找一个符合自己能力与找工作需求的实战项目视频或者博客跟着老师一起做。做的过程中，你要有自己的思考，不要浅尝辄止，对于很多知识点，别人的讲解可能只是满足项目就够了，你自己想多点知识的话，对于重要的知识点就要自己学会去往深出学。 Github或者码云上面有很多实战类别项目，你可以选择一个来研究，为了让自己对这个项目更加理解，在理解原有代码的基础上，你可以对原有项目进行改进或者增加功能。 自己动手去做一个自己想完成的东西，遇到不会的东西就临时去学，现学现卖。 注意代码规范 从学习编程的第一天起就要养成不错的编码习惯，包、类、方法的命名这些是最基本的。 推荐阅读： 阿里巴巴Java开发手册（详尽版）https://github.com/alibaba/p3c/blob/master/阿里巴巴Java开发手册（详尽版）.pdf Google Java编程风格指南：http://www.hawstein.com/posts/google-java-style.html Effective Java第三版中文版: https://legacy.gitbook.com/book/jiapengcai/effective-java 沟通能力 程序员也离不开沟通。你可能需要与客户交流需求，还要和同事交流项目问题，还有可能定期需要向领导汇报项目进展情况。所以，我觉得不错的沟通能力也是一个优秀的程序员应该有的基本素质。 学习方法和学习路线推荐 推荐查看我的这篇文章《可能是最适合你的Java学习方法和路线推荐》，文中提到的学习路线以及方法是笔主根据个人学习经历总结改进后得出，我相信照着这条学习路线来你的学习效率会非常高。 "},"zother6-JavaGuide/system-design/authority-certification/basis-of-authority-certification.html":{"url":"zother6-JavaGuide/system-design/authority-certification/basis-of-authority-certification.html","title":"Basis Of Authority Certification","keywords":"","body":"1. 认证 (Authentication) 和授权 (Authorization)的区别是什么？ 这是一个绝大多数人都会混淆的问题。首先先从读音上来认识这两个名词，很多人都会把它俩的读音搞混，所以我建议你先先去查一查这两个单词到底该怎么读，他们的具体含义是什么。 说简单点就是： 认证 (Authentication)： 你是谁。 授权 (Authorization)： 你有权限干什么。 稍微正式点（啰嗦点）的说法就是： Authentication（认证） 是验证您的身份的凭据（例如用户名/用户ID和密码），通过这个凭据，系统得以知道你就是你，也就是说系统存在你这个用户。所以，Authentication 被称为身份/用户验证。 Authorization（授权） 发生在 Authentication（认证） 之后。授权嘛，光看意思大家应该就明白，它主要掌管我们访问系统的权限。比如有些特定资源只能具有特定权限的人才能访问比如admin，有些对系统资源操作比如删除、添加、更新只能特定人才具有。 这两个一般在我们的系统中被结合在一起使用，目的就是为了保护我们系统的安全性。 2. 什么是Cookie ? Cookie的作用是什么?如何在服务端使用 Cookie ? 2.1 什么是Cookie ? Cookie的作用是什么? Cookie 和 Session都是用来跟踪浏览器用户身份的会话方式，但是两者的应用场景不太一样。 维基百科是这样定义 Cookie 的：Cookies是某些网站为了辨别用户身份而储存在用户本地终端上的数据（通常经过加密）。简单来说： Cookie 存放在客户端，一般用来保存用户信息。 下面是 Cookie 的一些应用案例： 我们在 Cookie 中保存已经登录过的用户信息，下次访问网站的时候页面可以自动帮你登录的一些基本信息给填了。除此之外，Cookie 还能保存用户首选项，主题和其他设置信息。 使用Cookie 保存 session 或者 token ，向后端发送请求的时候带上 Cookie，这样后端就能取到session或者token了。这样就能记录用户当前的状态了，因为 HTTP 协议是无状态的。 Cookie 还可以用来记录和分析用户行为。举个简单的例子你在网上购物的时候，因为HTTP协议是没有状态的，如果服务器想要获取你在某个页面的停留状态或者看了哪些商品，一种常用的实现方式就是将这些信息存放在Cookie 2.2 如何在服务端使用 Cookie 呢？ 这部分内容参考：https://attacomsian.com/blog/cookies-spring-boot，更多如何在Spring Boot中使用Cookie 的内容可以查看这篇文章。 1)设置cookie返回给客户端 @GetMapping(\"/change-username\") public String setCookie(HttpServletResponse response) { // 创建一个 cookie Cookie cookie = new Cookie(\"username\", \"Jovan\"); //设置 cookie过期时间 cookie.setMaxAge(7 * 24 * 60 * 60); // expires in 7 days //添加到 response 中 response.addCookie(cookie); return \"Username is changed!\"; } 2) 使用Spring框架提供的@CookieValue注解获取特定的 cookie的值 @GetMapping(\"/\") public String readCookie(@CookieValue(value = \"username\", defaultValue = \"Atta\") String username) { return \"Hey! My username is \" + username; } 3) 读取所有的 Cookie 值 @GetMapping(\"/all-cookies\") public String readAllCookies(HttpServletRequest request) { Cookie[] cookies = request.getCookies(); if (cookies != null) { return Arrays.stream(cookies) .map(c -> c.getName() + \"=\" + c.getValue()).collect(Collectors.joining(\", \")); } return \"No cookies\"; } 3. Cookie 和 Session 有什么区别？如何使用Session进行身份验证？ Session 的主要作用就是通过服务端记录用户的状态。 典型的场景是购物车，当你要添加商品到购物车的时候，系统不知道是哪个用户操作的，因为 HTTP 协议是无状态的。服务端给特定的用户创建特定的 Session 之后就可以标识这个用户并且跟踪这个用户了。 Cookie 数据保存在客户端(浏览器端)，Session 数据保存在服务器端。相对来说 Session 安全性更高。如果使用 Cookie 的一些敏感信息不要写入 Cookie 中，最好能将 Cookie 信息加密然后使用到的时候再去服务器端解密。 那么，如何使用Session进行身份验证？ 很多时候我们都是通过 SessionID 来实现特定的用户，SessionID 一般会选择存放在 Redis 中。举个例子：用户成功登陆系统，然后返回给客户端具有 SessionID 的 Cookie，当用户向后端发起请求的时候会把 SessionID 带上，这样后端就知道你的身份状态了。关于这种认证方式更详细的过程如下： 用户向服务器发送用户名和密码用于登陆系统。 服务器验证通过后，服务器为用户创建一个 Session，并将 Session信息存储 起来。 服务器向用户返回一个 SessionID，写入用户的 Cookie。 当用户保持登录状态时，Cookie 将与每个后续请求一起被发送出去。 服务器可以将存储在 Cookie 上的 Session ID 与存储在内存中或者数据库中的 Session 信息进行比较，以验证用户的身份，返回给用户客户端响应信息的时候会附带用户当前的状态。 使用 Session 的时候需要注意下面几个点： 依赖Session的关键业务一定要确保客户端开启了Cookie。 注意Session的过期时间 花了个图简单总结了一下Session认证涉及的一些东西。 另外，Spring Session提供了一种跨多个应用程序或实例管理用户会话信息的机制。如果想详细了解可以查看下面几篇很不错的文章： Getting Started with Spring Session Guide to Spring Session Sticky Sessions with Spring Session & Redis 4.如果没有Cookie的话Session还能用吗？ 这是一道经典的面试题！ 一般是通过 Cookie 来保存 SessionID ，假如你使用了 Cookie 保存 SessionID的方案的话， 如果客户端禁用了Cookie，那么Seesion就无法正常工作。 但是，并不是没有 Cookie 之后就不能用 Session 了，比如你可以将SessionID放在请求的 url 里面https://javaguide.cn/?session_id=xxx 。这种方案的话可行，但是安全性和用户体验感降低。当然，为了你也可以对 SessionID 进行一次加密之后再传入后端。 5.为什么Cookie 无法防止CSRF攻击，而token可以？ CSRF（Cross Site Request Forgery）一般被翻译为 跨站请求伪造 。那么什么是 跨站请求伪造 呢？说简单用你的身份去发送一些对你不友好的请求。举个简单的例子： 小壮登录了某网上银行，他来到了网上银行的帖子区，看到一个帖子下面有一个链接写着“科学理财，年盈利率过万”，小壮好奇的点开了这个链接，结果发现自己的账户少了10000元。这是这么回事呢？原来黑客在链接中藏了一个请求，这个请求直接利用小壮的身份给银行发送了一个转账请求,也就是通过你的 Cookie 向银行发出请求。 科学理财，年盈利率过万 上面也提到过，进行Session 认证的时候，我们一般使用 Cookie 来存储 SessionId,当我们登陆后后端生成一个SessionId放在Cookie中返回给客户端，服务端通过Redis或者其他存储工具记录保存着这个Sessionid，客户端登录以后每次请求都会带上这个SessionId，服务端通过这个SessionId来标示你这个人。如果别人通过 cookie拿到了 SessionId 后就可以代替你的身份访问系统了。 Session 认证中 Cookie 中的 SessionId是由浏览器发送到服务端的，借助这个特性，攻击者就可以通过让用户误点攻击链接，达到攻击效果。 但是，我们使用 token 的话就不会存在这个问题，在我们登录成功获得 token 之后，一般会选择存放在 local storage 中。然后我们在前端通过某些方式会给每个发到后端的请求加上这个 token,这样就不会出现 CSRF 漏洞的问题。因为，即使有个你点击了非法链接发送了请求到服务端，这个非法请求是不会携带 token 的，所以这个请求将是非法的。 需要注意的是不论是 Cookie 还是 token 都无法避免跨站脚本攻击（Cross Site Scripting）XSS。 跨站脚本攻击（Cross Site Scripting）缩写为 CSS 但这会与层叠样式表（Cascading Style Sheets，CSS）的缩写混淆。因此，有人将跨站脚本攻击缩写为XSS。 XSS中攻击者会用各种方式将恶意代码注入到其他用户的页面中。就可以通过脚本盗用信息比如cookie。 推荐阅读： 如何防止CSRF攻击？—美团技术团队 6. 什么是 Token?什么是 JWT?如何基于Token进行身份验证？ 我们在上一个问题中探讨了使用 Session 来鉴别用户的身份，并且给出了几个 Spring Session 的案例分享。 我们知道 Session 信息需要保存一份在服务器端。这种方式会带来一些麻烦，比如需要我们保证保存 Session 信息服务器的可用性、不适合移动端（依赖Cookie）等等。 有没有一种不需要自己存放 Session 信息就能实现身份验证的方式呢？使用 Token 即可！JWT （JSON Web Token） 就是这种方式的实现，通过这种方式服务器端就不需要保存 Session 数据了，只用在客户端保存服务端返回给客户的 Token 就可以了，扩展性得到提升。 JWT 本质上就一段签名的 JSON 格式的数据。由于它是带有签名的，因此接收者便可以验证它的真实性。 下面是 RFC 7519 对 JWT 做的较为正式的定义。 JSON Web Token (JWT) is a compact, URL-safe means of representing claims to be transferred between two parties. The claims in a JWT are encoded as a JSON object that is used as the payload of a JSON Web Signature (JWS) structure or as the plaintext of a JSON Web Encryption (JWE) structure, enabling the claims to be digitally signed or integrity protected with a Message Authentication Code (MAC) and/or encrypted. ——JSON Web Token (JWT) JWT 由 3 部分构成: Header :描述 JWT 的元数据。定义了生成签名的算法以及 Token 的类型。 Payload（负载）:用来存放实际需要传递的数据 Signature（签名）：服务器通过Payload、Header和一个密钥(secret)使用 Header 里面指定的签名算法（默认是 HMAC SHA256）生成。 在基于 Token 进行身份验证的的应用程序中，服务器通过Payload、Header和一个密钥(secret)创建令牌（Token）并将 Token 发送给客户端，客户端将 Token 保存在 Cookie 或者 localStorage 里面，以后客户端发出的所有请求都会携带这个令牌。你可以把它放在 Cookie 里面自动发送，但是这样不能跨域，所以更好的做法是放在 HTTP Header 的 Authorization字段中：Authorization: Bearer Token。 用户向服务器发送用户名和密码用于登陆系统。 身份验证服务响应并返回了签名的 JWT，上面包含了用户是谁的内容。 用户以后每次向后端发请求都在Header中带上 JWT。 服务端检查 JWT 并从中获取用户相关信息。 推荐阅读： JWT (JSON Web Tokens) Are Better Than Session Cookies JSON Web Tokens (JWT) 与 Sessions JSON Web Token 入门教程 彻底理解Cookie，Session，Token 7 什么是OAuth 2.0？ OAuth 是一个行业的标准授权协议，主要用来授权第三方应用获取有限的权限。而 OAuth 2.0是对 OAuth 1.0 的完全重新设计，OAuth 2.0更快，更容易实现，OAuth 1.0 已经被废弃。详情请见：rfc6749。 实际上它就是一种授权机制，它的最终目的是为第三方应用颁发一个有时效性的令牌 token，使得第三方应用能够通过该令牌获取相关的资源。 OAuth 2.0 比较常用的场景就是第三方登录，当你的网站接入了第三方登录的时候一般就是使用的 OAuth 2.0 协议。 另外，现在OAuth 2.0也常见于支付场景（微信支付、支付宝支付）和开发平台（微信开放平台、阿里开放平台等等）。 微信支付账户相关参数： 推荐阅读： OAuth 2.0 的一个简单解释 10 分钟理解什么是 OAuth 2.0 协议 OAuth 2.0 的四种方式 GitHub OAuth 第三方登录示例教程 8 什么是 SSO? SSO(Single Sign On)即单点登录说的是用户登陆多个子系统的其中一个就有权访问与其相关的其他系统。举个例子我们在登陆了京东金融之后，我们同时也成功登陆京东的京东超市、京东家电等子系统。 9.SSO与OAuth2.0的区别 OAuth 是一个行业的标准授权协议，主要用来授权第三方应用获取有限的权限。SSO解决的是一个公司的多个相关的自系统的之间的登陆问题比如京东旗下相关子系统京东金融、京东超市、京东家电等等。 参考 https://medium.com/@sherryhsu/session-vs-token-based-authentication-11a6c5ac45e4 https://www.varonis.com/blog/what-is-oauth/ https://tools.ietf.org/html/rfc6749 "},"zother6-JavaGuide/system-design/authority-certification/JWT-advantages-and-disadvantages.html":{"url":"zother6-JavaGuide/system-design/authority-certification/JWT-advantages-and-disadvantages.html","title":"JWT Advantages And Disadvantages","keywords":"","body":"JWT 身份认证优缺点分析以及常见问题解决方案 之前分享了一个使用 Spring Security 实现 JWT 身份认证的 Demo，文章地址：适合初学者入门 Spring Security With JWT 的 Demo。 Demo 非常简单，没有介绍到 JWT 存在的一些问题。所以，单独抽了一篇文章出来介绍。为了完成这篇文章，我查阅了很多资料和文献，我觉得应该对大家有帮助。 相关阅读： 《一问带你区分清楚Authentication,Authorization以及Cookie、Session、Token》 适合初学者入门 Spring Security With JWT 的 Demo Spring Boot 使用 JWT 进行身份和权限验证 Token 认证的优势 相比于 Session 认证的方式来说，使用 token 进行身份认证主要有下面三个优势： 1.无状态 token 自身包含了身份验证所需要的所有信息，使得我们的服务器不需要存储 Session 信息，这显然增加了系统的可用性和伸缩性，大大减轻了服务端的压力。但是，也正是由于 token 的无状态，也导致了它最大的缺点：当后端在token 有效期内废弃一个 token 或者更改它的权限的话，不会立即生效，一般需要等到有效期过后才可以。另外，当用户 Logout 的话，token 也还有效。除非，我们在后端增加额外的处理逻辑。 2.有效避免了CSRF 攻击 CSRF（Cross Site Request Forgery）一般被翻译为 跨站请求伪造，属于网络攻击领域范围。相比于 SQL 脚本注入、XSS等等安全攻击方式，CSRF 的知名度并没有它们高。但是,它的确是每个系统都要考虑的安全隐患，就连技术帝国 Google 的 Gmail 在早些年也被曝出过存在 CSRF 漏洞，这给 Gmail 的用户造成了很大的损失。 那么究竟什么是 跨站请求伪造 呢？说简单用你的身份去发送一些对你不友好的请求。举个简单的例子： 小壮登录了某网上银行，他来到了网上银行的帖子区，看到一个帖子下面有一个链接写着“科学理财，年盈利率过万”，小壮好奇的点开了这个链接，结果发现自己的账户少了10000元。这是这么回事呢？原来黑客在链接中藏了一个请求，这个请求直接利用小壮的身份给银行发送了一个转账请求,也就是通过你的 Cookie 向银行发出请求。 科学理财，年盈利率过万 导致这个问题很大的原因就是： Session 认证中 Cookie 中的 session_id 是由浏览器发送到服务端的，借助这个特性，攻击者就可以通过让用户误点攻击链接，达到攻击效果。 那为什么 token 不会存在这种问题呢？ 我是这样理解的：一般情况下我们使用 JWT 的话，在我们登录成功获得 token 之后，一般会选择存放在 local storage 中。然后我们在前端通过某些方式会给每个发到后端的请求加上这个 token,这样就不会出现 CSRF 漏洞的问题。因为，即使有个你点击了非法链接发送了请求到服务端，这个非法请求是不会携带 token 的，所以这个请求将是非法的。 但是这样会存在 XSS 攻击中被盗的风险，为了避免 XSS 攻击，你可以选择将 token 存储在标记为httpOnly 的cookie 中。但是，这样又导致了你必须自己提供CSRF保护。 具体采用上面哪两种方式存储 token 呢，大部分情况下存放在 local storage 下都是最好的选择，某些情况下可能需要存放在标记为httpOnly 的cookie 中会更好。 3.适合移动端应用 使用 Session 进行身份认证的话，需要保存一份信息在服务器端，而且这种方式会依赖到 Cookie（需要 Cookie 保存 SessionId），所以不适合移动端。 但是，使用 token 进行身份认证就不会存在这种问题，因为只要 token 可以被客户端存储就能够使用，而且 token 还可以跨语言使用。 4.单点登录友好 使用 Session 进行身份认证的话，实现单点登录，需要我们把用户的 Session 信息保存在一台电脑上，并且还会遇到常见的 Cookie 跨域的问题。但是，使用 token 进行认证的话， token 被保存在客户端，不会存在这些问题。 Token 认证常见问题以及解决办法 1.注销登录等场景下 token 还有效 与之类似的具体相关场景有： 退出登录; 修改密码; 服务端修改了某个用户具有的权限或者角色； 用户的帐户被删除/暂停。 用户由管理员注销； 这个问题不存在于 Session 认证方式中，因为在 Session 认证方式中，遇到这种情况的话服务端删除对应的 Session 记录即可。但是，使用 token 认证的方式就不好解决了。我们也说过了，token 一旦派发出去，如果后端不增加其他逻辑的话，它在失效之前都是有效的。那么，我们如何解决这个问题呢？查阅了很多资料，总结了下面几种方案： 将 token 存入内存数据库：将 token 存入 DB 中，redis 内存数据库在这里是是不错的选择。如果需要让某个 token 失效就直接从 redis 中删除这个 token 即可。但是，这样会导致每次使用 token 发送请求都要先从 DB 中查询 token 是否存在的步骤，而且违背了 JWT 的无状态原则。 黑名单机制：和上面的方式类似，使用内存数据库比如 redis 维护一个黑名单，如果想让某个 token 失效的话就直接将这个 token 加入到 黑名单 即可。然后，每次使用 token 进行请求的话都会先判断这个 token 是否存在于黑名单中。 修改密钥 (Secret) : 我们为每个用户都创建一个专属密钥，如果我们想让某个 token 失效，我们直接修改对应用户的密钥即可。但是，这样相比于前两种引入内存数据库带来了危害更大，比如：1⃣️如果服务是分布式的，则每次发出新的 token 时都必须在多台机器同步密钥。为此，你需要将必须将机密存储在数据库或其他外部服务中，这样和 Session 认证就没太大区别了。2⃣️如果用户同时在两个浏览器打开系统，或者在手机端也打开了系统，如果它从一个地方将账号退出，那么其他地方都要重新进行登录，这是不可取的。 保持令牌的有效期限短并经常轮换 ：很简单的一种方式。但是，会导致用户登录状态不会被持久记录，而且需要用户经常登录。 对于修改密码后 token 还有效问题的解决还是比较容易的，说一种我觉得比较好的方式：使用用户的密码的哈希值对 token 进行签名。因此，如果密码更改，则任何先前的令牌将自动无法验证。 2.token 的续签问题 token 有效期一般都建议设置的不太长，那么 token 过期后如何认证，如何实现动态刷新 token，避免用户经常需要重新登录？ 我们先来看看在 Session 认证中一般的做法：假如 session 的有效期30分钟，如果 30 分钟内用户有访问，就把 session 有效期被延长30分钟。 类似于 Session 认证中的做法：这种方案满足于大部分场景。假设服务端给的 token 有效期设置为30分钟，服务端每次进行校验时，如果发现 token 的有效期马上快过期了，服务端就重新生成 token 给客户端。客户端每次请求都检查新旧token，如果不一致，则更新本地的token。这种做法的问题是仅仅在快过期的时候请求才会更新 token ,对客户端不是很友好。 每次请求都返回新 token :这种方案的的思路很简单，但是，很明显，开销会比较大。 token 有效期设置到半夜 ：这种方案是一种折衷的方案，保证了大部分用户白天可以正常登录，适用于对安全性要求不高的系统。 用户登录返回两个 token ：第一个是 acessToken ，它的过期时间 token 本身的过期时间比如半个小时，另外一个是 refreshToken 它的过期时间更长一点比如为1天。客户端登录后，将 accessToken和refreshToken 保存在本地，每次访问将 accessToken 传给服务端。服务端校验 accessToken 的有效性，如果过期的话，就将 refreshToken 传给服务端。如果有效，服务端就生成新的 accessToken 给客户端。否则，客户端就重新登录即可。该方案的不足是：1⃣️需要客户端来配合；2⃣️用户注销的时候需要同时保证两个 token 都无效；3⃣️重新请求获取 token 的过程中会有短暂 token 不可用的情况（可以通过在客户端设置定时器，当accessToken 快过期的时候，提前去通过 refreshToken 获取新的accessToken）。 总结 JWT 最适合的场景是不需要服务端保存用户状态的场景，比如如果考虑到 token 注销和 token 续签的场景话，没有特别好的解决方案，大部分解决方案都给 token 加上了状态，这就有点类似 Session 认证了。 Reference JWT 超详细分析 https://medium.com/devgorilla/how-to-log-out-when-using-jwt-a8c7823e8a6 https://medium.com/@agungsantoso/csrf-protection-with-json-web-tokens-83e0f2fcbcc Invalidating JSON Web Tokens "},"zother6-JavaGuide/system-design/authority-certification/sso.html":{"url":"zother6-JavaGuide/system-design/authority-certification/sso.html","title":"Sso","keywords":"","body":" 本文授权转载自 ： https://ken.io/note/sso-design-implement 作者：ken.io 相关推荐阅读：系统的讲解 - SSO单点登录 一、前言 1、SSO说明 SSO英文全称Single Sign On，单点登录。SSO是在多个应用系统中，用户只需要登录一次就可以访问所有相互信任的应用系统。https://baike.baidu.com/item/SSO/3451380 例如访问在网易账号中心（http://reg.163.com/ ）登录之后 访问以下站点都是登录状态 网易直播 http://v.163.com 网易博客 http://blog.163.com 网易花田 http://love.163.com 网易考拉 https://www.kaola.com 网易Lofter http://www.lofter.com 2、单点登录系统的好处 用户角度 :用户能够做到一次登录多次使用，无需记录多套用户名和密码，省心。 系统管理员角度 : 管理员只需维护好一个统一的账号中心就可以了，方便。 新系统开发角度: 新系统开发时只需直接对接统一的账号中心即可，简化开发流程，省时。 3、设计目标 本篇文章也主要是为了探讨如何设计&实现一个SSO系统 以下为需要实现的核心功能： 单点登录 单点登出 支持跨域单点登录 支持跨域单点登出 二、SSO设计与实现 1、核心应用与依赖 应用/模块/对象 说明 前台站点 需要登录的站点 SSO站点-登录 提供登录的页面 SSO站点-登出 提供注销登录的入口 SSO服务-登录 提供登录服务 SSO服务-登录状态 提供登录状态校验/登录信息查询的服务 SSO服务-登出 提供用户注销登录的服务 数据库 存储用户账户信息 缓存 存储用户的登录信息，通常使用Redis 2、用户登录状态的存储与校验 常见的Web框架对于Session的实现都是生成一个SessionId存储在浏览器Cookie中。然后将Session内容存储在服务器端内存中，这个 ken.io 在之前Session工作原理中也提到过。整体也是借鉴这个思路。 用户登录成功之后，生成AuthToken交给客户端保存。如果是浏览器，就保存在Cookie中。如果是手机App就保存在App本地缓存中。本篇主要探讨基于Web站点的SSO。 用户在浏览需要登录的页面时，客户端将AuthToken提交给SSO服务校验登录状态/获取用户登录信息 对于登录信息的存储，建议采用Redis，使用Redis集群来存储登录信息，既可以保证高可用，又可以线性扩充。同时也可以让SSO服务满足负载均衡/可伸缩的需求。 对象 说明 AuthToken 直接使用UUID/GUID即可，如果有验证AuthToken合法性需求，可以将UserName+时间戳加密生成，服务端解密之后验证合法性 登录信息 通常是将UserId，UserName缓存起来 3、用户登录/登录校验 登录时序图 按照上图，用户登录后Authtoken保存在Cookie中。 domian= test. com 浏览器会将domain设置成 .test.com， 这样访问所有*.test.com的web站点，都会将Authtoken携带到服务器端。 然后通过SSO服务，完成对用户状态的校验/用户登录信息的获取 登录信息获取/登录状态校验 4、用户登出 用户登出时要做的事情很简单： 服务端清除缓存（Redis）中的登录状态 客户端清除存储的AuthToken 登出时序图 5、跨域登录、登出 前面提到过，核心思路是客户端存储AuthToken，服务器端通过Redis存储登录信息。由于客户端是将AuthToken存储在Cookie中的。所以跨域要解决的问题，就是如何解决Cookie的跨域读写问题。 Cookie是不能跨域的 ，比如我一个 解决跨域的核心思路就是： 登录完成之后通过回调的方式，将AuthToken传递给主域名之外的站点，该站点自行将AuthToken保存在当前域下的Cookie中。 登出完成之后通过回调的方式，调用非主域名站点的登出页面，完成设置Cookie中的AuthToken过期的操作。 跨域登录（主域名已登录） 跨域登录（主域名未登录） 跨域登出 三、备注 关于方案 这次设计方案更多是提供实现思路。如果涉及到APP用户登录等情况，在访问SSO服务时，增加对APP的签名验证就好了。当然，如果有无线网关，验证签名不是问题。 关于时序图 时序图中并没有包含所有场景，ken.io只列举了核心/主要场景，另外对于一些不影响理解思路的消息能省就省了。 "},"zother6-JavaGuide/system-design/data-communication/dubbo.html":{"url":"zother6-JavaGuide/system-design/data-communication/dubbo.html","title":"Dubbo","keywords":"","body":"本文是作者根据官方文档以及自己平时的使用情况，对 Dubbo 所做的一个总结。如果不懂 Dubbo 的使用的话，可以参考我的这篇文章《超详细，新手都能看懂 ！使用SpringBoot+Dubbo 搭建一个简单的分布式服务》 Dubbo 官网：http://dubbo.apache.org/zh-cn/index.html Dubbo 中文文档： http://dubbo.apache.org/zh-cn/index.html 一 重要的概念 1.1 什么是 Dubbo? 1.2 什么是 RPC?RPC原理是什么? 1.3 为什么要用 Dubbo? 1.4 什么是分布式? 1.5 为什么要分布式? 二 Dubbo 的架构 2.1 Dubbo 的架构图解 2.2 Dubbo 工作原理 三 Dubbo 的负载均衡策略 3.1 先来解释一下什么是负载均衡 3.2 再来看看 Dubbo 提供的负载均衡策略 3.2.1 Random LoadBalance(默认，基于权重的随机负载均衡机制) 3.2.2 RoundRobin LoadBalance(不推荐，基于权重的轮询负载均衡机制) 3.2.3 LeastActive LoadBalance 3.2.4 ConsistentHash LoadBalance 3.3 配置方式 四 zookeeper宕机与dubbo直连的情况 一 重要的概念 1.1 什么是 Dubbo? Apache Dubbo (incubating) |ˈdʌbəʊ| 是一款高性能、轻量级的开源Java RPC 框架，它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。简单来说 Dubbo 是一个分布式服务框架，致力于提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案。 Dubbo 目前已经有接近 23k 的 Star ，Dubbo的Github 地址：https://github.com/apache/incubator-dubbo 。 另外，在开源中国举行的2018年度最受欢迎中国开源软件这个活动的评选中，Dubbo 更是凭借其超高人气仅次于 vue.js 和 ECharts 获得第三名的好成绩。 Dubbo 是由阿里开源，后来加入了 Apache 。正式由于 Dubbo 的出现，才使得越来越多的公司开始使用以及接受分布式架构。 我们上面说了 Dubbo 实际上是 RPC 框架，那么什么是 RPC呢？ 1.2 什么是 RPC?RPC原理是什么? 什么是 RPC？ RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。比如两个不同的服务 A、B 部署在两台不同的机器上，那么服务 A 如果想要调用服务 B 中的某个方法该怎么办呢？使用 HTTP请求 当然可以，但是可能会比较慢而且一些优化做的并不好。 RPC 的出现就是为了解决这个问题。 RPC原理是什么？ 我这里这是简单的提一下。详细内容可以查看下面这篇文章： http://www.importnew.com/22003.html 服务消费方（client）调用以本地调用方式调用服务； client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体； client stub找到服务地址，并将消息发送到服务端； server stub收到消息后进行解码； server stub根据解码结果调用本地的服务； 本地服务执行并将结果返回给server stub； server stub将返回结果打包成消息并发送至消费方； client stub接收到消息，并进行解码； 服务消费方得到最终结果。 下面再贴一个网上的时序图： 说了这么多，我们为什么要用 Dubbo 呢？ 1.3 为什么要用 Dubbo? Dubbo 的诞生和 SOA 分布式架构的流行有着莫大的关系。SOA 面向服务的架构（Service Oriented Architecture），也就是把工程按照业务逻辑拆分成服务层、表现层两个工程。服务层中包含业务逻辑，只需要对外提供服务即可。表现层只需要处理和页面的交互，业务逻辑都是调用服务层的服务来实现。SOA架构中有两个主要角色：服务提供者（Provider）和服务使用者（Consumer）。 如果你要开发分布式程序，你也可以直接基于 HTTP 接口进行通信，但是为什么要用 Dubbo呢？ 我觉得主要可以从 Dubbo 提供的下面四点特性来说为什么要用 Dubbo： 负载均衡——同一个服务部署在不同的机器时该调用那一台机器上的服务。 服务调用链路生成——随着系统的发展，服务越来越多，服务间依赖关系变得错踪复杂，甚至分不清哪个应用要在哪个应用之前启动，架构师都不能完整的描述应用的架构关系。Dubbo 可以为我们解决服务之间互相是如何调用的。 服务访问压力以及时长统计、资源调度和治理——基于访问压力实时管理集群容量，提高集群利用率。 服务降级——某个服务挂掉之后调用备用服务。 另外，Dubbo 除了能够应用在分布式系统中，也可以应用在现在比较火的微服务系统中。不过，由于 Spring Cloud 在微服务中应用更加广泛，所以，我觉得一般我们提 Dubbo 的话，大部分是分布式系统的情况。 我们刚刚提到了分布式这个概念，下面再给大家介绍一下什么是分布式？为什么要分布式？ 1.4 什么是分布式? 分布式或者说 SOA 分布式重要的就是面向服务，说简单的分布式就是我们把整个系统拆分成不同的服务然后将这些服务放在不同的服务器上减轻单体服务的压力提高并发量和性能。比如电商系统可以简单地拆分成订单系统、商品系统、登录系统等等，拆分之后的每个服务可以部署在不同的机器上，如果某一个服务的访问量比较大的话也可以将这个服务同时部署在多台机器上。 1.5 为什么要分布式? 从开发角度来讲单体应用的代码都集中在一起，而分布式系统的代码根据业务被拆分。所以，每个团队可以负责一个服务的开发，这样提升了开发效率。另外，代码根据业务拆分之后更加便于维护和扩展。 另外，我觉得将系统拆分成分布式之后不光便于系统扩展和维护，更能提高整个系统的性能。你想一想嘛？把整个系统拆分成不同的服务/系统，然后每个服务/系统 单独部署在一台服务器上，是不是很大程度上提高了系统性能呢？ 二 Dubbo 的架构 2.1 Dubbo 的架构图解 上述节点简单说明： Provider： 暴露服务的服务提供方 Consumer： 调用远程服务的服务消费方 Registry： 服务注册与发现的注册中心 Monitor： 统计服务的调用次数和调用时间的监控中心 Container： 服务运行容器 调用关系说明： 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 重要知识点总结： 注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小 监控中心负责统计各服务调用次数，调用时间等，统计先在内存汇总后每分钟一次发送到监控中心服务器，并以报表展示 注册中心，服务提供者，服务消费者三者之间均为长连接，监控中心除外 注册中心通过长连接感知服务提供者的存在，服务提供者宕机，注册中心将立即推送事件通知消费者 注册中心和监控中心全部宕机，不影响已运行的提供者和消费者，消费者在本地缓存了提供者列表 注册中心和监控中心都是可选的，服务消费者可以直连服务提供者 服务提供者无状态，任意一台宕掉后，不影响使用 服务提供者全部宕掉后，服务消费者应用将无法使用，并无限次重连等待服务提供者恢复 2.2 Dubbo 工作原理 图中从下至上分为十层，各层均为单向依赖，右边的黑色箭头代表层之间的依赖关系，每一层都可以剥离上层被复用，其中，Service 和 Config 层为 API，其它各层均为 SPI。 各层说明： 第一层：service层，接口层，给服务提供者和消费者来实现的 第二层：config层，配置层，主要是对dubbo进行各种配置的 第三层：proxy层，服务接口透明代理，生成服务的客户端 Stub 和服务器端 Skeleton 第四层：registry层，服务注册层，负责服务的注册与发现 第五层：cluster层，集群层，封装多个服务提供者的路由以及负载均衡，将多个实例组合成一个服务 第六层：monitor层，监控层，对rpc接口的调用次数和调用时间进行监控 第七层：protocol层，远程调用层，封装rpc调用 第八层：exchange层，信息交换层，封装请求响应模式，同步转异步 第九层：transport层，网络传输层，抽象mina和netty为统一接口 第十层：serialize层，数据序列化层，网络传输需要 三 Dubbo 的负载均衡策略 3.1 先来解释一下什么是负载均衡 先来个官方的解释。 维基百科对负载均衡的定义：负载均衡改善了跨多个计算资源（例如计算机，计算机集群，网络链接，中央处理单元或磁盘驱动的的工作负载分布。负载平衡旨在优化资源使用，最大化吞吐量，最小化响应时间，并避免任何单个资源的过载。使用具有负载平衡而不是单个组件的多个组件可以通过冗余提高可靠性和可用性。负载平衡通常涉及专用软件或硬件。 上面讲的大家可能不太好理解，再用通俗的话给大家说一下。 比如我们的系统中的某个服务的访问量特别大，我们将这个服务部署在了多台服务器上，当客户端发起请求的时候，多台服务器都可以处理这个请求。那么，如何正确选择处理该请求的服务器就很关键。假如，你就要一台服务器来处理该服务的请求，那该服务部署在多台服务器的意义就不复存在了。负载均衡就是为了避免单个服务器响应同一请求，容易造成服务器宕机、崩溃等问题，我们从负载均衡的这四个字就能明显感受到它的意义。 3.2 再来看看 Dubbo 提供的负载均衡策略 在集群负载均衡时，Dubbo 提供了多种均衡策略，默认为 random 随机调用。可以自行扩展负载均衡策略，参见：负载均衡扩展。 备注:下面的图片来自于：尚硅谷2018Dubbo 视频。 3.2.1 Random LoadBalance(默认，基于权重的随机负载均衡机制) 随机，按权重设置随机概率。 在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。 3.2.2 RoundRobin LoadBalance(不推荐，基于权重的轮询负载均衡机制) 轮循，按公约后的权重设置轮循比率。 存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。 3.2.3 LeastActive LoadBalance 最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。 使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。 3.2.4 ConsistentHash LoadBalance 一致性 Hash，相同参数的请求总是发到同一提供者。(如果你需要的不是随机负载均衡，是要一类请求都到一个节点，那就走这个一致性hash策略。) 当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。 算法参见：http://en.wikipedia.org/wiki/Consistent_hashing 缺省只对第一个参数 Hash，如果要修改，请配置 缺省用 160 份虚拟节点，如果要修改，请配置 3.3 配置方式 xml 配置方式 服务端服务级别 客户端服务级别 服务端方法级别 客户端方法级别 注解配置方式： 消费方基于基于注解的服务级别配置方式： @Reference(loadbalance = \"roundrobin\") HelloService helloService; 四 zookeeper宕机与dubbo直连的情况 zookeeper宕机与dubbo直连的情况在面试中可能会被经常问到，所以要引起重视。 在实际生产中，假如zookeeper注册中心宕掉，一段时间内服务消费方还是能够调用提供方的服务的，实际上它使用的本地缓存进行通讯，这只是dubbo健壮性的一种体现。 dubbo的健壮性表现： 监控中心宕掉不影响使用，只是丢失部分采样数据 数据库宕掉后，注册中心仍能通过缓存提供服务列表查询，但不能注册新服务 注册中心对等集群，任意一台宕掉后，将自动切换到另一台 注册中心全部宕掉后，服务提供者和服务消费者仍能通过本地缓存通讯 服务提供者无状态，任意一台宕掉后，不影响使用 服务提供者全部宕掉后，服务消费者应用将无法使用，并无限次重连等待服务提供者恢复 我们前面提到过：注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小。所以，我们可以完全可以绕过注册中心——采用 dubbo 直连 ，即在服务消费方配置服务提供方的位置信息。 xml配置方式： 注解方式： @Reference(url = \"127.0.0.1:20880\") HelloService helloService; "},"zother6-JavaGuide/system-design/data-communication/kafka-inverview.html":{"url":"zother6-JavaGuide/system-design/data-communication/kafka-inverview.html","title":"Kafka Inverview","keywords":"","body":" Kafka面试题总结 Kafka 是什么？主要应用场景有哪些？ Kafka 是一个分布式流式处理平台。这到底是什么意思呢？ 流平台具有三个关键功能： 消息队列：发布和订阅消息流，这个功能类似于消息队列，这也是 Kafka 也被归类为消息队列的原因。 容错的持久方式存储记录消息流： Kafka 会把消息持久化到磁盘，有效避免了消息丢失的风险·。 流式处理平台： 在消息发布的时候进行处理，Kafka 提供了一个完整的流式处理类库。 Kafka 主要有两大应用场景： 消息队列 ：建立实时流数据管道，以可靠地在系统或应用程序之间获取数据。 数据处理： 构建实时的流数据处理程序来转换或处理数据流。 和其他消息队列相比,Kafka的优势在哪里？ 我们现在经常提到 Kafka 的时候就已经默认它是一个非常优秀的消息队列了，我们也会经常拿它给 RocketMQ、RabbitMQ 对比。我觉得 Kafka 相比其他消息队列主要的优势如下： 极致的性能 ：基于 Scala 和 Java 语言开发，设计中大量使用了批量处理和异步的思想，最高可以每秒处理千万级别的消息。 生态系统兼容性无可匹敌 ：Kafka 与周边生态系统的兼容性是最好的没有之一，尤其在大数据和流计算领域。 实际上在早期的时候 Kafka 并不是一个合格的消息队列，早期的 Kafka 在消息队列领域就像是一个衣衫褴褛的孩子一样，功能不完备并且有一些小问题比如丢失消息、不保证消息可靠性等等。当然，这也和 LinkedIn 最早开发 Kafka 用于处理海量的日志有很大关系，哈哈哈，人家本来最开始就不是为了作为消息队列滴，谁知道后面误打误撞在消息队列领域占据了一席之地。 随着后续的发展，这些短板都被 Kafka 逐步修复完善。所以，Kafka 作为消息队列不可靠这个说法已经过时！ 队列模型了解吗？Kafka 的消息模型知道吗？ 题外话：早期的 JMS 和 AMQP 属于消息服务领域权威组织所做的相关的标准，我在 JavaGuide的 《消息队列其实很简单》这篇文章中介绍过。但是，这些标准的进化跟不上消息队列的演进速度，这些标准实际上已经属于废弃状态。所以，可能存在的情况是：不同的消息队列都有自己的一套消息模型。 队列模型：早期的消息模型 使用队列（Queue）作为消息通信载体，满足生产者与消费者模式，一条消息只能被一个消费者使用，未被消费的消息在队列中保留直到被消费或超时。 比如：我们生产者发送 100 条消息的话，两个消费者来消费一般情况下两个消费者会按照消息发送的顺序各自消费一半（也就是你一个我一个的消费。） 队列模型存在的问题： 假如我们存在这样一种情况：我们需要将生产者产生的消息分发给多个消费者，并且每个消费者都能接收到完成的消息内容。 这种情况，队列模型就不好解决了。很多比较杠精的人就说：我们可以为每个消费者创建一个单独的队列，让生产者发送多份。这是一种非常愚蠢的做法，浪费资源不说，还违背了使用消息队列的目的。 发布-订阅模型:Kafka 消息模型 发布-订阅模型主要是为了解决队列模型存在的问题。 发布订阅模型（Pub-Sub） 使用主题（Topic） 作为消息通信载体，类似于广播模式；发布者发布一条消息，该消息通过主题传递给所有的订阅者，在一条消息广播之后才订阅的用户则是收不到该条消息的。 在发布 - 订阅模型中，如果只有一个订阅者，那它和队列模型就基本是一样的了。所以说，发布 - 订阅模型在功能层面上是可以兼容队列模型的。 Kafka 采用的就是发布 - 订阅模型。 RocketMQ 的消息模型和 Kafka 基本是完全一样的。唯一的区别是 Kafka 中没有队列这个概念，与之对应的是 Partition（分区）。 什么是Producer、Consumer、Broker、Topic、Partition？ Kafka 将生产者发布的消息发送到 Topic（主题） 中，需要这些消息的消费者可以订阅这些 Topic（主题），如下图所示： 上面这张图也为我们引出了，Kafka 比较重要的几个概念： Producer（生产者） : 产生消息的一方。 Consumer（消费者） : 消费消息的一方。 Broker（代理） : 可以看作是一个独立的 Kafka 实例。多个 Kafka Broker 组成一个 Kafka Cluster。 同时，你一定也注意到每个 Broker 中又包含了 Topic 以及 Partition 这两个重要的概念： Topic（主题） : Producer 将消息发送到特定的主题，Consumer 通过订阅特定的 Topic(主题) 来消费消息。 Partition（分区） : Partition 属于 Topic 的一部分。一个 Topic 可以有多个 Partition ，并且同一 Topic 下的 Partition 可以分布在不同的 Broker 上，这也就表明一个 Topic 可以横跨多个 Broker 。这正如我上面所画的图一样。 划重点：Kafka 中的 Partition（分区） 实际上可以对应成为消息队列中的队列。这样是不是更好理解一点？ Kafka 的多副本机制了解吗？带来了什么好处？ 还有一点我觉得比较重要的是 Kafka 为分区（Partition）引入了多副本（Replica）机制。分区（Partition）中的多个副本之间会有一个叫做 leader 的家伙，其他副本称为 follower。我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。 生产者和消费者只与 leader 副本交互。你可以理解为其他副本只是 leader 副本的拷贝，它们的存在只是为了保证消息存储的安全性。当 leader 副本发生故障时会从 follower 中选举出一个 leader,但是 follower 中如果有和 leader 同步程度达不到要求的参加不了 leader 的竞选。 Kafka 的多分区（Partition）以及多副本（Replica）机制有什么好处呢？ Kafka 通过给特定 Topic 指定多个 Partition, 而各个 Partition 可以分布在不同的 Broker 上, 这样便能提供比较好的并发能力（负载均衡）。 Partition 可以指定对应的 Replica 数, 这也极大地提高了消息存储的安全性, 提高了容灾能力，不过也相应的增加了所需要的存储空间。 Zookeeper 在 Kafka 中的作用知道吗？ 要想搞懂 zookeeper 在 Kafka 中的作用 一定要自己搭建一个 Kafka 环境然后自己进 zookeeper 去看一下有哪些文件夹和 Kafka 有关，每个节点又保存了什么信息。 一定不要光看不实践，这样学来的也终会忘记！这部分内容参考和借鉴了这篇文章：https://www.jianshu.com/p/a036405f989c 。 下图就是我的本地 Zookeeper ，它成功和我本地的 Kafka 关联上（以下文件夹结构借助 idea 插件 Zookeeper tool 实现）。 ZooKeeper 主要为 Kafka 提供元数据的管理的功能。 从图中我们可以看出，Zookeeper 主要为 Kafka 做了下面这些事情： Broker 注册 ：在 Zookeeper 上会有一个专门用来进行 Broker 服务器列表记录的节点。每个 Broker 在启动时，都会到 Zookeeper 上进行注册，即到/brokers/ids 下创建属于自己的节点。每个 Broker 就会将自己的 IP 地址和端口等信息记录到该节点中去 Topic 注册 ： 在 Kafka 中，同一个Topic 的消息会被分成多个分区并将其分布在多个 Broker 上，这些分区信息及与 Broker 的对应关系也都是由 Zookeeper 在维护。比如我创建了一个名字为 my-topic 的主题并且它有两个分区，对应到 zookeeper 中会创建这些文件夹：/brokers/topics/my-topic/Partitions/0、/brokers/topics/my-topic/Partitions/1 负载均衡 ：上面也说过了 Kafka 通过给特定 Topic 指定多个 Partition, 而各个 Partition 可以分布在不同的 Broker 上, 这样便能提供比较好的并发能力。 对于同一个 Topic 的不同 Partition，Kafka 会尽力将这些 Partition 分布到不同的 Broker 服务器上。当生产者产生消息后也会尽量投递到不同 Broker 的 Partition 里面。当 Consumer 消费的时候，Zookeeper 可以根据当前的 Partition 数量以及 Consumer 数量来实现动态负载均衡。 ...... Kafka 如何保证消息的消费顺序？ 我们在使用消息队列的过程中经常有业务场景需要严格保证消息的消费顺序，比如我们同时发了 2 个消息，这 2 个消息对应的操作分别对应的数据库操作是：更改用户会员等级、根据会员等级计算订单价格。假如这两条消息的消费顺序不一样造成的最终结果就会截然不同。 我们知道 Kafka 中 Partition(分区)是真正保存消息的地方，我们发送的消息都被放在了这里。而我们的 Partition(分区) 又存在于 Topic(主题) 这个概念中，并且我们可以给特定 Topic 指定多个 Partition。 每次添加消息到 Partition(分区) 的时候都会采用尾加法，如上图所示。Kafka 只能为我们保证 Partition(分区) 中的消息有序，而不能保证 Topic(主题) 中的 Partition(分区) 的有序。 消息在被追加到 Partition(分区)的时候都会分配一个特定的偏移量（offset）。Kafka 通过偏移量（offset）来保证消息在分区内的顺序性。 所以，我们就有一种很简单的保证消息消费顺序的方法：1 个 Topic 只对应一个 Partition。这样当然可以解决问题，但是破坏了 Kafka 的设计初衷。 Kafka 中发送 1 条消息的时候，可以指定 topic, partition, key,data（数据） 4 个参数。如果你发送消息的时候指定了 Partition 的话，所有消息都会被发送到指定的 Partition。并且，同一个 key 的消息可以保证只发送到同一个 partition，这个我们可以采用表/对象的 id 来作为 key 。 总结一下，对于如何保证 Kafka 中消息消费的顺序，有了下面两种方法： 1 个 Topic 只对应一个 Partition。 （推荐）发送消息的时候指定 key/Partition。 当然不仅仅只有上面两种方法，上面两种方法是我觉得比较好理解的， Kafka 如何保证消息不丢失 生产者丢失消息的情况 生产者(Producer) 调用send方法发送消息之后，消息可能因为网络问题并没有发送过去。 所以，我们不能默认在调用send方法发送消息之后消息消息发送成功了。为了确定消息是发送成功，我们要判断消息发送的结果。但是要注意的是 Kafka 生产者(Producer) 使用 send 方法发送消息实际上是异步的操作，我们可以通过 get()方法获取调用结果，但是这样也让它变为了同步操作，示例代码如下： 详细代码见我的这篇文章：Kafka系列第三篇！10 分钟学会如何在 Spring Boot 程序中使用 Kafka 作为消息队列? SendResult sendResult = kafkaTemplate.send(topic, o).get(); if (sendResult.getRecordMetadata() != null) { logger.info(\"生产者成功发送消息到\" + sendResult.getProducerRecord().topic() + \"-> \" + sendRe sult.getProducerRecord().value().toString()); } 但是一般不推荐这么做！可以采用为其添加回调函数的形式，示例代码如下： ListenableFuture> future = kafkaTemplate.send(topic, o); future.addCallback(result -> logger.info(\"生产者成功发送消息到topic:{} partition:{}的消息\", result.getRecordMetadata().topic(), result.getRecordMetadata().partition()), ex -> logger.error(\"生产者发送消失败，原因：{}\", ex.getMessage())); 如果消息发送失败的话，我们检查失败的原因之后重新发送即可！ 另外这里推荐为 Producer 的retries（重试次数）设置一个比较合理的值，一般是 3 ，但是为了保证消息不丢失的话一般会设置比较大一点。设置完成之后，当出现网络问题之后能够自动重试消息发送，避免消息丢失。另外，建议还要设置重试间隔，因为间隔太小的话重试的效果就不明显了，网络波动一次你3次一下子就重试完了 消费者丢失消息的情况 我们知道消息在被追加到 Partition(分区)的时候都会分配一个特定的偏移量（offset）。偏移量（offset)表示 Consumer 当前消费到的 Partition(分区)的所在的位置。Kafka 通过偏移量（offset）可以保证消息在分区内的顺序性。 当消费者拉取到了分区的某个消息之后，消费者会自动提交了 offset。自动提交的话会有一个问题，试想一下，当消费者刚拿到这个消息准备进行真正消费的时候，突然挂掉了，消息实际上并没有被消费，但是 offset 却被自动提交了。 解决办法也比较粗暴，我们手动关闭闭自动提交 offset，每次在真正消费完消息之后之后再自己手动提交 offset 。 但是，细心的朋友一定会发现，这样会带来消息被重新消费的问题。比如你刚刚消费完消息之后，还没提交 offset，结果自己挂掉了，那么这个消息理论上就会被消费两次。 Kafka 弄丢了消息 我们知道 Kafka 为分区（Partition）引入了多副本（Replica）机制。分区（Partition）中的多个副本之间会有一个叫做 leader 的家伙，其他副本称为 follower。我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。生产者和消费者只与 leader 副本交互。你可以理解为其他副本只是 leader 副本的拷贝，它们的存在只是为了保证消息存储的安全性。 试想一种情况：假如 leader 副本所在的 broker 突然挂掉，那么就要从 follower 副本重新选出一个 leader ，但是 leader 的数据还有一些没有被 follower 副本的同步的话，就会造成消息丢失。 设置 acks = all 解决办法就是我们设置 acks = all。acks 是 Kafka 生产者(Producer) 很重要的一个参数。 acks 的默认值即为1，代表我们的消息被leader副本接收之后就算被成功发送。当我们配置 acks = all 代表则所有副本都要接收到该消息之后该消息才算真正成功被发送。 设置 replication.factor >= 3 为了保证 leader 副本能有 follower 副本能同步消息，我们一般会为 topic 设置 replication.factor >= 3。这样就可以保证每个 分区(partition) 至少有 3 个副本。虽然造成了数据冗余，但是带来了数据的安全性。 设置 min.insync.replicas > 1 一般情况下我们还需要设置 min.insync.replicas> 1 ，这样配置代表消息至少要被写入到 2 个副本才算是被成功发送。min.insync.replicas 的默认值为 1 ，在实际生产中应尽量避免默认值 1。 但是，为了保证整个 Kafka 服务的高可用性，你需要确保 replication.factor > min.insync.replicas 。为什么呢？设想一下加入两者相等的话，只要是有一个副本挂掉，整个分区就无法正常工作了。这明显违反高可用性！一般推荐设置成 replication.factor = min.insync.replicas + 1。 设置 unclean.leader.election.enable = false Kafka 0.11.0.0版本开始 unclean.leader.election.enable 参数的默认值由原来的true 改为false 我们最开始也说了我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。多个 follower 副本之间的消息同步情况不一样，当我们配置了 unclean.leader.election.enable = false 的话，当 leader 副本发生故障时就不会从 follower 副本中和 leader 同步程度达不到要求的副本中选择出 leader ，这样降低了消息丢失的可能性。 Kafka 如何保证消息不重复消费 代办... Reference Kafka 官方文档： https://kafka.apache.org/documentation/ 极客时间—《Kafka核心技术与实战》第11节：无消息丢失配置怎么实现？ "},"zother6-JavaGuide/system-design/data-communication/Kafka入门看这一篇就够了.html":{"url":"zother6-JavaGuide/system-design/data-communication/Kafka入门看这一篇就够了.html","title":"Kafka入门看这一篇就够了","keywords":"","body":" 本文由 JavaGuide 读者推荐，JavaGuide 对文章进行了整理排版！原文地址：https://www.wmyskxz.com/2019/07/17/kafka-ru-men-jiu-zhe-yi-pian/ ， 作者：我没有三颗心脏。 一、Kafka 简介 Kafka 创建背景 Kafka 是一个消息系统，原本开发自 LinkedIn，用作 LinkedIn 的活动流（Activity Stream）和运营数据处理管道（Pipeline）的基础。现在它已被多家不同类型的公司 作为多种类型的数据管道和消息系统使用。 活动流数据是几乎所有站点在对其网站使用情况做报表时都要用到的数据中最常规的部分。活动数据包括页面访问量（Page View）、被查看内容方面的信息以及搜索情况等内容。这种数据通常的处理方式是先把各种活动以日志的形式写入某种文件，然后周期性地对这些文件进行统计分析。运营数据指的是服务器的性能数据（CPU、IO 使用率、请求时间、服务日志等等数据)。运营数据的统计方法种类繁多。 近年来，活动和运营数据处理已经成为了网站软件产品特性中一个至关重要的组成部分，这就需要一套稍微更加复杂的基础设施对其提供支持。 Kafka 简介 Kafka 是一种分布式的，基于发布 / 订阅的消息系统。 主要设计目标如下： 以时间复杂度为 O(1) 的方式提供消息持久化能力，即使对 TB 级以上数据也能保证常数时间复杂度的访问性能。 高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒 100K 条以上消息的传输。 支持 Kafka Server 间的消息分区，及分布式消费，同时保证每个 Partition 内的消息顺序传输。 同时支持离线数据处理和实时数据处理。 Scale out：支持在线水平扩展。 Kafka 基础概念 概念一：生产者与消费者 对于 Kafka 来说客户端有两种基本类型： 生产者（Producer） 消费者（Consumer）。 除此之外，还有用来做数据集成的 Kafka Connect API 和流式处理的 Kafka Streams 等高阶客户端，但这些高阶客户端底层仍然是生产者和消费者API，它们只不过是在上层做了封装。 这很容易理解，生产者（也称为发布者）创建消息，而消费者（也称为订阅者）负责消费or读取消息。 概念二：主题（Topic）与分区（Partition） 在 Kafka 中，消息以主题（Topic）来分类，每一个主题都对应一个 「消息队列」，这有点儿类似于数据库中的表。但是如果我们把所有同类的消息都塞入到一个“中心”队列中，势必缺少可伸缩性，无论是生产者/消费者数目的增加，还是消息数量的增加，都可能耗尽系统的性能或存储。 我们使用一个生活中的例子来说明：现在 A 城市生产的某商品需要运输到 B 城市，走的是公路，那么单通道的高速公路不论是在「A 城市商品增多」还是「现在 C 城市也要往 B 城市运输东西」这样的情况下都会出现「吞吐量不足」的问题。所以我们现在引入分区（Partition）的概念，类似“允许多修几条道”的方式对我们的主题完成了水平扩展。 概念三：Broker 和集群（Cluster） 一个 Kafka 服务器也称为 Broker，它接受生产者发送的消息并存入磁盘；Broker 同时服务消费者拉取分区消息的请求，返回目前已经提交的消息。使用特定的机器硬件，一个 Broker 每秒可以处理成千上万的分区和百万量级的消息。（现在动不动就百万量级..我特地去查了一把，好像确实集群的情况下吞吐量挺高的..嗯..） 若干个 Broker 组成一个集群（Cluster），其中集群内某个 Broker 会成为集群控制器（Cluster Controller），它负责管理集群，包括分配分区到 Broker、监控 Broker 故障等。在集群内，一个分区由一个 Broker 负责，这个 Broker 也称为这个分区的 Leader；当然一个分区可以被复制到多个 Broker 上来实现冗余，这样当存在 Broker 故障时可以将其分区重新分配到其他 Broker 来负责。下图是一个样例： Kafka 的一个关键性质是日志保留（retention），我们可以配置主题的消息保留策略，譬如只保留一段时间的日志或者只保留特定大小的日志。当超过这些限制时，老的消息会被删除。我们也可以针对某个主题单独设置消息过期策略，这样对于不同应用可以实现个性化。 概念四：多集群 随着业务发展，我们往往需要多集群，通常处于下面几个原因： 基于数据的隔离； 基于安全的隔离； 多数据中心（容灾） 当构建多个数据中心时，往往需要实现消息互通。举个例子，假如用户修改了个人资料，那么后续的请求无论被哪个数据中心处理，这个更新需要反映出来。又或者，多个数据中心的数据需要汇总到一个总控中心来做数据分析。 上面说的分区复制冗余机制只适用于同一个 Kafka 集群内部，对于多个 Kafka 集群消息同步可以使用 Kafka 提供的 MirrorMaker 工具。本质上来说，MirrorMaker 只是一个 Kafka 消费者和生产者，并使用一个队列连接起来而已。它从一个集群中消费消息，然后往另一个集群生产消息。 二、Kafka 的设计与实现 上面我们知道了 Kafka 中的一些基本概念，但作为一个成熟的「消息队列」中间件，其中有许多有意思的设计值得我们思考，下面我们简单列举一些。 讨论一：Kafka 存储在文件系统上 是的，您首先应该知道 Kafka 的消息是存在于文件系统之上的。Kafka 高度依赖文件系统来存储和缓存消息，一般的人认为 “磁盘是缓慢的”，所以对这样的设计持有怀疑态度。实际上，磁盘比人们预想的快很多也慢很多，这取决于它们如何被使用；一个好的磁盘结构设计可以使之跟网络速度一样快。 现代的操作系统针对磁盘的读写已经做了一些优化方案来加快磁盘的访问速度。比如，预读会提前将一个比较大的磁盘快读入内存。后写会将很多小的逻辑写操作合并起来组合成一个大的物理写操作。并且，操作系统还会将主内存剩余的所有空闲内存空间都用作磁盘缓存，所有的磁盘读写操作都会经过统一的磁盘缓存（除了直接 I/O 会绕过磁盘缓存）。综合这几点优化特点，如果是针对磁盘的顺序访问，某些情况下它可能比随机的内存访问都要快，甚至可以和网络的速度相差无几。 上述的 Topic 其实是逻辑上的概念，面相消费者和生产者，物理上存储的其实是 Partition，每一个 Partition 最终对应一个目录，里面存储所有的消息和索引文件。默认情况下，每一个 Topic 在创建时如果不指定 Partition 数量时只会创建 1 个 Partition。比如，我创建了一个 Topic 名字为 test ，没有指定 Partition 的数量，那么会默认创建一个 test-0 的文件夹，这里的命名规则是：-。 任何发布到 Partition 的消息都会被追加到 Partition 数据文件的尾部，这样的顺序写磁盘操作让 Kafka 的效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，这是 Kafka 高吞吐率的一个很重要的保证）。 每一条消息被发送到 Broker 中，会根据 Partition 规则选择被存储到哪一个 Partition。如果 Partition 规则设置的合理，所有消息可以均匀分布到不同的 Partition中。 讨论二：Kafka 中的底层存储设计 假设我们现在 Kafka 集群只有一个 Broker，我们创建 2 个 Topic 名称分别为：「topic1」和「topic2」，Partition 数量分别为 1、2，那么我们的根目录下就会创建如下三个文件夹： | --topic1-0 | --topic2-0 | --topic2-1 在 Kafka 的文件存储中，同一个 Topic 下有多个不同的 Partition，每个 Partition 都为一个目录，而每一个目录又被平均分配成多个大小相等的 Segment File 中，Segment File 又由 index file 和 data file 组成，他们总是成对出现，后缀 “.index” 和 “.log” 分表表示 Segment 索引文件和数据文件。 现在假设我们设置每个 Segment 大小为 500 MB，并启动生产者向 topic1 中写入大量数据，topic1-0 文件夹中就会产生类似如下的一些文件： | --topic1-0 | --00000000000000000000.index | --00000000000000000000.log | --00000000000000368769.index | --00000000000000368769.log | --00000000000000737337.index | --00000000000000737337.log | --00000000000001105814.index | --00000000000001105814.log | --topic2-0 | --topic2-1 Segment 是 Kafka 文件存储的最小单位。Segment 文件命名规则：Partition 全局的第一个 Segment 从 0 开始，后续每个 Segment 文件名为上一个 Segment 文件最后一条消息的 offset 值。数值最大为 64 位 long 大小，19 位数字字符长度，没有数字用0填充。如 00000000000000368769.index 和 00000000000000368769.log。 以上面的一对 Segment File 为例，说明一下索引文件和数据文件对应关系： 其中以索引文件中元数据 为例，依次在数据文件中表示第 3 个 message（在全局 Partition 表示第 368769 + 3 = 368772 个 message）以及该消息的物理偏移地址为 497。 注意该 index 文件并不是从0开始，也不是每次递增1的，这是因为 Kafka 采取稀疏索引存储的方式，每隔一定字节的数据建立一条索引，它减少了索引文件大小，使得能够把 index 映射到内存，降低了查询时的磁盘 IO 开销，同时也并没有给查询带来太多的时间消耗。 因为其文件名为上一个 Segment 最后一条消息的 offset ，所以当需要查找一个指定 offset 的 message 时，通过在所有 segment 的文件名中进行二分查找就能找到它归属的 segment ，再在其 index 文件中找到其对应到文件上的物理位置，就能拿出该 message 。 由于消息在 Partition 的 Segment 数据文件中是顺序读写的，且消息消费后不会删除（删除策略是针对过期的 Segment 文件），这种顺序磁盘 IO 存储设计师 Kafka 高性能很重要的原因。 Kafka 是如何准确的知道 message 的偏移的呢？这是因为在 Kafka 定义了标准的数据存储结构，在 Partition 中的每一条 message 都包含了以下三个属性： offset：表示 message 在当前 Partition 中的偏移量，是一个逻辑上的值，唯一确定了 Partition 中的一条 message，可以简单的认为是一个 id； MessageSize：表示 message 内容 data 的大小； data：message 的具体内容 讨论三：生产者设计概要 当我们发送消息之前，先问几个问题：每条消息都是很关键且不能容忍丢失么？偶尔重复消息可以么？我们关注的是消息延迟还是写入消息的吞吐量？ 举个例子，有一个信用卡交易处理系统，当交易发生时会发送一条消息到 Kafka，另一个服务来读取消息并根据规则引擎来检查交易是否通过，将结果通过 Kafka 返回。对于这样的业务，消息既不能丢失也不能重复，由于交易量大因此吞吐量需要尽可能大，延迟可以稍微高一点。 再举个例子，假如我们需要收集用户在网页上的点击数据，对于这样的场景，少量消息丢失或者重复是可以容忍的，延迟多大都不重要只要不影响用户体验，吞吐则根据实时用户数来决定。 不同的业务需要使用不同的写入方式和配置。具体的方式我们在这里不做讨论，现在先看下生产者写消息的基本流程： 图片来源：http://www.dengshenyu.com/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2017/11/12/kafka-producer.html 流程如下： 首先，我们需要创建一个ProducerRecord，这个对象需要包含消息的主题（topic）和值（value），可以选择性指定一个键值（key）或者分区（partition）。 发送消息时，生产者会对键值和值序列化成字节数组，然后发送到分配器（partitioner）。 如果我们指定了分区，那么分配器返回该分区即可；否则，分配器将会基于键值来选择一个分区并返回。 选择完分区后，生产者知道了消息所属的主题和分区，它将这条记录添加到相同主题和分区的批量消息中，另一个线程负责发送这些批量消息到对应的Kafka broker。 当broker接收到消息后，如果成功写入则返回一个包含消息的主题、分区及位移的RecordMetadata对象，否则返回异常。 生产者接收到结果后，对于异常可能会进行重试。 讨论四：消费者设计概要 消费者与消费组 假设这么个场景：我们从Kafka中读取消息，并且进行检查，最后产生结果数据。我们可以创建一个消费者实例去做这件事情，但如果生产者写入消息的速度比消费者读取的速度快怎么办呢？这样随着时间增长，消息堆积越来越严重。对于这种场景，我们需要增加多个消费者来进行水平扩展。 Kafka消费者是消费组的一部分，当多个消费者形成一个消费组来消费主题时，每个消费者会收到不同分区的消息。假设有一个T1主题，该主题有4个分区；同时我们有一个消费组G1，这个消费组只有一个消费者C1。那么消费者C1将会收到这4个分区的消息，如下所示： 如果我们增加新的消费者C2到消费组G1，那么每个消费者将会分别收到两个分区的消息，如下所示： 如果增加到4个消费者，那么每个消费者将会分别收到一个分区的消息，如下所示： 但如果我们继续增加消费者到这个消费组，剩余的消费者将会空闲，不会收到任何消息： 总而言之，我们可以通过增加消费组的消费者来进行水平扩展提升消费能力。这也是为什么建议创建主题时使用比较多的分区数，这样可以在消费负载高的情况下增加消费者来提升性能。另外，消费者的数量不应该比分区数多，因为多出来的消费者是空闲的，没有任何帮助。 Kafka一个很重要的特性就是，只需写入一次消息，可以支持任意多的应用读取这个消息。换句话说，每个应用都可以读到全量的消息。为了使得每个应用都能读到全量消息，应用需要有不同的消费组。对于上面的例子，假如我们新增了一个新的消费组G2，而这个消费组有两个消费者，那么会是这样的： 在这个场景中，消费组G1和消费组G2都能收到T1主题的全量消息，在逻辑意义上来说它们属于不同的应用。 最后，总结起来就是：如果应用需要读取全量消息，那么请为该应用设置一个消费组；如果该应用消费能力不足，那么可以考虑在这个消费组里增加消费者。 消费组与分区重平衡 可以看到，当新的消费者加入消费组，它会消费一个或多个分区，而这些分区之前是由其他消费者负责的；另外，当消费者离开消费组（比如重启、宕机等）时，它所消费的分区会分配给其他分区。这种现象称为重平衡（rebalance）。重平衡是 Kafka 一个很重要的性质，这个性质保证了高可用和水平扩展。不过也需要注意到，在重平衡期间，所有消费者都不能消费消息，因此会造成整个消费组短暂的不可用。而且，将分区进行重平衡也会导致原来的消费者状态过期，从而导致消费者需要重新更新状态，这段期间也会降低消费性能。后面我们会讨论如何安全的进行重平衡以及如何尽可能避免。 消费者通过定期发送心跳（hearbeat）到一个作为组协调者（group coordinator）的 broker 来保持在消费组内存活。这个 broker 不是固定的，每个消费组都可能不同。当消费者拉取消息或者提交时，便会发送心跳。 如果消费者超过一定时间没有发送心跳，那么它的会话（session）就会过期，组协调者会认为该消费者已经宕机，然后触发重平衡。可以看到，从消费者宕机到会话过期是有一定时间的，这段时间内该消费者的分区都不能进行消息消费；通常情况下，我们可以进行优雅关闭，这样消费者会发送离开的消息到组协调者，这样组协调者可以立即进行重平衡而不需要等待会话过期。 在 0.10.1 版本，Kafka 对心跳机制进行了修改，将发送心跳与拉取消息进行分离，这样使得发送心跳的频率不受拉取的频率影响。另外更高版本的 Kafka 支持配置一个消费者多长时间不拉取消息但仍然保持存活，这个配置可以避免活锁（livelock）。活锁，是指应用没有故障但是由于某些原因不能进一步消费。 Partition 与消费模型 上面提到，Kafka 中一个 topic 中的消息是被打散分配在多个 Partition(分区) 中存储的， Consumer Group 在消费时需要从不同的 Partition 获取消息，那最终如何重建出 Topic 中消息的顺序呢？ 答案是：没有办法。Kafka 只会保证在 Partition 内消息是有序的，而不管全局的情况。 下一个问题是：Partition 中的消息可以被（不同的 Consumer Group）多次消费，那 Partition中被消费的消息是何时删除的？ Partition 又是如何知道一个 Consumer Group 当前消费的位置呢？ 无论消息是否被消费，除非消息到期 Partition 从不删除消息。例如设置保留时间为 2 天，则消息发布 2 天内任何 Group 都可以消费，2 天后，消息自动被删除。 Partition 会为每个 Consumer Group 保存一个偏移量，记录 Group 消费到的位置。 如下图： 为什么 Kafka 是 pull 模型 消费者应该向 Broker 要数据（pull）还是 Broker 向消费者推送数据（push）？作为一个消息系统，Kafka 遵循了传统的方式，选择由 Producer 向 broker push 消息并由 Consumer 从 broker pull 消息。一些 logging-centric system，比如 Facebook 的Scribe和 Cloudera 的Flume，采用 push 模式。事实上，push 模式和 pull 模式各有优劣。 push 模式很难适应消费速率不同的消费者，因为消息发送速率是由 broker 决定的。push 模式的目标是尽可能以最快速度传递消息，但是这样很容易造成 Consumer 来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而 pull 模式则可以根据 Consumer 的消费能力以适当的速率消费消息。 对于 Kafka 而言，pull 模式更合适。pull 模式可简化 broker 的设计，Consumer 可自主控制消费消息的速率，同时 Consumer 可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。 讨论五：Kafka 如何保证可靠性 当我们讨论可靠性的时候，我们总会提到保证*这个词语。可靠性保证是基础，我们基于这些基础之上构建我们的应用。比如关系型数据库的可靠性保证是ACID，也就是原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability）。 Kafka 中的可靠性保证有如下四点： 对于一个分区来说，它的消息是有序的。如果一个生产者向一个分区先写入消息A，然后写入消息B，那么消费者会先读取消息A再读取消息B。 当消息写入所有in-sync状态的副本后，消息才会认为已提交（committed）。这里的写入有可能只是写入到文件系统的缓存，不一定刷新到磁盘。生产者可以等待不同时机的确认，比如等待分区主副本写入即返回，后者等待所有in-sync状态副本写入才返回。 一旦消息已提交，那么只要有一个副本存活，数据不会丢失。 消费者只能读取到已提交的消息。 使用这些基础保证，我们构建一个可靠的系统，这时候需要考虑一个问题：究竟我们的应用需要多大程度的可靠性？可靠性不是无偿的，它与系统可用性、吞吐量、延迟和硬件价格息息相关，得此失彼。因此，我们往往需要做权衡，一味的追求可靠性并不实际。 想了解更多戳这里：http://www.dengshenyu.com/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2017/11/21/kafka-data-delivery.html 三、动手搭一个 Kafka 通过上面的描述，我们已经大致了解到了「Kafka」是何方神圣了，现在我们开始尝试自己动手本地搭一个来实际体验一把。 第一步：下载 Kafka 这里以 Mac OS 为例，在安装了 Homebrew 的情况下执行下列代码： brew install kafka 由于 Kafka 依赖了 Zookeeper，所以在下载的时候会自动下载。 第二步：启动服务 我们在启动之前首先需要修改 Kafka 的监听地址和端口为 localhost:9092： vi /usr/local/etc/kafka/server.properties 然后修改成下图的样子： 依次启动 Zookeeper 和 Kafka： brew services start zookeeper brew services start kafka 然后执行下列语句来创建一个名字为 “test” 的 Topic： kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test 我们可以通过下列的命令查看我们的 Topic 列表： kafka-topics --list --zookeeper localhost:2181 第三步：发送消息 然后我们新建一个控制台，运行下列命令创建一个消费者关注刚才创建的 Topic： kafka-console-consumer --bootstrap-server localhost:9092 --topic test --from-beginning 用控制台往刚才创建的 Topic 中添加消息，并观察刚才创建的消费者窗口： kafka-console-producer --broker-list localhost:9092 --topic test 能通过消费者窗口观察到正确的消息： 参考资料 https://www.infoq.cn/article/kafka-analysis-part-1 - Kafka 设计解析（一）：Kafka 背景及架构介绍 http://www.dengshenyu.com/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2017/11/06/kafka-Meet-Kafka.html - Kafka系列（一）初识Kafka https://lotabout.me/2018/kafka-introduction/ - Kafka 入门介绍 https://www.zhihu.com/question/28925721 - Kafka 中的 Topic 为什么要进行分区? - 知乎 https://blog.joway.io/posts/kafka-design-practice/ - Kafka 的设计与实践思考 http://www.dengshenyu.com/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2017/11/21/kafka-data-delivery.html - Kafka系列（六）可靠的数据传输 "},"zother6-JavaGuide/system-design/data-communication/message-queue.html":{"url":"zother6-JavaGuide/system-design/data-communication/message-queue.html","title":"Message Queue","keywords":"","body":" 消息队列其实很简单 一 什么是消息队列 二 为什么要用消息队列 (1) 通过异步处理提高系统性能（削峰、减少响应所需时间） (2) 降低系统耦合性 三 使用消息队列带来的一些问题 四 JMS VS AMQP 4.1 JMS 4.1.1 JMS 简介 4.1.2 JMS两种消息模型 4.1.3 JMS 五种不同的消息正文格式 4.2 AMQP 4.3 JMS vs AMQP 五 常见的消息队列对比 消息队列其实很简单 　　“RabbitMQ？”“Kafka？”“RocketMQ？”...在日常学习与开发过程中，我们常常听到消息队列这个关键词。我也在我的多篇文章中提到了这个概念。可能你是熟练使用消息队列的老手，又或者你是不懂消息队列的新手，不论你了不了解消息队列，本文都将带你搞懂消息队列的一些基本理论。如果你是老手，你可能从本文学到你之前不曾注意的一些关于消息队列的重要概念，如果你是新手，相信本文将是你打开消息队列大门的一板砖。 一 什么是消息队列 　　我们可以把消息队列比作是一个存放消息的容器，当我们需要使用消息的时候可以取出消息供自己使用。消息队列是分布式系统中重要的组件，使用消息队列主要是为了通过异步处理提高系统性能和削峰、降低系统耦合性。目前使用较多的消息队列有ActiveMQ，RabbitMQ，Kafka，RocketMQ，我们后面会一一对比这些消息队列。 　　另外，我们知道队列 Queue 是一种先进先出的数据结构，所以消费消息时也是按照顺序来消费的。比如生产者发送消息1,2,3...对于消费者就会按照1,2,3...的顺序来消费。但是偶尔也会出现消息被消费的顺序不对的情况，比如某个消息消费失败又或者一个 queue 多个consumer 也会导致消息被消费的顺序不对，我们一定要保证消息被消费的顺序正确。 　　除了上面说的消息消费顺序的问题，使用消息队列，我们还要考虑如何保证消息不被重复消费？如何保证消息的可靠性传输（如何处理消息丢失的问题）？......等等问题。所以说使用消息队列也不是十全十美的，使用它也会让系统可用性降低、复杂度提高，另外需要我们保障一致性等问题。 二 为什么要用消息队列 　　我觉得使用消息队列主要有两点好处：1.通过异步处理提高系统性能（削峰、减少响应所需时间）;2.降低系统耦合性。如果在面试的时候你被面试官问到这个问题的话，一般情况是你在你的简历上涉及到消息队列这方面的内容，这个时候推荐你结合你自己的项目来回答。 　　《大型网站技术架构》第四章和第七章均有提到消息队列对应用性能及扩展性的提升。 (1) 通过异步处理提高系统性能（削峰、减少响应所需时间） 　　如上图，在不使用消息队列服务器的时候，用户的请求数据直接写入数据库，在高并发的情况下数据库压力剧增，使得响应速度变慢。但是在使用消息队列之后，用户的请求数据发送给消息队列之后立即 返回，再由消息队列的消费者进程从消息队列中获取数据，异步写入数据库。由于消息队列服务器处理速度快于数据库（消息队列也比数据库有更好的伸缩性），因此响应速度得到大幅改善。 　　通过以上分析我们可以得出消息队列具有很好的削峰作用的功能——即通过异步处理，将短时间高并发产生的事务消息存储在消息队列中，从而削平高峰期的并发事务。 举例：在电子商务一些秒杀、促销活动中，合理使用消息队列可以有效抵御促销活动刚开始大量订单涌入对系统的冲击。如下图所示： 　　因为用户请求数据写入消息队列之后就立即返回给用户了，但是请求数据在后续的业务校验、写数据库等操作中可能失败。因此使用消息队列进行异步处理之后，需要适当修改业务流程进行配合，比如用户在提交订单之后，订单数据写入消息队列，不能立即返回用户订单提交成功，需要在消息队列的订单消费者进程真正处理完该订单之后，甚至出库后，再通过电子邮件或短信通知用户订单成功，以免交易纠纷。这就类似我们平时手机订火车票和电影票。 (2) 降低系统耦合性 　　使用消息队列还可以降低系统耦合性。我们知道如果模块之间不存在直接调用，那么新增模块或者修改模块就对其他模块影响较小，这样系统的可扩展性无疑更好一些。还是直接上图吧： 　　生产者（客户端）发送消息到消息队列中去，接受者（服务端）处理消息，需要消费的系统直接去消息队列取消息进行消费即可而不需要和其他系统有耦合， 这显然也提高了系统的扩展性。 　　消息队列使利用发布-订阅模式工作，消息发送者（生产者）发布消息，一个或多个消息接受者（消费者）订阅消息。 从上图可以看到消息发送者（生产者）和消息接受者（消费者）之间没有直接耦合，消息发送者将消息发送至分布式消息队列即结束对消息的处理，消息接受者从分布式消息队列获取该消息后进行后续处理，并不需要知道该消息从何而来。对新增业务，只要对该类消息感兴趣，即可订阅该消息，对原有系统和业务没有任何影响，从而实现网站业务的可扩展性设计。 　　消息接受者对消息进行过滤、处理、包装后，构造成一个新的消息类型，将消息继续发送出去，等待其他消息接受者订阅该消息。因此基于事件（消息对象）驱动的业务架构可以是一系列流程。 　　另外为了避免消息队列服务器宕机造成消息丢失，会将成功发送到消息队列的消息存储在消息生产者服务器上，等消息真正被消费者服务器处理后才删除消息。在消息队列服务器宕机后，生产者服务器会选择分布式消息队列服务器集群中的其他服务器发布消息。 备注： 不要认为消息队列只能利用发布-订阅模式工作，只不过在解耦这个特定业务环境下是使用发布-订阅模式的。除了发布-订阅模式，还有点对点订阅模式（一个消息只有一个消费者），我们比较常用的是发布-订阅模式。 另外，这两种消息模型是 JMS 提供的，AMQP 协议还提供了 5 种消息模型。 三 使用消息队列带来的一些问题 系统可用性降低： 系统可用性在某种程度上降低，为什么这样说呢？在加入MQ之前，你不用考虑消息丢失或者说MQ挂掉等等的情况，但是，引入MQ之后你就需要去考虑了！ 系统复杂性提高： 加入MQ之后，你需要保证消息没有被重复消费、处理消息丢失的情况、保证消息传递的顺序性等等问题！ 一致性问题： 我上面讲了消息队列可以实现异步，消息队列带来的异步确实可以提高系统响应速度。但是，万一消息的真正消费者并没有正确消费消息怎么办？这样就会导致数据不一致的情况了! 四 JMS VS AMQP 4.1 JMS 4.1.1 JMS 简介 　　JMS（JAVA Message Service,java消息服务）是java的消息服务，JMS的客户端之间可以通过JMS服务进行异步的消息传输。JMS（JAVA Message Service，Java消息服务）API是一个消息服务的标准或者说是规范，允许应用程序组件基于JavaEE平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。 ActiveMQ 就是基于 JMS 规范实现的。 4.1.2 JMS两种消息模型 ①点到点（P2P）模型 　　 使用队列（Queue）作为消息通信载体；满足生产者与消费者模式，一条消息只能被一个消费者使用，未被消费的消息在队列中保留直到被消费或超时。比如：我们生产者发送100条消息的话，两个消费者来消费一般情况下两个消费者会按照消息发送的顺序各自消费一半（也就是你一个我一个的消费。） ② 发布/订阅（Pub/Sub）模型 　　 发布订阅模型（Pub/Sub） 使用主题（Topic）作为消息通信载体，类似于广播模式；发布者发布一条消息，该消息通过主题传递给所有的订阅者，在一条消息广播之后才订阅的用户则是收不到该条消息的。 4.1.3 JMS 五种不同的消息正文格式 　　JMS定义了五种不同的消息正文格式，以及调用的消息类型，允许你发送并接收以一些不同形式的数据，提供现有消息格式的一些级别的兼容性。 StreamMessage -- Java原始值的数据流 MapMessage--一套名称-值对 TextMessage--一个字符串对象 ObjectMessage--一个序列化的 Java对象 BytesMessage--一个字节的数据流 4.2 AMQP 　　​ AMQP，即Advanced Message Queuing Protocol，一个提供统一消息服务的应用层标准 高级消息队列协议（二进制应用层协议），是应用层协议的一个开放标准,为面向消息的中间件设计，兼容 JMS。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件同产品，不同的开发语言等条件的限制。 RabbitMQ 就是基于 AMQP 协议实现的。 4.3 JMS vs AMQP 对比方向 JMS AMQP 定义 Java API 协议 跨语言 否 是 跨平台 否 是 支持消息类型 提供两种消息模型：①Peer-2-Peer;②Pub/sub 提供了五种消息模型：①direct exchange；②fanout exchange；③topic change；④headers exchange；⑤system exchange。本质来讲，后四种和JMS的pub/sub模型没有太大差别，仅是在路由机制上做了更详细的划分； 支持消息类型 支持多种消息类型 ，我们在上面提到过 byte[]（二进制） 总结： AMQP 为消息定义了线路层（wire-level protocol）的协议，而JMS所定义的是API规范。在 Java 体系中，多个client均可以通过JMS进行交互，不需要应用修改代码，但是其对跨平台的支持较差。而AMQP天然具有跨平台、跨语言特性。 JMS 支持TextMessage、MapMessage 等复杂的消息类型；而 AMQP 仅支持 byte[] 消息类型（复杂的类型可序列化后发送）。 由于Exchange 提供的路由算法，AMQP可以提供多样化的路由方式来传递消息到消息队列，而 JMS 仅支持 队列 和 主题/订阅 方式两种。 五 常见的消息队列对比 对比方向 概要 吞吐量 万级的 ActiveMQ 和 RabbitMQ 的吞吐量（ActiveMQ 的性能最差）要比 十万级甚至是百万级的 RocketMQ 和 Kafka 低一个数量级。 可用性 都可以实现高可用。ActiveMQ 和 RabbitMQ 都是基于主从架构实现高可用性。RocketMQ 基于分布式架构。 kafka 也是分布式的，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 时效性 RabbitMQ 基于erlang开发，所以并发能力很强，性能极其好，延时很低，达到微秒级。其他三个都是 ms 级。 功能支持 除了 Kafka，其他三个功能都较为完备。 Kafka 功能较为简单，主要支持简单的MQ功能，在大数据领域的实时计算以及日志采集被大规模使用，是事实上的标准 消息丢失 ActiveMQ 和 RabbitMQ 丢失的可能性非常低， RocketMQ 和 Kafka 理论上不会丢失。 总结： ActiveMQ 的社区算是比较成熟，但是较目前来说，ActiveMQ 的性能比较差，而且版本迭代很慢，不推荐使用。 RabbitMQ 在吞吐量方面虽然稍逊于 Kafka 和 RocketMQ ，但是由于它基于 erlang 开发，所以并发能力很强，性能极其好，延时很低，达到微秒级。但是也因为 RabbitMQ 基于 erlang 开发，所以国内很少有公司有实力做erlang源码级别的研究和定制。如果业务场景对并发量要求不是太高（十万级、百万级），那这四种消息队列中，RabbitMQ 一定是你的首选。如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。 RocketMQ 阿里出品，Java 系开源项目，源代码我们可以直接阅读，然后可以定制自己公司的MQ，并且 RocketMQ 有阿里巴巴的实际业务场景的实战考验。RocketMQ 社区活跃度相对较为一般，不过也还可以，文档相对来说简单一些，然后接口这块不是按照标准 JMS 规范走的有些系统要迁移需要修改大量代码。还有就是阿里出台的技术，你得做好这个技术万一被抛弃，社区黄掉的风险，那如果你们公司有技术实力我觉得用RocketMQ 挺好的 kafka 的特点其实很明显，就是仅仅提供较少的核心功能，但是提供超高的吞吐量，ms 级的延迟，极高的可用性以及可靠性，而且分布式可以任意扩展。同时 kafka 最好是支撑较少的 topic 数量即可，保证其超高吞吐量。kafka 唯一的一点劣势是有可能消息重复消费，那么对数据准确性会造成极其轻微的影响，在大数据领域中以及日志采集中，这点轻微影响可以忽略这个特性天然适合大数据实时计算以及日志收集。 参考：《Java工程师面试突击第1季-中华石杉老师》 "},"zother6-JavaGuide/system-design/data-communication/rabbitmq.html":{"url":"zother6-JavaGuide/system-design/data-communication/rabbitmq.html","title":"Rabbitmq","keywords":"","body":" 一文搞懂 RabbitMQ 的重要概念以及安装 一 RabbitMQ 介绍 1.1 RabbitMQ 简介 1.2 RabbitMQ 核心概念 1.2.1 Producer(生产者) 和 Consumer(消费者) 1.2.2 Exchange(交换器) 1.2.3 Queue(消息队列) 1.2.4 Broker（消息中间件的服务节点） 1.2.5 Exchange Types(交换器类型) ① fanout ② direct ③ topic ④ headers(不推荐) 二 安装 RabbitMq 2.1 安装 erlang 2.2 安装 RabbitMQ 一文搞懂 RabbitMQ 的重要概念以及安装 一 RabbitMQ 介绍 这部分参考了 《RabbitMQ实战指南》这本书的第 1 章和第 2 章。 1.1 RabbitMQ 简介 RabbitMQ 是采用 Erlang 语言实现 AMQP(Advanced Message Queuing Protocol，高级消息队列协议）的消息中间件，它最初起源于金融系统，用于在分布式系统中存储转发消息。 RabbitMQ 发展到今天，被越来越多的人认可，这和它在易用性、扩展性、可靠性和高可用性等方面的卓著表现是分不开的。RabbitMQ 的具体特点可以概括为以下几点： 可靠性： RabbitMQ使用一些机制来保证消息的可靠性，如持久化、传输确认及发布确认等。 灵活的路由： 在消息进入队列之前，通过交换器来路由消息。对于典型的路由功能，RabbitMQ 己经提供了一些内置的交换器来实现。针对更复杂的路由功能，可以将多个交换器绑定在一起，也可以通过插件机制来实现自己的交换器。这个后面会在我们将 RabbitMQ 核心概念的时候详细介绍到。 扩展性： 多个RabbitMQ节点可以组成一个集群，也可以根据实际业务情况动态地扩展集群中节点。 高可用性： 队列可以在集群中的机器上设置镜像，使得在部分节点出现问题的情况下队列仍然可用。 支持多种协议： RabbitMQ 除了原生支持 AMQP 协议，还支持 STOMP、MQTT 等多种消息中间件协议。 多语言客户端： RabbitMQ几乎支持所有常用语言，比如 Java、Python、Ruby、PHP、C#、JavaScript等。 易用的管理界面： RabbitMQ提供了一个易用的用户界面，使得用户可以监控和管理消息、集群中的节点等。在安装 RabbitMQ 的时候会介绍到，安装好 RabbitMQ 就自带管理界面。 插件机制： RabbitMQ 提供了许多插件，以实现从多方面进行扩展，当然也可以编写自己的插件。感觉这个有点类似 Dubbo 的 SPI机制。 1.2 RabbitMQ 核心概念 RabbitMQ 整体上是一个生产者与消费者模型，主要负责接收、存储和转发消息。可以把消息传递的过程想象成：当你将一个包裹送到邮局，邮局会暂存并最终将邮件通过邮递员送到收件人的手上，RabbitMQ就好比由邮局、邮箱和邮递员组成的一个系统。从计算机术语层面来说，RabbitMQ 模型更像是一种交换机模型。 下面再来看看图1—— RabbitMQ 的整体模型架构。 下面我会一一介绍上图中的一些概念。 1.2.1 Producer(生产者) 和 Consumer(消费者) Producer(生产者) :生产消息的一方（邮件投递者） Consumer(消费者) :消费消息的一方（邮件收件人） 消息一般由 2 部分组成：消息头（或者说是标签 Label）和 消息体。消息体也可以称为 payLoad ,消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括 routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。生产者把消息交由 RabbitMQ 后，RabbitMQ 会根据消息头把消息发送给感兴趣的 Consumer(消费者)。 1.2.2 Exchange(交换器) 在 RabbitMQ 中，消息并不是直接被投递到 Queue(消息队列) 中的，中间还必须经过 Exchange(交换器) 这一层，Exchange(交换器) 会把我们的消息分配到对应的 Queue(消息队列) 中。 Exchange(交换器) 用来接收生产者发送的消息并将这些消息路由给服务器中的队列中，如果路由不到，或许会返回给 Producer(生产者) ，或许会被直接丢弃掉 。这里可以将RabbitMQ中的交换器看作一个简单的实体。 RabbitMQ 的 Exchange(交换器) 有4种类型，不同的类型对应着不同的路由策略：direct(默认)，fanout, topic, 和 headers，不同类型的Exchange转发消息的策略有所区别。这个会在介绍 Exchange Types(交换器类型) 的时候介绍到。 Exchange(交换器) 示意图如下： 生产者将消息发给交换器的时候，一般会指定一个 RoutingKey(路由键)，用来指定这个消息的路由规则，而这个 RoutingKey 需要与交换器类型和绑定键(BindingKey)联合使用才能最终生效。 RabbitMQ 中通过 Binding(绑定) 将 Exchange(交换器) 与 Queue(消息队列) 关联起来，在绑定的时候一般会指定一个 BindingKey(绑定建) ,这样 RabbitMQ 就知道如何正确将消息路由到队列了,如下图所示。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。Exchange 和 Queue 的绑定可以是多对多的关系。 Binding(绑定) 示意图： 生产者将消息发送给交换器时，需要一个RoutingKey,当 BindingKey 和 RoutingKey 相匹配时，消息会被路由到对应的队列中。在绑定多个队列到同一个交换器的时候，这些绑定允许使用相同的 BindingKey。BindingKey 并不是在所有的情况下都生效，它依赖于交换器类型，比如fanout类型的交换器就会无视，而是将消息路由到所有绑定到该交换器的队列中。 1.2.3 Queue(消息队列) Queue(消息队列) 用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。 RabbitMQ 中消息只能存储在 队列 中，这一点和 Kafka 这种消息中间件相反。Kafka 将消息存储在 topic（主题） 这个逻辑层面，而相对应的队列逻辑只是topic实际存储文件中的位移标识。 RabbitMQ 的生产者生产消息并最终投递到队列中，消费者可以从队列中获取消息并消费。 多个消费者可以订阅同一个队列，这时队列中的消息会被平均分摊（Round-Robin，即轮询）给多个消费者进行处理，而不是每个消费者都收到所有的消息并处理，这样避免的消息被重复消费。 RabbitMQ 不支持队列层面的广播消费,如果有广播消费的需求，需要在其上进行二次开发,这样会很麻烦，不建议这样做。 1.2.4 Broker（消息中间件的服务节点） 对于 RabbitMQ 来说，一个 RabbitMQ Broker 可以简单地看作一个 RabbitMQ 服务节点，或者RabbitMQ服务实例。大多数情况下也可以将一个 RabbitMQ Broker 看作一台 RabbitMQ 服务器。 下图展示了生产者将消息存入 RabbitMQ Broker,以及消费者从Broker中消费数据的整个流程。 这样图1中的一些关于 RabbitMQ 的基本概念我们就介绍完毕了，下面再来介绍一下 Exchange Types(交换器类型) 。 1.2.5 Exchange Types(交换器类型) RabbitMQ 常用的 Exchange Type 有 fanout、direct、topic、headers 这四种（AMQP规范里还提到两种 Exchange Type，分别为 system 与 自定义，这里不予以描述）。 ① fanout fanout 类型的Exchange路由规则非常简单，它会把所有发送到该Exchange的消息路由到所有与它绑定的Queue中，不需要做任何判断操作，所以 fanout 类型是所有的交换机类型里面速度最快的。fanout 类型常用来广播消息。 ② direct direct 类型的Exchange路由规则也很简单，它会把消息路由到那些 Bindingkey 与 RoutingKey 完全匹配的 Queue 中。 以上图为例，如果发送消息的时候设置路由键为“warning”,那么消息会路由到 Queue1 和 Queue2。如果在发送消息的时候设置路由键为\"Info”或者\"debug”，消息只会路由到Queue2。如果以其他的路由键发送消息，则消息不会路由到这两个队列中。 direct 类型常用在处理有优先级的任务，根据任务的优先级把消息发送到对应的队列，这样可以指派更多的资源去处理高优先级的队列。 ③ topic 前面讲到direct类型的交换器路由规则是完全匹配 BindingKey 和 RoutingKey ，但是这种严格的匹配方式在很多情况下不能满足实际业务的需求。topic类型的交换器在匹配规则上进行了扩展，它与 direct 类型的交换器相似，也是将消息路由到 BindingKey 和 RoutingKey 相匹配的队列中，但这里的匹配规则有些不同，它约定： RoutingKey 为一个点号“．”分隔的字符串（被点号“．”分隔开的每一段独立的字符串称为一个单词），如 “com.rabbitmq.client”、“java.util.concurrent”、“com.hidden.client”; BindingKey 和 RoutingKey 一样也是点号“．”分隔的字符串； BindingKey 中可以存在两种特殊字符串“”和“#”，用于做模糊匹配，其中“”用于匹配一个单词，“#”用于匹配多个单词(可以是零个)。 以上图为例： 路由键为 “com.rabbitmq.client” 的消息会同时路由到 Queuel 和 Queue2; 路由键为 “com.hidden.client” 的消息只会路由到 Queue2 中； 路由键为 “com.hidden.demo” 的消息只会路由到 Queue2 中； 路由键为 “java.rabbitmq.demo” 的消息只会路由到Queuel中； 路由键为 “java.util.concurrent” 的消息将会被丢弃或者返回给生产者（需要设置 mandatory 参数），因为它没有匹配任何路由键。 ④ headers(不推荐) headers 类型的交换器不依赖于路由键的匹配规则来路由消息，而是根据发送的消息内容中的 headers 属性进行匹配。在绑定队列和交换器时制定一组键值对，当发送消息到交换器时，RabbitMQ会获取到该消息的 headers（也是一个键值对的形式)'对比其中的键值对是否完全匹配队列和交换器绑定时指定的键值对，如果完全匹配则消息会路由到该队列，否则不会路由到该队列。headers 类型的交换器性能会很差，而且也不实用，基本上不会看到它的存在。 二 安装 RabbitMq 通过 Docker 安装非常方便，只需要几条命令就好了，我这里是只说一下常规安装方法。 前面提到了 RabbitMQ 是由 Erlang语言编写的，也正因如此，在安装RabbitMQ 之前需要安装 Erlang。 注意：在安装 RabbitMQ 的时候需要注意 RabbitMQ 和 Erlang 的版本关系，如果不注意的话会导致出错，两者对应关系如下: 2.1 安装 erlang 1 下载 erlang 安装包 在官网下载然后上传到 Linux 上或者直接使用下面的命令下载对应的版本。 [root@SnailClimb local]#wget http://erlang.org/download/otp_src_19.3.tar.gz erlang 官网下载：http://www.erlang.org/downloads 2 解压 erlang 安装包 [root@SnailClimb local]#tar -xvzf otp_src_19.3.tar.gz 3 删除 erlang 安装包 [root@SnailClimb local]#rm -rf otp_src_19.3.tar.gz 4 安装 erlang 的依赖工具 [root@SnailClimb local]#yum -y install make gcc gcc-c++ kernel-devel m4 ncurses-devel openssl-devel unixODBC-devel 5 进入erlang 安装包解压文件对 erlang 进行安装环境的配置 新建一个文件夹 [root@SnailClimb local]# mkdir erlang 对 erlang 进行安装环境的配置 [root@SnailClimb otp_src_19.3]# ./configure --prefix=/usr/local/erlang --without-javac 6 编译安装 [root@SnailClimb otp_src_19.3]# make && make install 7 验证一下 erlang 是否安装成功了 [root@SnailClimb otp_src_19.3]# ./bin/erl 运行下面的语句输出“hello world” io:format(\"hello world~n\", []). 大功告成，我们的 erlang 已经安装完成。 8 配置 erlang 环境变量 [root@SnailClimb etc]# vim profile 追加下列环境变量到文件末尾 #erlang ERL_HOME=/usr/local/erlang PATH=$ERL_HOME/bin:$PATH export ERL_HOME PATH 运行下列命令使配置文件profile生效 [root@SnailClimb etc]# source /etc/profile 输入 erl 查看 erlang 环境变量是否配置正确 [root@SnailClimb etc]# erl 2.2 安装 RabbitMQ 1. 下载rpm wget https://www.rabbitmq.com/releases/rabbitmq-server/v3.6.8/rabbitmq-server-3.6.8-1.el7.noarch.rpm 或者直接在官网下载 https://www.rabbitmq.com/install-rpm.html 2. 安装rpm rpm --import https://www.rabbitmq.com/rabbitmq-release-signing-key.asc 紧接着执行： yum install rabbitmq-server-3.6.8-1.el7.noarch.rpm 中途需要你输入\"y\"才能继续安装。 3 开启 web 管理插件 rabbitmq-plugins enable rabbitmq_management 4 设置开机启动 chkconfig rabbitmq-server on 4. 启动服务 service rabbitmq-server start 5. 查看服务状态 service rabbitmq-server status 6. 访问 RabbitMQ 控制台 浏览器访问：http://你的ip地址:15672/ 默认用户名和密码： guest/guest;但是需要注意的是：guestuest用户只是被容许从localhost访问。官网文档描述如下： “guest” user can only connect via localhost 解决远程访问 RabbitMQ 远程访问密码错误 新建用户并授权 [root@SnailClimb rabbitmq]# rabbitmqctl add_user root root Creating user \"root\" ... [root@SnailClimb rabbitmq]# rabbitmqctl set_user_tags root administrator Setting tags for user \"root\" to [administrator] ... [root@SnailClimb rabbitmq]# [root@SnailClimb rabbitmq]# rabbitmqctl set_permissions -p / root \".*\" \".*\" \".*\" Setting permissions for user \"root\" in vhost \"/\" ... 再次访问:http://你的ip地址:15672/ ,输入用户名和密码：root root "},"zother6-JavaGuide/system-design/data-communication/RocketMQ-Questions.html":{"url":"zother6-JavaGuide/system-design/data-communication/RocketMQ-Questions.html","title":"Rocket MQ Questions","keywords":"","body":"本文来自读者 PR。 1 单机版消息中心 2 分布式消息中心 2.1 问题与解决 2.1.1 消息丢失的问题 2.1.2 同步落盘怎么才能快 2.1.3 消息堆积的问题 2.1.4 定时消息的实现 2.1.5 顺序消息的实现 2.1.6 分布式消息的实现 2.1.7 消息的 push 实现 2.1.8 消息重复发送的避免 2.1.9 广播消费与集群消费 2.1.10 RocketMQ 不使用 ZooKeeper 作为注册中心的原因，以及自制的 NameServer 优缺点？ 2.1.11 其它 3 参考 1 单机版消息中心 一个消息中心，最基本的需要支持多生产者、多消费者，例如下： class Scratch { public static void main(String[] args) { // 实际中会有 nameserver 服务来找到 broker 具体位置以及 broker 主从信息 Broker broker = new Broker(); Producer producer1 = new Producer(); producer1.connectBroker(broker); Producer producer2 = new Producer(); producer2.connectBroker(broker); Consumer consumer1 = new Consumer(); consumer1.connectBroker(broker); Consumer consumer2 = new Consumer(); consumer2.connectBroker(broker); for (int i = 0; i { broker.sendMsg(msg); }).start(); } } class Consumer { private Broker broker; public void connectBroker(Broker broker) { this.broker = broker; } public String syncPullMsg() { return broker.getMsg(); } } class Broker { // 对应 RocketMQ 中 MessageQueue，默认情况下 1 个 Topic 包含 4 个 MessageQueue private LinkedBlockingQueue messageQueue = new LinkedBlockingQueue(Integer.MAX_VALUE); // 实际发送消息到 broker 服务器使用 Netty 发送 public void sendMsg(String msg) { try { messageQueue.put(msg); // 实际会同步或异步落盘，异步落盘使用的定时任务定时扫描落盘 } catch (InterruptedException e) { } } public String getMsg() { try { return messageQueue.take(); } catch (InterruptedException e) { } return null; } public String getAllMagByDisk() { StringBuilder sb = new StringBuilder(\"\\n\"); messageQueue.iterator().forEachRemaining((msg) -> { sb.append(msg + \"\\n\"); }); return sb.toString(); } } 问题： 没有实现真正执行消息存储落盘 没有实现 NameServer 去作为注册中心，定位服务 使用 LinkedBlockingQueue 作为消息队列，注意，参数是无限大，在真正 RocketMQ 也是如此是无限大，理论上不会出现对进来的数据进行抛弃，但是会有内存泄漏问题（阿里巴巴开发手册也因为这个问题，建议我们使用自制线程池） 没有使用多个队列（即多个 LinkedBlockingQueue），RocketMQ 的顺序消息是通过生产者和消费者同时使用同一个 MessageQueue 来实现，但是如果我们只有一个 MessageQueue，那我们天然就支持顺序消息 没有使用 MappedByteBuffer 来实现文件映射从而使消息数据落盘非常的快（实际 RocketMQ 使用的是 FileChannel+DirectBuffer） 2 分布式消息中心 2.1 问题与解决 2.1.1 消息丢失的问题 当你系统需要保证百分百消息不丢失，你可以使用生产者每发送一个消息，Broker 同步返回一个消息发送成功的反馈消息 即每发送一个消息，同步落盘后才返回生产者消息发送成功，这样只要生产者得到了消息发送生成的返回，事后除了硬盘损坏，都可以保证不会消息丢失 但是这同时引入了一个问题，同步落盘怎么才能快？ 2.1.2 同步落盘怎么才能快 使用 FileChannel + DirectBuffer 池，使用堆外内存，加快内存拷贝 使用数据和索引分离，当消息需要写入时，使用 commitlog 文件顺序写，当需要定位某个消息时，查询index 文件来定位，从而减少文件IO随机读写的性能损耗 2.1.3 消息堆积的问题 后台定时任务每隔72小时，删除旧的没有使用过的消息信息 根据不同的业务实现不同的丢弃任务，具体参考线程池的 AbortPolicy，例如FIFO/LRU等（RocketMQ没有此策略） 消息定时转移，或者对某些重要的 TAG 型（支付型）消息真正落库 2.1.4 定时消息的实现 实际 RocketMQ 没有实现任意精度的定时消息，它只支持某些特定的时间精度的定时消息 实现定时消息的原理是：创建特定时间精度的 MessageQueue，例如生产者需要定时1s之后被消费者消费，你只需要将此消息发送到特定的 Topic，例如：MessageQueue-1 表示这个 MessageQueue 里面的消息都会延迟一秒被消费，然后 Broker 会在 1s 后发送到消费者消费此消息，使用 newSingleThreadScheduledExecutor 实现 2.1.5 顺序消息的实现 与定时消息同原理，生产者生产消息时指定特定的 MessageQueue ，消费者消费消息时，消费特定的 MessageQueue，其实单机版的消息中心在一个 MessageQueue 就天然支持了顺序消息 注意：同一个 MessageQueue 保证里面的消息是顺序消费的前提是：消费者是串行的消费该 MessageQueue，因为就算 MessageQueue 是顺序的，但是当并行消费时，还是会有顺序问题，但是串行消费也同时引入了两个问题： 引入锁来实现串行 前一个消费阻塞时后面都会被阻塞 2.1.6 分布式消息的实现 需要前置知识：2PC RocketMQ4.3 起支持，原理为2PC，即两阶段提交，prepared->commit/rollback 生产者发送事务消息，假设该事务消息 Topic 为 Topic1-Trans，Broker 得到后首先更改该消息的 Topic 为 Topic1-Prepared，该 Topic1-Prepared 对消费者不可见。然后定时回调生产者的本地事务A执行状态，根据本地事务A执行状态，来是否将该消息修改为 Topic1-Commit 或 Topic1-Rollback，消费者就可以正常找到该事务消息或者不执行等 注意，就算是事务消息最后回滚了也不会物理删除，只会逻辑删除该消息 2.1.7 消息的 push 实现 注意，RocketMQ 已经说了自己会有低延迟问题，其中就包括这个消息的 push 延迟问题 因为这并不是真正的将消息主动的推送到消费者，而是 Broker 定时任务每5s将消息推送到消费者 2.1.8 消息重复发送的避免 RocketMQ 会出现消息重复发送的问题，因为在网络延迟的情况下，这种问题不可避免的发生，如果非要实现消息不可重复发送，那基本太难，因为网络环境无法预知，还会使程序复杂度加大，因此默认允许消息重复发送 RocketMQ 让使用者在消费者端去解决该问题，即需要消费者端在消费消息时支持幂等性的去消费消息 最简单的解决方案是每条消费记录有个消费状态字段，根据这个消费状态字段来是否消费或者使用一个集中式的表，来存储所有消息的消费状态，从而避免重复消费 具体实现可以查询关于消息幂等消费的解决方案 2.1.9 广播消费与集群消费 消息消费区别：广播消费，订阅该 Topic 的消息者们都会消费每个消息。集群消费，订阅该 Topic 的消息者们只会有一个去消费某个消息 消息落盘区别：具体表现在消息消费进度的保存上。广播消费，由于每个消费者都独立的去消费每个消息，因此每个消费者各自保存自己的消息消费进度。而集群消费下，订阅了某个 Topic，而旗下又有多个 MessageQueue，每个消费者都可能会去消费不同的 MessageQueue，因此总体的消费进度保存在 Broker 上集中的管理 2.1.10 RocketMQ 不使用 ZooKeeper 作为注册中心的原因，以及自制的 NameServer 优缺点？ ZooKeeper 作为支持顺序一致性的中间件，在某些情况下，它为了满足一致性，会丢失一定时间内的可用性，RocketMQ 需要注册中心只是为了发现组件地址，在某些情况下，RocketMQ 的注册中心可以出现数据不一致性，这同时也是 NameServer 的缺点，因为 NameServer 集群间互不通信，它们之间的注册信息可能会不一致 另外，当有新的服务器加入时，NameServer 并不会立马通知到 Produer，而是由 Produer 定时去请求 NameServer 获取最新的 Broker/Consumer 信息（这种情况是通过 Producer 发送消息时，负载均衡解决） 2.1.11 其它 加分项咯 包括组件通信间使用 Netty 的自定义协议 消息重试负载均衡策略（具体参考 Dubbo 负载均衡策略） 消息过滤器（Producer 发送消息到 Broker，Broker 存储消息信息，Consumer 消费时请求 Broker 端从磁盘文件查询消息文件时,在 Broker 端就使用过滤服务器进行过滤） Broker 同步双写和异步双写中 Master 和 Slave 的交互 Broker 在 4.5.0 版本更新中引入了\b基于 Raft 协议的多副本选举，之前这是商业版才有的特性 ISSUE-1046 3 参考 《RocketMQ技术内幕》：https://blog.csdn.net/prestigeding/article/details/85233529 关于 RocketMQ 对 MappedByteBuffer 的一点优化：https://lishoubo.github.io/2017/09/27/MappedByteBuffer%E7%9A%84%E4%B8%80%E7%82%B9%E4%BC%98%E5%8C%96/ 阿里中间件团队博客-十分钟入门RocketMQ：http://jm.taobao.org/2017/01/12/rocketmq-quick-start-in-10-minutes/ 分布式事务的种类以及 RocketMQ 支持的分布式消息：https://www.infoq.cn/article/2018/08/rocketmq-4.3-release 滴滴出行基于RocketMQ构建企业级消息队列服务的实践：https://yq.aliyun.com/articles/664608 基于《RocketMQ技术内幕》源码注释：https://github.com/LiWenGu/awesome-rocketmq "},"zother6-JavaGuide/system-design/data-communication/RocketMQ.html":{"url":"zother6-JavaGuide/system-design/data-communication/RocketMQ.html","title":"Rocket MQ","keywords":"","body":" 文章很长，点赞再看，养成好习惯😋😋😋 本文由 FrancisQ 老哥投稿！ 消息队列扫盲 消息队列顾名思义就是存放消息的队列，队列我就不解释了，别告诉我你连队列都不知道似啥吧？ 所以问题并不是消息队列是什么，而是 消息队列为什么会出现？消息队列能用来干什么？用它来干这些事会带来什么好处？消息队列会带来副作用吗？ 消息队列为什么会出现？ 消息队列算是作为后端程序员的一个必备技能吧，因为分布式应用必定涉及到各个系统之间的通信问题，这个时候消息队列也应运而生了。可以说分布式的产生是消息队列的基础，而分布式怕是一个很古老的概念了吧，所以消息队列也是一个很古老的中间件了。 消息队列能用来干什么？ 异步 你可能会反驳我，应用之间的通信又不是只能由消息队列解决，好好的通信为什么中间非要插一个消息队列呢？我不能直接进行通信吗？ 很好👍，你又提出了一个概念，同步通信。就比如现在业界使用比较多的 Dubbo 就是一个适用于各个系统之间同步通信的 RPC 框架。 我来举个🌰吧，比如我们有一个购票系统，需求是用户在购买完之后能接收到购买完成的短信。 我们省略中间的网络通信时间消耗，假如购票系统处理需要 150ms ，短信系统处理需要 200ms ，那么整个处理流程的时间消耗就是 150ms + 200ms = 350ms。 当然，乍看没什么问题。可是仔细一想你就感觉有点问题，我用户购票在购票系统的时候其实就已经完成了购买，而我现在通过同步调用非要让整个请求拉长时间，而短息系统这玩意又不是很有必要，它仅仅是一个辅助功能增强用户体验感而已。我现在整个调用流程就有点 头重脚轻 的感觉了，购票是一个不太耗时的流程，而我现在因为同步调用，非要等待发送短信这个比较耗时的操作才返回结果。那我如果再加一个发送邮件呢？ 这样整个系统的调用链又变长了，整个时间就变成了550ms。 当我们在学生时代需要在食堂排队的时候，我们和食堂大妈就是一个同步的模型。 我们需要告诉食堂大妈：“姐姐，给我加个鸡腿，再加个酸辣土豆丝，帮我浇点汁上去，多打点饭哦😋😋😋” 咦~~~ 为了多吃点，真恶心。 然后大妈帮我们打饭配菜，我们看着大妈那颤抖的手和掉落的土豆丝不禁咽了咽口水。 最终我们从大妈手中接过饭菜然后去寻找座位了... 回想一下，我们在给大妈发送需要的信息之后我们是 同步等待大妈给我配好饭菜 的，上面我们只是加了鸡腿和土豆丝，万一我再加一个番茄牛腩，韭菜鸡蛋，这样是不是大妈打饭配菜的流程就会变长，我们等待的时间也会相应的变长。 那后来，我们工作赚钱了有钱去饭店吃饭了，我们告诉服务员来一碗牛肉面加个荷包蛋 (传达一个消息) ，然后我们就可以在饭桌上安心的玩手机了 (干自己其他事情) ，等到我们的牛肉面上了我们就可以吃了。这其中我们也就传达了一个消息，然后我们又转过头干其他事情了。这其中虽然做面的时间没有变短，但是我们只需要传达一个消息就可以看其他事情了，这是一个 异步 的概念。 所以，为了解决这一个问题，聪明的程序员在中间也加了个类似于服务员的中间件——消息队列。这个时候我们就可以把模型给改造了。 这样，我们在将消息存入消息队列之后我们就可以直接返回了(我们告诉服务员我们要吃什么然后玩手机)，所以整个耗时只是 150ms + 10ms = 160ms。 但是你需要注意的是，整个流程的时长是没变的，就像你仅仅告诉服务员要吃什么是不会影响到做面的速度的。 解耦 回到最初同步调用的过程，我们写个伪代码简单概括一下。 那么第二步，我们又添加了一个发送邮件，我们就得重新去修改代码，如果我们又加一个需求：用户购买完还需要给他加积分，这个时候我们是不是又得改代码？ 如果你觉得还行，那么我这个时候不要发邮件这个服务了呢，我是不是又得改代码，又得重启应用？ 这样改来改去是不是很麻烦，那么 此时我们就用一个消息队列在中间进行解耦 。你需要注意的是，我们后面的发送短信、发送邮件、添加积分等一些操作都依赖于上面的 result ，这东西抽象出来就是购票的处理结果呀，比如订单号，用户账号等等，也就是说我们后面的一系列服务都是需要同样的消息来进行处理。既然这样，我们是不是可以通过 “广播消息” 来实现。 我上面所讲的“广播”并不是真正的广播，而是接下来的系统作为消费者去 订阅 特定的主题。比如我们这里的主题就可以叫做 订票 ，我们购买系统作为一个生产者去生产这条消息放入消息队列，然后消费者订阅了这个主题，会从消息队列中拉取消息并消费。就比如我们刚刚画的那张图，你会发现，在生产者这边我们只需要关注 生产消息到指定主题中 ，而 消费者只需要关注从指定主题中拉取消息 就行了。 如果没有消息队列，每当一个新的业务接入，我们都要在主系统调用新接口、或者当我们取消某些业务，我们也得在主系统删除某些接口调用。有了消息队列，我们只需要关心消息是否送达了队列，至于谁希望订阅，接下来收到消息如何处理，是下游的事情，无疑极大地减少了开发和联调的工作量。 削峰 我们再次回到一开始我们使用同步调用系统的情况，并且思考一下，如果此时有大量用户请求购票整个系统会变成什么样？ 如果，此时有一万的请求进入购票系统，我们知道运行我们主业务的服务器配置一般会比较好，所以这里我们假设购票系统能承受这一万的用户请求，那么也就意味着我们同时也会出现一万调用发短信服务的请求。而对于短信系统来说并不是我们的主要业务，所以我们配备的硬件资源并不会太高，那么你觉得现在这个短信系统能承受这一万的峰值么，且不说能不能承受，系统会不会 直接崩溃 了？ 短信业务又不是我们的主业务，我们能不能 折中处理 呢？如果我们把购买完成的信息发送到消息队列中，而短信系统 尽自己所能地去消息队列中取消息和消费消息 ，即使处理速度慢一点也无所谓，只要我们的系统没有崩溃就行了。 留得江山在，还怕没柴烧？你敢说每次发送验证码的时候是一发你就收到了的么？ 消息队列能带来什么好处？ 其实上面我已经说了。异步、解耦、削峰。 哪怕你上面的都没看懂也千万要记住这六个字，因为他不仅是消息队列的精华，更是编程和架构的精华。 消息队列会带来副作用吗？ 没有哪一门技术是“银弹”，消息队列也有它的副作用。 比如，本来好好的两个系统之间的调用，我中间加了个消息队列，如果消息队列挂了怎么办呢？是不是 降低了系统的可用性 ？ 那这样是不是要保证HA(高可用)？是不是要搞集群？那么我 整个系统的复杂度是不是上升了 ？ 抛开上面的问题不讲，万一我发送方发送失败了，然后执行重试，这样就可能产生重复的消息。 或者我消费端处理失败了，请求重发，这样也会产生重复的消息。 对于一些微服务来说，消费重复消息会带来更大的麻烦，比如增加积分，这个时候我加了多次是不是对其他用户不公平？ 那么，又 如何解决重复消费消息的问题 呢？ 如果我们此时的消息需要保证严格的顺序性怎么办呢？比如生产者生产了一系列的有序消息(对一个id为1的记录进行删除增加修改)，但是我们知道在发布订阅模型中，对于主题是无顺序的，那么这个时候就会导致对于消费者消费消息的时候没有按照生产者的发送顺序消费，比如这个时候我们消费的顺序为修改删除增加，如果该记录涉及到金额的话是不是会出大事情？ 那么，又 如何解决消息的顺序消费问题 呢？ 就拿我们上面所讲的分布式系统来说，用户购票完成之后是不是需要增加账户积分？在同一个系统中我们一般会使用事务来进行解决，如果用 Spring 的话我们在上面伪代码中加入 @Transactional 注解就好了。但是在不同系统中如何保证事务呢？总不能这个系统我扣钱成功了你那积分系统积分没加吧？或者说我这扣钱明明失败了，你那积分系统给我加了积分。 那么，又如何 解决分布式事务问题 呢？ 我们刚刚说了，消息队列可以进行削峰操作，那如果我的消费者如果消费很慢或者生产者生产消息很快，这样是不是会将消息堆积在消息队列中？ 那么，又如何 解决消息堆积的问题 呢？ 可用性降低，复杂度上升，又带来一系列的重复消费，顺序消费，分布式事务，消息堆积的问题，这消息队列还怎么用啊😵？ 别急，办法总是有的。 RocketMQ是什么？ 哇，你个混蛋！上面给我抛出那么多问题，你现在又讲 RocketMQ ，还让不让人活了？！🤬 别急别急，话说你现在清楚 MQ 的构造吗，我还没讲呢，我们先搞明白 MQ 的内部构造，再来看看如何解决上面的一系列问题吧，不过你最好带着问题去阅读和了解喔。 RocketMQ 是一个 队列模型 的消息中间件，具有高性能、高可靠、高实时、分布式 的特点。它是一个采用 Java 语言开发的分布式的消息系统，由阿里巴巴团队开发，在2016年底贡献给 Apache，成为了 Apache 的一个顶级项目。 在阿里内部，RocketMQ 很好地服务了集团大大小小上千个应用，在每年的双十一当天，更有不可思议的万亿级消息通过 RocketMQ 流转。 废话不多说，想要了解 RocketMQ 历史的同学可以自己去搜寻资料。听完上面的介绍，你只要知道 RocketMQ 很快、很牛、而且经历过双十一的实践就行了！ 队列模型和主题模型 在谈 RocketMQ 的技术架构之前，我们先来了解一下两个名词概念——队列模型 和 主题模型 。 首先我问一个问题，消息队列为什么要叫消息队列？ 你可能觉得很弱智，这玩意不就是存放消息的队列嘛？不叫消息队列叫什么？ 的确，早期的消息中间件是通过 队列 这一模型来实现的，可能是历史原因，我们都习惯把消息中间件成为消息队列。 但是，如今例如 RocketMQ 、Kafka 这些优秀的消息中间件不仅仅是通过一个 队列 来实现消息存储的。 队列模型 就像我们理解队列一样，消息中间件的队列模型就真的只是一个队列。。。我画一张图给大家理解。 在一开始我跟你提到了一个 “广播” 的概念，也就是说如果我们此时我们需要将一个消息发送给多个消费者(比如此时我需要将信息发送给短信系统和邮件系统)，这个时候单个队列即不能满足需求了。 当然你可以让 Producer 生产消息放入多个队列中，然后每个队列去对应每一个消费者。问题是可以解决，创建多个队列并且复制多份消息是会很影响资源和性能的。而且，这样子就会导致生产者需要知道具体消费者个数然后去复制对应数量的消息队列，这就违背我们消息中间件的 解耦 这一原则。 主题模型 那么有没有好的方法去解决这一个问题呢？有，那就是 主题模型 或者可以称为 发布订阅模型 。 感兴趣的同学可以去了解一下设计模式里面的观察者模式并且手动实现一下，我相信你会有所收获的。 在主题模型中，消息的生产者称为 发布者(Publisher) ，消息的消费者称为 订阅者(Subscriber) ，存放消息的容器称为 主题(Topic) 。 其中，发布者将消息发送到指定主题中，订阅者需要 提前订阅主题 才能接受特定主题的消息。 RocketMQ中的消息模型 RockerMQ 中的消息模型就是按照 主题模型 所实现的。你可能会好奇这个 主题 到底是怎么实现的呢？你上面也没有讲到呀！ 其实对于主题模型的实现来说每个消息中间件的底层设计都是不一样的，就比如 Kafka 中的 分区 ，RocketMQ 中的 队列 ，RabbitMQ 中的 Exchange 。我们可以理解为 主题模型/发布订阅模型 就是一个标准，那些中间件只不过照着这个标准去实现而已。 所以，RocketMQ 中的 主题模型 到底是如何实现的呢？首先我画一张图，大家尝试着去理解一下。 我们可以看到在整个图中有 Producer Group 、Topic 、Consumer Group 三个角色，我来分别介绍一下他们。 Producer Group 生产者组： 代表某一类的生产者，比如我们有多个秒杀系统作为生产者，这多个合在一起就是一个 Producer Group 生产者组，它们一般生产相同的消息。 Consumer Group 消费者组： 代表某一类的消费者，比如我们有多个短信系统作为消费者，这多个合在一起就是一个 Consumer Group 消费者组，它们一般消费相同的消息。 Topic 主题： 代表一类消息，比如订单消息，物流消息等等。 你可以看到图中生产者组中的生产者会向主题发送消息，而 主题中存在多个队列，生产者每次生产消息之后是指定主题中的某个队列发送消息的。 每个主题中都有多个队列(这里还不涉及到 Broker)，集群消费模式下，一个消费者集群多台机器共同消费一个 topic 的多个队列，一个队列只会被一个消费者消费。如果某个消费者挂掉，分组内其它消费者会接替挂掉的消费者继续消费。就像上图中 Consumer1 和 Consumer2 分别对应着两个队列，而 Consuer3 是没有队列对应的，所以一般来讲要控制 消费者组中的消费者个数和主题中队列个数相同 。 当然也可以消费者个数小于队列个数，只不过不太建议。如下图。 每个消费组在每个队列上维护一个消费位置 ，为什么呢？ 因为我们刚刚画的仅仅是一个消费者组，我们知道在发布订阅模式中一般会涉及到多个消费者组，而每个消费者组在每个队列中的消费位置都是不同的。如果此时有多个消费者组，那么消息被一个消费者组消费完之后是不会删除的(因为其它消费者组也需要呀)，它仅仅是为每个消费者组维护一个 消费位移(offset) ，每次消费者组消费完会返回一个成功的响应，然后队列再把维护的消费位移加一，这样就不会出现刚刚消费过的消息再一次被消费了。 可能你还有一个问题，为什么一个主题中需要维护多个队列 ？ 答案是 提高并发能力 。的确，每个主题中只存在一个队列也是可行的。你想一下，如果每个主题中只存在一个队列，这个队列中也维护着每个消费者组的消费位置，这样也可以做到 发布订阅模式 。如下图。 但是，这样我生产者是不是只能向一个队列发送消息？又因为需要维护消费位置所以一个队列只能对应一个消费者组中的消费者，这样是不是其他的 Consumer 就没有用武之地了？从这两个角度来讲，并发度一下子就小了很多。 所以总结来说，RocketMQ 通过使用在一个 Topic 中配置多个队列并且每个队列维护每个消费者组的消费位置 实现了 主题模式/发布订阅模式 。 RocketMQ的架构图 讲完了消息模型，我们理解起 RocketMQ 的技术架构起来就容易多了。 RocketMQ 技术架构中有四大角色 NameServer 、Broker 、Producer 、Consumer 。我来向大家分别解释一下这四个角色是干啥的。 Broker： 主要负责消息的存储、投递和查询以及服务高可用保证。说白了就是消息队列服务器嘛，生产者生产消息到 Broker ，消费者从 Broker 拉取消息并消费。 这里，我还得普及一下关于 Broker 、Topic 和 队列的关系。上面我讲解了 Topic 和队列的关系——一个 Topic 中存在多个队列，那么这个 Topic 和队列存放在哪呢？ 一个 Topic 分布在多个 Broker上，一个 Broker 可以配置多个 Topic ，它们是多对多的关系。 如果某个 Topic 消息量很大，应该给它多配置几个队列(上文中提到了提高并发能力)，并且 尽量多分布在不同 Broker 上，以减轻某个 Broker 的压力 。 Topic 消息量都比较均匀的情况下，如果某个 broker 上的队列越多，则该 broker 压力越大。 所以说我们需要配置多个Broker。 NameServer： 不知道你们有没有接触过 ZooKeeper 和 Spring Cloud 中的 Eureka ，它其实也是一个 注册中心 ，主要提供两个功能：Broker管理 和 路由信息管理 。说白了就是 Broker 会将自己的信息注册到 NameServer 中，此时 NameServer 就存放了很多 Broker 的信息(Broker的路由表)，消费者和生产者就从 NameServer 中获取路由表然后照着路由表的信息和对应的 Broker 进行通信(生产者和消费者定期会向 NameServer 去查询相关的 Broker 的信息)。 Producer： 消息发布的角色，支持分布式集群方式部署。说白了就是生产者。 Consumer： 消息消费的角色，支持分布式集群方式部署。支持以push推，pull拉两种模式对消息进行消费。同时也支持集群方式和广播方式的消费，它提供实时消息订阅机制。说白了就是消费者。 听完了上面的解释你可能会觉得，这玩意好简单。不就是这样的么？ 嗯？你可能会发现一个问题，这老家伙 NameServer 干啥用的，这不多余吗？直接 Producer 、Consumer 和 Broker 直接进行生产消息，消费消息不就好了么？ 但是，我们上文提到过 Broker 是需要保证高可用的，如果整个系统仅仅靠着一个 Broker 来维持的话，那么这个 Broker 的压力会不会很大？所以我们需要使用多个 Broker 来保证 负载均衡 。 如果说，我们的消费者和生产者直接和多个 Broker 相连，那么当 Broker 修改的时候必定会牵连着每个生产者和消费者，这样就会产生耦合问题，而 NameServer 注册中心就是用来解决这个问题的。 如果还不是很理解的话，可以去看我介绍 Spring Cloud 的那篇文章，其中介绍了 Eureka 注册中心。 当然，RocketMQ 中的技术架构肯定不止前面那么简单，因为上面图中的四个角色都是需要做集群的。我给出一张官网的架构图，大家尝试理解一下。 其实和我们最开始画的那张乞丐版的架构图也没什么区别，主要是一些细节上的差别。听我细细道来🤨。 第一、我们的 Broker 做了集群并且还进行了主从部署 ，由于消息分布在各个 Broker 上，一旦某个 Broker 宕机，则该Broker 上的消息读写都会受到影响。所以 Rocketmq 提供了 master/slave 的结构，salve 定时从 master 同步数据(同步刷盘或者异步刷盘)，如果 master 宕机，则 slave 提供消费服务，但是不能写入消息 (后面我还会提到哦)。 第二、为了保证 HA ，我们的 NameServer 也做了集群部署，但是请注意它是 去中心化 的。也就意味着它没有主节点，你可以很明显地看出 NameServer 的所有节点是没有进行 Info Replicate 的，在 RocketMQ 中是通过 单个Broker和所有NameServer保持长连接 ，并且在每隔30秒 Broker 会向所有 Nameserver 发送心跳，心跳包含了自身的 Topic 配置信息，这个步骤就对应这上面的 Routing Info 。 第三、在生产者需要向 Broker 发送消息的时候，需要先从 NameServer 获取关于 Broker 的路由信息，然后通过 轮询 的方法去向每个队列中生产数据以达到 负载均衡 的效果。 第四、消费者通过 NameServer 获取所有 Broker 的路由信息后，向 Broker 发送 Pull 请求来获取消息数据。Consumer 可以以两种模式启动—— 广播（Broadcast）和集群（Cluster）。广播模式下，一条消息会发送给 同一个消费组中的所有消费者 ，集群模式下消息只会发送给一个消费者。 如何解决 顺序消费、重复消费 其实，这些东西都是我在介绍消息队列带来的一些副作用的时候提到的，也就是说，这些问题不仅仅挂钩于 RocketMQ ，而是应该每个消息中间件都需要去解决的。 在上面我介绍 RocketMQ 的技术架构的时候我已经向你展示了 它是如何保证高可用的 ，这里不涉及运维方面的搭建，如果你感兴趣可以自己去官网上照着例子搭建属于你自己的 RocketMQ 集群。 其实 Kafka 的架构基本和 RocketMQ 类似，只是它注册中心使用了 Zookeeper 、它的 分区 就相当于 RocketMQ 中的 队列 。还有一些小细节不同会在后面提到。 顺序消费 在上面的技术架构介绍中，我们已经知道了 RocketMQ 在主题上是无序的、它只有在队列层面才是保证有序 的。 这又扯到两个概念——普通顺序 和 严格顺序 。 所谓普通顺序是指 消费者通过 同一个消费队列收到的消息是有顺序的 ，不同消息队列收到的消息则可能是无顺序的。普通顺序消息在 Broker 重启情况下不会保证消息顺序性 (短暂时间) 。 所谓严格顺序是指 消费者收到的 所有消息 均是有顺序的。严格顺序消息 即使在异常情况下也会保证消息的顺序性 。 但是，严格顺序看起来虽好，实现它可会付出巨大的代价。如果你使用严格顺序模式，Broker 集群中只要有一台机器不可用，则整个集群都不可用。你还用啥？现在主要场景也就在 binlog 同步。 一般而言，我们的 MQ 都是能容忍短暂的乱序，所以推荐使用普通顺序模式。 那么，我们现在使用了 普通顺序模式 ，我们从上面学习知道了在 Producer 生产消息的时候会进行轮询(取决你的负载均衡策略)来向同一主题的不同消息队列发送消息。那么如果此时我有几个消息分别是同一个订单的创建、支付、发货，在轮询的策略下这 三个消息会被发送到不同队列 ，因为在不同的队列此时就无法使用 RocketMQ 带来的队列有序特性来保证消息有序性了。 那么，怎么解决呢？ 其实很简单，我们需要处理的仅仅是将同一语义下的消息放入同一个队列(比如这里是同一个订单)，那我们就可以使用 Hash取模法 来保证同一个订单在同一个队列中就行了。 重复消费 emmm，就两个字—— 幂等 。在编程中一个幂等 操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。比如说，这个时候我们有一个订单的处理积分的系统，每当来一个消息的时候它就负责为创建这个订单的用户的积分加上相应的数值。可是有一次，消息队列发送给订单系统 FrancisQ 的订单信息，其要求是给 FrancisQ 的积分加上 500。但是积分系统在收到 FrancisQ 的订单信息处理完成之后返回给消息队列处理成功的信息的时候出现了网络波动(当然还有很多种情况，比如Broker意外重启等等)，这条回应没有发送成功。 那么，消息队列没收到积分系统的回应会不会尝试重发这个消息？问题就来了，我再发这个消息，万一它又给 FrancisQ 的账户加上 500 积分怎么办呢？ 所以我们需要给我们的消费者实现 幂等 ，也就是对同一个消息的处理结果，执行多少次都不变。 那么如何给业务实现幂等呢？这个还是需要结合具体的业务的。你可以使用 写入 Redis 来保证，因为 Redis 的 key 和 value 就是天然支持幂等的。当然还有使用 数据库插入法 ，基于数据库的唯一键来保证重复数据不会被插入多条。 不过最主要的还是需要 根据特定场景使用特定的解决方案 ，你要知道你的消息消费是否是完全不可重复消费还是可以忍受重复消费的，然后再选择强校验和弱校验的方式。毕竟在 CS 领域还是很少有技术银弹的说法。 而在整个互联网领域，幂等不仅仅适用于消息队列的重复消费问题，这些实现幂等的方法，也同样适用于，在其他场景中来解决重复请求或者重复调用的问题 。比如将HTTP服务设计成幂等的，解决前端或者APP重复提交表单数据的问题 ，也可以将一个微服务设计成幂等的，解决 RPC 框架自动重试导致的 重复调用问题 。 分布式事务 如何解释分布式事务呢？事务大家都知道吧？要么都执行要么都不执行 。在同一个系统中我们可以轻松地实现事务，但是在分布式架构中，我们有很多服务是部署在不同系统之间的，而不同服务之间又需要进行调用。比如此时我下订单然后增加积分，如果保证不了分布式事务的话，就会出现A系统下了订单，但是B系统增加积分失败或者A系统没有下订单，B系统却增加了积分。前者对用户不友好，后者对运营商不利，这是我们都不愿意见到的。 那么，如何去解决这个问题呢？ 如今比较常见的分布式事务实现有 2PC、TCC 和事务消息(half 半消息机制)。每一种实现都有其特定的使用场景，但是也有各自的问题，都不是完美的解决方案。 在 RocketMQ 中使用的是 事务消息加上事务反查机制 来解决分布式事务问题的。我画了张图，大家可以对照着图进行理解。 在第一步发送的 half 消息 ，它的意思是 在事务提交之前，对于消费者来说，这个消息是不可见的 。 那么，如何做到写入消息但是对用户不可见呢？RocketMQ事务消息的做法是：如果消息是half消息，将备份原消息的主题与消息消费队列，然后 改变主题 为RMQ_SYS_TRANS_HALF_TOPIC。由于消费组未订阅该主题，故消费端无法消费half类型的消息，然后RocketMQ会开启一个定时任务，从Topic为RMQ_SYS_TRANS_HALF_TOPIC中拉取消息进行消费，根据生产者组获取一个服务提供者发送回查事务状态请求，根据事务状态来决定是提交或回滚消息。 你可以试想一下，如果没有从第5步开始的 事务反查机制 ，如果出现网路波动第4步没有发送成功，这样就会产生 MQ 不知道是不是需要给消费者消费的问题，他就像一个无头苍蝇一样。在 RocketMQ 中就是使用的上述的事务反查来解决的，而在 Kafka 中通常是直接抛出一个异常让用户来自行解决。 你还需要注意的是，在 MQ Server 指向系统B的操作已经和系统A不相关了，也就是说在消息队列中的分布式事务是——本地事务和存储消息到消息队列才是同一个事务。这样也就产生了事务的最终一致性，因为整个过程是异步的，每个系统只要保证它自己那一部分的事务就行了。 消息堆积问题 在上面我们提到了消息队列一个很重要的功能——削峰 。那么如果这个峰值太大了导致消息堆积在队列中怎么办呢？ 其实这个问题可以将它广义化，因为产生消息堆积的根源其实就只有两个——生产者生产太快或者消费者消费太慢。 我们可以从多个角度去思考解决这个问题，当流量到峰值的时候是因为生产者生产太快，我们可以使用一些 限流降级 的方法，当然你也可以增加多个消费者实例去水平扩展增加消费能力来匹配生产的激增。如果消费者消费过慢的话，我们可以先检查 是否是消费者出现了大量的消费错误 ，或者打印一下日志查看是否是哪一个线程卡死，出现了锁资源不释放等等的问题。 当然，最快速解决消息堆积问题的方法还是增加消费者实例，不过 同时你还需要增加每个主题的队列数量 。 别忘了在 RocketMQ 中，一个队列只会被一个消费者消费 ，如果你仅仅是增加消费者实例就会出现我一开始给你画架构图的那种情况。 回溯消费 回溯消费是指 Consumer 已经消费成功的消息，由于业务上需求需要重新消费，在RocketMQ 中， Broker 在向Consumer 投递成功消息后，消息仍然需要保留 。并且重新消费一般是按照时间维度，例如由于 Consumer 系统故障，恢复后需要重新消费1小时前的数据，那么 Broker 要提供一种机制，可以按照时间维度来回退消费进度。RocketMQ 支持按照时间回溯消费，时间维度精确到毫秒。 这是官方文档的解释，我直接照搬过来就当科普了😁😁😁。 RocketMQ 的刷盘机制 上面我讲了那么多的 RocketMQ 的架构和设计原理，你有没有好奇 在 Topic 中的 队列是以什么样的形式存在的？ 队列中的消息又是如何进行存储持久化的呢？ 我在上文中提到的 同步刷盘 和 异步刷盘 又是什么呢？它们会给持久化带来什么样的影响呢？ 下面我将给你们一一解释。 同步刷盘和异步刷盘 如上图所示，在同步刷盘中需要等待一个刷盘成功的 ACK ，同步刷盘对 MQ 消息可靠性来说是一种不错的保障，但是 性能上会有较大影响 ，一般地适用于金融等特定业务场景。 而异步刷盘往往是开启一个线程去异步地执行刷盘操作。消息刷盘采用后台异步线程提交的方式进行， 降低了读写延迟 ，提高了 MQ 的性能和吞吐量，一般适用于如发验证码等对于消息保证要求不太高的业务场景。 一般地，异步刷盘只有在 Broker 意外宕机的时候会丢失部分数据，你可以设置 Broker 的参数 FlushDiskType 来调整你的刷盘策略(ASYNC_FLUSH 或者 SYNC_FLUSH)。 同步复制和异步复制 上面的同步刷盘和异步刷盘是在单个结点层面的，而同步复制和异步复制主要是指的 Borker 主从模式下，主节点返回消息给客户端的时候是否需要同步从节点。 同步复制： 也叫 “同步双写”，也就是说，只有消息同步双写到主从结点上时才返回写入成功 。 异步复制： 消息写入主节点之后就直接返回写入成功 。 然而，很多事情是没有完美的方案的，就比如我们进行消息写入的节点越多就更能保证消息的可靠性，但是随之的性能也会下降，所以需要程序员根据特定业务场景去选择适应的主从复制方案。 那么，异步复制会不会也像异步刷盘那样影响消息的可靠性呢？ 答案是不会的，因为两者就是不同的概念，对于消息可靠性是通过不同的刷盘策略保证的，而像异步同步复制策略仅仅是影响到了 可用性 。为什么呢？其主要原因是 RocketMQ 是不支持自动主从切换的，当主节点挂掉之后，生产者就不能再给这个主节点生产消息了。 比如这个时候采用异步复制的方式，在主节点还未发送完需要同步的消息的时候主节点挂掉了，这个时候从节点就少了一部分消息。但是此时生产者无法再给主节点生产消息了，消费者可以自动切换到从节点进行消费(仅仅是消费)，所以在主节点挂掉的时间只会产生主从结点短暂的消息不一致的情况，降低了可用性，而当主节点重启之后，从节点那部分未来得及复制的消息还会继续复制。 在单主从架构中，如果一个主节点挂掉了，那么也就意味着整个系统不能再生产了。那么这个可用性的问题能否解决呢？一个主从不行那就多个主从的呗，别忘了在我们最初的架构图中，每个 Topic 是分布在不同 Broker 中的。 但是这种复制方式同样也会带来一个问题，那就是无法保证 严格顺序 。在上文中我们提到了如何保证的消息顺序性是通过将一个语义的消息发送在同一个队列中，使用 Topic 下的队列来保证顺序性的。如果此时我们主节点A负责的是订单A的一系列语义消息，然后它挂了，这样其他节点是无法代替主节点A的，如果我们任意节点都可以存入任何消息，那就没有顺序性可言了。 而在 RocketMQ 中采用了 Dledger 解决这个问题。他要求在写入消息的时候，要求至少消息复制到半数以上的节点之后，才给客⼾端返回写⼊成功，并且它是⽀持通过选举来动态切换主节点的。这里我就不展开说明了，读者可以自己去了解。 也不是说 Dledger 是个完美的方案，至少在 Dledger 选举过程中是无法提供服务的，而且他必须要使用三个节点或以上，如果多数节点同时挂掉他也是无法保证可用性的，而且要求消息复制板书以上节点的效率和直接异步复制还是有一定的差距的。 存储机制 还记得上面我们一开始的三个问题吗？到这里第三个问题已经解决了。 但是，在 Topic 中的 队列是以什么样的形式存在的？队列中的消息又是如何进行存储持久化的呢？ 还未解决，其实这里涉及到了 RocketMQ 是如何设计它的存储结构了。我首先想大家介绍 RocketMQ 消息存储架构中的三大角色——CommitLog 、ConsumeQueue 和 IndexFile 。 CommitLog： 消息主体以及元数据的存储主体，存储 Producer 端写入的消息主体内容,消息内容不是定长的。单个文件大小默认1G ，文件名长度为20位，左边补零，剩余为起始偏移量，比如00000000000000000000代表了第一个文件，起始偏移量为0，文件大小为1G=1073741824；当第一个文件写满了，第二个文件为00000000001073741824，起始偏移量为1073741824，以此类推。消息主要是顺序写入日志文件，当文件满了，写入下一个文件。 ConsumeQueue： 消息消费队列，引入的目的主要是提高消息消费的性能(我们再前面也讲了)，由于RocketMQ 是基于主题 Topic 的订阅模式，消息消费是针对主题进行的，如果要遍历 commitlog 文件中根据 Topic 检索消息是非常低效的。Consumer 即可根据 ConsumeQueue 来查找待消费的消息。其中，ConsumeQueue（逻辑消费队列）作为消费消息的索引，保存了指定 Topic 下的队列消息在 CommitLog 中的起始物理偏移量 offset ，消息大小 size 和消息 Tag 的 HashCode 值。consumequeue 文件可以看成是基于 topic 的 commitlog 索引文件，故 consumequeue 文件夹的组织方式如下：topic/queue/file三层组织结构，具体存储路径为：$HOME/store/consumequeue/{topic}/{queueId}/{fileName}。同样 consumequeue 文件采取定长设计，每一个条目共20个字节，分别为8字节的 commitlog 物理偏移量、4字节的消息长度、8字节tag hashcode，单个文件由30W个条目组成，可以像数组一样随机访问每一个条目，每个 ConsumeQueue文件大小约5.72M； IndexFile： IndexFile（索引文件）提供了一种可以通过key或时间区间来查询消息的方法。这里只做科普不做详细介绍。 总结来说，整个消息存储的结构，最主要的就是 CommitLoq 和 ConsumeQueue 。而 ConsumeQueue 你可以大概理解为 Topic 中的队列。 RocketMQ 采用的是 混合型的存储结构 ，即为 Broker 单个实例下所有的队列共用一个日志数据文件来存储消息。有意思的是在同样高并发的 Kafka 中会为每个 Topic 分配一个存储文件。这就有点类似于我们有一大堆书需要装上书架，RockeMQ 是不分书的种类直接成批的塞上去的，而 Kafka 是将书本放入指定的分类区域的。 而 RocketMQ 为什么要这么做呢？原因是 提高数据的写入效率 ，不分 Topic 意味着我们有更大的几率获取 成批 的消息进行数据写入，但也会带来一个麻烦就是读取消息的时候需要遍历整个大文件，这是非常耗时的。 所以，在 RocketMQ 中又使用了 ConsumeQueue 作为每个队列的索引文件来 提升读取消息的效率。我们可以直接根据队列的消息序号，计算出索引的全局位置（索引序号*索引固定⻓度20），然后直接读取这条索引，再根据索引中记录的消息的全局位置，找到消息。 讲到这里，你可能对 RockeMQ 的存储架构还有些模糊，没事，我们结合着图来理解一下。 emmm，是不是有一点复杂🤣，看英文图片和英文文档的时候就不要怂，硬着头皮往下看就行。 如果上面没看懂的读者一定要认真看下面的流程分析！ 首先，在最上面的那一块就是我刚刚讲的你现在可以直接 把 ConsumerQueue 理解为 Queue。 在图中最左边说明了 红色方块 代表被写入的消息，虚线方块代表等待被写入的。左边的生产者发送消息会指定 Topic 、QueueId 和具体消息内容，而在 Broker 中管你是哪门子消息，他直接 全部顺序存储到了 CommitLog 。而根据生产者指定的 Topic 和 QueueId 将这条消息本身在 CommitLog 的偏移(offset)，消息本身大小，和tag的hash值存入对应的 ConsumeQueue 索引文件中。而在每个队列中都保存了 ConsumeOffset 即每个消费者组的消费位置(我在架构那里提到了，忘了的同学可以回去看一下)，而消费者拉取消息进行消费的时候只需要根据 ConsumeOffset 获取下一个未被消费的消息就行了。 上述就是我对于整个消息存储架构的大概理解(这里不涉及到一些细节讨论，比如稀疏索引等等问题)，希望对你有帮助。 因为有一个知识点因为写嗨了忘讲了，想想在哪里加也不好，所以我留给大家去思考🤔🤔一下吧。 为什么 CommitLog 文件要设计成固定大小的长度呢？提醒：内存映射机制。 总结 总算把这篇博客写完了。我讲的你们还记得吗😅？ 这篇文章中我主要想大家介绍了 消息队列出现的原因 消息队列的作用(异步，解耦，削峰) 消息队列带来的一系列问题(消息堆积、重复消费、顺序消费、分布式事务等等) 消息队列的两种消息模型——队列和主题模式 分析了 RocketMQ 的技术架构(NameServer 、Broker 、Producer 、Comsumer) 结合 RocketMQ 回答了消息队列副作用的解决方案 介绍了 RocketMQ 的存储机制和刷盘策略。 等等。。。 如果喜欢可以点赞哟👍👍👍。 "},"zother6-JavaGuide/system-design/data-communication/why-use-rpc.html":{"url":"zother6-JavaGuide/system-design/data-communication/why-use-rpc.html","title":"Why Use Rpc","keywords":"","body":"什么是 RPC?RPC原理是什么? 什么是 RPC？ RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。比如两个不同的服务 A、B 部署在两台不同的机器上，那么服务 A 如果想要调用服务 B 中的某个方法该怎么办呢？使用 HTTP请求 当然可以，但是可能会比较慢而且一些优化做的并不好。 RPC 的出现就是为了解决这个问题。 RPC原理是什么？ 我这里这是简单的提一下，详细内容可以查看下面这篇文章： http://www.importnew.com/22003.html 服务消费方（client）调用以本地调用方式调用服务； client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体； client stub找到服务地址，并将消息发送到服务端； server stub收到消息后进行解码； server stub根据解码结果调用本地的服务； 本地服务执行并将结果返回给server stub； server stub将返回结果打包成消息并发送至消费方； client stub接收到消息，并进行解码； 服务消费方得到最终结果。 下面再贴一个网上的时序图： RPC 解决了什么问题？ 从上面对 RPC 介绍的内容中，概括来讲RPC 主要解决了：让分布式或者微服务系统中不同服务之间的调用像本地调用一样简单。 常见的 RPC 框架总结? RMI（JDK自带）： JDK自带的RPC，有很多局限性，不推荐使用。 Dubbo: Dubbo是 阿里巴巴公司开源的一个高性能优秀的服务框架，使得应用可通过高性能的 RPC 实现服务的输出和输入功能，可以和 Spring框架无缝集成。目前 Dubbo 已经成为 Spring Cloud Alibaba 中的官方组件。 gRPC ：gRPC是可以在任何环境中运行的现代开源高性能RPC框架。它可以通过可插拔的支持来有效地连接数据中心内和跨数据中心的服务，以实现负载平衡，跟踪，运行状况检查和身份验证。它也适用于分布式计算的最后一英里，以将设备，移动应用程序和浏览器连接到后端服务。 Hessian： Hessian是一个轻量级的remotingonhttp工具，使用简单的方法提供了RMI的功能。 相比WebService，Hessian更简单、快捷。采用的是二进制RPC协议，因为采用的是二进制协议，所以它很适合于发送二进制数据。 Thrift： Apache Thrift是Facebook开源的跨语言的RPC通信框架，目前已经捐献给Apache基金会管理，由于其跨语言特性和出色的性能，在很多互联网公司得到应用，有能力的公司甚至会基于thrift研发一套分布式服务框架，增加诸如服务注册、服务发现等功能。 既有 HTTP ,为啥用 RPC 进行服务调用? RPC 只是一种设计而已 RPC 只是一种概念、一种设计，就是为了解决 不同服务之间的调用问题, 它一般会包含有 传输协议 和 序列化协议 这两个。 但是，HTTP 是一种协议，RPC框架可以使用 HTTP协议作为传输协议或者直接使用TCP作为传输协议，使用不同的协议一般也是为了适应不同的场景。 HTTP 和 TCP 可能现在很多对计算机网络不太熟悉的朋友已经被搞蒙了，要想真正搞懂，还需要来简单复习一下计算机网络基础知识： 我们通常谈计算机网络的五层协议的体系结构是指：应用层、传输层、网络层、数据链路层、物理层。 应用层(application-layer）的任务是通过应用进程间的交互来完成特定网络应用。HTTP 属于应用层协议，它会基于TCP/IP通信协议来传递数据（HTML 文件, 图片文件, 查询结果等）。HTTP协议工作于客户端-服务端架构为上。浏览器作为HTTP客户端通过 URL 向HTTP服务端即WEB服务器发送所有请求。Web服务器根据接收到的请求后，向客户端发送响应信息。HTTP协议建立在 TCP 协议之上。 运输层(transport layer)的主要任务就是负责向两台主机进程之间的通信提供通用的数据传输服务。TCP是传输层协议，主要解决数据如何在网络中传输。相比于UDP,TCP 提供的是面向连接的，可靠的数据传输服务。 RPC框架功能更齐全 成熟的 RPC框架还提供好了“服务自动注册与发现”、\"智能负载均衡\"、“可视化的服务治理和运维”、“运行期流量调度”等等功能，这些也算是选择 RPC 进行服务注册和发现的一方面原因吧！ 相关阅读： http://www.ruanyifeng.com/blog/2016/08/http.html （HTTP 协议入门- 阮一峰） 一个常见的错误观点 很多文章中还会提到说 HTTP 协议相较于自定义 TCP 报文协议，增加的开销在于连接的建立与断开，但是这个观点已经被否认，下面截取自知乎中一个回答，原回答地址：https://www.zhihu.com/question/41609070/answer/191965937。 首先要否认一点 HTTP 协议相较于自定义 TCP 报文协议，增加的开销在于连接的建立与断开。HTTP 协议是支持连接池复用的，也就是建立一定数量的连接不断开，并不会频繁的创建和销毁连接。二一要说的是 HTTP 也可以使用 Protobuf 这种二进制编码协议对内容进行编码，因此二者最大的区别还是在传输协议上。 "},"zother6-JavaGuide/system-design/framework/mybatis/mybatis-interview.html":{"url":"zother6-JavaGuide/system-design/framework/mybatis/mybatis-interview.html","title":"Mybatis Interview","keywords":"","body":" 本篇文章是JavaGuide收集自网络，原出处不明。 Mybatis 技术内幕系列博客，从原理和源码角度，介绍了其内部实现细节，无论是写的好与不好，我确实是用心写了，由于并不是介绍如何使用 Mybatis 的文章，所以，一些参数使用细节略掉了，我们的目标是介绍 Mybatis 的技术架构和重要组成部分，以及基本运行原理。 博客写的很辛苦，但是写出来却不一定好看，所谓开始很兴奋，过程很痛苦，结束很遗憾。要求不高，只要读者能从系列博客中，学习到一点其他博客所没有的技术点，作为作者，我就很欣慰了，我也读别人写的博客，通常对自己当前研究的技术，是很有帮助的。 尽管还有很多可写的内容，但是，我认为再写下去已经没有意义，任何其他小的功能点，都是在已经介绍的基本框架和基本原理下运行的，只有结束，才能有新的开始。写博客也积攒了一些经验，源码多了感觉就是复制黏贴，源码少了又觉得是空谈原理，将来再写博客，我希望是“精炼博文”，好读好懂美观读起来又不累，希望自己能再写一部开源分布式框架原理系列博客。 有胆就来，我出几道 Mybatis 面试题，看你能回答上来几道（都是我出的，可不是网上找的）。 1、#{}和${}的区别是什么？ 注：这道题是面试官面试我同事的。 答： ${}是 Properties 文件中的变量占位符，它可以用于标签属性值和 sql 内部，属于静态文本替换，比如${driver}会被静态替换为com.mysql.jdbc.Driver。 #{}是 sql 的参数占位符，Mybatis 会将 sql 中的#{}替换为?号，在 sql 执行前会使用 PreparedStatement 的参数设置方法，按序给 sql 的?号占位符设置参数值，比如 ps.setInt(0, parameterValue)，#{item.name} 的取值方式为使用反射从参数对象中获取 item 对象的 name 属性值，相当于 param.getItem().getName()。 2、Xml 映射文件中，除了常见的 select|insert|updae|delete 标签之外，还有哪些标签？ 注：这道题是京东面试官面试我时问的。 答：还有很多其他的标签，、、、、，加上动态 sql 的 9 个标签，trim|where|set|foreach|if|choose|when|otherwise|bind等，其中为 sql 片段标签，通过标签引入 sql 片段，为不支持自增的主键生成策略标签。 3、最佳实践中，通常一个 Xml 映射文件，都会写一个 Dao 接口与之对应，请问，这个 Dao 接口的工作原理是什么？Dao 接口里的方法，参数不同时，方法能重载吗？ 注：这道题也是京东面试官面试我时问的。 答：Dao 接口，就是人们常说的 Mapper接口，接口的全限名，就是映射文件中的 namespace 的值，接口的方法名，就是映射文件中MappedStatement的 id 值，接口方法内的参数，就是传递给 sql 的参数。Mapper接口是没有实现类的，当调用接口方法时，接口全限名+方法名拼接字符串作为 key 值，可唯一定位一个MappedStatement，举例：com.mybatis3.mappers.StudentDao.findStudentById，可以唯一找到 namespace 为com.mybatis3.mappers.StudentDao下面id = findStudentById的MappedStatement。在 Mybatis 中，每一个、、、标签，都会被解析为一个MappedStatement对象。 Dao 接口里的方法，是不能重载的，因为是全限名+方法名的保存和寻找策略。 Dao 接口的工作原理是 JDK 动态代理，Mybatis 运行时会使用 JDK 动态代理为 Dao 接口生成代理 proxy 对象，代理对象 proxy 会拦截接口方法，转而执行MappedStatement所代表的 sql，然后将 sql 执行结果返回。 4、Mybatis 是如何进行分页的？分页插件的原理是什么？ 注：我出的。 答：Mybatis 使用 RowBounds 对象进行分页，它是针对 ResultSet 结果集执行的内存分页，而非物理分页，可以在 sql 内直接书写带有物理分页的参数来完成物理分页功能，也可以使用分页插件来完成物理分页。 分页插件的基本原理是使用 Mybatis 提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的 sql，然后重写 sql，根据 dialect 方言，添加对应的物理分页语句和物理分页参数。 举例：select _ from student，拦截 sql 后重写为：select t._ from （select \\* from student）t limit 0，10 5、简述 Mybatis 的插件运行原理，以及如何编写一个插件。 注：我出的。 答：Mybatis 仅可以编写针对 ParameterHandler、ResultSetHandler、StatementHandler、Executor 这 4 种接口的插件，Mybatis 使用 JDK 的动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这 4 种接口对象的方法时，就会进入拦截方法，具体就是 InvocationHandler 的 invoke()方法，当然，只会拦截那些你指定需要拦截的方法。 实现 Mybatis 的 Interceptor 接口并复写intercept()方法，然后在给插件编写注解，指定要拦截哪一个接口的哪些方法即可，记住，别忘了在配置文件中配置你编写的插件。 6、Mybatis 执行批量插入，能返回数据库主键列表吗？ 注：我出的。 答：能，JDBC 都能，Mybatis 当然也能。 7、Mybatis 动态 sql 是做什么的？都有哪些动态 sql？能简述一下动态 sql 的执行原理不？ 注：我出的。 答：Mybatis 动态 sql 可以让我们在 Xml 映射文件内，以标签的形式编写动态 sql，完成逻辑判断和动态拼接 sql 的功能，Mybatis 提供了 9 种动态 sql 标签 trim|where|set|foreach|if|choose|when|otherwise|bind。 其执行原理为，使用 OGNL 从 sql 参数对象中计算表达式的值，根据表达式的值动态拼接 sql，以此来完成动态 sql 的功能。 8、Mybatis 是如何将 sql 执行结果封装为目标对象并返回的？都有哪些映射形式？ 注：我出的。 答：第一种是使用标签，逐一定义列名和对象属性名之间的映射关系。第二种是使用 sql 列的别名功能，将列别名书写为对象属性名，比如 T_NAME AS NAME，对象属性名一般是 name，小写，但是列名不区分大小写，Mybatis 会忽略列名大小写，智能找到与之对应对象属性名，你甚至可以写成 T_NAME AS NaMe，Mybatis 一样可以正常工作。 有了列名与属性名的映射关系后，Mybatis 通过反射创建对象，同时使用反射给对象的属性逐一赋值并返回，那些找不到映射关系的属性，是无法完成赋值的。 9、Mybatis 能执行一对一、一对多的关联查询吗？都有哪些实现方式，以及它们之间的区别。 注：我出的。 答：能，Mybatis 不仅可以执行一对一、一对多的关联查询，还可以执行多对一，多对多的关联查询，多对一查询，其实就是一对一查询，只需要把 selectOne()修改为 selectList()即可；多对多查询，其实就是一对多查询，只需要把 selectOne()修改为 selectList()即可。 关联对象查询，有两种实现方式，一种是单独发送一个 sql 去查询关联对象，赋给主对象，然后返回主对象。另一种是使用嵌套查询，嵌套查询的含义为使用 join 查询，一部分列是 A 对象的属性值，另外一部分列是关联对象 B 的属性值，好处是只发一个 sql 查询，就可以把主对象和其关联对象查出来。 那么问题来了，join 查询出来 100 条记录，如何确定主对象是 5 个，而不是 100 个？其去重复的原理是标签内的子标签，指定了唯一确定一条记录的 id 列，Mybatis 根据列值来完成 100 条记录的去重复功能，可以有多个，代表了联合主键的语意。 同样主对象的关联对象，也是根据这个原理去重复的，尽管一般情况下，只有主对象会有重复记录，关联对象一般不会重复。 举例：下面 join 查询出来 6 条记录，一、二列是 Teacher 对象列，第三列为 Student 对象列，Mybatis 去重复处理后，结果为 1 个老师 6 个学生，而不是 6 个老师 6 个学生。 t_id t_name s_id | 1 | teacher | 38 | | 1 | teacher | 39 | | 1 | teacher | 40 | | 1 | teacher | 41 | | 1 | teacher | 42 | | 1 | teacher | 43 | 10、Mybatis 是否支持延迟加载？如果支持，它的实现原理是什么？ 注：我出的。 答：Mybatis 仅支持 association 关联对象和 collection 关联集合对象的延迟加载，association 指的就是一对一，collection 指的就是一对多查询。在 Mybatis 配置文件中，可以配置是否启用延迟加载 lazyLoadingEnabled=true|false。 它的原理是，使用CGLIB 创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用 a.getB().getName()，拦截器 invoke()方法发现 a.getB()是 null 值，那么就会单独发送事先保存好的查询关联 B 对象的 sql，把 B 查询上来，然后调用 a.setB(b)，于是 a 的对象 b 属性就有值了，接着完成 a.getB().getName()方法的调用。这就是延迟加载的基本原理。 当然了，不光是 Mybatis，几乎所有的包括 Hibernate，支持延迟加载的原理都是一样的。 11、Mybatis 的 Xml 映射文件中，不同的 Xml 映射文件，id 是否可以重复？ 注：我出的。 答：不同的 Xml 映射文件，如果配置了 namespace，那么 id 可以重复；如果没有配置 namespace，那么 id 不能重复；毕竟 namespace 不是必须的，只是最佳实践而已。 原因就是 namespace+id 是作为 Map的 key 使用的，如果没有 namespace，就剩下 id，那么，id 重复会导致数据互相覆盖。有了 namespace，自然 id 就可以重复，namespace 不同，namespace+id 自然也就不同。 12、Mybatis 中如何执行批处理？ 注：我出的。 答：使用 BatchExecutor 完成批处理。 13、Mybatis 都有哪些 Executor 执行器？它们之间的区别是什么？ 注：我出的 答：Mybatis 有三种基本的 Executor 执行器，SimpleExecutor、ReuseExecutor、BatchExecutor。 SimpleExecutor：每执行一次 update 或 select，就开启一个 Statement 对象，用完立刻关闭 Statement 对象。 `ReuseExecutor：执行 update 或 select，以 sql 作为 key 查找 Statement 对象，存在就使用，不存在就创建，用完后，不关闭 Statement 对象，而是放置于 Map内，供下一次使用。简言之，就是重复使用 Statement 对象。 BatchExecutor：执行 update（没有 select，JDBC 批处理不支持 select），将所有 sql 都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个 Statement 对象，每个 Statement 对象都是 addBatch()完毕后，等待逐一执行 executeBatch()批处理。与 JDBC 批处理相同。 作用范围：Executor 的这些特点，都严格限制在 SqlSession 生命周期范围内。 14、Mybatis 中如何指定使用哪一种 Executor 执行器？ 注：我出的 答：在 Mybatis 配置文件中，可以指定默认的 ExecutorType 执行器类型，也可以手动给 DefaultSqlSessionFactory 的创建 SqlSession 的方法传递 ExecutorType 类型参数。 15、Mybatis 是否可以映射 Enum 枚举类？ 注：我出的 答：Mybatis 可以映射枚举类，不单可以映射枚举类，Mybatis 可以映射任何对象到表的一列上。映射方式为自定义一个 TypeHandler，实现 TypeHandler 的 setParameter()和 getResult()接口方法。TypeHandler 有两个作用，一是完成从 javaType 至 jdbcType 的转换，二是完成 jdbcType 至 javaType 的转换，体现为 setParameter()和 getResult()两个方法，分别代表设置 sql 问号占位符参数和获取列查询结果。 16、Mybatis 映射文件中，如果 A 标签通过 include 引用了 B 标签的内容，请问，B 标签能否定义在 A 标签的后面，还是说必须定义在 A 标签的前面？ 注：我出的 答：虽然 Mybatis 解析 Xml 映射文件是按照顺序解析的，但是，被引用的 B 标签依然可以定义在任何地方，Mybatis 都可以正确识别。 原理是，Mybatis 解析 A 标签，发现 A 标签引用了 B 标签，但是 B 标签尚未解析到，尚不存在，此时，Mybatis 会将 A 标签标记为未解析状态，然后继续解析余下的标签，包含 B 标签，待所有标签解析完毕，Mybatis 会重新解析那些被标记为未解析的标签，此时再解析 A 标签时，B 标签已经存在，A 标签也就可以正常解析完成了。 17、简述 Mybatis 的 Xml 映射文件和 Mybatis 内部数据结构之间的映射关系？ 注：我出的 答：Mybatis 将所有 Xml 配置信息都封装到 All-In-One 重量级对象 Configuration 内部。在 Xml 映射文件中，标签会被解析为 ParameterMap 对象，其每个子元素会被解析为 ParameterMapping 对象。标签会被解析为 ResultMap 对象，其每个子元素会被解析为 ResultMapping 对象。每一个、、、标签均会被解析为 MappedStatement 对象，标签内的 sql 会被解析为 BoundSql 对象。 18、为什么说 Mybatis 是半自动 ORM 映射工具？它与全自动的区别在哪里？ 注：我出的 答：Hibernate 属于全自动 ORM 映射工具，使用 Hibernate 查询关联对象或者关联集合对象时，可以根据对象关系模型直接获取，所以它是全自动的。而 Mybatis 在查询关联对象或关联集合对象时，需要手动编写 sql 来完成，所以，称之为半自动 ORM 映射工具。 面试题看似都很简单，但是想要能正确回答上来，必定是研究过源码且深入的人，而不是仅会使用的人或者用的很熟的人，以上所有面试题及其答案所涉及的内容，在我的 Mybatis 系列博客中都有详细讲解和原理分析。 "},"zother6-JavaGuide/system-design/framework/spring/spring-annotations.html":{"url":"zother6-JavaGuide/system-design/framework/spring/spring-annotations.html","title":"Spring Annotations","keywords":"","body":"文章目录 文章目录 0.前言 1. @SpringBootApplication 2. Spring Bean 相关 2.1. @Autowired 2.2. Component,@Repository,@Service, @Controller 2.3. @RestController 2.4. @Scope 2.5. Configuration 3. 处理常见的 HTTP 请求类型 3.1. GET 请求 3.2. POST 请求 3.3. PUT 请求 3.4. DELETE 请求 3.5. PATCH 请求 4. 前后端传值 4.1. @PathVariable 和 @RequestParam 4.2. @RequestBody 5. 读取配置信息 5.1. @value(常用) 5.2. @ConfigurationProperties(常用) 5.3. PropertySource（不常用） 6. 参数校验 6.1. 一些常用的字段验证的注解 6.2. 验证请求体(RequestBody) 6.3. 验证请求参数(Path Variables 和 Request Parameters) 7. 全局处理 Controller 层异常 8. JPA 相关 8.1. 创建表 8.2. 创建主键 8.3. 设置字段类型 8.4. 指定不持久化特定字段 8.5. 声明大字段 8.6. 创建枚举类型的字段 8.7. 增加审计功能 8.8. 删除/修改数据 8.9. 关联关系 9. 事务 @Transactional 10. json 数据处理 10.1. 过滤 json 数据 10.2. 格式化 json 数据 10.3. 扁平化对象 11. 测试相关 0.前言 大家好，我是 Guide 哥！这是我的 221 篇优质原创文章。如需转载，请在文首注明地址，蟹蟹！ 本文已经收录进我的 75K Star 的 Java 开源项目 JavaGuide：https://github.com/Snailclimb/JavaGuide。 可以毫不夸张地说，这篇文章介绍的 Spring/SpringBoot 常用注解基本已经涵盖你工作中遇到的大部分常用的场景。对于每一个注解我都说了具体用法，掌握搞懂，使用 SpringBoot 来开发项目基本没啥大问题了！ 为什么要写这篇文章？ 最近看到网上有一篇关于 SpringBoot 常用注解的文章被转载的比较多，我看了文章内容之后属实觉得质量有点低，并且有点会误导没有太多实际使用经验的人（这些人又占据了大多数）。所以，自己索性花了大概 两天时间简单总结一下了。 因为我个人的能力和精力有限，如果有任何不对或者需要完善的地方，请帮忙指出！Guide 哥感激不尽！ 1. @SpringBootApplication 这里先单独拎出@SpringBootApplication 注解说一下，虽然我们一般不会主动去使用它。 Guide 哥：这个注解是 Spring Boot 项目的基石，创建 SpringBoot 项目之后会默认在主类加上。 @SpringBootApplication public class SpringSecurityJwtGuideApplication { public static void main(java.lang.String[] args) { SpringApplication.run(SpringSecurityJwtGuideApplication.class, args); } } 我们可以把 @SpringBootApplication看作是 @Configuration、@EnableAutoConfiguration、@ComponentScan 注解的集合。 package org.springframework.boot.autoconfigure; @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) }) public @interface SpringBootApplication { ...... } package org.springframework.boot; @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Configuration public @interface SpringBootConfiguration { } 根据 SpringBoot 官网，这三个注解的作用分别是： @EnableAutoConfiguration：启用 SpringBoot 的自动配置机制 @ComponentScan： 扫描被@Component (@Service,@Controller)注解的 bean，注解默认会扫描该类所在的包下所有的类。 @Configuration：允许在 Spring 上下文中注册额外的 bean 或导入其他配置类 2. Spring Bean 相关 2.1. @Autowired 自动导入对象到类中，被注入进的类同样要被 Spring 容器管理比如：Service 类注入到 Controller 类中。 @Service public class UserService { ...... } @RestController @RequestMapping(\"/users\") public class UserController { @Autowired private UserService userService; ...... } 2.2. Component,@Repository,@Service, @Controller 我们一般使用 @Autowired 注解让 Spring 容器帮我们自动装配 bean。要想把类标识成可用于 @Autowired 注解自动装配的 bean 的类,可以采用以下注解实现： @Component ：通用的注解，可标注任意类为 Spring 组件。如果一个 Bean 不知道属于哪个层，可以使用@Component 注解标注。 @Repository : 对应持久层即 Dao 层，主要用于数据库相关操作。 @Service : 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao 层。 @Controller : 对应 Spring MVC 控制层，主要用户接受用户请求并调用 Service 层返回数据给前端页面。 2.3. @RestController @RestController注解是@Controller和@ResponseBody的合集,表示这是个控制器 bean,并且是将函数的返回值直 接填入 HTTP 响应体中,是 REST 风格的控制器。 Guide 哥：现在都是前后端分离，说实话我已经很久没有用过@Controller。如果你的项目太老了的话，就当我没说。 单独使用 @Controller 不加 @ResponseBody的话一般使用在要返回一个视图的情况，这种情况属于比较传统的 Spring MVC 的应用，对应于前后端不分离的情况。@Controller +@ResponseBody 返回 JSON 或 XML 形式数据 关于@RestController 和 @Controller的对比，请看这篇文章：@RestController vs @Controller。 2.4. @Scope 声明 Spring Bean 的作用域，使用方法: @Bean @Scope(\"singleton\") public Person personSingleton() { return new Person(); } 四种常见的 Spring Bean 的作用域： singleton : 唯一 bean 实例，Spring 中的 bean 默认都是单例的。 prototype : 每次请求都会创建一个新的 bean 实例。 request : 每一次 HTTP 请求都会产生一个新的 bean，该 bean 仅在当前 HTTP request 内有效。 session : 每一次 HTTP 请求都会产生一个新的 bean，该 bean 仅在当前 HTTP session 内有效。 2.5. Configuration 一般用来声明配置类，可以使用 @Component注解替代，不过使用Configuration注解声明配置类更加语义化。 @Configuration public class AppConfig { @Bean public TransferService transferService() { return new TransferServiceImpl(); } } 3. 处理常见的 HTTP 请求类型 5 种常见的请求类型: GET ：请求从服务器获取特定资源。举个例子：GET /users（获取所有学生） POST ：在服务器上创建一个新的资源。举个例子：POST /users（创建学生） PUT ：更新服务器上的资源（客户端提供更新后的整个资源）。举个例子：PUT /users/12（更新编号为 12 的学生） DELETE ：从服务器删除特定的资源。举个例子：DELETE /users/12（删除编号为 12 的学生） PATCH ：更新服务器上的资源（客户端提供更改的属性，可以看做作是部分更新），使用的比较少，这里就不举例子了。 3.1. GET 请求 @GetMapping(\"users\") 等价于@RequestMapping(value=\"/users\",method=RequestMethod.GET) @GetMapping(\"/users\") public ResponseEntity> getAllUsers() { return userRepository.findAll(); } 3.2. POST 请求 @PostMapping(\"users\") 等价于@RequestMapping(value=\"/users\",method=RequestMethod.POST) 关于@RequestBody注解的使用，在下面的“前后端传值”这块会讲到。 @PostMapping(\"/users\") public ResponseEntity createUser(@Valid @RequestBody UserCreateRequest userCreateRequest) { return userRespository.save(user); } 3.3. PUT 请求 @PutMapping(\"/users/{userId}\") 等价于@RequestMapping(value=\"/users/{userId}\",method=RequestMethod.PUT) @PutMapping(\"/users/{userId}\") public ResponseEntity updateUser(@PathVariable(value = \"userId\") Long userId, @Valid @RequestBody UserUpdateRequest userUpdateRequest) { ...... } 3.4. DELETE 请求 @DeleteMapping(\"/users/{userId}\")等价于@RequestMapping(value=\"/users/{userId}\",method=RequestMethod.DELETE) @DeleteMapping(\"/users/{userId}\") public ResponseEntity deleteUser(@PathVariable(value = \"userId\") Long userId){ ...... } 3.5. PATCH 请求 一般实际项目中，我们都是 PUT 不够用了之后才用 PATCH 请求去更新数据。 @PatchMapping(\"/profile\") public ResponseEntity updateStudent(@RequestBody StudentUpdateRequest studentUpdateRequest) { studentRepository.updateDetail(studentUpdateRequest); return ResponseEntity.ok().build(); } 4. 前后端传值 掌握前后端传值的正确姿势，是你开始 CRUD 的第一步！ 4.1. @PathVariable 和 @RequestParam @PathVariable用于获取路径参数，@RequestParam用于获取查询参数。 举个简单的例子： @GetMapping(\"/klasses/{klassId}/teachers\") public List getKlassRelatedTeachers( @PathVariable(\"klassId\") Long klassId, @RequestParam(value = \"type\", required = false) String type ) { ... } 如果我们请求的 url 是：/klasses/{123456}/teachers?type=web 那么我们服务获取到的数据就是：klassId=123456,type=web。 4.2. @RequestBody 用于读取 Request 请求（可能是 POST,PUT,DELETE,GET 请求）的 body 部分并且Content-Type 为 application/json 格式的数据，接收到数据之后会自动将数据绑定到 Java 对象上去。系统会使用HttpMessageConverter或者自定义的HttpMessageConverter将请求的 body 中的 json 字符串转换为 java 对象。 我用一个简单的例子来给演示一下基本使用！ 我们有一个注册的接口： @PostMapping(\"/sign-up\") public ResponseEntity signUp(@RequestBody @Valid UserRegisterRequest userRegisterRequest) { userService.save(userRegisterRequest); return ResponseEntity.ok().build(); } UserRegisterRequest对象： @Data @AllArgsConstructor @NoArgsConstructor public class UserRegisterRequest { @NotBlank private String userName; @NotBlank private String password; @FullName @NotBlank private String fullName; } 我们发送 post 请求到这个接口，并且 body 携带 JSON 数据： {\"userName\":\"coder\",\"fullName\":\"shuangkou\",\"password\":\"123456\"} 这样我们的后端就可以直接把 json 格式的数据映射到我们的 UserRegisterRequest 类上。 👉 需要注意的是：一个请求方法只可以有一个@RequestBody，但是可以有多个@RequestParam和@PathVariable。 如果你的方法必须要用两个 @RequestBody来接受数据的话，大概率是你的数据库设计或者系统设计出问题了！ 5. 读取配置信息 很多时候我们需要将一些常用的配置信息比如阿里云 oss、发送短信、微信认证的相关配置信息等等放到配置文件中。 下面我们来看一下 Spring 为我们提供了哪些方式帮助我们从配置文件中读取这些配置信息。 我们的数据源application.yml内容如下：： wuhan2020: 2020年初武汉爆发了新型冠状病毒，疫情严重，但是，我相信一切都会过去！武汉加油！中国加油！ my-profile: name: Guide哥 email: koushuangbwcx@163.com library: location: 湖北武汉加油中国加油 books: - name: 天才基本法 description: 二十二岁的林朝夕在父亲确诊阿尔茨海默病这天，得知自己暗恋多年的校园男神裴之即将出国深造的消息——对方考取的学校，恰是父亲当年为她放弃的那所。 - name: 时间的秩序 description: 为什么我们记得过去，而非未来？时间“流逝”意味着什么？是我们存在于时间之内，还是时间存在于我们之中？卡洛·罗韦利用诗意的文字，邀请我们思考这一亘古难题——时间的本质。 - name: 了不起的我 description: 如何养成一个新习惯？如何让心智变得更成熟？如何拥有高质量的关系？ 如何走出人生的艰难时刻？ 5.1. @value(常用) 使用 @Value(\"${property}\") 读取比较简单的配置信息： @Value(\"${wuhan2020}\") String wuhan2020; 5.2. @ConfigurationProperties(常用) 通过@ConfigurationProperties读取配置信息并与 bean 绑定。 @Component @ConfigurationProperties(prefix = \"library\") class LibraryProperties { @NotEmpty private String location; private List books; @Setter @Getter @ToString static class Book { String name; String description; } 省略getter/setter ...... } 你可以像使用普通的 Spring bean 一样，将其注入到类中使用。 5.3. PropertySource（不常用） @PropertySource读取指定 properties 文件 @Component @PropertySource(\"classpath:website.properties\") class WebSite { @Value(\"${url}\") private String url; 省略getter/setter ...... } 更多内容请查看我的这篇文章：《10 分钟搞定 SpringBoot 如何优雅读取配置文件？》 。 6. 参数校验 数据的校验的重要性就不用说了，即使在前端对数据进行校验的情况下，我们还是要对传入后端的数据再进行一遍校验，避免用户绕过浏览器直接通过一些 HTTP 工具直接向后端请求一些违法数据。 JSR(Java Specification Requests） 是一套 JavaBean 参数校验的标准，它定义了很多常用的校验注解，我们可以直接将这些注解加在我们 JavaBean 的属性上面，这样就可以在需要校验的时候进行校验了，非常方便！ 校验的时候我们实际用的是 Hibernate Validator 框架。Hibernate Validator 是 Hibernate 团队最初的数据校验框架，Hibernate Validator 4.x 是 Bean Validation 1.0（JSR 303）的参考实现，Hibernate Validator 5.x 是 Bean Validation 1.1（JSR 349）的参考实现，目前最新版的 Hibernate Validator 6.x 是 Bean Validation 2.0（JSR 380）的参考实现。 SpringBoot 项目的 spring-boot-starter-web 依赖中已经有 hibernate-validator 包，不需要引用相关依赖。如下图所示（通过 idea 插件—Maven Helper 生成）： 非 SpringBoot 项目需要自行引入相关依赖包，这里不多做讲解，具体可以查看我的这篇文章：《如何在 Spring/Spring Boot 中做参数校验？你需要了解的都在这里！》。 👉 需要注意的是： 所有的注解，推荐使用 JSR 注解，即javax.validation.constraints，而不是org.hibernate.validator.constraints 6.1. 一些常用的字段验证的注解 @NotEmpty 被注释的字符串的不能为 null 也不能为空 @NotBlank 被注释的字符串非 null，并且必须包含一个非空白字符 @Null 被注释的元素必须为 null @NotNull 被注释的元素必须不为 null @AssertTrue 被注释的元素必须为 true @AssertFalse 被注释的元素必须为 false @Pattern(regex=,flag=)被注释的元素必须符合指定的正则表达式 @Email 被注释的元素必须是 Email 格式。 @Min(value)被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @Max(value)被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @DecimalMin(value)被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @DecimalMax(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @Size(max=, min=)被注释的元素的大小必须在指定的范围内 @Digits (integer, fraction)被注释的元素必须是一个数字，其值必须在可接受的范围内 @Past被注释的元素必须是一个过去的日期 @Future 被注释的元素必须是一个将来的日期 ...... 6.2. 验证请求体(RequestBody) @Data @AllArgsConstructor @NoArgsConstructor public class Person { @NotNull(message = \"classId 不能为空\") private String classId; @Size(max = 33) @NotNull(message = \"name 不能为空\") private String name; @Pattern(regexp = \"((^Man$|^Woman$|^UGM$))\", message = \"sex 值不在可选范围\") @NotNull(message = \"sex 不能为空\") private String sex; @Email(message = \"email 格式不正确\") @NotNull(message = \"email 不能为空\") private String email; } 我们在需要验证的参数上加上了@Valid注解，如果验证失败，它将抛出MethodArgumentNotValidException。 @RestController @RequestMapping(\"/api\") public class PersonController { @PostMapping(\"/person\") public ResponseEntity getPerson(@RequestBody @Valid Person person) { return ResponseEntity.ok().body(person); } } 6.3. 验证请求参数(Path Variables 和 Request Parameters) 一定一定不要忘记在类上加上 Validated 注解了，这个参数可以告诉 Spring 去校验方法参数。 @RestController @RequestMapping(\"/api\") @Validated public class PersonController { @GetMapping(\"/person/{id}\") public ResponseEntity getPersonByID(@Valid @PathVariable(\"id\") @Max(value = 5,message = \"超过 id 的范围了\") Integer id) { return ResponseEntity.ok().body(id); } } 更多关于如何在 Spring 项目中进行参数校验的内容，请看《如何在 Spring/Spring Boot 中做参数校验？你需要了解的都在这里！》这篇文章。 7. 全局处理 Controller 层异常 介绍一下我们 Spring 项目必备的全局处理 Controller 层异常。 相关注解： @ControllerAdvice :注解定义全局异常处理类 @ExceptionHandler :注解声明异常处理方法 如何使用呢？拿我们在第 5 节参数校验这块来举例子。如果方法参数不对的话就会抛出MethodArgumentNotValidException，我们来处理这个异常。 @ControllerAdvice @ResponseBody public class GlobalExceptionHandler { /** * 请求参数异常处理 */ @ExceptionHandler(MethodArgumentNotValidException.class) public ResponseEntity handleMethodArgumentNotValidException(MethodArgumentNotValidException ex, HttpServletRequest request) { ...... } } 更多关于 Spring Boot 异常处理的内容，请看我的这两篇文章： SpringBoot 处理异常的几种常见姿势 使用枚举简单封装一个优雅的 Spring Boot 全局异常处理！ 8. JPA 相关 8.1. 创建表 @Entity声明一个类对应一个数据库实体。 @Table 设置表明 @Entity @Table(name = \"role\") public class Role { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String name; private String description; 省略getter/setter...... } 8.2. 创建主键 @Id ：声明一个字段为主键。 使用@Id声明之后，我们还需要定义主键的生成策略。我们可以使用 @GeneratedValue 指定主键生成策略。 1.通过 @GeneratedValue直接使用 JPA 内置提供的四种主键生成策略来指定主键生成策略。 @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; JPA 使用枚举定义了 4 中常见的主键生成策略，如下： Guide 哥：枚举替代常量的一种用法 public enum GenerationType { /** * 使用一个特定的数据库表格来保存主键 * 持久化引擎通过关系数据库的一张特定的表格来生成主键, */ TABLE, /** *在某些数据库中,不支持主键自增长,比如Oracle、PostgreSQL其提供了一种叫做\"序列(sequence)\"的机制生成主键 */ SEQUENCE, /** * 主键自增长 */ IDENTITY, /** *把主键生成策略交给持久化引擎(persistence engine), *持久化引擎会根据数据库在以上三种主键生成 策略中选择其中一种 */ AUTO } @GeneratedValue注解默认使用的策略是GenerationType.AUTO public @interface GeneratedValue { GenerationType strategy() default AUTO; String generator() default \"\"; } 一般使用 MySQL 数据库的话，使用GenerationType.IDENTITY策略比较普遍一点（分布式系统的话需要另外考虑使用分布式 ID）。 2.通过 @GenericGenerator声明一个主键策略，然后 @GeneratedValue使用这个策略 @Id @GeneratedValue(generator = \"IdentityIdGenerator\") @GenericGenerator(name = \"IdentityIdGenerator\", strategy = \"identity\") private Long id; 等价于： @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; jpa 提供的主键生成策略有如下几种： public class DefaultIdentifierGeneratorFactory implements MutableIdentifierGeneratorFactory, Serializable, ServiceRegistryAwareService { @SuppressWarnings(\"deprecation\") public DefaultIdentifierGeneratorFactory() { register( \"uuid2\", UUIDGenerator.class ); register( \"guid\", GUIDGenerator.class ); // can be done with UUIDGenerator + strategy register( \"uuid\", UUIDHexGenerator.class ); // \"deprecated\" for new use register( \"uuid.hex\", UUIDHexGenerator.class ); // uuid.hex is deprecated register( \"assigned\", Assigned.class ); register( \"identity\", IdentityGenerator.class ); register( \"select\", SelectGenerator.class ); register( \"sequence\", SequenceStyleGenerator.class ); register( \"seqhilo\", SequenceHiLoGenerator.class ); register( \"increment\", IncrementGenerator.class ); register( \"foreign\", ForeignGenerator.class ); register( \"sequence-identity\", SequenceIdentityGenerator.class ); register( \"enhanced-sequence\", SequenceStyleGenerator.class ); register( \"enhanced-table\", TableGenerator.class ); } public void register(String strategy, Class generatorClass) { LOG.debugf( \"Registering IdentifierGenerator strategy [%s] -> [%s]\", strategy, generatorClass.getName() ); final Class previous = generatorStrategyToClassNameMap.put( strategy, generatorClass ); if ( previous != null ) { LOG.debugf( \" - overriding [%s]\", previous.getName() ); } } } 8.3. 设置字段类型 @Column 声明字段。 示例： 设置属性 userName 对应的数据库字段名为 user_name，长度为 32，非空 @Column(name = \"user_name\", nullable = false, length=32) private String userName; 设置字段类型并且加默认值，这个还是挺常用的。 Column(columnDefinition = \"tinyint(1) default 1\") private Boolean enabled; 8.4. 指定不持久化特定字段 @Transient ：声明不需要与数据库映射的字段，在保存的时候不需要保存进数据库 。 如果我们想让secrect 这个字段不被持久化，可以使用 @Transient关键字声明。 Entity(name=\"USER\") public class User { ...... @Transient private String secrect; // not persistent because of @Transient } 除了 @Transient关键字声明， 还可以采用下面几种方法： static String secrect; // not persistent because of static final String secrect = “Satish”; // not persistent because of final transient String secrect; // not persistent because of transient 一般使用注解的方式比较多。 8.5. 声明大字段 @Lob:声明某个字段为大字段。 @Lob private String content; 更详细的声明： @Lob //指定 Lob 类型数据的获取策略， FetchType.EAGER 表示非延迟 加载，而 FetchType. LAZY 表示延迟加载 ； @Basic(fetch = FetchType.EAGER) //columnDefinition 属性指定数据表对应的 Lob 字段类型 @Column(name = \"content\", columnDefinition = \"LONGTEXT NOT NULL\") private String content; 8.6. 创建枚举类型的字段 可以使用枚举类型的字段，不过枚举字段要用@Enumerated注解修饰。 public enum Gender { MALE(\"男性\"), FEMALE(\"女性\"); private String value; Gender(String str){ value=str; } } @Entity @Table(name = \"role\") public class Role { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String name; private String description; @Enumerated(EnumType.STRING) private Gender gender; 省略getter/setter...... } 数据库里面对应存储的是 MAIL/FEMAIL。 8.7. 增加审计功能 只要继承了 AbstractAuditBase的类都会默认加上下面四个字段。 @Data @AllArgsConstructor @NoArgsConstructor @MappedSuperclass @EntityListeners(value = AuditingEntityListener.class) public abstract class AbstractAuditBase { @CreatedDate @Column(updatable = false) @JsonIgnore private Instant createdAt; @LastModifiedDate @JsonIgnore private Instant updatedAt; @CreatedBy @Column(updatable = false) @JsonIgnore private String createdBy; @LastModifiedBy @JsonIgnore private String updatedBy; } 我们对应的审计功能对应地配置类可能是下面这样的（Spring Security 项目）: @Configuration @EnableJpaAuditing public class AuditSecurityConfiguration { @Bean AuditorAware auditorAware() { return () -> Optional.ofNullable(SecurityContextHolder.getContext()) .map(SecurityContext::getAuthentication) .filter(Authentication::isAuthenticated) .map(Authentication::getName); } } 简单介绍一下上面设计到的一些注解： @CreatedDate: 表示该字段为创建时间时间字段，在这个实体被 insert 的时候，会设置值 @CreatedBy :表示该字段为创建人，在这个实体被 insert 的时候，会设置值 @LastModifiedDate、@LastModifiedBy同理。 @EnableJpaAuditing：开启 JPA 审计功能。 8.8. 删除/修改数据 @Modifying 注解提示 JPA 该操作是修改操作,注意还要配合@Transactional注解使用。 @Repository public interface UserRepository extends JpaRepository { @Modifying @Transactional(rollbackFor = Exception.class) void deleteByUserName(String userName); } 8.9. 关联关系 @OneToOne 声明一对一关系 @OneToMany 声明一对多关系 @ManyToOne声明多对一关系 MangToMang声明多对多关系 更多关于 Spring Boot JPA 的文章请看我的这篇文章：一文搞懂如何在 Spring Boot 正确中使用 JPA 。 9. 事务 @Transactional 在要开启事务的方法上使用@Transactional注解即可! @Transactional(rollbackFor = Exception.class) public void save() { ...... } 我们知道 Exception 分为运行时异常 RuntimeException 和非运行时异常。在@Transactional注解中如果不配置rollbackFor属性,那么事物只会在遇到RuntimeException的时候才会回滚,加上rollbackFor=Exception.class,可以让事物在遇到非运行时异常时也回滚。 @Transactional 注解一般用在可以作用在类或者方法上。 作用于类：当把@Transactional 注解放在类上时，表示所有该类的public 方法都配置相同的事务属性信息。 作用于方法：当类配置了@Transactional，方法也配置了@Transactional，方法的事务会覆盖类的事务配置信息。 更多关于关于 Spring 事务的内容请查看： 可能是最漂亮的 Spring 事务管理详解 一口气说出 6 种 @Transactional 注解失效场景 10. json 数据处理 10.1. 过滤 json 数据 @JsonIgnoreProperties 作用在类上用于过滤掉特定字段不返回或者不解析。 //生成json时将userRoles属性过滤 @JsonIgnoreProperties({\"userRoles\"}) public class User { private String userName; private String fullName; private String password; @JsonIgnore private List userRoles = new ArrayList<>(); } @JsonIgnore一般用于类的属性上，作用和上面的@JsonIgnoreProperties 一样。 public class User { private String userName; private String fullName; private String password; //生成json时将userRoles属性过滤 @JsonIgnore private List userRoles = new ArrayList<>(); } 10.2. 格式化 json 数据 @JsonFormat一般用来格式化 json 数据。： 比如： @JsonFormat(shape=JsonFormat.Shape.STRING, pattern=\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\", timezone=\"GMT\") private Date date; 10.3. 扁平化对象 @Getter @Setter @ToString public class Account { @JsonUnwrapped private Location location; @JsonUnwrapped private PersonInfo personInfo; @Getter @Setter @ToString public static class Location { private String provinceName; private String countyName; } @Getter @Setter @ToString public static class PersonInfo { private String userName; private String fullName; } } 未扁平化之前： { \"location\": { \"provinceName\":\"湖北\", \"countyName\":\"武汉\" }, \"personInfo\": { \"userName\": \"coder1234\", \"fullName\": \"shaungkou\" } } 使用@JsonUnwrapped 扁平对象之后： @Getter @Setter @ToString public class Account { @JsonUnwrapped private Location location; @JsonUnwrapped private PersonInfo personInfo; ...... } { \"provinceName\":\"湖北\", \"countyName\":\"武汉\", \"userName\": \"coder1234\", \"fullName\": \"shaungkou\" } 11. 测试相关 @ActiveProfiles一般作用于测试类上， 用于声明生效的 Spring 配置文件。 @SpringBootTest(webEnvironment = RANDOM_PORT) @ActiveProfiles(\"test\") @Slf4j public abstract class TestBase { ...... } @Test声明一个方法为测试方法 @Transactional被声明的测试方法的数据会回滚，避免污染测试数据。 @WithMockUser Spring Security 提供的，用来模拟一个真实用户，并且可以赋予权限。 @Test @Transactional @WithMockUser(username = \"user-id-18163138155\", authorities = \"ROLE_TEACHER\") void should_import_student_success() throws Exception { ...... } 暂时总结到这里吧！虽然花了挺长时间才写完，不过可能还是会一些常用的注解的被漏掉，所以，我将文章也同步到了 Github 上去，Github 地址： 欢迎完善！ 本文已经收录进我的 75K Star 的 Java 开源项目 JavaGuide：https://github.com/Snailclimb/JavaGuide。 "},"zother6-JavaGuide/system-design/framework/spring/Spring-Design-Patterns.html":{"url":"zother6-JavaGuide/system-design/framework/spring/Spring-Design-Patterns.html","title":"Spring Design Patterns","keywords":"","body":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。 控制反转(IoC)和依赖注入(DI) 工厂设计模式 单例设计模式 代理设计模式 代理模式在 AOP 中的应用 Spring AOP 和 AspectJ AOP 有什么区别? 模板方法 观察者模式 Spring 事件驱动模型中的三种角色 事件角色 事件监听者角色 事件发布者角色 Spring 的事件流程总结 适配器模式 spring AOP中的适配器模式 spring MVC中的适配器模式 装饰者模式 总结 参考 JDK 中用到了那些设计模式?Spring 中用到了那些设计模式?这两个问题，在面试中比较常见。我在网上搜索了一下关于 Spring 中设计模式的讲解几乎都是千篇一律，而且大部分都年代久远。所以，花了几天时间自己总结了一下，由于我的个人能力有限，文中如有任何错误各位都可以指出。另外，文章篇幅有限，对于设计模式以及一些源码的解读我只是一笔带过，这篇文章的主要目的是回顾一下 Spring 中的设计模式。 Design Patterns(设计模式) 表示面向对象软件开发中最好的计算机编程实践。 Spring 框架中广泛使用了不同类型的设计模式，下面我们来看看到底有哪些设计模式? 控制反转(IoC)和依赖注入(DI) IoC(Inversion of Control,控制翻转) 是Spring 中一个非常非常重要的概念，它不是什么技术，而是一种解耦的设计思想。它的主要目的是借助于“第三方”(Spring 中的 IOC 容器) 实现具有依赖关系的对象之间的解耦(IOC容易管理对象，你只管使用即可)，从而降低代码之间的耦合度。IOC 是一个原则，而不是一个模式，以下模式（但不限于）实现了IoC原则。 Spring IOC 容器就像是一个工厂一样，当我们需要创建一个对象的时候，只需要配置好配置文件/注解即可，完全不用考虑对象是如何被创建出来的。 IOC 容器负责创建对象，将对象连接在一起，配置这些对象，并从创建中处理这些对象的整个生命周期，直到它们被完全销毁。 在实际项目中一个 Service 类如果有几百甚至上千个类作为它的底层，我们需要实例化这个 Service，你可能要每次都要搞清这个 Service 所有底层类的构造函数，这可能会把人逼疯。如果利用 IOC 的话，你只需要配置好，然后在需要的地方引用就行了，这大大增加了项目的可维护性且降低了开发难度。关于Spring IOC 的理解，推荐看这一下知乎的一个回答：https://www.zhihu.com/question/23277575/answer/169698662 ，非常不错。 控制翻转怎么理解呢? 举个例子：\"对象a 依赖了对象 b，当对象 a 需要使用 对象 b的时候必须自己去创建。但是当系统引入了 IOC 容器后， 对象a 和对象 b 之前就失去了直接的联系。这个时候，当对象 a 需要使用 对象 b的时候， 我们可以指定 IOC 容器去创建一个对象b注入到对象 a 中\"。 对象 a 获得依赖对象 b 的过程,由主动行为变为了被动行为，控制权翻转，这就是控制反转名字的由来。 DI(Dependecy Inject,依赖注入)是实现控制反转的一种设计模式，依赖注入就是将实例变量传入到一个对象中去。 工厂设计模式 Spring使用工厂模式可以通过 BeanFactory 或 ApplicationContext 创建 bean 对象。 两者对比： BeanFactory ：延迟注入(使用到某个 bean 的时候才会注入),相比于ApplicationContext 来说会占用更少的内存，程序启动速度更快。 ApplicationContext ：容器启动的时候，不管你用没用到，一次性创建所有 bean 。BeanFactory 仅提供了最基本的依赖注入支持，ApplicationContext 扩展了 BeanFactory ,除了有BeanFactory的功能还有额外更多功能，所以一般开发人员使用ApplicationContext会更多。 ApplicationContext的三个实现类： ClassPathXmlApplication：把上下文文件当成类路径资源。 FileSystemXmlApplication：从文件系统中的 XML 文件载入上下文定义信息。 XmlWebApplicationContext：从Web系统中的XML文件载入上下文定义信息。 Example: import org.springframework.context.ApplicationContext; import org.springframework.context.support.FileSystemXmlApplicationContext; public class App { public static void main(String[] args) { ApplicationContext context = new FileSystemXmlApplicationContext( \"C:/work/IOC Containers/springframework.applicationcontext/src/main/resources/bean-factory-config.xml\"); HelloApplicationContext obj = (HelloApplicationContext) context.getBean(\"helloApplicationContext\"); obj.getMsg(); } } 单例设计模式 在我们的系统中，有一些对象其实我们只需要一个，比如说：线程池、缓存、对话框、注册表、日志对象、充当打印机、显卡等设备驱动程序的对象。事实上，这一类对象只能有一个实例，如果制造出多个实例就可能会导致一些问题的产生，比如：程序的行为异常、资源使用过量、或者不一致性的结果。 使用单例模式的好处: 对于频繁使用的对象，可以省略创建对象所花费的时间，这对于那些重量级对象而言，是非常可观的一笔系统开销； 由于 new 操作的次数减少，因而对系统内存的使用频率也会降低，这将减轻 GC 压力，缩短 GC 停顿时间。 Spring 中 bean 的默认作用域就是 singleton(单例)的。 除了 singleton 作用域，Spring 中 bean 还有下面几种作用域： prototype : 每次请求都会创建一个新的 bean 实例。 request : 每一次HTTP请求都会产生一个新的bean，该bean仅在当前HTTP request内有效。 session : 每一次HTTP请求都会产生一个新的 bean，该bean仅在当前 HTTP session 内有效。 global-session： 全局session作用域，仅仅在基于portlet的web应用中才有意义，Spring5已经没有了。Portlet是能够生成语义代码(例如：HTML)片段的小型Java Web插件。它们基于portlet容器，可以像servlet一样处理HTTP请求。但是，与 servlet 不同，每个 portlet 都有不同的会话 Spring 实现单例的方式： xml : 注解：@Scope(value = \"singleton\") Spring 通过 ConcurrentHashMap 实现单例注册表的特殊方式实现单例模式。Spring 实现单例的核心代码如下 // 通过 ConcurrentHashMap（线程安全） 实现单例注册表 private final Map singletonObjects = new ConcurrentHashMap(64); public Object getSingleton(String beanName, ObjectFactory singletonFactory) { Assert.notNull(beanName, \"'beanName' must not be null\"); synchronized (this.singletonObjects) { // 检查缓存中是否存在实例 Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) { //...省略了很多代码 try { singletonObject = singletonFactory.getObject(); } //...省略了很多代码 // 如果实例对象在不存在，我们注册到单例注册表中。 addSingleton(beanName, singletonObject); } return (singletonObject != NULL_OBJECT ? singletonObject : null); } } //将对象添加到单例注册表 protected void addSingleton(String beanName, Object singletonObject) { synchronized (this.singletonObjects) { this.singletonObjects.put(beanName, (singletonObject != null ? singletonObject : NULL_OBJECT)); } } } 代理设计模式 代理模式在 AOP 中的应用 AOP(Aspect-Oriented Programming:面向切面编程)能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。 Spring AOP 就是基于动态代理的，如果要代理的对象，实现了某个接口，那么Spring AOP会使用JDK Proxy，去创建代理对象，而对于没有实现接口的对象，就无法使用 JDK Proxy 去进行代理了，这时候Spring AOP会使用Cglib ，这时候Spring AOP会使用 Cglib 生成一个被代理对象的子类来作为代理，如下图所示： 当然你也可以使用 AspectJ ,Spring AOP 已经集成了AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。 使用 AOP 之后我们可以把一些通用功能抽象出来，在需要用到的地方直接使用即可，这样大大简化了代码量。我们需要增加新功能时也方便，这样也提高了系统扩展性。日志功能、事务管理等等场景都用到了 AOP 。 Spring AOP 和 AspectJ AOP 有什么区别? Spring AOP 属于运行时增强，而 AspectJ 是编译时增强。 Spring AOP 基于代理(Proxying)，而 AspectJ 基于字节码操作(Bytecode Manipulation)。 Spring AOP 已经集成了 AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。AspectJ 相比于 Spring AOP 功能更加强大，但是 Spring AOP 相对来说更简单， 如果我们的切面比较少，那么两者性能差异不大。但是，当切面太多的话，最好选择 AspectJ ，它比Spring AOP 快很多。 模板方法 模板方法模式是一种行为设计模式，它定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。 模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤的实现方式。 public abstract class Template { //这是我们的模板方法 public final void TemplateMethod(){ PrimitiveOperation1(); PrimitiveOperation2(); PrimitiveOperation3(); } protected void PrimitiveOperation1(){ //当前类实现 } //被子类实现的方法 protected abstract void PrimitiveOperation2(); protected abstract void PrimitiveOperation3(); } public class TemplateImpl extends Template { @Override public void PrimitiveOperation2() { //当前类实现 } @Override public void PrimitiveOperation3() { //当前类实现 } } Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。一般情况下，我们都是使用继承的方式来实现模板模式，但是 Spring 并没有使用这种方式，而是使用Callback 模式与模板方法模式配合，既达到了代码复用的效果，同时增加了灵活性。 观察者模式 观察者模式是一种对象行为型模式。它表示的是一种对象与对象之间具有依赖关系，当一个对象发生改变的时候，这个对象所依赖的对象也会做出反应。Spring 事件驱动模型就是观察者模式很经典的一个应用。Spring 事件驱动模型非常有用，在很多场景都可以解耦我们的代码。比如我们每次添加商品的时候都需要重新更新商品索引，这个时候就可以利用观察者模式来解决这个问题。 Spring 事件驱动模型中的三种角色 事件角色 ApplicationEvent (org.springframework.context包下)充当事件的角色,这是一个抽象类，它继承了java.util.EventObject并实现了 java.io.Serializable接口。 Spring 中默认存在以下事件，他们都是对 ApplicationContextEvent 的实现(继承自ApplicationContextEvent)： ContextStartedEvent：ApplicationContext 启动后触发的事件; ContextStoppedEvent：ApplicationContext 停止后触发的事件; ContextRefreshedEvent：ApplicationContext 初始化或刷新完成后触发的事件; ContextClosedEvent：ApplicationContext 关闭后触发的事件。 事件监听者角色 ApplicationListener 充当了事件监听者角色，它是一个接口，里面只定义了一个 onApplicationEvent（）方法来处理ApplicationEvent。ApplicationListener接口类源码如下，可以看出接口定义看出接口中的事件只要实现了 ApplicationEvent就可以了。所以，在 Spring中我们只要实现 ApplicationListener 接口实现 onApplicationEvent() 方法即可完成监听事件 package org.springframework.context; import java.util.EventListener; @FunctionalInterface public interface ApplicationListener extends EventListener { void onApplicationEvent(E var1); } 事件发布者角色 ApplicationEventPublisher 充当了事件的发布者，它也是一个接口。 @FunctionalInterface public interface ApplicationEventPublisher { default void publishEvent(ApplicationEvent event) { this.publishEvent((Object)event); } void publishEvent(Object var1); } ApplicationEventPublisher 接口的publishEvent（）这个方法在AbstractApplicationContext类中被实现，阅读这个方法的实现，你会发现实际上事件真正是通过ApplicationEventMulticaster来广播出去的。具体内容过多，就不在这里分析了，后面可能会单独写一篇文章提到。 Spring 的事件流程总结 定义一个事件: 实现一个继承自 ApplicationEvent，并且写相应的构造函数； 定义一个事件监听者：实现 ApplicationListener 接口，重写 onApplicationEvent() 方法； 使用事件发布者发布消息: 可以通过 ApplicationEventPublisher 的 publishEvent() 方法发布消息。 Example: // 定义一个事件,继承自ApplicationEvent并且写相应的构造函数 public class DemoEvent extends ApplicationEvent{ private static final long serialVersionUID = 1L; private String message; public DemoEvent(Object source,String message){ super(source); this.message = message; } public String getMessage() { return message; } // 定义一个事件监听者,实现ApplicationListener接口，重写 onApplicationEvent() 方法； @Component public class DemoListener implements ApplicationListener{ //使用onApplicationEvent接收消息 @Override public void onApplicationEvent(DemoEvent event) { String msg = event.getMessage(); System.out.println(\"接收到的信息是：\"+msg); } } // 发布事件，可以通过ApplicationEventPublisher 的 publishEvent() 方法发布消息。 @Component public class DemoPublisher { @Autowired ApplicationContext applicationContext; public void publish(String message){ //发布事件 applicationContext.publishEvent(new DemoEvent(this, message)); } } 当调用 DemoPublisher 的 publish() 方法的时候，比如 demoPublisher.publish(\"你好\") ，控制台就会打印出:接收到的信息是：你好 。 适配器模式 适配器模式(Adapter Pattern) 将一个接口转换成客户希望的另一个接口，适配器模式使接口不兼容的那些类可以一起工作，其别名为包装器(Wrapper)。 spring AOP中的适配器模式 我们知道 Spring AOP 的实现是基于代理模式，但是 Spring AOP 的增强或通知(Advice)使用到了适配器模式，与之相关的接口是AdvisorAdapter 。Advice 常用的类型有：BeforeAdvice（目标方法调用前,前置通知）、AfterAdvice（目标方法调用后,后置通知）、AfterReturningAdvice(目标方法执行结束后，return之前)等等。每个类型Advice（通知）都有对应的拦截器:MethodBeforeAdviceInterceptor、AfterReturningAdviceAdapter、AfterReturningAdviceInterceptor。Spring预定义的通知要通过对应的适配器，适配成 MethodInterceptor接口(方法拦截器)类型的对象（如：MethodBeforeAdviceInterceptor 负责适配 MethodBeforeAdvice）。 spring MVC中的适配器模式 在Spring MVC中，DispatcherServlet 根据请求信息调用 HandlerMapping，解析请求对应的 Handler。解析到对应的 Handler（也就是我们平常说的 Controller 控制器）后，开始由HandlerAdapter 适配器处理。HandlerAdapter 作为期望接口，具体的适配器实现类用于对目标类进行适配，Controller 作为需要适配的类。 为什么要在 Spring MVC 中使用适配器模式？ Spring MVC 中的 Controller 种类众多，不同类型的 Controller 通过不同的方法来对请求进行处理。如果不利用适配器模式的话，DispatcherServlet 直接获取对应类型的 Controller，需要的自行来判断，像下面这段代码一样： if(mappedHandler.getHandler() instanceof MultiActionController){ ((MultiActionController)mappedHandler.getHandler()).xxx }else if(mappedHandler.getHandler() instanceof XXX){ ... }else if(...){ ... } 假如我们再增加一个 Controller类型就要在上面代码中再加入一行 判断语句，这种形式就使得程序难以维护，也违反了设计模式中的开闭原则 – 对扩展开放，对修改关闭。 装饰者模式 装饰者模式可以动态地给对象添加一些额外的属性或行为。相比于使用继承，装饰者模式更加灵活。简单点儿说就是当我们需要修改原有的功能，但我们又不愿直接去修改原有的代码时，设计一个Decorator套在原有代码外面。其实在 JDK 中就有很多地方用到了装饰者模式，比如 InputStream家族，InputStream 类下有 FileInputStream (读取文件)、BufferedInputStream (增加缓存,使读取文件速度大大提升)等子类都在不修改InputStream 代码的情况下扩展了它的功能。 Spring 中配置 DataSource 的时候，DataSource 可能是不同的数据库和数据源。我们能否根据客户的需求在少修改原有类的代码下动态切换不同的数据源？这个时候就要用到装饰者模式(这一点我自己还没太理解具体原理)。Spring 中用到的包装器模式在类名上含有 Wrapper或者 Decorator。这些类基本上都是动态地给一个对象添加一些额外的职责 总结 Spring 框架中用到了哪些设计模式？ 工厂设计模式 : Spring使用工厂模式通过 BeanFactory、ApplicationContext 创建 bean 对象。 代理设计模式 : Spring AOP 功能的实现。 单例设计模式 : Spring 中的 Bean 默认都是单例的。 模板方法模式 : Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。 包装器设计模式 : 我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。 观察者模式: Spring 事件驱动模型就是观察者模式很经典的一个应用。 适配器模式 :Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中也是用到了适配器模式适配Controller。 ...... 参考 《Spring技术内幕》 https://blog.eduonix.com/java-programming-2/learn-design-patterns-used-spring-framework/ http://blog.yeamin.top/2018/03/27/单例模式-Spring单例实现原理分析/ https://www.tutorialsteacher.com/ioc/inversion-of-control https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/observer.html https://juejin.im/post/5a8eb261f265da4e9e307230 https://juejin.im/post/5ba28986f265da0abc2b6084 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \"Java面试突击\" 即可免费领取！ Java工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/system-design/framework/spring/spring-transaction.html":{"url":"zother6-JavaGuide/system-design/framework/spring/spring-transaction.html","title":"Spring Transaction","keywords":"","body":"什么是事务？ 事务是逻辑上的一组操作，要么都执行，要么都不执行。 Guide哥：大家应该都能背上面这句话了，下面我结合我们日常的真实开发来谈一谈！ 具体对应到我们日常开发过程中是这样的：我们系统的每个业务方法可能包括了多个原子性的数据库操作，并且原子性的数据库操作是有依赖的，它们要不都执行，要不就都不执行。 另外，需要格外注意的是：事务能否生效数据库引擎是否支持事务是关键。常用的MySQL数据库默认使用支持事务的innodb引擎。但是，如果把数据库引擎变为myisam，那么程序也就不再支持事务了！ 事务最经典也经常被拿出来说例子就是转账了。假如小明要给小红转账1000元，这个转账会涉及到两个关键操作就是：将小明的余额减少1000元，将小红的余额增加1000元。万一在这两个操作之间突然出现错误比如银行系统崩溃，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。 public class OrdersService { private AccountDao accountDao; public void setOrdersDao(AccountDao accountDao) { this.accountDao = accountDao; } @Transactional(propagation = Propagation.REQUIRED, isolation = Isolation.DEFAULT, readOnly = false, timeout = -1) public void accountMoney() { //小红账户多1000 accountDao.addMoney(1000,xiaohong); //模拟突然出现的异常，比如银行中可能为突然停电等等 //如果没有配置事务管理的话会造成，小红账户多了1000而小明账户没有少钱 int i = 10 / 0; //小王账户少1000 accountDao.reduceMoney(1000,xiaoming); } } 事物的特性（ACID）了解么? 原子性： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性： 执行事务前后，数据保持一致； 隔离性： 并发访问数据库时，一个用户的事物不被其他事务所干扰也就是说多个事务并发执行时，一个事务的执行不应影响其他事务的执行； 持久性: 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 详谈Spring对事务的支持 Spring支持两种方式的事务管理 编程式事务管理： 通过Transaction Template手动管理事务，实际应用中很少使用， 置声明式事务： 推荐使用（代码侵入性最小），实际是通过AOP实现（基于@Transactional 的全注解方式使用最多，后文主要介绍这个注解的使用）。 Spring事务管理接口介绍 Spring 框架中，事务管理相关最重要的3个接口如下： PlatformTransactionManager： （平台）事务管理器，Spring事务策略的核心。 TransactionDefinition： 事务定义信息(事务隔离级别、传播行为、超时、只读、回滚规则) TransactionStatus： 事务运行状态 PlatformTransactionManager Spring并不直接管理事务，而是提供了多种事务管理器 。Spring事务管理器的接口是： org.springframework.transaction.PlatformTransactionManager ，通过这个接口，Spring为各个平台如JDBC、Hibernate等都提供了对应的事务管理器，但是具体的实现就是各个平台自己的事情了。 PlatformTransactionManager接口中定义了三个方法： package org.springframework.transaction; import org.springframework.lang.Nullable; public interface PlatformTransactionManager { //获得事务 TransactionStatus getTransaction(@Nullable TransactionDefinition var1) throws TransactionException; //提交事务 void commit(TransactionStatus var1) throws TransactionException; //回滚事务 void rollback(TransactionStatus var1) throws TransactionException; } TransactionDefinition 事务管理器接口 PlatformTransactionManager 通过 getTransaction(TransactionDefinition definition) 方法来得到一个事务，这个方法里面的参数是 TransactionDefinition类 ，这个类就定义了一些基本的事务属性。 那么什么是事务属性呢？ 事务属性可以理解成事务的一些基本配置，描述了事务策略如何应用到方法上。事务属性包含了5个方面。 TransactionDefinition 接口中定义了5个方法以及一些表示事务属性的常量比如隔离级别、传播行为等等。 package org.springframework.transaction; import org.springframework.lang.Nullable; public interface TransactionDefinition { int PROPAGATION_REQUIRED = 0; int PROPAGATION_SUPPORTS = 1; int PROPAGATION_MANDATORY = 2; int PROPAGATION_REQUIRES_NEW = 3; int PROPAGATION_NOT_SUPPORTED = 4; int PROPAGATION_NEVER = 5; int PROPAGATION_NESTED = 6; int ISOLATION_DEFAULT = -1; int ISOLATION_READ_UNCOMMITTED = 1; int ISOLATION_READ_COMMITTED = 2; int ISOLATION_REPEATABLE_READ = 4; int ISOLATION_SERIALIZABLE = 8; int TIMEOUT_DEFAULT = -1; // 返回事务的传播行为，默认值为 REQUIRED。 int getPropagationBehavior(); //返回事务的隔离级别，默认值是 DEFAULT int getIsolationLevel(); // 返回事务的超时时间，默认值为-1。如果超过该时间限制但事务还没有完成，则自动回滚事务。 int getTimeout(); // 返回是否为只读事务，默认值为 false boolean isReadOnly(); @Nullable String getName(); } TransactionStatus TransactionStatus接口用来记录事务的状态 该接口定义了一组方法,用来获取或判断事务的相应状态信息. PlatformTransactionManager.getTransaction(…)方法返回一个 TransactionStatus 对象。返回的 TransactionStatus 对象可能代表一个新的或已经存在的事务（如果在当前调用堆栈有一个符合条件的事务）。 TransactionStatus接口接口内容如下： public interface TransactionStatus{ boolean isNewTransaction(); // 是否是新的事物 boolean hasSavepoint(); // 是否有恢复点 void setRollbackOnly(); // 设置为只回滚 boolean isRollbackOnly(); // 是否为只回滚 boolean isCompleted; // 是否已完成 } 事务属性详解 事务传播行为 事务传播行为是为了解决业务层方法之间互相调用的事务问题。 当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。 举个例子！ 我们在 A 类的aMethod（）方法中调用了 B 类的 bMethod() 方法。这个时候就涉及到业务层方法之间互相调用的事务问题。我们的 bMethod()如果发生异常需要回滚，如何配置事务传播行为才能让 aMethod()也跟着回滚呢？ Class A { @Transactional(propagation=propagation.xxx) public void aMethod { //do something B b = new B(); b.bMethod(); } } Class B { @Transactional(propagation=propagation.xxx) public void bMethod { //do something } } 在TransactionDefinition定义中包括了如下几个表示传播行为的常量： public interface TransactionDefinition { int PROPAGATION_REQUIRED = 0; int PROPAGATION_SUPPORTS = 1; int PROPAGATION_MANDATORY = 2; int PROPAGATION_REQUIRES_NEW = 3; int PROPAGATION_NOT_SUPPORTED = 4; int PROPAGATION_NEVER = 5; int PROPAGATION_NESTED = 6; ...... } 不过如此，为了方便使用，Spring会相应地定义了一个枚举类：Propagation package org.springframework.transaction.annotation; import org.springframework.transaction.TransactionDefinition; public enum Propagation { REQUIRED(TransactionDefinition.PROPAGATION_REQUIRED), SUPPORTS(TransactionDefinition.PROPAGATION_SUPPORTS), MANDATORY(TransactionDefinition.PROPAGATION_MANDATORY), REQUIRES_NEW(TransactionDefinition.PROPAGATION_REQUIRES_NEW), NOT_SUPPORTED(TransactionDefinition.PROPAGATION_NOT_SUPPORTED), NEVER(TransactionDefinition.PROPAGATION_NEVER), NESTED(TransactionDefinition.PROPAGATION_NESTED); private final int value; Propagation(int value) { this.value = value; } public int value() { return this.value; } } 正确的事务传播行为可能的值如下 ： 1.TransactionDefinition.PROPAGATION_REQUIRED 使用的最多的一个事务传播行为，我们平时经常使用的@Transactional注解默认使用就是这个事务传播行为。如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。也就是说： 如果外部方法没有开启事务的话，Propagation.REQUIRED修饰的内部方法会新开启自己的事务，且开启的事务相互独立，互不干扰。 如果外部方法开启事务并且被Propagation.REQUIRED的话，所有Propagation.REQUIRED修饰的内部方法和外部方法均属于同一事务 ，只要一个方法回滚，整个事务均回滚。 举个例子：如果我们上面的aMethod()和bMethod()使用的都是PROPAGATION_REQUIRED传播行为的话，两者使用的就是同一个事务，只要其中一个方法回滚，整个事务均回滚。 Class A { @Transactional(propagation=propagation.PROPAGATION_REQUIRED) public void aMethod { //do something B b = new B(); b.bMethod(); } } Class B { @Transactional(propagation=propagation.PROPAGATION_REQUIRED) public void bMethod { //do something } } 2.TransactionDefinition.PROPAGATION_REQUIRES_NEW 创建一个新的事务，如果当前存在事务，则把当前事务挂起。也就是说不管外部方法是否开启事务，Propagation.REQUIRES_NEW修饰的内部方法会新开启自己的事务，且开启的事务相互独立，互不干扰。 举个例子：如果我们上面的bMethod()使用PROPAGATION_REQUIRES_NEW事务传播行为修饰，aMethod还是用PROPAGATION_REQUIRED修饰的话。如果aMethod()发生异常回滚，bMethod()不会跟着回滚，因为 bMethod()开启了独立的事务。但是，如果 bMethod()抛出了未被捕获的异常并且这个异常满足事务回滚规则的话,aMethod()同样也会回滚，因为这个异常被 aMethod()的事务管理机制检测到了。 Class A { @Transactional(propagation=propagation.PROPAGATION_REQUIRED) public void aMethod { //do something B b = new B(); b.bMethod(); } } Class B { @Transactional(propagation=propagation.REQUIRES_NEW) public void bMethod { //do something } } 4.TransactionDefinition.PROPAGATION_NESTED: 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。也就是说： 在外部方法未开启事务的情况下Propagation.NESTED和Propagation.REQUIRED作用相同，修饰的内部方法都会新开启自己的事务，且开启的事务相互独立，互不干扰。 如果外部方法开启事务的话，Propagation.NESTED修饰的内部方法属于外部事务的子事务，外部主事务回滚的话，子事务也会回滚，而内部子事务可以单独回滚而不影响外部主事务和其他子事务。 这里还是简单举个例子： 如果 aMethod() 回滚的话，bMethod()和bMethod2()都要回滚，而bMethod()回滚的话，并不会造成 aMethod() 和bMethod()回滚。 Class A { @Transactional(propagation=propagation.PROPAGATION_REQUIRED) public void aMethod { //do something B b = new B(); b.bMethod(); b.bMethod2(); } } Class B { @Transactional(propagation=propagation.PROPAGATION_NESTED) public void bMethod { //do something } @Transactional(propagation=propagation.PROPAGATION_NESTED) public void bMethod2 { //do something } } 5.TransactionDefinition.PROPAGATION_MANDATORY 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性） 这个使用的很少，就不举例子来说了。 若是错误的配置以下三种事务传播行为，事务将不会发生回滚，这里不对照案例讲解了，使用的很少。 TransactionDefinition.PROPAGATION_SUPPORTS: 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED: 以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER: 以非事务方式运行，如果当前存在事务，则抛出异常。 事务超时属性 所谓事务超时，就是指一个事务所允许执行的最长时间，如果超过该时间限制但事务还没有完成，则自动回滚事务。在 TransactionDefinition 中以 int 的值来表示超时时间，其单位是秒。 事务只读属性 package org.springframework.transaction; import org.springframework.lang.Nullable; public interface TransactionDefinition { ...... // 返回是否为只读事务，默认值为 false boolean isReadOnly(); } 对于只有读取数据查询的事务，可以指定事务类型为readonly，即只读事务。只读事务不涉及数据的修改，数据库会提供一些优化手段，适合用在有多条数据库查询操作的方法中。 很多人就会疑问了，为什么我一个数据查询操作还要启用事务支持呢？ 拿 MySQL 的 innodb 举例子，根据官网 https://dev.mysql.com/doc/refman/5.7/en/innodb-autocommit-commit-rollback.html 描述： MySQL默认对每一个新建立的连接都启用了autocommit模式。在该模式下，每一个发送到MySQL服务器的sql语句都会在一个单独的事务中进行处理，执行结束后会自动提交事务，并开启一个新的事务。 但是，如果你给方法加上了Transactional注解的话，这个方法执行的所有sql会被放在一个事务中。如果声明了只读事务的话，数据库就会去优化它的执行，并不会带来其他的什么收益。只读事务并不能避免幻读。如果不加Transactional，每条sql会开启一个单独的事务，中间被其它事务改了数据，都会实时读取到最新值。 分享一下关于事务只读属性，其他人的解答： 如果你一次执行单条查询语句，则没有必要启用事务支持，数据库默认支持SQL执行期间的读一致性； 如果你一次执行多条查询语句，例如统计查询，报表查询，在这种场景下，多条查询SQL必须保证整体的读一致性，否则，在前条SQL查询之后，后条SQL查询之前，数据被其他用户改变，则该次整体的统计查询将会出现读数据不一致的状态，此时，应该启用事务支持 事务回滚规则 这些规则定义了哪些异常会导致事务回滚而哪些不会。默认情况下，事务只有遇到运行期异常时才会回滚，而在遇到检查型异常时不会回滚（这一行为与EJB的回滚行为是一致的）。 但是你可以声明事务在遇到特定的检查型异常时像遇到运行期异常那样回滚。同样，你还可以声明事务遇到特定的异常不回滚，即使这些异常是运行期异常。 @Transactional注解使用详解 @Transactional注解就代表支持事务管理，@Transactional 注解可以被应用于接口定义和接口方法、类定义和类的 public 方法上。如果这个注解在类上，那么表示该注解对于所有该类中的public方法都生效；如果注解出现在方法上，则代表该注解仅对该方法有效，会覆盖先前从类层次继承下来的注解。 Reference 可能是最漂亮的Spring事务管理详解:https://juejin.im/post/5b00c52ef265da0b95276091 Spring 事务管理机制概述 : https://blog.csdn.net/justloveyou/article/details/73733278https://blog.csdn.net/justloveyou/article/details/73733278 [总结]Spring事务管理中@Transactional的参数:http://www.mobabel.net/spring事务管理中transactional的参数/ 透彻的掌握 Spring 中@transactional 的使用: https://www.ibm.com/developerworks/cn/java/j-master-spring-transactional-use/index.html Spring事务的传播特性：https://github.com/love-somnus/Spring/wiki/Spring事务的传播特性 Spring事务传播行为详解 ：https://segmentfault.com/a/1190000013341344 "},"zother6-JavaGuide/system-design/framework/spring/Spring.html":{"url":"zother6-JavaGuide/system-design/framework/spring/Spring.html","title":"Spring","keywords":"","body":"Spring相关教程/资料 官网相关 Spring官网、Spring系列主要项目、Spring官网指南、官方文档 spring-framework-reference Spring Framework 4.3.17.RELEASE API 系统学习教程 文档 极客学院Spring Wiki Spring W3Cschool教程 视频 网易云课堂——58集精通java教程Spring框架开发 慕课网相关视频 黑马视频和尚硅谷视频（非常推荐）： 微信公众号：“JavaGuide”后台回复关键字 “1” 免费领取。 面试必备知识点 SpringAOP,IOC实现原理 AOP实现原理、动态代理和静态代理、Spring IOC的初始化过程、IOC原理、自己实现怎么实现一个IOC容器？这些东西都是经常会被问到的。 推荐阅读： 自己动手实现的 Spring IOC 和 AOP - 上篇 自己动手实现的 Spring IOC 和 AOP - 下篇 AOP AOP思想的实现一般都是基于 代理模式 ，在JAVA中一般采用JDK动态代理模式，但是我们都知道，JDK动态代理模式只能代理接口而不能代理类。因此，Spring AOP 会这样子来进行切换，因为Spring AOP 同时支持 CGLIB、ASPECTJ、JDK动态代理。 如果目标对象的实现类实现了接口，Spring AOP 将会采用 JDK 动态代理来生成 AOP 代理类； 如果目标对象的实现类没有实现接口，Spring AOP 将会采用 CGLIB 来生成 AOP 代理类——不过这个选择过程对开发者完全透明、开发者也无需关心。 推荐阅读： 静态代理、JDK动态代理、CGLIB动态代理讲解 ：我们知道AOP思想的实现一般都是基于 代理模式 ，所以在看下面的文章之前建议先了解一下静态代理以及JDK动态代理、CGLIB动态代理的实现方式。 Spring AOP 入门 ：带你入门的一篇文章。这篇文章主要介绍了AOP中的基本概念：5种类型的通知（Before，After，After-returning，After-throwing，Around）；Spring中对AOP的支持：AOP思想的实现一般都是基于代理模式，在Java中一般采用JDK动态代理模式，Spring AOP 同时支持 CGLIB、ASPECTJ、JDK动态代理， Spring AOP 基于AspectJ注解如何实现AOP ： AspectJ是一个AOP框架，它能够对java代码进行AOP编译（一般在编译期进行），让java代码具有AspectJ的AOP功能（当然需要特殊的编译器），可以这样说AspectJ是目前实现AOP框架中最成熟，功能最丰富的语言，更幸运的是，AspectJ与java程序完全兼容，几乎是无缝关联，因此对于有java编程基础的工程师，上手和使用都非常容易。Spring注意到AspectJ在AOP的实现方式上依赖于特殊编译器(ajc编译器)，因此Spring很机智回避了这点，转向采用动态代理技术的实现原理来构建Spring AOP的内部机制（动态织入），这是与AspectJ（静态织入）最根本的区别。Spring 只是使用了与 AspectJ 5 一样的注解，但仍然没有使用 AspectJ 的编译器，底层依是动态代理技术的实现，因此并不依赖于 AspectJ 的编译器。 Spring AOP虽然是使用了那一套注解，其实实现AOP的底层是使用了动态代理(JDK或者CGLib)来动态植入。至于AspectJ的静态植入，不是本文重点，所以只提一提。 探秘Spring AOP（慕课网视频，很不错）:慕课网视频，讲解的很不错，详细且深入 spring源码剖析（六）AOP实现原理剖析 :通过源码分析Spring AOP的原理 IOC [Spring框架]Spring IOC的原理及详解。 Spring IOC核心源码学习 :比较简短，推荐阅读。 Spring IOC 容器源码分析 :强烈推荐，内容详尽，而且便于阅读。 Bean初始化过程 Spring事务管理 可能是最漂亮的Spring事务管理详解 Spring编程式和声明式事务实例讲解 Spring单例与线程安全 Spring框架中的单例模式（源码解读）:单例模式是一种常用的软件设计模式。通过单例模式可以保证系统中一个类只有一个实例。spring依赖注入时，使用了 多重判断加锁 的单例模式。 Spring源码阅读 阅读源码不仅可以加深我们对Spring设计思想的理解，提高自己的编码水平，还可以让自己在面试中如鱼得水。下面的是Github上的一个开源的Spring源码阅读，大家有时间可以看一下，当然你如果有时间也可以自己慢慢研究源码。 spring-core spring-aop spring-context spring-task spring-transaction spring-mvc guava-cache "},"zother6-JavaGuide/system-design/framework/spring/SpringBean.html":{"url":"zother6-JavaGuide/system-design/framework/spring/SpringBean.html","title":"Spring Bean","keywords":"","body":" 前言 一 bean的作用域 1. singleton——唯一 bean 实例 2. prototype——每次请求都会创建一个新的 bean 实例 3. request——每一次HTTP请求都会产生一个新的bean，该bean仅在当前HTTP request内有效 4. session——每一次HTTP请求都会产生一个新的 bean，该bean仅在当前 HTTP session 内有效 5. globalSession 二 bean的生命周期 initialization 和 destroy 实现*Aware接口 在Bean中使用Spring框架的一些对象 BeanPostProcessor 总结 单例管理的对象 非单例管理的对象 三 说明 前言 在 Spring 中，那些组成应用程序的主体及由 Spring IOC 容器所管理的对象，被称之为 bean。简单地讲，bean 就是由 IOC 容器初始化、装配及管理的对象，除此之外，bean 就与应用程序中的其他对象没有什么区别了。而 bean 的定义以及 bean 相互间的依赖关系将通过配置元数据来描述。 Spring中的bean默认都是单例的，这些单例Bean在多线程程序下如何保证线程安全呢？ 例如对于Web应用来说，Web容器对于每个用户请求都创建一个单独的Sevlet线程来处理请求，引入Spring框架之后，每个Action都是单例的，那么对于Spring托管的单例Service Bean，如何保证其安全呢？ Spring的单例是基于BeanFactory也就是Spring容器的，单例Bean在此容器内只有一个，Java的单例是基于 JVM，每个 JVM 内只有一个实例。 在大多数情况下。单例 bean 是很理想的方案。不过，有时候你可能会发现你所使用的类是易变的，它们会保持一些状态，因此重用是不安全的。在这种情况下，将 class 声明为单例的就不是那么明智了。因为对象会被污染，稍后重用的时候会出现意想不到的问题。所以 Spring 定义了多种作用域的bean。 一 bean的作用域 创建一个bean定义，其实质是用该bean定义对应的类来创建真正实例的“配方”。把bean定义看成一个配方很有意义，它与class很类似，只根据一张“处方”就可以创建多个实例。不仅可以控制注入到对象中的各种依赖和配置值，还可以控制该对象的作用域。这样可以灵活选择所建对象的作用域，而不必在Java Class级定义作用域。Spring Framework支持五种作用域，分别阐述如下表。 五种作用域中，request、session 和 global session 三种作用域仅在基于web的应用中使用（不必关心你所采用的是什么web应用框架），只能用在基于 web 的 Spring ApplicationContext 环境。 1. singleton——唯一 bean 实例 当一个 bean 的作用域为 singleton，那么Spring IoC容器中只会存在一个共享的 bean 实例，并且所有对 bean 的请求，只要 id 与该 bean 定义相匹配，则只会返回bean的同一实例。 singleton 是单例类型(对应于单例模式)，就是在创建起容器时就同时自动创建了一个bean的对象，不管你是否使用，但我们可以指定Bean节点的 lazy-init=”true” 来延迟初始化bean，这时候，只有在第一次获取bean时才会初始化bean，即第一次请求该bean时才初始化。 每次获取到的对象都是同一个对象。注意，singleton 作用域是Spring中的缺省作用域。要在XML中将 bean 定义成 singleton ，可以这样配置： 也可以通过 @Scope 注解（它可以显示指定bean的作用范围。）的方式 @Service @Scope(\"singleton\") public class ServiceImpl{ } 2. prototype——每次请求都会创建一个新的 bean 实例 当一个bean的作用域为 prototype，表示一个 bean 定义对应多个对象实例。 prototype 作用域的 bean 会导致在每次对该 bean 请求（将其注入到另一个 bean 中，或者以程序的方式调用容器的 getBean() 方法）时都会创建一个新的 bean 实例。prototype 是原型类型，它在我们创建容器的时候并没有实例化，而是当我们获取bean的时候才会去创建一个对象，而且我们每次获取到的对象都不是同一个对象。根据经验，对有状态的 bean 应该使用 prototype 作用域，而对无状态的 bean 则应该使用 singleton 作用域。 在 XML 中将 bean 定义成 prototype ，可以这样配置： 或者 通过 @Scope 注解的方式实现就不做演示了。 3. request——每一次HTTP请求都会产生一个新的bean，该bean仅在当前HTTP request内有效 request只适用于Web程序，每一次 HTTP 请求都会产生一个新的bean，同时该bean仅在当前HTTP request内有效，当请求结束后，该对象的生命周期即告结束。 在 XML 中将 bean 定义成 request ，可以这样配置： 4. session——每一次HTTP请求都会产生一个新的 bean，该bean仅在当前 HTTP session 内有效 session只适用于Web程序，session 作用域表示该针对每一次 HTTP 请求都会产生一个新的 bean，同时该 bean 仅在当前 HTTP session 内有效.与request作用域一样，可以根据需要放心的更改所创建实例的内部状态，而别的 HTTP session 中根据 userPreferences 创建的实例，将不会看到这些特定于某个 HTTP session 的状态变化。当HTTP session最终被废弃的时候，在该HTTP session作用域内的bean也会被废弃掉。 5. globalSession global session 作用域类似于标准的 HTTP session 作用域，不过仅仅在基于 portlet 的 web 应用中才有意义。Portlet 规范定义了全局 Session 的概念，它被所有构成某个 portlet web 应用的各种不同的 portle t所共享。在global session 作用域中定义的 bean 被限定于全局portlet Session的生命周期范围内。 二 bean的生命周期 Spring Bean是Spring应用中最最重要的部分了。所以来看看Spring容器在初始化一个bean的时候会做那些事情，顺序是怎样的，在容器关闭的时候，又会做哪些事情。 spring版本：4.2.3.RELEASE 鉴于Spring源码是用gradle构建的，我也决定舍弃我大maven，尝试下洪菊推荐过的gradle。运行beanLifeCycle模块下的junit test即可在控制台看到如下输出，可以清楚了解Spring容器在创建，初始化和销毁Bean的时候依次做了那些事情。 Spring容器初始化 ===================================== 调用GiraffeService无参构造函数 GiraffeService中利用set方法设置属性值 调用setBeanName:: Bean Name defined in context=giraffeService 调用setBeanClassLoader,ClassLoader Name = sun.misc.Launcher$AppClassLoader 调用setBeanFactory,setBeanFactory:: giraffe bean singleton=true 调用setEnvironment 调用setResourceLoader:: Resource File Name=spring-beans.xml 调用setApplicationEventPublisher 调用setApplicationContext:: Bean Definition Names=[giraffeService, org.springframework.context.annotation.CommonAnnotationBeanPostProcessor#0, com.giraffe.spring.service.GiraffeServicePostProcessor#0] 执行BeanPostProcessor的postProcessBeforeInitialization方法,beanName=giraffeService 调用PostConstruct注解标注的方法 执行InitializingBean接口的afterPropertiesSet方法 执行配置的init-method 执行BeanPostProcessor的postProcessAfterInitialization方法,beanName=giraffeService Spring容器初始化完毕 ===================================== 从容器中获取Bean giraffe Name=李光洙 ===================================== 调用preDestroy注解标注的方法 执行DisposableBean接口的destroy方法 执行配置的destroy-method Spring容器关闭 先来看看，Spring在Bean从创建到销毁的生命周期中可能做得事情。 initialization 和 destroy 有时我们需要在Bean属性值set好之后和Bean销毁之前做一些事情，比如检查Bean中某个属性是否被正常的设置好值了。Spring框架提供了多种方法让我们可以在Spring Bean的生命周期中执行initialization和pre-destroy方法。 1.实现InitializingBean和DisposableBean接口 这两个接口都只包含一个方法。通过实现InitializingBean接口的afterPropertiesSet()方法可以在Bean属性值设置好之后做一些操作，实现DisposableBean接口的destroy()方法可以在销毁Bean之前做一些操作。 例子如下： public class GiraffeService implements InitializingBean,DisposableBean { @Override public void afterPropertiesSet() throws Exception { System.out.println(\"执行InitializingBean接口的afterPropertiesSet方法\"); } @Override public void destroy() throws Exception { System.out.println(\"执行DisposableBean接口的destroy方法\"); } } 这种方法比较简单，但是不建议使用。因为这样会将Bean的实现和Spring框架耦合在一起。 2.在bean的配置文件中指定init-method和destroy-method方法 Spring允许我们创建自己的 init 方法和 destroy 方法，只要在 Bean 的配置文件中指定 init-method 和 destroy-method 的值就可以在 Bean 初始化时和销毁之前执行一些操作。 例子如下： public class GiraffeService { //通过的destroy-method属性指定的销毁方法 public void destroyMethod() throws Exception { System.out.println(\"执行配置的destroy-method\"); } //通过的init-method属性指定的初始化方法 public void initMethod() throws Exception { System.out.println(\"执行配置的init-method\"); } } 配置文件中的配置： 需要注意的是自定义的init-method和post-method方法可以抛异常但是不能有参数。 这种方式比较推荐，因为可以自己创建方法，无需将Bean的实现直接依赖于spring的框架。 3.使用@PostConstruct和@PreDestroy注解 除了xml配置的方式，Spring 也支持用 @PostConstruct和 @PreDestroy注解来指定 init 和 destroy 方法。这两个注解均在javax.annotation 包中。为了注解可以生效，需要在配置文件中定义org.springframework.context.annotation.CommonAnnotationBeanPostProcessor或context:annotation-config 例子如下： public class GiraffeService { @PostConstruct public void initPostConstruct(){ System.out.println(\"执行PostConstruct注解标注的方法\"); } @PreDestroy public void preDestroy(){ System.out.println(\"执行preDestroy注解标注的方法\"); } } 配置文件： 实现*Aware接口 在Bean中使用Spring框架的一些对象 有些时候我们需要在 Bean 的初始化中使用 Spring 框架自身的一些对象来执行一些操作，比如获取 ServletContext 的一些参数，获取 ApplicaitionContext 中的 BeanDefinition 的名字，获取 Bean 在容器中的名字等等。为了让 Bean 可以获取到框架自身的一些对象，Spring 提供了一组名为*Aware的接口。 这些接口均继承于org.springframework.beans.factory.Aware标记接口，并提供一个将由 Bean 实现的set*方法,Spring通过基于setter的依赖注入方式使相应的对象可以被Bean使用。 网上说，这些接口是利用观察者模式实现的，类似于servlet listeners，目前还不明白，不过这也不在本文的讨论范围内。 介绍一些重要的Aware接口： ApplicationContextAware: 获得ApplicationContext对象,可以用来获取所有Bean definition的名字。 BeanFactoryAware:获得BeanFactory对象，可以用来检测Bean的作用域。 BeanNameAware:获得Bean在配置文件中定义的名字。 ResourceLoaderAware:获得ResourceLoader对象，可以获得classpath中某个文件。 ServletContextAware:在一个MVC应用中可以获取ServletContext对象，可以读取context中的参数。 ServletConfigAware： 在一个MVC应用中可以获取ServletConfig对象，可以读取config中的参数。 public class GiraffeService implements ApplicationContextAware, ApplicationEventPublisherAware, BeanClassLoaderAware, BeanFactoryAware, BeanNameAware, EnvironmentAware, ImportAware, ResourceLoaderAware{ @Override public void setBeanClassLoader(ClassLoader classLoader) { System.out.println(\"执行setBeanClassLoader,ClassLoader Name = \" + classLoader.getClass().getName()); } @Override public void setBeanFactory(BeanFactory beanFactory) throws BeansException { System.out.println(\"执行setBeanFactory,setBeanFactory:: giraffe bean singleton=\" + beanFactory.isSingleton(\"giraffeService\")); } @Override public void setBeanName(String s) { System.out.println(\"执行setBeanName:: Bean Name defined in context=\" + s); } @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException { System.out.println(\"执行setApplicationContext:: Bean Definition Names=\" + Arrays.toString(applicationContext.getBeanDefinitionNames())); } @Override public void setApplicationEventPublisher(ApplicationEventPublisher applicationEventPublisher) { System.out.println(\"执行setApplicationEventPublisher\"); } @Override public void setEnvironment(Environment environment) { System.out.println(\"执行setEnvironment\"); } @Override public void setResourceLoader(ResourceLoader resourceLoader) { Resource resource = resourceLoader.getResource(\"classpath:spring-beans.xml\"); System.out.println(\"执行setResourceLoader:: Resource File Name=\" + resource.getFilename()); } @Override public void setImportMetadata(AnnotationMetadata annotationMetadata) { System.out.println(\"执行setImportMetadata\"); } } BeanPostProcessor 上面的*Aware接口是针对某个实现这些接口的Bean定制初始化的过程， Spring同样可以针对容器中的所有Bean，或者某些Bean定制初始化过程，只需提供一个实现BeanPostProcessor接口的类即可。 该接口中包含两个方法，postProcessBeforeInitialization和postProcessAfterInitialization。 postProcessBeforeInitialization方法会在容器中的Bean初始化之前执行， postProcessAfterInitialization方法在容器中的Bean初始化之后执行。 例子如下： public class CustomerBeanPostProcessor implements BeanPostProcessor { @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { System.out.println(\"执行BeanPostProcessor的postProcessBeforeInitialization方法,beanName=\" + beanName); return bean; } @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { System.out.println(\"执行BeanPostProcessor的postProcessAfterInitialization方法,beanName=\" + beanName); return bean; } } 要将BeanPostProcessor的Bean像其他Bean一样定义在配置文件中 总结 所以。。。结合第一节控制台输出的内容，Spring Bean的生命周期是这样纸的： Bean容器找到配置文件中 Spring Bean 的定义。 Bean容器利用Java Reflection API创建一个Bean的实例。 如果涉及到一些属性值 利用set方法设置一些属性值。 如果Bean实现了BeanNameAware接口，调用setBeanName()方法，传入Bean的名字。 如果Bean实现了BeanClassLoaderAware接口，调用setBeanClassLoader()方法，传入ClassLoader对象的实例。 如果Bean实现了BeanFactoryAware接口，调用setBeanFactory()方法，传入BeanFactory对象的实例。 与上面的类似，如果实现了其他*Aware接口，就调用相应的方法。 如果有和加载这个Bean的Spring容器相关的BeanPostProcessor对象，执行postProcessBeforeInitialization()方法 如果Bean实现了InitializingBean接口，执行afterPropertiesSet()方法。 如果Bean在配置文件中的定义包含init-method属性，执行指定的方法。 如果有和加载这个Bean的Spring容器相关的BeanPostProcessor对象，执行postProcessAfterInitialization()方法 当要销毁Bean的时候，如果Bean实现了DisposableBean接口，执行destroy()方法。 当要销毁Bean的时候，如果Bean在配置文件中的定义包含destroy-method属性，执行指定的方法。 用图表示一下(图来源:http://www.jianshu.com/p/d00539babca5)： 与之比较类似的中文版本: 其实很多时候我们并不会真的去实现上面说描述的那些接口，那么下面我们就除去那些接口，针对bean的单例和非单例来描述下bean的生命周期： 单例管理的对象 当scope=”singleton”，即默认情况下，会在启动容器时（即实例化容器时）时实例化。但我们可以指定Bean节点的lazy-init=”true”来延迟初始化bean，这时候，只有在第一次获取bean时才会初始化bean，即第一次请求该bean时才初始化。如下配置： 如果想对所有的默认单例bean都应用延迟初始化，可以在根节点beans设置default-lazy-init属性为true，如下所示： 默认情况下，Spring 在读取 xml 文件的时候，就会创建对象。在创建对象的时候先调用构造器，然后调用 init-method 属性值中所指定的方法。对象在被销毁的时候，会调用 destroy-method 属性值中所指定的方法（例如调用Container.destroy()方法的时候）。写一个测试类，代码如下： public class LifeBean { private String name; public LifeBean(){ System.out.println(\"LifeBean()构造函数\"); } public String getName() { return name; } public void setName(String name) { System.out.println(\"setName()\"); this.name = name; } public void init(){ System.out.println(\"this is init of lifeBean\"); } public void destory(){ System.out.println(\"this is destory of lifeBean \" + this); } } 　life.xml配置如下： 测试代码： public class LifeTest { @Test public void test() { AbstractApplicationContext container = new ClassPathXmlApplicationContext(\"life.xml\"); LifeBean life1 = (LifeBean)container.getBean(\"life\"); System.out.println(life1); container.close(); } } 运行结果： LifeBean()构造函数 this is init of lifeBean com.bean.LifeBean@573f2bb1 …… this is destory of lifeBean com.bean.LifeBean@573f2bb1 非单例管理的对象 当scope=”prototype”时，容器也会延迟初始化 bean，Spring 读取xml 文件的时候，并不会立刻创建对象，而是在第一次请求该 bean 时才初始化（如调用getBean方法时）。在第一次请求每一个 prototype 的bean 时，Spring容器都会调用其构造器创建这个对象，然后调用init-method属性值中所指定的方法。对象销毁的时候，Spring 容器不会帮我们调用任何方法，因为是非单例，这个类型的对象有很多个，Spring容器一旦把这个对象交给你之后，就不再管理这个对象了。 为了测试prototype bean的生命周期life.xml配置如下： 测试程序： public class LifeTest { @Test public void test() { AbstractApplicationContext container = new ClassPathXmlApplicationContext(\"life.xml\"); LifeBean life1 = (LifeBean)container.getBean(\"life_singleton\"); System.out.println(life1); LifeBean life3 = (LifeBean)container.getBean(\"life_prototype\"); System.out.println(life3); container.close(); } } 运行结果： LifeBean()构造函数 this is init of lifeBean com.bean.LifeBean@573f2bb1 LifeBean()构造函数 this is init of lifeBean com.bean.LifeBean@5ae9a829 …… this is destory of lifeBean com.bean.LifeBean@573f2bb1 可以发现，对于作用域为 prototype 的 bean ，其destroy方法并没有被调用。如果 bean 的 scope 设为prototype时，当容器关闭时，destroy 方法不会被调用。对于 prototype 作用域的 bean，有一点非常重要，那就是 Spring不能对一个 prototype bean 的整个生命周期负责：容器在初始化、配置、装饰或者是装配完一个prototype实例后，将它交给客户端，随后就对该prototype实例不闻不问了。 不管何种作用域，容器都会调用所有对象的初始化生命周期回调方法。但对prototype而言，任何配置好的析构生命周期回调方法都将不会被调用。清除prototype作用域的对象并释放任何prototype bean所持有的昂贵资源，都是客户端代码的职责（让Spring容器释放被prototype作用域bean占用资源的一种可行方式是，通过使用bean的后置处理器，该处理器持有要被清除的bean的引用）。谈及prototype作用域的bean时，在某些方面你可以将Spring容器的角色看作是Java new操作的替代者，任何迟于该时间点的生命周期事宜都得交由客户端来处理。 Spring 容器可以管理 singleton 作用域下 bean 的生命周期，在此作用域下，Spring 能够精确地知道bean何时被创建，何时初始化完成，以及何时被销毁。而对于 prototype 作用域的bean，Spring只负责创建，当容器创建了 bean 的实例后，bean 的实例就交给了客户端的代码管理，Spring容器将不再跟踪其生命周期，并且不会管理那些被配置成prototype作用域的bean的生命周期。 三 说明 本文的完成结合了下面两篇文章，并做了相应修改： https://blog.csdn.net/fuzhongmin05/article/details/73389779 https://yemengying.com/2016/07/14/spring-bean-life-cycle/ 由于本文非本人独立原创，所以未声明为原创！在此说明！ "},"zother6-JavaGuide/system-design/framework/spring/SpringInterviewQuestions.html":{"url":"zother6-JavaGuide/system-design/framework/spring/SpringInterviewQuestions.html","title":"Spring Interview Questions","keywords":"","body":"这篇文章主要是想通过一些问题，加深大家对于 Spring 的理解，所以不会涉及太多的代码！这篇文章整理了挺长时间，下面的很多问题我自己在使用 Spring 的过程中也并没有注意，自己也是临时查阅了很多资料和书籍补上的。网上也有一些很多关于 Spring 常见问题/面试题整理的文章，我感觉大部分都是互相 copy，而且很多问题也不是很好，有些回答也存在问题。所以，自己花了一周的业余时间整理了一下，希望对大家有帮助。 1. 什么是 Spring 框架? Spring 是一种轻量级开发框架，旨在提高开发人员的开发效率以及系统的可维护性。Spring 官网：https://spring.io/。 我们一般说 Spring 框架指的都是 Spring Framework，它是很多模块的集合，使用这些模块可以很方便地协助我们进行开发。这些模块是：核心容器、数据访问/集成,、Web、AOP（面向切面编程）、工具、消息和测试模块。比如：Core Container 中的 Core 组件是Spring 所有组件的核心，Beans 组件和 Context 组件是实现IOC和依赖注入的基础，AOP组件用来实现面向切面编程。 Spring 官网列出的 Spring 的 6 个特征: 核心技术 ：依赖注入(DI)，AOP，事件(events)，资源，i18n，验证，数据绑定，类型转换，SpEL。 测试 ：模拟对象，TestContext框架，Spring MVC 测试，WebTestClient。 数据访问 ：事务，DAO支持，JDBC，ORM，编组XML。 Web支持 : Spring MVC和Spring WebFlux Web框架。 集成 ：远程处理，JMS，JCA，JMX，电子邮件，任务，调度，缓存。 语言 ：Kotlin，Groovy，动态语言。 2. 列举一些重要的Spring模块？ 下图对应的是 Spring4.x 版本。目前最新的5.x版本中 Web 模块的 Portlet 组件已经被废弃掉，同时增加了用于异步响应式处理的 WebFlux 组件。 Spring Core： 基础,可以说 Spring 其他所有的功能都需要依赖于该类库。主要提供 IoC 依赖注入功能。 Spring Aspects ： 该模块为与AspectJ的集成提供支持。 Spring AOP ：提供了面向切面的编程实现。 Spring JDBC : Java数据库连接。 Spring JMS ：Java消息服务。 Spring ORM : 用于支持Hibernate等ORM工具。 Spring Web : 为创建Web应用程序提供支持。 Spring Test : 提供了对 JUnit 和 TestNG 测试的支持。 3. @RestController vs @Controller Controller 返回一个页面 单独使用 @Controller 不加 @ResponseBody的话一般使用在要返回一个视图的情况，这种情况属于比较传统的Spring MVC 的应用，对应于前后端不分离的情况。 @RestController 返回JSON 或 XML 形式数据 但@RestController只返回对象，对象数据直接以 JSON 或 XML 形式写入 HTTP 响应(Response)中，这种情况属于 RESTful Web服务，这也是目前日常开发所接触的最常用的情况（前后端分离）。 @Controller +@ResponseBody 返回JSON 或 XML 形式数据 如果你需要在Spring4之前开发 RESTful Web服务的话，你需要使用@Controller 并结合@ResponseBody注解，也就是说@Controller +@ResponseBody= @RestController（Spring 4 之后新加的注解）。 @ResponseBody 注解的作用是将 Controller 的方法返回的对象通过适当的转换器转换为指定的格式之后，写入到HTTP 响应(Response)对象的 body 中，通常用来返回 JSON 或者 XML 数据，返回 JSON 数据的情况比较多。 Reference: https://dzone.com/articles/spring-framework-restcontroller-vs-controller（图片来源） https://javarevisited.blogspot.com/2017/08/difference-between-restcontroller-and-controller-annotations-spring-mvc-rest.html?m=1 4. Spring IOC & AOP 4.1 谈谈自己对于 Spring IoC 和 AOP 的理解 IoC IoC（Inverse of Control:控制反转）是一种设计思想，就是 将原本在程序中手动创建对象的控制权，交由Spring框架来管理。 IoC 在其他语言中也有应用，并非 Spring 特有。 IoC 容器是 Spring 用来实现 IoC 的载体， IoC 容器实际上就是个Map（key，value）,Map 中存放的是各种对象。 将对象之间的相互依赖关系交给 IoC 容器来管理，并由 IoC 容器完成对象的注入。这样可以很大程度上简化应用的开发，把应用从复杂的依赖关系中解放出来。 IoC 容器就像是一个工厂一样，当我们需要创建一个对象的时候，只需要配置好配置文件/注解即可，完全不用考虑对象是如何被创建出来的。 在实际项目中一个 Service 类可能有几百甚至上千个类作为它的底层，假如我们需要实例化这个 Service，你可能要每次都要搞清这个 Service 所有底层类的构造函数，这可能会把人逼疯。如果利用 IoC 的话，你只需要配置好，然后在需要的地方引用就行了，这大大增加了项目的可维护性且降低了开发难度。 Spring 时代我们一般通过 XML 文件来配置 Bean，后来开发人员觉得 XML 文件来配置不太好，于是 SpringBoot 注解配置就慢慢开始流行起来。 推荐阅读：https://www.zhihu.com/question/23277575/answer/169698662 Spring IoC的初始化过程： IoC源码阅读 https://javadoop.com/post/spring-ioc AOP AOP(Aspect-Oriented Programming:面向切面编程)能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。 Spring AOP就是基于动态代理的，如果要代理的对象，实现了某个接口，那么Spring AOP会使用JDK Proxy，去创建代理对象，而对于没有实现接口的对象，就无法使用 JDK Proxy 去进行代理了，这时候Spring AOP会使用Cglib ，这时候Spring AOP会使用 Cglib 生成一个被代理对象的子类来作为代理，如下图所示： 当然你也可以使用 AspectJ ,Spring AOP 已经集成了AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。 使用 AOP 之后我们可以把一些通用功能抽象出来，在需要用到的地方直接使用即可，这样大大简化了代码量。我们需要增加新功能时也方便，这样也提高了系统扩展性。日志功能、事务管理等等场景都用到了 AOP 。 4.2 Spring AOP 和 AspectJ AOP 有什么区别？ Spring AOP 属于运行时增强，而 AspectJ 是编译时增强。 Spring AOP 基于代理(Proxying)，而 AspectJ 基于字节码操作(Bytecode Manipulation)。 Spring AOP 已经集成了 AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。AspectJ 相比于 Spring AOP 功能更加强大，但是 Spring AOP 相对来说更简单， 如果我们的切面比较少，那么两者性能差异不大。但是，当切面太多的话，最好选择 AspectJ ，它比Spring AOP 快很多。 5. Spring bean 5.1 Spring 中的 bean 的作用域有哪些? singleton : 唯一 bean 实例，Spring 中的 bean 默认都是单例的。 prototype : 每次请求都会创建一个新的 bean 实例。 request : 每一次HTTP请求都会产生一个新的bean，该bean仅在当前HTTP request内有效。 session : 每一次HTTP请求都会产生一个新的 bean，该bean仅在当前 HTTP session 内有效。 global-session： 全局session作用域，仅仅在基于portlet的web应用中才有意义，Spring5已经没有了。Portlet是能够生成语义代码(例如：HTML)片段的小型Java Web插件。它们基于portlet容器，可以像servlet一样处理HTTP请求。但是，与 servlet 不同，每个 portlet 都有不同的会话 5.2 Spring 中的单例 bean 的线程安全问题了解吗？ 大部分时候我们并没有在系统中使用多线程，所以很少有人会关注这个问题。单例 bean 存在线程问题，主要是因为当多个线程操作同一个对象的时候，对这个对象的非静态成员变量的写操作会存在线程安全问题。 常见的有两种解决办法： 在Bean对象中尽量避免定义可变的成员变量（不太现实）。 在类中定义一个ThreadLocal成员变量，将需要的可变成员变量保存在 ThreadLocal 中（推荐的一种方式）。 5.3 @Component 和 @Bean 的区别是什么？ 作用对象不同: @Component 注解作用于类，而@Bean注解作用于方法。 @Component通常是通过类路径扫描来自动侦测以及自动装配到Spring容器中（我们可以使用 @ComponentScan 注解定义要扫描的路径从中找出标识了需要装配的类自动装配到 Spring 的 bean 容器中）。@Bean 注解通常是我们在标有该注解的方法中定义产生这个 bean,@Bean告诉了Spring这是某个类的示例，当我需要用它的时候还给我。 @Bean 注解比 Component 注解的自定义性更强，而且很多地方我们只能通过 @Bean 注解来注册bean。比如当我们引用第三方库中的类需要装配到 Spring容器时，则只能通过 @Bean来实现。 @Bean注解使用示例： @Configuration public class AppConfig { @Bean public TransferService transferService() { return new TransferServiceImpl(); } } 上面的代码相当于下面的 xml 配置 下面这个例子是通过 @Component 无法实现的。 @Bean public OneService getService(status) { case (status) { when 1: return new serviceImpl1(); when 2: return new serviceImpl2(); when 3: return new serviceImpl3(); } } 5.4 将一个类声明为Spring的 bean 的注解有哪些? 我们一般使用 @Autowired 注解自动装配 bean，要想把类标识成可用于 @Autowired 注解自动装配的 bean 的类,采用以下注解可实现： @Component ：通用的注解，可标注任意类为 Spring 组件。如果一个Bean不知道属于哪个层，可以使用@Component 注解标注。 @Repository : 对应持久层即 Dao 层，主要用于数据库相关操作。 @Service : 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao层。 @Controller : 对应 Spring MVC 控制层，主要用户接受用户请求并调用 Service 层返回数据给前端页面。 5.5 Spring 中的 bean 生命周期? 这部分网上有很多文章都讲到了，下面的内容整理自：https://yemengying.com/2016/07/14/spring-bean-life-cycle/ ，除了这篇文章，再推荐一篇很不错的文章 ：https://www.cnblogs.com/zrtqsk/p/3735273.html 。 Bean 容器找到配置文件中 Spring Bean 的定义。 Bean 容器利用 Java Reflection API 创建一个Bean的实例。 如果涉及到一些属性值 利用 set()方法设置一些属性值。 如果 Bean 实现了 BeanNameAware 接口，调用 setBeanName()方法，传入Bean的名字。 如果 Bean 实现了 BeanClassLoaderAware 接口，调用 setBeanClassLoader()方法，传入 ClassLoader对象的实例。 与上面的类似，如果实现了其他 *.Aware接口，就调用相应的方法。 如果有和加载这个 Bean 的 Spring 容器相关的 BeanPostProcessor 对象，执行postProcessBeforeInitialization() 方法 如果Bean实现了InitializingBean接口，执行afterPropertiesSet()方法。 如果 Bean 在配置文件中的定义包含 init-method 属性，执行指定的方法。 如果有和加载这个 Bean的 Spring 容器相关的 BeanPostProcessor 对象，执行postProcessAfterInitialization() 方法 当要销毁 Bean 的时候，如果 Bean 实现了 DisposableBean 接口，执行 destroy() 方法。 当要销毁 Bean 的时候，如果 Bean 在配置文件中的定义包含 destroy-method 属性，执行指定的方法。 图示： 与之比较类似的中文版本: 6. Spring MVC 6.1 说说自己对于 Spring MVC 了解? 谈到这个问题，我们不得不提提之前 Model1 和 Model2 这两个没有 Spring MVC 的时代。 Model1 时代 : 很多学 Java 后端比较晚的朋友可能并没有接触过 Model1 模式下的 JavaWeb 应用开发。在 Model1 模式下，整个 Web 应用几乎全部用 JSP 页面组成，只用少量的 JavaBean 来处理数据库连接、访问等操作。这个模式下 JSP 即是控制层又是表现层。显而易见，这种模式存在很多问题。比如①将控制逻辑和表现逻辑混杂在一起，导致代码重用率极低；②前端和后端相互依赖，难以进行测试并且开发效率极低； Model2 时代 ：学过 Servlet 并做过相关 Demo 的朋友应该了解“Java Bean(Model)+ JSP（View,）+Servlet（Controller） ”这种开发模式,这就是早期的 JavaWeb MVC 开发模式。Model:系统涉及的数据，也就是 dao 和 bean。View：展示模型中的数据，只是用来展示。Controller：处理用户请求都发送给 ，返回数据给 JSP 并展示给用户。 Model2 模式下还存在很多问题，Model2的抽象和封装程度还远远不够，使用Model2进行开发时不可避免地会重复造轮子，这就大大降低了程序的可维护性和复用性。于是很多JavaWeb开发相关的 MVC 框架应运而生比如Struts2，但是 Struts2 比较笨重。随着 Spring 轻量级开发框架的流行，Spring 生态圈出现了 Spring MVC 框架， Spring MVC 是当前最优秀的 MVC 框架。相比于 Struts2 ， Spring MVC 使用更加简单和方便，开发效率更高，并且 Spring MVC 运行速度更快。 MVC 是一种设计模式,Spring MVC 是一款很优秀的 MVC 框架。Spring MVC 可以帮助我们进行更简洁的Web层的开发，并且它天生与 Spring 框架集成。Spring MVC 下我们一般把后端项目分为 Service层（处理业务）、Dao层（数据库操作）、Entity层（实体类）、Controller层(控制层，返回数据给前台页面)。 Spring MVC 的简单原理图如下： 6.2 SpringMVC 工作原理了解吗? 原理如下图所示： 上图的一个笔误的小问题：Spring MVC 的入口函数也就是前端控制器 DispatcherServlet 的作用是接收请求，响应结果。 流程说明（重要）： 客户端（浏览器）发送请求，直接请求到 DispatcherServlet。 DispatcherServlet 根据请求信息调用 HandlerMapping，解析请求对应的 Handler。 解析到对应的 Handler（也就是我们平常说的 Controller 控制器）后，开始由 HandlerAdapter 适配器处理。 HandlerAdapter 会根据 Handler来调用真正的处理器开处理请求，并处理相应的业务逻辑。 处理器处理完业务后，会返回一个 ModelAndView 对象，Model 是返回的数据对象，View 是个逻辑上的 View。 ViewResolver 会根据逻辑 View 查找实际的 View。 DispaterServlet 把返回的 Model 传给 View（视图渲染）。 把 View 返回给请求者（浏览器） 7. Spring 框架中用到了哪些设计模式？ 关于下面一些设计模式的详细介绍，可以看笔主前段时间的原创文章《面试官:“谈谈Spring中都用到了那些设计模式?”。》 。 工厂设计模式 : Spring使用工厂模式通过 BeanFactory、ApplicationContext 创建 bean 对象。 代理设计模式 : Spring AOP 功能的实现。 单例设计模式 : Spring 中的 Bean 默认都是单例的。 模板方法模式 : Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。 包装器设计模式 : 我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。 观察者模式: Spring 事件驱动模型就是观察者模式很经典的一个应用。 适配器模式 :Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中也是用到了适配器模式适配Controller。 ...... 8. Spring 事务 8.1 Spring 管理事务的方式有几种？ 编程式事务，在代码中硬编码。(不推荐使用) 声明式事务，在配置文件中配置（推荐使用） 声明式事务又分为两种： 基于XML的声明式事务 基于注解的声明式事务 8.2 Spring 事务中的隔离级别有哪几种? TransactionDefinition 接口中定义了五个表示隔离级别的常量： TransactionDefinition.ISOLATION_DEFAULT: 使用后端数据库默认的隔离级别，Mysql 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别. TransactionDefinition.ISOLATION_READ_UNCOMMITTED: 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 TransactionDefinition.ISOLATION_READ_COMMITTED: 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 TransactionDefinition.ISOLATION_REPEATABLE_READ: 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 TransactionDefinition.ISOLATION_SERIALIZABLE: 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 8.3 Spring 事务中哪几种事务传播行为? 支持当前事务的情况： TransactionDefinition.PROPAGATION_REQUIRED： 如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。 TransactionDefinition.PROPAGATION_SUPPORTS： 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_MANDATORY： 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性） 不支持当前事务的情况： TransactionDefinition.PROPAGATION_REQUIRES_NEW： 创建一个新的事务，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED： 以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER： 以非事务方式运行，如果当前存在事务，则抛出异常。 其他情况： TransactionDefinition.PROPAGATION_NESTED： 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。 8.4 @Transactional(rollbackFor = Exception.class)注解了解吗？ 我们知道：Exception分为运行时异常RuntimeException和非运行时异常。事务管理对于企业应用来说是至关重要的，即使出现异常情况，它也可以保证数据的一致性。 当@Transactional注解作用于类上时，该类的所有 public 方法将都具有该类型的事务属性，同时，我们也可以在方法级别使用该标注来覆盖类级别的定义。如果类或者方法加了这个注解，那么这个类里面的方法抛出异常，就会回滚，数据库里面的数据也会回滚。 在@Transactional注解中如果不配置rollbackFor属性,那么事物只会在遇到RuntimeException的时候才会回滚,加上rollbackFor=Exception.class,可以让事物在遇到非运行时异常时也回滚。 关于 @Transactional 注解推荐阅读的文章： 透彻的掌握 Spring 中@transactional 的使用 9. JPA 9.1 如何使用JPA在数据库中非持久化一个字段？ 假如我们有有下面一个类： Entity(name=\"USER\") public class User { @Id @GeneratedValue(strategy = GenerationType.AUTO) @Column(name = \"ID\") private Long id; @Column(name=\"USER_NAME\") private String userName; @Column(name=\"PASSWORD\") private String password; private String secrect; } 如果我们想让secrect 这个字段不被持久化，也就是不被数据库存储怎么办？我们可以采用下面几种方法： static String transient1; // not persistent because of static final String transient2 = “Satish”; // not persistent because of final transient String transient3; // not persistent because of transient @Transient String transient4; // not persistent because of @Transient 一般使用后面两种方式比较多，我个人使用注解的方式比较多。 参考 《Spring 技术内幕》 http://www.cnblogs.com/wmyskxz/p/8820371.html https://www.journaldev.com/2696/spring-interview-questions-and-answers https://www.edureka.co/blog/interview-questions/spring-interview-questions/ https://www.cnblogs.com/clwydjgs/p/9317849.html https://howtodoinjava.com/interview-questions/top-spring-interview-questions-with-answers/ http://www.tomaszezula.com/2014/02/09/spring-series-part-5-component-vs-bean/ https://stackoverflow.com/questions/34172888/difference-between-bean-and-autowired 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \"Java面试突击\" 即可免费领取！ Java工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/system-design/framework/spring/SpringMVC-Principle.html":{"url":"zother6-JavaGuide/system-design/framework/spring/SpringMVC-Principle.html","title":"Spring MVC Principle","keywords":"","body":" 本文整理自网络，原文出处暂不知，对原文做了较大的改动，在此说明！ 先来看一下什么是 MVC 模式 MVC 是一种设计模式. MVC 的原理图如下： SpringMVC 简单介绍 SpringMVC 框架是以请求为驱动，围绕 Servlet 设计，将请求发给控制器，然后通过模型对象，分派器来展示请求结果视图。其中核心类是 DispatcherServlet，它是一个 Servlet，顶层是实现的Servlet接口。 SpringMVC 使用 需要在 web.xml 中配置 DispatcherServlet 。并且需要配置 Spring 监听器ContextLoaderListener org.springframework.web.context.ContextLoaderListener springmvc org.springframework.web.servlet.DispatcherServlet contextConfigLocation classpath:spring/springmvc-servlet.xml 1 springmvc / SpringMVC 工作原理（重要） 简单来说： 客户端发送请求-> 前端控制器 DispatcherServlet 接受客户端请求 -> 找到处理器映射 HandlerMapping 解析请求对应的 Handler-> HandlerAdapter 会根据 Handler 来调用真正的处理器开处理请求，并处理相应的业务逻辑 -> 处理器返回一个模型视图 ModelAndView -> 视图解析器进行解析 -> 返回一个视图对象->前端控制器 DispatcherServlet 渲染数据（Moder）->将得到视图对象返回给用户 如下图所示： 上图的一个笔误的小问题：Spring MVC 的入口函数也就是前端控制器 DispatcherServlet 的作用是接收请求，响应结果。 流程说明（重要）： （1）客户端（浏览器）发送请求，直接请求到 DispatcherServlet。 （2）DispatcherServlet 根据请求信息调用 HandlerMapping，解析请求对应的 Handler。 （3）解析到对应的 Handler（也就是我们平常说的 Controller 控制器）后，开始由 HandlerAdapter 适配器处理。 （4）HandlerAdapter 会根据 Handler 来调用真正的处理器开处理请求，并处理相应的业务逻辑。 （5）处理器处理完业务后，会返回一个 ModelAndView 对象，Model 是返回的数据对象，View 是个逻辑上的 View。 （6）ViewResolver 会根据逻辑 View 查找实际的 View。 （7）DispaterServlet 把返回的 Model 传给 View（视图渲染）。 （8）把 View 返回给请求者（浏览器） SpringMVC 重要组件说明 1、前端控制器DispatcherServlet（不需要工程师开发）,由框架提供（重要） 作用：Spring MVC 的入口函数。接收请求，响应结果，相当于转发器，中央处理器。有了 DispatcherServlet 减少了其它组件之间的耦合度。用户请求到达前端控制器，它就相当于mvc模式中的c，DispatcherServlet是整个流程控制的中心，由它调用其它组件处理用户的请求，DispatcherServlet的存在降低了组件之间的耦合性。 2、处理器映射器HandlerMapping(不需要工程师开发),由框架提供 作用：根据请求的url查找Handler。HandlerMapping负责根据用户请求找到Handler即处理器（Controller），SpringMVC提供了不同的映射器实现不同的映射方式，例如：配置文件方式，实现接口方式，注解方式等。 3、处理器适配器HandlerAdapter 作用：按照特定规则（HandlerAdapter要求的规则）去执行Handler 通过HandlerAdapter对处理器进行执行，这是适配器模式的应用，通过扩展适配器可以对更多类型的处理器进行执行。 4、处理器Handler(需要工程师开发) 注意：编写Handler时按照HandlerAdapter的要求去做，这样适配器才可以去正确执行Handler Handler 是继DispatcherServlet前端控制器的后端控制器，在DispatcherServlet的控制下Handler对具体的用户请求进行处理。 由于Handler涉及到具体的用户业务请求，所以一般情况需要工程师根据业务需求开发Handler。 5、视图解析器View resolver(不需要工程师开发),由框架提供 作用：进行视图解析，根据逻辑视图名解析成真正的视图（view） View Resolver负责将处理结果生成View视图，View Resolver首先根据逻辑视图名解析成物理视图名即具体的页面地址，再生成View视图对象，最后对View进行渲染将处理结果通过页面展示给用户。 springmvc框架提供了很多的View视图类型，包括：jstlView、freemarkerView、pdfView等。 一般情况下需要通过页面标签或页面模版技术将模型数据通过页面展示给用户，需要由工程师根据业务需求开发具体的页面。 6、视图View(需要工程师开发) View是一个接口，实现类支持不同的View类型（jsp、freemarker、pdf...） 注意：处理器Handler（也就是我们平常说的Controller控制器）以及视图层view都是需要我们自己手动开发的。其他的一些组件比如：前端控制器DispatcherServlet、处理器映射器HandlerMapping、处理器适配器HandlerAdapter等等都是框架提供给我们的，不需要自己手动开发。 DispatcherServlet详细解析 首先看下源码： package org.springframework.web.servlet; @SuppressWarnings(\"serial\") public class DispatcherServlet extends FrameworkServlet { public static final String MULTIPART_RESOLVER_BEAN_NAME = \"multipartResolver\"; public static final String LOCALE_RESOLVER_BEAN_NAME = \"localeResolver\"; public static final String THEME_RESOLVER_BEAN_NAME = \"themeResolver\"; public static final String HANDLER_MAPPING_BEAN_NAME = \"handlerMapping\"; public static final String HANDLER_ADAPTER_BEAN_NAME = \"handlerAdapter\"; public static final String HANDLER_EXCEPTION_RESOLVER_BEAN_NAME = \"handlerExceptionResolver\"; public static final String REQUEST_TO_VIEW_NAME_TRANSLATOR_BEAN_NAME = \"viewNameTranslator\"; public static final String VIEW_RESOLVER_BEAN_NAME = \"viewResolver\"; public static final String FLASH_MAP_MANAGER_BEAN_NAME = \"flashMapManager\"; public static final String WEB_APPLICATION_CONTEXT_ATTRIBUTE = DispatcherServlet.class.getName() + \".CONTEXT\"; public static final String LOCALE_RESOLVER_ATTRIBUTE = DispatcherServlet.class.getName() + \".LOCALE_RESOLVER\"; public static final String THEME_RESOLVER_ATTRIBUTE = DispatcherServlet.class.getName() + \".THEME_RESOLVER\"; public static final String THEME_SOURCE_ATTRIBUTE = DispatcherServlet.class.getName() + \".THEME_SOURCE\"; public static final String INPUT_FLASH_MAP_ATTRIBUTE = DispatcherServlet.class.getName() + \".INPUT_FLASH_MAP\"; public static final String OUTPUT_FLASH_MAP_ATTRIBUTE = DispatcherServlet.class.getName() + \".OUTPUT_FLASH_MAP\"; public static final String FLASH_MAP_MANAGER_ATTRIBUTE = DispatcherServlet.class.getName() + \".FLASH_MAP_MANAGER\"; public static final String EXCEPTION_ATTRIBUTE = DispatcherServlet.class.getName() + \".EXCEPTION\"; public static final String PAGE_NOT_FOUND_LOG_CATEGORY = \"org.springframework.web.servlet.PageNotFound\"; private static final String DEFAULT_STRATEGIES_PATH = \"DispatcherServlet.properties\"; protected static final Log pageNotFoundLogger = LogFactory.getLog(PAGE_NOT_FOUND_LOG_CATEGORY); private static final Properties defaultStrategies; static { try { ClassPathResource resource = new ClassPathResource(DEFAULT_STRATEGIES_PATH, DispatcherServlet.class); defaultStrategies = PropertiesLoaderUtils.loadProperties(resource); } catch (IOException ex) { throw new IllegalStateException(\"Could not load 'DispatcherServlet.properties': \" + ex.getMessage()); } } /** Detect all HandlerMappings or just expect \"handlerMapping\" bean? */ private boolean detectAllHandlerMappings = true; /** Detect all HandlerAdapters or just expect \"handlerAdapter\" bean? */ private boolean detectAllHandlerAdapters = true; /** Detect all HandlerExceptionResolvers or just expect \"handlerExceptionResolver\" bean? */ private boolean detectAllHandlerExceptionResolvers = true; /** Detect all ViewResolvers or just expect \"viewResolver\" bean? */ private boolean detectAllViewResolvers = true; /** Throw a NoHandlerFoundException if no Handler was found to process this request? **/ private boolean throwExceptionIfNoHandlerFound = false; /** Perform cleanup of request attributes after include request? */ private boolean cleanupAfterInclude = true; /** MultipartResolver used by this servlet */ private MultipartResolver multipartResolver; /** LocaleResolver used by this servlet */ private LocaleResolver localeResolver; /** ThemeResolver used by this servlet */ private ThemeResolver themeResolver; /** List of HandlerMappings used by this servlet */ private List handlerMappings; /** List of HandlerAdapters used by this servlet */ private List handlerAdapters; /** List of HandlerExceptionResolvers used by this servlet */ private List handlerExceptionResolvers; /** RequestToViewNameTranslator used by this servlet */ private RequestToViewNameTranslator viewNameTranslator; private FlashMapManager flashMapManager; /** List of ViewResolvers used by this servlet */ private List viewResolvers; public DispatcherServlet() { super(); } public DispatcherServlet(WebApplicationContext webApplicationContext) { super(webApplicationContext); } @Override protected void onRefresh(ApplicationContext context) { initStrategies(context); } protected void initStrategies(ApplicationContext context) { initMultipartResolver(context); initLocaleResolver(context); initThemeResolver(context); initHandlerMappings(context); initHandlerAdapters(context); initHandlerExceptionResolvers(context); initRequestToViewNameTranslator(context); initViewResolvers(context); initFlashMapManager(context); } } DispatcherServlet类中的属性beans： HandlerMapping：用于handlers映射请求和一系列的对于拦截器的前处理和后处理，大部分用@Controller注解。 HandlerAdapter：帮助DispatcherServlet处理映射请求处理程序的适配器，而不用考虑实际调用的是 哪个处理程序。- - - ViewResolver：根据实际配置解析实际的View类型。 ThemeResolver：解决Web应用程序可以使用的主题，例如提供个性化布局。 MultipartResolver：解析多部分请求，以支持从HTML表单上传文件。- FlashMapManager：存储并检索可用于将一个请求属性传递到另一个请求的input和output的FlashMap，通常用于重定向。 在Web MVC框架中，每个DispatcherServlet都拥自己的WebApplicationContext，它继承了ApplicationContext。WebApplicationContext包含了其上下文和Servlet实例之间共享的所有的基础框架beans。 HandlerMapping HandlerMapping接口处理请求的映射HandlerMapping接口的实现类： SimpleUrlHandlerMapping类通过配置文件把URL映射到Controller类。 DefaultAnnotationHandlerMapping类通过注解把URL映射到Controller类。 HandlerAdapter HandlerAdapter接口-处理请求映射 AnnotationMethodHandlerAdapter：通过注解，把请求URL映射到Controller类的方法上。 HandlerExceptionResolver HandlerExceptionResolver接口-异常处理接口 SimpleMappingExceptionResolver通过配置文件进行异常处理。 AnnotationMethodHandlerExceptionResolver：通过注解进行异常处理。 ViewResolver ViewResolver接口解析View视图。 UrlBasedViewResolver类 通过配置文件，把一个视图名交给到一个View来处理。 "},"zother6-JavaGuide/system-design/framework/ZooKeeper-plus.html":{"url":"zother6-JavaGuide/system-design/framework/ZooKeeper-plus.html","title":"Zoo Keeper Plus","keywords":"","body":"FrancisQ 投稿。 ZooKeeper 好久不见 离上一篇文章的发布也快一个月了，想想已经快一个月没写东西了，其中可能有期末考试、课程设计和驾照考试，但这都不是借口！ 一到冬天就懒的不行，望广大掘友督促我🙄🙄✍️✍️。 文章很长，先赞后看，养成习惯。❤️ 🧡 💛 💚 💙 💜 什么是ZooKeeper ZooKeeper 由 Yahoo 开发，后来捐赠给了 Apache ，现已成为 Apache 顶级项目。ZooKeeper 是一个开源的分布式应用程序协调服务器，其为分布式系统提供一致性服务。其一致性是通过基于 Paxos 算法的 ZAB 协议完成的。其主要功能包括：配置维护、分布式同步、集群管理、分布式事务等。 简单来说， ZooKeeper 是一个 分布式协调服务框架 。分布式？协调服务？这啥玩意？🤔🤔 其实解释到分布式这个概念的时候，我发现有些同学并不是能把 分布式和集群 这两个概念很好的理解透。前段时间有同学和我探讨起分布式的东西，他说分布式不就是加机器吗？一台机器不够用再加一台抗压呗。当然加机器这种说法也无可厚非，你一个分布式系统必定涉及到多个机器，但是你别忘了，计算机学科中还有一个相似的概念—— Cluster ，集群不也是加机器吗？但是 集群 和 分布式 其实就是两个完全不同的概念。 比如，我现在有一个秒杀服务，并发量太大单机系统承受不住，那我加几台服务器也 一样 提供秒杀服务，这个时候就是 Cluster 集群 。 但是，我现在换一种方式，我将一个秒杀服务 拆分成多个子服务 ，比如创建订单服务，增加积分服务，扣优惠券服务等等，然后我将这些子服务都部署在不同的服务器上 ，这个时候就是 Distributed 分布式 。 而我为什么反驳同学所说的分布式就是加机器呢？因为我认为加机器更加适用于构建集群，因为它真是只有加机器。而对于分布式来说，你首先需要将业务进行拆分，然后再加机器（不仅仅是加机器那么简单），同时你还要去解决分布式带来的一系列问题。 比如各个分布式组件如何协调起来，如何减少各个系统之间的耦合度，分布式事务的处理，如何去配置整个分布式系统等等。ZooKeeper 主要就是解决这些问题的。 一致性问题 设计一个分布式系统必定会遇到一个问题—— 因为分区容忍性（partition tolerance）的存在，就必定要求我们需要在系统可用性（availability）和数据一致性（consistency）中做出权衡 。这就是著名的 CAP 定理。 理解起来其实很简单，比如说把一个班级作为整个系统，而学生是系统中的一个个独立的子系统。这个时候班里的小红小明偷偷谈恋爱被班里的大嘴巴小花发现了，小花欣喜若狂告诉了周围的人，然后小红小明谈恋爱的消息在班级里传播起来了。当在消息的传播（散布）过程中，你抓到一个同学问他们的情况，如果回答你不知道，那么说明整个班级系统出现了数据不一致的问题（因为小花已经知道这个消息了）。而如果他直接不回答你，因为整个班级有消息在进行传播（为了保证一致性，需要所有人都知道才可提供服务），这个时候就出现了系统的可用性问题。 而上述前者就是 Eureka 的处理方式，它保证了AP（可用性），后者就是我们今天所要将的 ZooKeeper 的处理方式，它保证了CP（数据一致性）。 一致性协议和算法 而为了解决数据一致性问题，在科学家和程序员的不断探索中，就出现了很多的一致性协议和算法。比如 2PC（两阶段提交），3PC（三阶段提交），Paxos算法等等。 这时候请你思考一个问题，同学之间如果采用传纸条的方式去传播消息，那么就会出现一个问题——我咋知道我的小纸条有没有传到我想要传递的那个人手中呢？万一被哪个小家伙给劫持篡改了呢，对吧？ 这个时候就引申出一个概念—— 拜占庭将军问题 。它意指 在不可靠信道上试图通过消息传递的方式达到一致性是不可能的， 所以所有的一致性算法的 必要前提 就是安全可靠的消息通道。 而为什么要去解决数据一致性的问题？你想想，如果一个秒杀系统将服务拆分成了下订单和加积分服务，这两个服务部署在不同的机器上了，万一在消息的传播过程中积分系统宕机了，总不能你这边下了订单却没加积分吧？你总得保证两边的数据需要一致吧？ 2PC（两阶段提交） 两阶段提交是一种保证分布式系统数据一致性的协议，现在很多数据库都是采用的两阶段提交协议来完成 分布式事务 的处理。 在介绍2PC之前，我们先来想想分布式事务到底有什么问题呢？ 还拿秒杀系统的下订单和加积分两个系统来举例吧（我想你们可能都吐了🤮🤮🤮），我们此时下完订单会发个消息给积分系统告诉它下面该增加积分了。如果我们仅仅是发送一个消息也不收回复，那么我们的订单系统怎么能知道积分系统的收到消息的情况呢？如果我们增加一个收回复的过程，那么当积分系统收到消息后返回给订单系统一个 Response ，但在中间出现了网络波动，那个回复消息没有发送成功，订单系统是不是以为积分系统消息接收失败了？它是不是会回滚事务？但此时积分系统是成功收到消息的，它就会去处理消息然后给用户增加积分，这个时候就会出现积分加了但是订单没下成功。 所以我们所需要解决的是在分布式系统中，整个调用链中，我们所有服务的数据处理要么都成功要么都失败，即所有服务的 原子性问题 。 在两阶段提交中，主要涉及到两个角色，分别是协调者和参与者。 第一阶段：当要执行一个分布式事务的时候，事务发起者首先向协调者发起事务请求，然后协调者会给所有参与者发送 prepare 请求（其中包括事务内容）告诉参与者你们需要执行事务了，如果能执行我发的事务内容那么就先执行但不提交，执行后请给我回复。然后参与者收到 prepare 消息后，他们会开始执行事务（但不提交），并将 Undo 和 Redo 信息记入事务日志中，之后参与者就向协调者反馈是否准备好了。 第二阶段：第二阶段主要是协调者根据参与者反馈的情况来决定接下来是否可以进行事务的提交操作，即提交事务或者回滚事务。 比如这个时候 所有的参与者 都返回了准备好了的消息，这个时候就进行事务的提交，协调者此时会给所有的参与者发送 Commit 请求 ，当参与者收到 Commit 请求的时候会执行前面执行的事务的 提交操作 ，提交完毕之后将给协调者发送提交成功的响应。 而如果在第一阶段并不是所有参与者都返回了准备好了的消息，那么此时协调者将会给所有参与者发送 回滚事务的 rollback 请求，参与者收到之后将会 回滚它在第一阶段所做的事务处理 ，然后再将处理情况返回给协调者，最终协调者收到响应后便给事务发起者返回处理失败的结果。 个人觉得 2PC 实现得还是比较鸡肋的，因为事实上它只解决了各个事务的原子性问题，随之也带来了很多的问题。 单点故障问题，如果协调者挂了那么整个系统都处于不可用的状态了。 阻塞问题，即当协调者发送 prepare 请求，参与者收到之后如果能处理那么它将会进行事务的处理但并不提交，这个时候会一直占用着资源不释放，如果此时协调者挂了，那么这些资源都不会再释放了，这会极大影响性能。 数据不一致问题，比如当第二阶段，协调者只发送了一部分的 commit 请求就挂了，那么也就意味着，收到消息的参与者会进行事务的提交，而后面没收到的则不会进行事务提交，那么这时候就会产生数据不一致性问题。 3PC（三阶段提交） 因为2PC存在的一系列问题，比如单点，容错机制缺陷等等，从而产生了 3PC（三阶段提交） 。那么这三阶段又分别是什么呢？ 千万不要吧PC理解成个人电脑了，其实他们是 phase-commit 的缩写，即阶段提交。 CanCommit阶段：协调者向所有参与者发送 CanCommit 请求，参与者收到请求后会根据自身情况查看是否能执行事务，如果可以则返回 YES 响应并进入预备状态，否则返回 NO 。 PreCommit阶段：协调者根据参与者返回的响应来决定是否可以进行下面的 PreCommit 操作。如果上面参与者返回的都是 YES，那么协调者将向所有参与者发送 PreCommit 预提交请求，参与者收到预提交请求后，会进行事务的执行操作，并将 Undo 和 Redo 信息写入事务日志中 ，最后如果参与者顺利执行了事务则给协调者返回成功的响应。如果在第一阶段协调者收到了 任何一个 NO 的信息，或者 在一定时间内 并没有收到全部的参与者的响应，那么就会中断事务，它会向所有参与者发送中断请求（abort），参与者收到中断请求之后会立即中断事务，或者在一定时间内没有收到协调者的请求，它也会中断事务。 DoCommit阶段：这个阶段其实和 2PC 的第二阶段差不多，如果协调者收到了所有参与者在 PreCommit 阶段的 YES 响应，那么协调者将会给所有参与者发送 DoCommit 请求，参与者收到 DoCommit 请求后则会进行事务的提交工作，完成后则会给协调者返回响应，协调者收到所有参与者返回的事务提交成功的响应之后则完成事务。若协调者在 PreCommit 阶段 收到了任何一个 NO 或者在一定时间内没有收到所有参与者的响应 ，那么就会进行中断请求的发送，参与者收到中断请求后则会 通过上面记录的回滚日志 来进行事务的回滚操作，并向协调者反馈回滚状况，协调者收到参与者返回的消息后，中断事务。 这里是 3PC 在成功的环境下的流程图，你可以看到 3PC 在很多地方进行了超时中断的处理，比如协调者在指定时间内为收到全部的确认消息则进行事务中断的处理，这样能 减少同步阻塞的时间 。还有需要注意的是，3PC 在 DoCommit 阶段参与者如未收到协调者发送的提交事务的请求，它会在一定时间内进行事务的提交。为什么这么做呢？是因为这个时候我们肯定保证了在第一阶段所有的协调者全部返回了可以执行事务的响应，这个时候我们有理由相信其他系统都能进行事务的执行和提交，所以不管协调者有没有发消息给参与者，进入第三阶段参与者都会进行事务的提交操作。 总之，3PC 通过一系列的超时机制很好的缓解了阻塞问题，但是最重要的一致性并没有得到根本的解决，比如在 PreCommit 阶段，当一个参与者收到了请求之后其他参与者和协调者挂了或者出现了网络分区，这个时候收到消息的参与者都会进行事务提交，这就会出现数据不一致性问题。 所以，要解决一致性问题还需要靠 Paxos 算法⭐️ ⭐️ ⭐️ 。 Paxos 算法 Paxos 算法是基于消息传递且具有高度容错特性的一致性算法，是目前公认的解决分布式一致性问题最有效的算法之一，其解决的问题就是在分布式系统中如何就某个值（决议）达成一致 。 在 Paxos 中主要有三个角色，分别为 Proposer提案者、Acceptor表决者、Learner学习者。Paxos 算法和 2PC 一样，也有两个阶段，分别为 Prepare 和 accept 阶段。 prepare 阶段 Proposer提案者：负责提出 proposal，每个提案者在提出提案时都会首先获取到一个 具有全局唯一性的、递增的提案编号N，即在整个集群中是唯一的编号 N，然后将该编号赋予其要提出的提案，在第一阶段是只将提案编号发送给所有的表决者。 Acceptor表决者：每个表决者在 accept 某提案后，会将该提案编号N记录在本地，这样每个表决者中保存的已经被 accept 的提案中会存在一个编号最大的提案，其编号假设为 maxN。每个表决者仅会 accept 编号大于自己本地 maxN 的提案，在批准提案时表决者会将以前接受过的最大编号的提案作为响应反馈给 Proposer 。 下面是 prepare 阶段的流程图，你可以对照着参考一下。 accept 阶段 当一个提案被 Proposer 提出后，如果 Proposer 收到了超过半数的 Acceptor 的批准（Proposer 本身同意），那么此时 Proposer 会给所有的 Acceptor 发送真正的提案（你可以理解为第一阶段为试探），这个时候 Proposer 就会发送提案的内容和提案编号。 表决者收到提案请求后会再次比较本身已经批准过的最大提案编号和该提案编号，如果该提案编号 大于等于 已经批准过的最大提案编号，那么就 accept 该提案（此时执行提案内容但不提交），随后将情况返回给 Proposer 。如果不满足则不回应或者返回 NO 。 当 Proposer 收到超过半数的 accept ，那么它这个时候会向所有的 acceptor 发送提案的提交请求。需要注意的是，因为上述仅仅是超过半数的 acceptor 批准执行了该提案内容，其他没有批准的并没有执行该提案内容，所以这个时候需要向未批准的 acceptor 发送提案内容和提案编号并让它无条件执行和提交，而对于前面已经批准过该提案的 acceptor 来说 仅仅需要发送该提案的编号 ，让 acceptor 执行提交就行了。 而如果 Proposer 如果没有收到超过半数的 accept 那么它将会将 递增 该 Proposal 的编号，然后 重新进入 Prepare 阶段 。 对于 Learner 来说如何去学习 Acceptor 批准的提案内容，这有很多方式，读者可以自己去了解一下，这里不做过多解释。 paxos 算法的死循环问题 其实就有点类似于两个人吵架，小明说我是对的，小红说我才是对的，两个人据理力争的谁也不让谁🤬🤬。 比如说，此时提案者 P1 提出一个方案 M1，完成了 Prepare 阶段的工作，这个时候 acceptor 则批准了 M1，但是此时提案者 P2 同时也提出了一个方案 M2，它也完成了 Prepare 阶段的工作。然后 P1 的方案已经不能在第二阶段被批准了（因为 acceptor 已经批准了比 M1 更大的 M2），所以 P1 自增方案变为 M3 重新进入 Prepare 阶段，然后 acceptor ，又批准了新的 M3 方案，它又不能批准 M2 了，这个时候 M2 又自增进入 Prepare 阶段。。。 就这样无休无止的永远提案下去，这就是 paxos 算法的死循环问题。 那么如何解决呢？很简单，人多了容易吵架，我现在 就允许一个能提案 就行了。 引出 ZAB Zookeeper 架构 作为一个优秀高效且可靠的分布式协调框架，ZooKeeper 在解决分布式数据一致性问题时并没有直接使用 Paxos ，而是专门定制了一致性协议叫做 ZAB(ZooKeeper Automic Broadcast) 原子广播协议，该协议能够很好地支持 崩溃恢复 。 ZAB 中的三个角色 和介绍 Paxos 一样，在介绍 ZAB 协议之前，我们首先来了解一下在 ZAB 中三个主要的角色，Leader 领导者、Follower跟随者、Observer观察者 。 Leader ：集群中 唯一的写请求处理者 ，能够发起投票（投票也是为了进行写请求）。 Follower：能够接收客户端的请求，如果是读请求则可以自己处理，如果是写请求则要转发给 Leader 。在选举过程中会参与投票，有选举权和被选举权 。 Observer ：就是没有选举权和被选举权的 Follower 。 在 ZAB 协议中对 zkServer(即上面我们说的三个角色的总称) 还有两种模式的定义，分别是 消息广播 和 崩溃恢复 。 消息广播模式 说白了就是 ZAB 协议是如何处理写请求的，上面我们不是说只有 Leader 能处理写请求嘛？那么我们的 Follower 和 Observer 是不是也需要 同步更新数据 呢？总不能数据只在 Leader 中更新了，其他角色都没有得到更新吧？ 不就是 在整个集群中保持数据的一致性 嘛？如果是你，你会怎么做呢？ 废话，第一步肯定需要 Leader 将写请求 广播 出去呀，让 Leader 问问 Followers 是否同意更新，如果超过半数以上的同意那么就进行 Follower 和 Observer 的更新（和 Paxos 一样）。当然这么说有点虚，画张图理解一下。 嗯。。。看起来很简单，貌似懂了🤥🤥🤥。这两个 Queue 哪冒出来的？答案是 ZAB 需要让 Follower 和 Observer 保证顺序性 。何为顺序性，比如我现在有一个写请求A，此时 Leader 将请求A广播出去，因为只需要半数同意就行，所以可能这个时候有一个 Follower F1因为网络原因没有收到，而 Leader 又广播了一个请求B，因为网络原因，F1竟然先收到了请求B然后才收到了请求A，这个时候请求处理的顺序不同就会导致数据的不同，从而 产生数据不一致问题 。 所以在 Leader 这端，它为每个其他的 zkServer 准备了一个 队列 ，采用先进先出的方式发送消息。由于协议是 通过 TCP 来进行网络通信的，保证了消息的发送顺序性，接受顺序性也得到了保证。 除此之外，在 ZAB 中还定义了一个 全局单调递增的事务ID ZXID ，它是一个64位long型，其中高32位表示 epoch 年代，低32位表示事务id。epoch 是会根据 Leader 的变化而变化的，当一个 Leader 挂了，新的 Leader 上位的时候，年代（epoch）就变了。而低32位可以简单理解为递增的事务id。 定义这个的原因也是为了顺序性，每个 proposal 在 Leader 中生成后需要 通过其 ZXID 来进行排序 ，才能得到处理。 崩溃恢复模式 说到崩溃恢复我们首先要提到 ZAB 中的 Leader 选举算法，当系统出现崩溃影响最大应该是 Leader 的崩溃，因为我们只有一个 Leader ，所以当 Leader 出现问题的时候我们势必需要重新选举 Leader 。 Leader 选举可以分为两个不同的阶段，第一个是我们提到的 Leader 宕机需要重新选举，第二则是当 Zookeeper 启动时需要进行系统的 Leader 初始化选举。下面我先来介绍一下 ZAB 是如何进行初始化选举的。 假设我们集群中有3台机器，那也就意味着我们需要两台以上同意（超过半数）。比如这个时候我们启动了 server1 ，它会首先 投票给自己 ，投票内容为服务器的 myid 和 ZXID ，因为初始化所以 ZXID 都为0，此时 server1 发出的投票为 (1,0)。但此时 server1 的投票仅为1，所以不能作为 Leader ，此时还在选举阶段所以整个集群处于 Looking 状态。 接着 server2 启动了，它首先也会将投票选给自己(2,0)，并将投票信息广播出去（server1也会，只是它那时没有其他的服务器了），server1 在收到 server2 的投票信息后会将投票信息与自己的作比较。首先它会比较 ZXID ，ZXID 大的优先为 Leader，如果相同则比较 myid，myid 大的优先作为 Leader。所以此时server1 发现 server2 更适合做 Leader，它就会将自己的投票信息更改为(2,0)然后再广播出去，之后server2 收到之后发现和自己的一样无需做更改，并且自己的 投票已经超过半数 ，则 确定 server2 为 Leader，server1 也会将自己服务器设置为 Following 变为 Follower。整个服务器就从 Looking 变为了正常状态。 当 server3 启动发现集群没有处于 Looking 状态时，它会直接以 Follower 的身份加入集群。 还是前面三个 server 的例子，如果在整个集群运行的过程中 server2 挂了，那么整个集群会如何重新选举 Leader 呢？其实和初始化选举差不多。 首先毫无疑问的是剩下的两个 Follower 会将自己的状态 从 Following 变为 Looking 状态 ，然后每个 server 会向初始化投票一样首先给自己投票（这不过这里的 zxid 可能不是0了，这里为了方便随便取个数字）。 假设 server1 给自己投票为(1,99)，然后广播给其他 server，server3 首先也会给自己投票(3,95)，然后也广播给其他 server。server1 和 server3 此时会收到彼此的投票信息，和一开始选举一样，他们也会比较自己的投票和收到的投票（zxid 大的优先，如果相同那么就 myid 大的优先）。这个时候 server1 收到了 server3 的投票发现没自己的合适故不变，server3 收到 server1 的投票结果后发现比自己的合适于是更改投票为(1,99)然后广播出去，最后 server1 收到了发现自己的投票已经超过半数就把自己设为 Leader，server3 也随之变为 Follower。 请注意 ZooKeeper 为什么要设置奇数个结点？比如这里我们是三个，挂了一个我们还能正常工作，挂了两个我们就不能正常工作了（已经没有超过半数的节点数了，所以无法进行投票等操作了）。而假设我们现在有四个，挂了一个也能工作，但是挂了两个也不能正常工作了，这是和三个一样的，而三个比四个还少一个，带来的效益是一样的，所以 Zookeeper 推荐奇数个 server 。 那么说完了 ZAB 中的 Leader 选举方式之后我们再来了解一下 崩溃恢复 是什么玩意？ 其实主要就是 当集群中有机器挂了，我们整个集群如何保证数据一致性？ 如果只是 Follower 挂了，而且挂的没超过半数的时候，因为我们一开始讲了在 Leader 中会维护队列，所以不用担心后面的数据没接收到导致数据不一致性。 如果 Leader 挂了那就麻烦了，我们肯定需要先暂停服务变为 Looking 状态然后进行 Leader 的重新选举（上面我讲过了），但这个就要分为两种情况了，分别是 确保已经被Leader提交的提案最终能够被所有的Follower提交 和 跳过那些已经被丢弃的提案 。 确保已经被Leader提交的提案最终能够被所有的Follower提交是什么意思呢？ 假设 Leader (server2) 发送 commit 请求（忘了请看上面的消息广播模式），他发送给了 server3，然后要发给 server1 的时候突然挂了。这个时候重新选举的时候我们如果把 server1 作为 Leader 的话，那么肯定会产生数据不一致性，因为 server3 肯定会提交刚刚 server2 发送的 commit 请求的提案，而 server1 根本没收到所以会丢弃。 那怎么解决呢？ 聪明的同学肯定会质疑，这个时候 server1 已经不可能成为 Leader 了，因为 server1 和 server3 进行投票选举的时候会比较 ZXID ，而此时 server3 的 ZXID 肯定比 server1 的大了。(不理解可以看前面的选举算法) 那么跳过那些已经被丢弃的提案又是什么意思呢？ 假设 Leader (server2) 此时同意了提案N1，自身提交了这个事务并且要发送给所有 Follower 要 commit 的请求，却在这个时候挂了，此时肯定要重新进行 Leader 的选举，比如说此时选 server1 为 Leader （这无所谓）。但是过了一会，这个 挂掉的 Leader 又重新恢复了 ，此时它肯定会作为 Follower 的身份进入集群中，需要注意的是刚刚 server2 已经同意提交了提案N1，但其他 server 并没有收到它的 commit 信息，所以其他 server 不可能再提交这个提案N1了，这样就会出现数据不一致性问题了，所以 该提案N1最终需要被抛弃掉 。 Zookeeper的几个理论知识 了解了 ZAB 协议还不够，它仅仅是 Zookeeper 内部实现的一种方式，而我们如何通过 Zookeeper 去做一些典型的应用场景呢？比如说集群管理，分布式锁，Master 选举等等。 这就涉及到如何使用 Zookeeper 了，但在使用之前我们还需要掌握几个概念。比如 Zookeeper 的 数据模型 、会话机制、ACL、Watcher机制 等等。 数据模型 zookeeper 数据存储结构与标准的 Unix 文件系统非常相似，都是在根节点下挂很多子节点(树型)。但是 zookeeper 中没有文件系统中目录与文件的概念，而是 使用了 znode 作为数据节点 。znode 是 zookeeper 中的最小数据单元，每个 znode 上都可以保存数据，同时还可以挂载子节点，形成一个树形化命名空间。 每个 znode 都有自己所属的 节点类型 和 节点状态。 其中节点类型可以分为 持久节点、持久顺序节点、临时节点 和 临时顺序节点。 持久节点：一旦创建就一直存在，直到将其删除。 持久顺序节点：一个父节点可以为其子节点 维护一个创建的先后顺序 ，这个顺序体现在 节点名称 上，是节点名称后自动添加一个由 10 位数字组成的数字串，从 0 开始计数。 临时节点：临时节点的生命周期是与 客户端会话 绑定的，会话消失则节点消失 。临时节点 只能做叶子节点 ，不能创建子节点。 临时顺序节点：父节点可以创建一个维持了顺序的临时节点(和前面的持久顺序性节点一样)。 节点状态中包含了很多节点的属性比如 czxid 、mzxid 等等，在 zookeeper 中是使用 Stat 这个类来维护的。下面我列举一些属性解释。 czxid：Created ZXID，该数据节点被 创建 时的事务ID。 mzxid：Modified ZXID，节点 最后一次被更新时 的事务ID。 ctime：Created Time，该节点被创建的时间。 mtime： Modified Time，该节点最后一次被修改的时间。 version：节点的版本号。 cversion：子节点 的版本号。 aversion：节点的 ACL 版本号。 ephemeralOwner：创建该节点的会话的 sessionID ，如果该节点为持久节点，该值为0。 dataLength：节点数据内容的长度。 numChildre：该节点的子节点个数，如果为临时节点为0。 pzxid：该节点子节点列表最后一次被修改时的事务ID，注意是子节点的 列表 ，不是内容。 会话 我想这个对于后端开发的朋友肯定不陌生，不就是 session 吗？只不过 zk 客户端和服务端是通过 TCP 长连接 维持的会话机制，其实对于会话来说你可以理解为 保持连接状态 。 在 zookeeper 中，会话还有对应的事件，比如 CONNECTION_LOSS 连接丢失事件 、SESSION_MOVED 会话转移事件 、SESSION_EXPIRED 会话超时失效事件 。 ACL ACL 为 Access Control Lists ，它是一种权限控制。在 zookeeper 中定义了5种权限，它们分别为： CREATE ：创建子节点的权限。 READ：获取节点数据和子节点列表的权限。 WRITE：更新节点数据的权限。 DELETE：删除子节点的权限。 ADMIN：设置节点 ACL 的权限。 Watcher机制 Watcher 为事件监听器，是 zk 非常重要的一个特性，很多功能都依赖于它，它有点类似于订阅的方式，即客户端向服务端 注册 指定的 watcher ，当服务端符合了 watcher 的某些事件或要求则会 向客户端发送事件通知 ，客户端收到通知后找到自己定义的 Watcher 然后 执行相应的回调方法 。 Zookeeper的几个典型应用场景 前面说了这么多的理论知识，你可能听得一头雾水，这些玩意有啥用？能干啥事？别急，听我慢慢道来。 选主 还记得上面我们的所说的临时节点吗？因为 Zookeeper 的强一致性，能够很好地在保证 在高并发的情况下保证节点创建的全局唯一性 (即无法重复创建同样的节点)。 利用这个特性，我们可以 让多个客户端创建一个指定的节点 ，创建成功的就是 master。 但是，如果这个 master 挂了怎么办？？？ 你想想为什么我们要创建临时节点？还记得临时节点的生命周期吗？master 挂了是不是代表会话断了？会话断了是不是意味着这个节点没了？还记得 watcher 吗？我们是不是可以 让其他不是 master 的节点监听节点的状态 ，比如说我们监听这个临时节点的父节点，如果子节点个数变了就代表 master 挂了，这个时候我们 触发回调函数进行重新选举 ，或者我们直接监听节点的状态，我们可以通过节点是否已经失去连接来判断 master 是否挂了等等。 总的来说，我们可以完全 利用 临时节点、节点状态 和 watcher 来实现选主的功能，临时节点主要用来选举，节点状态和watcher 可以用来判断 master 的活性和进行重新选举。 分布式锁 分布式锁的实现方式有很多种，比如 Redis 、数据库 、zookeeper 等。个人认为 zookeeper 在实现分布式锁这方面是非常非常简单的。 上面我们已经提到过了 zk在高并发的情况下保证节点创建的全局唯一性，这玩意一看就知道能干啥了。实现互斥锁呗，又因为能在分布式的情况下，所以能实现分布式锁呗。 如何实现呢？这玩意其实跟选主基本一样，我们也可以利用临时节点的创建来实现。 首先肯定是如何获取锁，因为创建节点的唯一性，我们可以让多个客户端同时创建一个临时节点，创建成功的就说明获取到了锁 。然后没有获取到锁的客户端也像上面选主的非主节点创建一个 watcher 进行节点状态的监听，如果这个互斥锁被释放了（可能获取锁的客户端宕机了，或者那个客户端主动释放了锁）可以调用回调函数重新获得锁。 zk 中不需要向 redis 那样考虑锁得不到释放的问题了，因为当客户端挂了，节点也挂了，锁也释放了。是不是很简答？ 那能不能使用 zookeeper 同时实现 共享锁和独占锁 呢？答案是可以的，不过稍微有点复杂而已。 还记得 有序的节点 吗？ 这个时候我规定所有创建节点必须有序，当你是读请求（要获取共享锁）的话，如果 没有比自己更小的节点，或比自己小的节点都是读请求 ，则可以获取到读锁，然后就可以开始读了。若比自己小的节点中有写请求 ，则当前客户端无法获取到读锁，只能等待前面的写请求完成。 如果你是写请求（获取独占锁），若 没有比自己更小的节点 ，则表示当前客户端可以直接获取到写锁，对数据进行修改。若发现 有比自己更小的节点，无论是读操作还是写操作，当前客户端都无法获取到写锁 ，等待所有前面的操作完成。 这就很好地同时实现了共享锁和独占锁，当然还有优化的地方，比如当一个锁得到释放它会通知所有等待的客户端从而造成 羊群效应 。此时你可以通过让等待的节点只监听他们前面的节点。 具体怎么做呢？其实也很简单，你可以让 读请求监听比自己小的最后一个写请求节点，写请求只监听比自己小的最后一个节点 ，感兴趣的小伙伴可以自己去研究一下。 命名服务 如何给一个对象设置ID，大家可能都会想到 UUID，但是 UUID 最大的问题就在于它太长了。。。(太长不一定是好事，嘿嘿嘿)。那么在条件允许的情况下，我们能不能使用 zookeeper 来实现呢？ 我们之前提到过 zookeeper 是通过 树形结构 来存储数据节点的，那也就是说，对于每个节点的 全路径，它必定是唯一的，我们可以使用节点的全路径作为命名方式了。而且更重要的是，路径是我们可以自己定义的，这对于我们对有些有语意的对象的ID设置可以更加便于理解。 集群管理和注册中心 看到这里是不是觉得 zookeeper 实在是太强大了，它怎么能这么能干！ 别急，它能干的事情还很多呢。可能我们会有这样的需求，我们需要了解整个集群中有多少机器在工作，我们想对及群众的每台机器的运行时状态进行数据采集，对集群中机器进行上下线操作等等。 而 zookeeper 天然支持的 watcher 和 临时节点能很好的实现这些需求。我们可以为每条机器创建临时节点，并监控其父节点，如果子节点列表有变动（我们可能创建删除了临时节点），那么我们可以使用在其父节点绑定的 watcher 进行状态监控和回调。 至于注册中心也很简单，我们同样也是让 服务提供者 在 zookeeper 中创建一个临时节点并且将自己的 ip、port、调用方式 写入节点，当 服务消费者 需要进行调用的时候会 通过注册中心找到相应的服务的地址列表(IP端口什么的) ，并缓存到本地(方便以后调用)，当消费者调用服务时，不会再去请求注册中心，而是直接通过负载均衡算法从地址列表中取一个服务提供者的服务器调用服务。 当服务提供者的某台服务器宕机或下线时，相应的地址会从服务提供者地址列表中移除。同时，注册中心会将新的服务地址列表发送给服务消费者的机器并缓存在消费者本机（当然你可以让消费者进行节点监听，我记得 Eureka 会先试错，然后再更新）。 总结 看到这里的同学实在是太有耐心了👍👍👍，如果觉得我写得不错的话点个赞哈。 不知道大家是否还记得我讲了什么😒。 这篇文章中我带大家入门了 zookeeper 这个强大的分布式协调框架。现在我们来简单梳理一下整篇文章的内容。 分布式与集群的区别 2PC 、3PC 以及 paxos 算法这些一致性框架的原理和实现。 zookeeper 专门的一致性算法 ZAB 原子广播协议的内容（Leader 选举、崩溃恢复、消息广播）。 zookeeper 中的一些基本概念，比如 ACL，数据节点，会话，watcher机制等等。 zookeeper 的典型应用场景，比如选主，注册中心等等。 如果忘了可以回去看看再次理解一下，如果有疑问和建议欢迎提出🤝🤝🤝。 "},"zother6-JavaGuide/system-design/framework/ZooKeeper.html":{"url":"zother6-JavaGuide/system-design/framework/ZooKeeper.html","title":"Zoo Keeper","keywords":"","body":" 前言 相信大家对 ZooKeeper 应该不算陌生。但是你真的了解 ZooKeeper 是个什么东西吗？如果别人/面试官让你给他讲讲 ZooKeeper 是个什么东西，你能回答到什么地步呢？ 我本人曾经使用过 ZooKeeper 作为 Dubbo 的注册中心，另外在搭建 solr 集群的时候，我使用到了 ZooKeeper 作为 solr 集群的管理工具。前几天，总结项目经验的时候，我突然问自己 ZooKeeper 到底是个什么东西？想了半天，脑海中只是简单的能浮现出几句话：“①Zookeeper 可以被用作注册中心。 ②Zookeeper 是 Hadoop 生态系统的一员；③构建 Zookeeper 集群的时候，使用的服务器最好是奇数台。” 可见，我对于 Zookeeper 的理解仅仅是停留在了表面。 所以，通过本文，希望带大家稍微详细的了解一下 ZooKeeper 。如果没有学过 ZooKeeper ，那么本文将会是你进入 ZooKeeper 大门的垫脚砖。如果你已经接触过 ZooKeeper ，那么本文将带你回顾一下 ZooKeeper 的一些基础概念。 最后，本文只涉及 ZooKeeper 的一些概念，并不涉及 ZooKeeper 的使用以及 ZooKeeper 集群的搭建。 网上有介绍 ZooKeeper 的使用以及搭建 ZooKeeper 集群的文章，大家有需要可以自行查阅。 一 什么是 ZooKeeper ZooKeeper 的由来 下面这段内容摘自《从Paxos到Zookeeper 》第四章第一节的某段内容，推荐大家阅读以下： Zookeeper最早起源于雅虎研究院的一个研究小组。在当时，研究人员发现，在雅虎内部很多大型系统基本都需要依赖一个类似的系统来进行分布式协调，但是这些系统往往都存在分布式单点问题。所以，雅虎的开发人员就试图开发一个通用的无单点问题的分布式协调框架，以便让开发人员将精力集中在处理业务逻辑上。 关于“ZooKeeper”这个项目的名字，其实也有一段趣闻。在立项初期，考虑到之前内部很多项目都是使用动物的名字来命名的（例如著名的Pig项目),雅虎的工程师希望给这个项目也取一个动物的名字。时任研究院的首席科学家RaghuRamakrishnan开玩笑地说：“在这样下去，我们这儿就变成动物园了！”此话一出，大家纷纷表示就叫动物园管理员吧一一一因为各个以动物命名的分布式组件放在一起，雅虎的整个分布式系统看上去就像一个大型的动物园了，而Zookeeper正好要用来进行分布式环境的协调一一于是，Zookeeper的名字也就由此诞生了。 1.1 ZooKeeper 概览 ZooKeeper 是一个开源的分布式协调服务，ZooKeeper框架最初是在“Yahoo!\"上构建的，用于以简单而稳健的方式访问他们的应用程序。 后来，Apache ZooKeeper成为Hadoop，HBase和其他分布式框架使用的有组织服务的标准。 例如，Apache HBase使用ZooKeeper跟踪分布式数据的状态。ZooKeeper 的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。 原语： 操作系统或计算机网络用语范畴。是由若干条指令组成的，用于完成一定功能的一个过程。具有不可分割性·即原语的执行必须是连续的，在执行过程中不允许被中断。 ZooKeeper 是一个典型的分布式数据一致性解决方案，分布式应用程序可以基于 ZooKeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。 Zookeeper 一个最常用的使用场景就是用于担任服务生产者和服务消费者的注册中心(提供发布订阅服务)。 服务生产者将自己提供的服务注册到Zookeeper中心，服务的消费者在进行服务调用的时候先到Zookeeper中查找服务，获取到服务生产者的详细信息之后，再去调用服务生产者的内容与数据。如下图所示，在 Dubbo架构中 Zookeeper 就担任了注册中心这一角色。 1.2 结合个人使用情况的讲一下 ZooKeeper 在我自己做过的项目中，主要使用到了 ZooKeeper 作为 Dubbo 的注册中心(Dubbo 官方推荐使用 ZooKeeper注册中心)。另外在搭建 solr 集群的时候，我使用 ZooKeeper 作为 solr 集群的管理工具。这时，ZooKeeper 主要提供下面几个功能：1、集群管理：容错、负载均衡。2、配置文件的集中管理3、集群的入口。 我个人觉得在使用 ZooKeeper 的时候，最好是使用 集群版的 ZooKeeper 而不是单机版的。官网给出的架构图就描述的是一个集群版的 ZooKeeper 。通常 3 台服务器就可以构成一个 ZooKeeper 集群了。 为什么最好使用奇数台服务器构成 ZooKeeper 集群？ 所谓的zookeeper容错是指，当宕掉几个zookeeper服务器之后，剩下的个数必须大于宕掉的个数的话整个zookeeper才依然可用。假如我们的集群中有n台zookeeper服务器，那么也就是剩下的服务数必须大于n/2。先说一下结论，2n和2n-1的容忍度是一样的，都是n-1，大家可以先自己仔细想一想，这应该是一个很简单的数学问题了。 比如假如我们有3台，那么最大允许宕掉1台zookeeper服务器，如果我们有4台的的时候也同样只允许宕掉1台。 假如我们有5台，那么最大允许宕掉2台zookeeper服务器，如果我们有6台的的时候也同样只允许宕掉2台。 综上，何必增加那一个不必要的zookeeper呢？ 二 关于 ZooKeeper 的一些重要概念 2.1 重要概念总结 ZooKeeper 本身就是一个分布式程序（只要半数以上节点存活，ZooKeeper 就能正常服务）。 为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么 ZooKeeper 本身仍然是可用的。 ZooKeeper 将数据保存在内存中，这也就保证了 高吞吐量和低延迟（但是内存限制了能够存储的容量不太大，此限制也是保持znode中存储的数据量较小的进一步原因）。 ZooKeeper 是高性能的。 在“读”多于“写”的应用程序中尤其地高性能，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景。） ZooKeeper有临时节点的概念。 当创建临时节点的客户端会话一直保持活动，瞬时节点就一直存在。而当会话终结时，瞬时节点被删除。持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上。 ZooKeeper 底层其实只提供了两个功能：①管理（存储、读取）用户程序提交的数据；②为用户程序提供数据节点监听服务。 下面关于会话（Session）、 Znode、版本、Watcher、ACL概念的总结都在《从Paxos到Zookeeper 》第四章第一节以及第七章第八节有提到，感兴趣的可以看看！ 2.2 会话（Session） Session 指的是 ZooKeeper 服务器与客户端会话。在 ZooKeeper 中，一个客户端连接是指客户端和服务器之间的一个 TCP 长连接。客户端启动的时候，首先会与服务器建立一个 TCP 连接，从第一次连接建立开始，客户端会话的生命周期也开始了。通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向Zookeeper服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的Watch事件通知。 Session的sessionTimeout值用来设置一个客户端会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在sessionTimeout规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。 在为客户端创建会话之前，服务端首先会为每个客户端都分配一个sessionID。由于 sessionID 是 Zookeeper 会话的一个重要标识，许多与会话相关的运行机制都是基于这个 sessionID 的，因此，无论是哪台服务器为客户端分配的 sessionID，都务必保证全局唯一。 2.3 Znode 在谈到分布式的时候，我们通常说的“节点\"是指组成集群的每一台机器。然而，在Zookeeper中，“节点\"分为两类，第一类同样是指构成集群的机器，我们称之为机器节点；第二类则是指数据模型中的数据单元，我们称之为数据节点一一ZNode。 Zookeeper将所有数据存储在内存中，数据模型是一棵树（Znode Tree)，由斜杠（/）的进行分割的路径，就是一个Znode，例如/foo/path1。每个上都会保存自己的数据内容，同时还会保存一系列属性信息。 在Zookeeper中，node可以分为持久节点和临时节点两类。所谓持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上。而临时节点就不一样了，它的生命周期和客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。 另外，ZooKeeper还允许用户为每个节点添加一个特殊的属性：SEQUENTIAL.一旦节点被标记上这个属性，那么在这个节点被创建的时候，Zookeeper会自动在其节点名后面追加上一个整型数字，这个整型数字是一个由父节点维护的自增数字。 2.4 版本 在前面我们已经提到，Zookeeper 的每个 ZNode 上都会存储数据，对应于每个ZNode，Zookeeper 都会为其维护一个叫作 Stat 的数据结构，Stat 中记录了这个 ZNode 的三个数据版本，分别是version（当前ZNode的版本）、cversion（当前ZNode子节点的版本）和 aversion（当前ZNode的ACL版本）。 2.5 Watcher Watcher（事件监听器），是Zookeeper中的一个很重要的特性。Zookeeper允许用户在指定节点上注册一些Watcher，并且在一些特定事件触发的时候，ZooKeeper服务端会将事件通知到感兴趣的客户端上去，该机制是Zookeeper实现分布式协调服务的重要特性。 2.6 ACL Zookeeper采用ACL（AccessControlLists）策略来进行权限控制，类似于 UNIX 文件系统的权限控制。Zookeeper 定义了如下5种权限。 其中尤其需要注意的是，CREATE和DELETE这两种权限都是针对子节点的权限控制。 三 ZooKeeper 特点 顺序一致性： 从同一客户端发起的事务请求，最终将会严格地按照顺序被应用到 ZooKeeper 中去。 原子性： 所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群中所有的机器都成功应用了某一个事务，要么都没有应用。 单一系统映像 ： 无论客户端连到哪一个 ZooKeeper 服务器上，其看到的服务端数据模型都是一致的。 可靠性： 一旦一次更改请求被应用，更改的结果就会被持久化，直到被下一次更改覆盖。 四 ZooKeeper 设计目标 4.1 简单的数据模型 ZooKeeper 允许分布式进程通过共享的层次结构命名空间进行相互协调，这与标准文件系统类似。 名称空间由 ZooKeeper 中的数据寄存器组成 - 称为znode，这些类似于文件和目录。 与为存储设计的典型文件系统不同，ZooKeeper数据保存在内存中，这意味着ZooKeeper可以实现高吞吐量和低延迟。 4.2 可构建集群 为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么zookeeper本身仍然是可用的。 客户端在使用 ZooKeeper 时，需要知道集群机器列表，通过与集群中的某一台机器建立 TCP 连接来使用服务，客户端使用这个TCP链接来发送请求、获取结果、获取监听事件以及发送心跳包。如果这个连接异常断开了，客户端可以连接到另外的机器上。 ZooKeeper 官方提供的架构图： 上图中每一个Server代表一个安装Zookeeper服务的服务器。组成 ZooKeeper 服务的服务器都会在内存中维护当前的服务器状态，并且每台服务器之间都互相保持着通信。集群间通过 Zab 协议（Zookeeper Atomic Broadcast）来保持数据的一致性。 4.3 顺序访问 对于来自客户端的每个更新请求，ZooKeeper 都会分配一个全局唯一的递增编号，这个编号反应了所有事务操作的先后顺序，应用程序可以使用 ZooKeeper 这个特性来实现更高层次的同步原语。 这个编号也叫做时间戳——zxid（Zookeeper Transaction Id） 4.4 高性能 ZooKeeper 是高性能的。 在“读”多于“写”的应用程序中尤其地高性能，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景。） 五 ZooKeeper 集群角色介绍 最典型集群模式： Master/Slave 模式（主备模式）。在这种模式中，通常 Master服务器作为主服务器提供写服务，其他的 Slave 服务器从服务器通过异步复制的方式获取 Master 服务器最新的数据提供读服务。 但是，在 ZooKeeper 中没有选择传统的 Master/Slave 概念，而是引入了Leader、Follower 和 Observer 三种角色。如下图所示 ZooKeeper 集群中的所有机器通过一个 Leader 选举过程来选定一台称为 “Leader” 的机器，Leader 既可以为客户端提供写服务又能提供读服务。除了 Leader 外，Follower 和 Observer 都只能提供读服务。Follower 和 Observer 唯一的区别在于 Observer 机器不参与 Leader 的选举过程，也不参与写操作的“过半写成功”策略，因此 Observer 机器可以在不影响写性能的情况下提升集群的读性能。 当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进人恢复模式并选举产生新的Leader服务器。这个过程大致是这样的： Leader election（选举阶段）：节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准 leader。 Discovery（发现阶段）：在这个阶段，followers 跟准 leader 进行通信，同步 followers 最近接收的事务提议。 Synchronization（同步阶段）:同步阶段主要是利用 leader 前一阶段获得的最新提议历史，同步集群中所有的副本。同步完成之后 准 leader 才会成为真正的 leader。 Broadcast（广播阶段） 到了这个阶段，Zookeeper 集群才能正式对外提供事务服务，并且 leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步。 六 ZooKeeper &ZAB 协议&Paxos算法 6.1 ZAB 协议&Paxos算法 Paxos 算法应该可以说是 ZooKeeper 的灵魂了。但是，ZooKeeper 并没有完全采用 Paxos算法 ，而是使用 ZAB 协议作为其保证数据一致性的核心算法。另外，在ZooKeeper的官方文档中也指出，ZAB协议并不像 Paxos 算法那样，是一种通用的分布式一致性算法，它是一种特别为Zookeeper设计的崩溃可恢复的原子消息广播算法。 6.2 ZAB 协议介绍 ZAB（ZooKeeper Atomic Broadcast 原子广播） 协议是为分布式协调服务 ZooKeeper 专门设计的一种支持崩溃恢复的原子广播协议。 在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。 6.3 ZAB 协议两种基本的模式：崩溃恢复和消息广播 ZAB协议包括两种基本的模式，分别是 崩溃恢复和消息广播。当整个服务框架在启动过程中，或是当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进人恢复模式并选举产生新的Leader服务器。当选举产生了新的 Leader 服务器，同时集群中已经有过半的机器与该Leader服务器完成了状态同步之后，ZAB协议就会退出恢复模式。其中，所谓的状态同步是指数据同步，用来保证集群中存在过半的机器能够和Leader服务器的数据状态保持一致。 当集群中已经有过半的Follower服务器完成了和Leader服务器的状态同步，那么整个服务框架就可以进人消息广播模式了。 当一台同样遵守ZAB协议的服务器启动后加人到集群中时，如果此时集群中已经存在一个Leader服务器在负责进行消息广播，那么新加人的服务器就会自觉地进人数据恢复模式：找到Leader所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。正如上文介绍中所说的，ZooKeeper设计成只允许唯一的一个Leader服务器来进行事务请求的处理。Leader服务器在接收到客户端的事务请求后，会生成对应的事务提案并发起一轮广播协议；而如果集群中的其他机器接收到客户端的事务请求，那么这些非Leader服务器会首先将这个事务请求转发给Leader服务器。 关于 ZAB 协议&Paxos算法 需要讲和理解的东西太多了，说实话，笔主到现在不太清楚这俩兄弟的具体原理和实现过程。推荐阅读下面两篇文章： 图解 Paxos 一致性协议 Zookeeper ZAB 协议分析 关于如何使用 zookeeper 实现分布式锁，可以查看下面这篇文章： 10分钟看懂！基于Zookeeper的分布式锁 六 总结 通过阅读本文，想必大家已从 ①ZooKeeper的由来。 -> ②ZooKeeper 到底是什么 。-> ③ ZooKeeper 的一些重要概念（会话（Session）、 Znode、版本、Watcher、ACL）-> ④ZooKeeper 的特点。 -> ⑤ZooKeeper 的设计目标。-> ⑥ ZooKeeper 集群角色介绍 （Leader、Follower 和 Observer 三种角色）-> ⑦ZooKeeper &ZAB 协议&Paxos算法。 这七点了解了 ZooKeeper 。 参考 《从Paxos到Zookeeper 》 https://cwiki.apache.org/confluence/display/ZOOKEEPER/ProjectDescription https://cwiki.apache.org/confluence/display/ZOOKEEPER/Index https://www.cnblogs.com/raphael5200/p/5285583.html https://zhuanlan.zhihu.com/p/30024403 "},"zother6-JavaGuide/system-design/framework/ZooKeeper数据模型和常见命令.html":{"url":"zother6-JavaGuide/system-design/framework/ZooKeeper数据模型和常见命令.html","title":"ZooKeeper数据模型和常见命令","keywords":"","body":" ZooKeeper 数据模型 ZNode(数据节点)的结构 测试 ZooKeeper 中的常见操作 连接 ZooKeeper 服务 查看常用命令(help 命令) 创建节点(create 命令) 更新节点数据内容(set 命令) 获取节点的数据(get 命令) 查看某个目录下的子节点(ls 命令) 查看节点状态(stat 命令) 查看节点信息和状态(ls2 命令) 删除节点(delete 命令) 参考 看本文之前如果你没有安装 ZooKeeper 的话，可以参考这篇文章：《使用 SpringBoot+Dubbo 搭建一个简单分布式服务》 的 “开始实战 1 ：zookeeper 环境安装搭建” 这部分进行安装（Centos7.4 环境下）。如果你想对 ZooKeeper 有一个整体了解的话，可以参考这篇文章：《可能是把 ZooKeeper 概念讲的最清楚的一篇文章》 ZooKeeper 数据模型 ZNode（数据节点）是 ZooKeeper 中数据的最小单元，每个ZNode上都可以保存数据，同时还是可以有子节点（这就像树结构一样，如下图所示）。可以看出，节点路径标识方式和Unix文件 系统路径非常相似，都是由一系列使用斜杠\"/\"进行分割的路径表示，开发人员可以向这个节点中写人数据，也可以在节点下面创建子节点。这些操作我们后面都会介绍到。 提到 ZooKeeper 数据模型，还有一个不得不得提的东西就是 事务 ID 。事务的ACID（Atomic：原子性；Consistency:一致性；Isolation：隔离性；Durability：持久性）四大特性我在这里就不多说了，相信大家也已经挺腻了。 在Zookeeper中，事务是指能够改变 ZooKeeper 服务器状态的操作，我们也称之为事务操作或更新操作，一般包括数据节点创建与删除、数据节点内容更新和客户端会话创建与失效等操作。对于每一个事务请求，ZooKeeper 都会为其分配一个全局唯一的事务ID,用 ZXID 来表示，通常是一个64位的数字。每一个ZXID对应一次更新操作，从这些 ZXID 中可以间接地识别出Zookeeper处理这些更新操作请求的全局顺序。 ZNode(数据节点)的结构 每个 ZNode 由2部分组成: stat：状态信息 data：数据内容 如下所示，我通过 get 命令来获取 根目录下的 dubbo 节点的内容。（get 命令在下面会介绍到） [zk: 127.0.0.1:2181(CONNECTED) 6] get /dubbo # 该数据节点关联的数据内容为空 null # 下面是该数据节点的一些状态信息，其实就是 Stat 对象的格式化输出 cZxid = 0x2 ctime = Tue Nov 27 11:05:34 CST 2018 mZxid = 0x2 mtime = Tue Nov 27 11:05:34 CST 2018 pZxid = 0x3 cversion = 1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 0 numChildren = 1 这些状态信息其实就是 Stat 对象的格式化输出。Stat 类中包含了一个数据节点的所有状态信息的字段，包括事务ID、版本信息和子节点个数等，如下图所示（图源：《从Paxos到Zookeeper 分布式一致性原理与实践》，下面会介绍通过 stat 命令查看数据节点的状态）。 Stat 类： 关于数据节点的状态信息说明（也就是对Stat 类中的各字段进行说明），可以参考下图（图源：《从Paxos到Zookeeper 分布式一致性原理与实践》）。 测试 ZooKeeper 中的常见操作 连接 ZooKeeper 服务 进入安装 ZooKeeper文件夹的 bin 目录下执行下面的命令连接 ZooKeeper 服务（Linux环境下）（连接之前首选要确定你的 ZooKeeper 服务已经启动成功）。 ./zkCli.sh -server 127.0.0.1:2181 从上图可以看出控制台打印出了很多信息，包括我们的主机名称、JDK 版本、操作系统等等。如果你成功看到这些信息，说明你成功连接到 ZooKeeper 服务。 查看常用命令(help 命令) help 命令查看 zookeeper 常用命令 创建节点(create 命令) 通过 create 命令在根目录创建了node1节点，与它关联的字符串是\"node1\" [zk: 127.0.0.1:2181(CONNECTED) 34] create /node1 “node1” 通过 create 命令在根目录创建了node1节点，与它关联的内容是数字 123 [zk: 127.0.0.1:2181(CONNECTED) 1] create /node1/node1.1 123 Created /node1/node1.1 更新节点数据内容(set 命令) [zk: 127.0.0.1:2181(CONNECTED) 11] set /node1 \"set node1\" 获取节点的数据(get 命令) get 命令可以获取指定节点的数据内容和节点的状态,可以看出我们通过set 命令已经将节点数据内容改为 \"set node1\"。 set node1 cZxid = 0x47 ctime = Sun Jan 20 10:22:59 CST 2019 mZxid = 0x4b mtime = Sun Jan 20 10:41:10 CST 2019 pZxid = 0x4a cversion = 1 dataVersion = 1 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 9 numChildren = 1 查看某个目录下的子节点(ls 命令) 通过 ls 命令查看根目录下的节点 [zk: 127.0.0.1:2181(CONNECTED) 37] ls / [dubbo, zookeeper, node1] 通过 ls 命令查看 node1 目录下的节点 [zk: 127.0.0.1:2181(CONNECTED) 5] ls /node1 [node1.1] zookeeper 中的 ls 命令和 linux 命令中的 ls 类似， 这个命令将列出绝对路径path下的所有子节点信息（列出1级，并不递归） 查看节点状态(stat 命令) 通过 stat 命令查看节点状态 [zk: 127.0.0.1:2181(CONNECTED) 10] stat /node1 cZxid = 0x47 ctime = Sun Jan 20 10:22:59 CST 2019 mZxid = 0x47 mtime = Sun Jan 20 10:22:59 CST 2019 pZxid = 0x4a cversion = 1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 11 numChildren = 1 上面显示的一些信息比如cversion、aclVersion、numChildren等等，我在上面 “ZNode(数据节点)的结构” 这部分已经介绍到。 查看节点信息和状态(ls2 命令) ls2 命令更像是 ls 命令和 stat 命令的结合。ls2 命令返回的信息包括2部分：子节点列表 + 当前节点的stat信息。 [zk: 127.0.0.1:2181(CONNECTED) 7] ls2 /node1 [node1.1] cZxid = 0x47 ctime = Sun Jan 20 10:22:59 CST 2019 mZxid = 0x47 mtime = Sun Jan 20 10:22:59 CST 2019 pZxid = 0x4a cversion = 1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 11 numChildren = 1 删除节点(delete 命令) 这个命令很简单，但是需要注意的一点是如果你要删除某一个节点，那么这个节点必须无子节点才行。 [zk: 127.0.0.1:2181(CONNECTED) 3] delete /node1/node1.1 在后面我会介绍到 Java 客户端 API的使用以及开源 Zookeeper 客户端 ZkClient 和 Curator 的使用。 参考 《从Paxos到Zookeeper 分布式一致性原理与实践》 "},"zother6-JavaGuide/system-design/micro-service/api-gateway-intro.html":{"url":"zother6-JavaGuide/system-design/micro-service/api-gateway-intro.html","title":"Api Gateway Intro","keywords":"","body":"为什么要网关？ 微服务下一个系统被拆分为多个服务，但是像 安全认证，流量控制，日志，监控等功能是每个服务都需要的，没有网关的话，我们就需要在每个服务中单独实现，这使得我们做了很多重复的事情并且没有一个全局的视图来统一管理这些功能。 综上：一般情况下，网关一般都会提供请求转发、安全认证（身份/权限认证）、流量控制、负载均衡、容灾、日志、监控这些功能。 上面介绍了这么多功能实际上网关主要做了一件事情：请求过滤 。权限校验、流量控制这些都可以通过过滤器实现，请求转也是通过过滤器实现的。 你知道有哪些常见的网关系统？ 我所了解的目前经常用到的开源 API 网关系统有： Kong Netflix zuul 下图来源：https://www.stackshare.io/stackups/kong-vs-zuul 。 可以看出不论是社区活跃度还是 Star数， Kong 都是略胜一筹。总的来说，Kong 相比于 Zuul 更加强大并且简单易用。Kong 基于 Openresty ，Zuul 基于 Java。 OpenResty（也称为 ngx_openresty）是一个全功能的 Web 应用服务器。它打包了标准的 Nginx 核心，很多的常用的第三方模块，以及它们的大多数依赖项。 通过揉和众多设计良好的 Nginx 模块，OpenResty 有效地把 Nginx 服务器转变为一个强大的 Web 应用服务器，基于它开发人员可以使用 Lua 编程语言对 Nginx 核心以及现有的各种 Nginx C 模块进行脚本编程，构建出可以处理一万以上并发请求的极端高性能的 Web 应用。——OpenResty 另外， Kong 还提供了插件机制来扩展其功能。 比如、在服务上启用 Zipkin 插件 $ curl -X POST http://kong:8001/services/{service}/plugins \\ --data \"name=zipkin\" \\ --data \"config.http_endpoint=http://your.zipkin.collector:9411/api/v2/spans\" \\ --data \"config.sample_ratio=0.001\" ps:这里没有太深入去探讨，需要深入了解的话可以自行查阅相关资料。 "},"zother6-JavaGuide/system-design/micro-service/API网关.html":{"url":"zother6-JavaGuide/system-design/micro-service/API网关.html","title":"API网关","keywords":"","body":" 点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。 本文授权转载自：https://github.com/javagrowing/JGrowing/blob/master/服务端开发/浅析如何设计一个亿级网关.md。 1.背景 1.1 什么是API网关 API网关可以看做系统与外界联通的入口，我们可以在网关进行处理一些非业务逻辑的逻辑，比如权限验证，监控，缓存，请求路由等等。 1.2 为什么需要API网关 RPC协议转成HTTP。 由于在内部开发中我们都是以RPC协议(thrift or dubbo)去做开发，暴露给内部服务，当外部服务需要使用这个接口的时候往往需要将RPC协议转换成HTTP协议。 请求路由 在我们的系统中由于同一个接口新老两套系统都在使用，我们需要根据请求上下文将请求路由到对应的接口。 统一鉴权 对于鉴权操作不涉及到业务逻辑，那么可以在网关层进行处理，不用下层到业务逻辑。 统一监控 由于网关是外部服务的入口，所以我们可以在这里监控我们想要的数据，比如入参出参，链路时间。 流量控制，熔断降级 对于流量控制，熔断降级非业务逻辑可以统一放到网关层。 有很多业务都会自己去实现一层网关层，用来接入自己的服务，但是对于整个公司来说这还不够。 1.3 统一API网关 统一的API网关不仅有API网关的所有的特点，还有下面几个好处: 统一技术组件升级 在公司中如果有某个技术组件需要升级，那么是需要和每个业务线沟通，通常几个月都搞不定。举个例子如果对于入口的安全鉴权有重大安全隐患需要升级，如果速度还是这么慢肯定是不行，那么有了统一的网关升级是很快的。 统一服务接入 对于某个服务的接入也比较困难，比如公司已经研发出了比较稳定的服务组件，正在公司大力推广，这个周期肯定也特别漫长，由于有了统一网关，那么只需要统一网关统一接入。 节约资源 不同业务不同部门如果按照我们上面的做法应该会都自己搞一个网关层，用来做这个事，可以想象如果一个公司有100个这种业务，每个业务配备4台机器，那么就需要400台机器。并且每个业务的开发RD都需要去开发这个网关层，去随时去维护，增加人力。如果有了统一网关层，那么也许只需要50台机器就可以做这100个业务的网关层的事，并且业务RD不需要随时关注开发，上线的步骤。 2.统一网关的设计 2.1 异步化请求 对于我们自己实现的网关层，由于只有我们自己使用，对于吞吐量的要求并不高所以，我们一般同步请求调用即可。 对于我们统一的网关层，如何用少量的机器接入更多的服务，这就需要我们的异步，用来提高更多的吞吐量。对于异步化一般有下面两种策略： Tomcat/Jetty+NIO+servlet3 这种策略使用的比较普遍，京东，有赞，Zuul，都选取的是这个策略，这种策略比较适合HTTP。在Servlet3中可以开启异步。 Netty+NIO Netty为高并发而生，目前唯品会的网关使用这个策略，在唯品会的技术文章中在相同的情况下Netty是每秒30w+的吞吐量，Tomcat是13w+,可以看出是有一定的差距的，但是Netty需要自己处理HTTP协议，这一块比较麻烦。 对于网关是HTTP请求场景比较多的情况可以采用Servlet，毕竟有更加成熟的处理HTTP协议。如果更加重视吞吐量那么可以采用Netty。 2.1.1 全链路异步 对于来的请求我们已经使用异步了，为了达到全链路异步所以我们需要对去的请求也进行异步处理，对于去的请求我们可以利用我们rpc的异步支持进行异步请求所以基本可以达到下图: 由在web容器中开启servlet异步，然后进入到网关的业务线程池中进行业务处理，然后进行rpc的异步调用并注册需要回调的业务，最后在回调线程池中进行回调处理。 2.2 链式处理 在设计模式中有一个模式叫责任链模式，他的作用是避免请求发送者与接收者耦合在一起，让多个对象都有可能接收请求，将这些对象连接成一条链，并且沿着这条链传递请求，直到有对象处理它为止。通过这种模式将请求的发送者和请求的处理者解耦了。在我们的各个框架中对此模式都有实现，比如servlet里面的filter，springmvc里面的Interceptor。 在Netflix Zuul中也应用了这种模式，如下图所示: 这种模式在网关的设计中我们可以借鉴到自己的网关设计: preFilters：前置过滤器，用来处理一些公共的业务，比如统一鉴权，统一限流，熔断降级，缓存处理等，并且提供业务方扩展。 routingFilters: 用来处理一些泛化调用，主要是做协议的转换，请求的路由工作。 postFilters: 后置过滤器，主要用来做结果的处理，日志打点，记录时间等等。 errorFilters: 错误过滤器，用来处理调用异常的情况。 这种设计在有赞的网关也有应用。 2.3 业务隔离 上面在全链路异步的情况下不同业务之间的影响很小，但是如果在提供的自定义FiIlter中进行了某些同步调用，一旦超时频繁那么就会对其他业务产生影响。所以我们需要采用隔离之术，降低业务之间的互相影响。 2.3.1 信号量隔离 信号量隔离只是限制了总的并发数，服务还是主线程进行同步调用。这个隔离如果远程调用超时依然会影响主线程，从而会影响其他业务。因此，如果只是想限制某个服务的总并发调用量或者调用的服务不涉及远程调用的话，可以使用轻量级的信号量来实现。有赞的网关由于没有自定义filter所以选取的是信号量隔离。 2.3.2 线程池隔离 最简单的就是不同业务之间通过不同的线程池进行隔离，就算业务接口出现了问题由于线程池已经进行了隔离那么也不会影响其他业务。在京东的网关实现之中就是采用的线程池隔离，比较重要的业务比如商品或者订单 都是单独的通过线程池去处理。但是由于是统一网关平台，如果业务线众多，大家都觉得自己的业务比较重要需要单独的线程池隔离，如果使用的是Java语言开发的话那么，在Java中线程是比较重的资源比较受限，如果需要隔离的线程池过多不是很适用。如果使用一些其他语言比如Golang进行开发网关的话，线程是比较轻的资源，所以比较适合使用线程池隔离。 2.3.3 集群隔离 如果有某些业务就需要使用隔离但是统一网关又没有线程池隔离那么应该怎么办呢？那么可以使用集群隔离，如果你的某些业务真的很重要那么可以为这一系列业务单独申请一个集群或者多个集群，通过机器之间进行隔离。 2.4 请求限流 流量控制可以采用很多开源的实现，比如阿里最近开源的Sentinel和比较成熟的Hystrix。 一般限流分为集群限流和单机限流: 利用统一存储保存当前流量的情况，一般可以采用Redis，这个一般会有一些性能损耗。 单机限流:限流每台机器我们可以直接利用Guava的令牌桶去做，由于没有远程调用性能消耗较小。 2.5 熔断降级 这一块也可以参照开源的实现Sentinel和Hystrix，这里不是重点就不多提了。 2.6 泛化调用 泛化调用指的是一些通信协议的转换，比如将HTTP转换成Thrift。在一些开源的网关中比如Zuul是没有实现的，因为各个公司的内部服务通信协议都不同。比如在唯品会中支持HTTP1,HTTP2,以及二进制的协议，然后转化成内部的协议，淘宝的支持HTTPS,HTTP1,HTTP2这些协议都可以转换成，HTTP,HSF,Dubbo等协议。 2.6.1泛化调用 如何去实现泛化调用呢？由于协议很难自动转换，那么其实每个协议对应的接口需要提供一种映射。简单来说就是把两个协议都能转换成共同语言，从而互相转换。 一般来说共同语言有三种方式指定: json：json数据格式比较简单,解析速度快，较轻量级。在Dubbo的生态中有一个HTTP转Dubbo的项目是用JsonRpc做的，将HTTP转化成JsonRpc再转化成Dubbo。 比如可以将一个 www.baidu.com/id = 1 GET 可以映射为json： 代码块 { “method”: \"getBaidu\" \"param\" : { \"id\" : 1 } } xml:xml数据比较重，解析比较困难，这里不过多讨论。 自定义描述语言:一般来说这个成本比较高需要自己定义语言来进行描述并进行解析，但是其扩展性，自定义个性化性都是最高。例:spring自定义了一套自己的SPEL表达式语言 对于泛化调用如果要自己设计的话JSON基本可以满足，如果对于个性化的需要特别多的话倒是可以自己定义一套语言。 2.7 管理平台 上面介绍的都是如何实现一个网关的技术关键。这里需要介绍网关的一个业务关键。有了网关之后，需要一个管理平台如何去对我们上面所描述的技术关键进行配置,包括但不限于下面这些配置: 限流 熔断 缓存 日志 自定义filter 泛化调用 3.总结 最后一个合理的标准网关应该按照如下去实现: --- 京东 唯品会 有赞 阿里 Zuul 实现关键 servlet3.0 netty servlet3.0 servlet3.0 servlet3.0 异步情况 servlet异步，rpc是否异步不清楚 全链路异步 全链路异步 全链路异步 Zuul1同步阻塞,Zuul2异步非阻塞 限流 --- --- 平滑限流。最初是codis，后续换到每个单机的令牌桶限流。 1.基本流控:基于API的QPS做限流。2.运营流控:支持APP流量包，APP+API+USER的流控33.大促流控:APP访问API的权重流控。阿里开源:Sentinel 提供了jar包:spring-cloud-zuul-ratelimit。1.对请求的目标URL进行限流（例如：某个URL每分钟只允许调用多少次)。2.对客户端的访问IP进行限流（例如：某个IP每分钟只允许请求多少次）3.对某些特定用户或者用户组进行限流（例如：非VIP用户限制每分钟只允许调用100次某个API等）4.多维度混合的限流。此时，就需要实现一些限流规则的编排机制。与、或、非等关系。支持四种存储方式ConcurrentHashMap,Consul,Redis,数据库。 熔断降级 --- --- Hystrix --- 只支持服务级别熔断，不支持URL级别。 隔离 线程池隔离 --- 信号量隔离 --- 线程池隔离，信号量隔离 缓存 redis --- 二级缓存，本地缓存+Codis HDCC 本地缓存，远程缓存，数据库 需要自己开发 泛化调用 --- http,https,http1,http2,二进制 dubbo,http,nova hsf,dubbo,http,https,http2,http1 只支持http 4.参考 京东:http://www.yunweipai.com/archives/23653.html 有赞网关:https://tech.youzan.com/api-gateway-in-practice/ 唯品会:https://mp.weixin.qq.com/s/gREMe-G7nqNJJLzbZ3ed3A Zuul:http://www.scienjus.com/api-gateway-and-netflix-zuul/ 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \"Java面试突击\" 即可免费领取！ Java工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/system-design/micro-service/limit-request.html":{"url":"zother6-JavaGuide/system-design/micro-service/limit-request.html","title":"Limit Request","keywords":"","body":"限流的算法有哪些？ 简单介绍 4 种非常好理解并且容易实现的限流算法！ 下图的图片不是 Guide 哥自己画的哦！图片来源于 InfoQ 的一篇文章《分布式服务限流实战，已经为你排好坑了》。 固定窗口计数器算法 规定我们单位时间处理的请求数量。比如我们规定我们的一个接口一分钟只能访问10次的话。使用固定窗口计数器算法的话可以这样实现：给定一个变量counter来记录处理的请求数量，当1分钟之内处理一个请求之后counter+1，1分钟之内的如果counter=100的话，后续的请求就会被全部拒绝。等到 1分钟结束后，将counter回归成0，重新开始计数（ps：只要过了一个周期就讲counter回归成0）。 这种限流算法无法保证限流速率，因而无法保证突然激增的流量。比如我们限制一个接口一分钟只能访问10次的话，前半分钟一个请求没有接收，后半分钟接收了10个请求。 滑动窗口计数器算法 算的上是固定窗口计数器算法的升级版。滑动窗口计数器算法相比于固定窗口计数器算法的优化在于：它把时间以一定比例分片。例如我们的借口限流每分钟处理60个请求，我们可以把 1 分钟分为60个窗口。每隔1秒移动一次，每个窗口一秒只能处理 不大于 60(请求数)/60（窗口数） 的请求， 如果当前窗口的请求计数总和超过了限制的数量的话就不再处理其他请求。 很显然：当滑动窗口的格子划分的越多，滑动窗口的滚动就越平滑，限流的统计就会越精确。 漏桶算法 我们可以把发请求的动作比作成注水到桶中，我们处理请求的过程可以比喻为漏桶漏水。我们往桶中以任意速率流入水，以一定速率流出水。当水超过桶流量则丢弃，因为桶容量是不变的，保证了整体的速率。如果想要实现这个算法的话也很简单，准备一个队列用来保存请求，然后我们定期从队列中拿请求来执行就好了。 令牌桶算法 令牌桶算法也比较简单。和漏桶算法算法一样，我们的主角还是桶（这限流算法和桶过不去啊）。不过现在桶里装的是令牌了，请求在被处理之前需要拿到一个令牌，请求处理完毕之后将这个令牌丢弃（删除）。我们根据限流大小，按照一定的速率往桶里添加令牌。 "},"zother6-JavaGuide/system-design/micro-service/spring-cloud.html":{"url":"zother6-JavaGuide/system-design/micro-service/spring-cloud.html","title":"Spring Cloud","keywords":"","body":" 本文基于 Spring Cloud Netflix 。Spring Cloud Alibaba 也是非常不错的选择哦！ 授权转载自：https://juejin.im/post/5de2553e5188256e885f4fa3 首先我给大家看一张图，如果大家对这张图有些地方不太理解的话，我希望你们看完我这篇文章会恍然大悟。 什么是Spring cloud 构建分布式系统不需要复杂和容易出错。Spring Cloud 为最常见的分布式系统模式提供了一种简单且易于接受的编程模型，帮助开发人员构建有弹性的、可靠的、协调的应用程序。Spring Cloud 构建于 Spring Boot 之上，使得开发者很容易入手并快速应用于生产中。 官方果然官方，介绍都这么有板有眼的。 我所理解的 Spring Cloud 就是微服务系统架构的一站式解决方案，在平时我们构建微服务的过程中需要做如 服务发现注册 、配置中心 、消息总线 、负载均衡 、断路器 、数据监控 等操作，而 Spring Cloud 为我们提供了一套简易的编程模型，使我们能在 Spring Boot 的基础上轻松地实现微服务项目的构建。 Spring Cloud 的版本 当然这个只是个题外话。 Spring Cloud 的版本号并不是我们通常见的数字版本号，而是一些很奇怪的单词。这些单词均为英国伦敦地铁站的站名。同时根据字母表的顺序来对应版本时间顺序，比如：最早 的 Release 版本 Angel，第二个 Release 版本 Brixton（英国地名），然后是 Camden、 Dalston、Edgware、Finchley、Greenwich、Hoxton。 Spring Cloud 的服务发现框架——Eureka Eureka是基于REST（代表性状态转移）的服务，主要在 AWS 云中用于定位服务，以实现负载均衡和中间层服务器的故障转移。我们称此服务为Eureka服务器。Eureka还带有一个基于 Java 的客户端组件 Eureka Client，它使与服务的交互变得更加容易。客户端还具有一个内置的负载平衡器，可以执行基本的循环负载平衡。在 Netflix，更复杂的负载均衡器将 Eureka 包装起来，以基于流量，资源使用，错误条件等多种因素提供加权负载均衡，以提供出色的弹性。 总的来说，Eureka 就是一个服务发现框架。何为服务，何又为发现呢？ 举一个生活中的例子，就比如我们平时租房子找中介的事情。 在没有中介的时候我们需要一个一个去寻找是否有房屋要出租的房东，这显然会非常的费力，一你找凭一个人的能力是找不到很多房源供你选择，再者你也懒得这么找下去(找了这么久，没有合适的只能将就)。这里的我们就相当于微服务中的 Consumer ，而那些房东就相当于微服务中的 Provider 。消费者 Consumer 需要调用提供者 Provider 提供的一些服务，就像我们现在需要租他们的房子一样。 但是如果只是租客和房东之间进行寻找的话，他们的效率是很低的，房东找不到租客赚不到钱，租客找不到房东住不了房。所以，后来房东肯定就想到了广播自己的房源信息(比如在街边贴贴小广告)，这样对于房东来说已经完成他的任务(将房源公布出去)，但是有两个问题就出现了。第一、其他不是租客的都能收到这种租房消息，这在现实世界没什么，但是在计算机的世界中就会出现 资源消耗 的问题了。第二、租客这样还是很难找到你，试想一下我需要租房，我还需要东一个西一个地去找街边小广告，麻不麻烦？ 那怎么办呢？我们当然不会那么傻乎乎的，第一时间就是去找 中介 呀，它为我们提供了统一房源的地方，我们消费者只需要跑到它那里去找就行了。而对于房东来说，他们也只需要把房源在中介那里发布就行了。 那么现在，我们的模式就是这样的了。 但是，这个时候还会出现一些问题。 房东注册之后如果不想卖房子了怎么办？我们是不是需要让房东 定期续约 ？如果房东不进行续约是不是要将他们从中介那里的注册列表中 移除 。 租客是不是也要进行 注册 呢？不然合同乙方怎么来呢？ 中介可不可以做 连锁店 呢？如果这一个店因为某些不可抗力因素而无法使用，那么我们是否可以换一个连锁店呢？ 针对上面的问题我们来重新构建一下上面的模式图 好了，举完这个:chestnut:我们就可以来看关于 Eureka 的一些基础概念了，你会发现这东西理解起来怎么这么简单。:punch::punch::punch: 服务发现：其实就是一个“中介”，整个过程中有三个角色：服务提供者(出租房子的)、服务消费者(租客)、服务中介(房屋中介)。 服务提供者： 就是提供一些自己能够执行的一些服务给外界。 服务消费者： 就是需要使用一些服务的“用户”。 服务中介： 其实就是服务提供者和服务消费者之间的“桥梁”，服务提供者可以把自己注册到服务中介那里，而服务消费者如需要消费一些服务(使用一些功能)就可以在服务中介中寻找注册在服务中介的服务提供者。 服务注册 Register： 官方解释：当 Eureka 客户端向 Eureka Server 注册时，它提供自身的元数据，比如IP地址、端口，运行状况指示符URL，主页等。 结合中介理解：房东 (提供者 Eureka Client Provider)在中介 (服务器 Eureka Server) 那里登记房屋的信息，比如面积，价格，地段等等(元数据 metaData)。 服务续约 Renew： 官方解释：Eureka 客户会每隔30秒(默认情况下)发送一次心跳来续约。 通过续约来告知 Eureka Server 该 Eureka 客户仍然存在，没有出现问题。 正常情况下，如果 Eureka Server 在90秒没有收到 Eureka 客户的续约，它会将实例从其注册表中删除。 结合中介理解：房东 (提供者 Eureka Client Provider) 定期告诉中介 (服务器 Eureka Server) 我的房子还租(续约) ，中介 (服务器Eureka Server) 收到之后继续保留房屋的信息。 获取注册列表信息 Fetch Registries： 官方解释：Eureka 客户端从服务器获取注册表信息，并将其缓存在本地。客户端会使用该信息查找其他服务，从而进行远程调用。该注册列表信息定期（每30秒钟）更新一次。每次返回注册列表信息可能与 Eureka 客户端的缓存信息不同, Eureka 客户端自动处理。如果由于某种原因导致注册列表信息不能及时匹配，Eureka 客户端则会重新获取整个注册表信息。 Eureka 服务器缓存注册列表信息，整个注册表以及每个应用程序的信息进行了压缩，压缩内容和没有压缩的内容完全相同。Eureka 客户端和 Eureka 服务器可以使用JSON / XML格式进行通讯。在默认的情况下 Eureka 客户端使用压缩 JSON 格式来获取注册列表的信息。 结合中介理解：租客(消费者 Eureka Client Consumer) 去中介 (服务器 Eureka Server) 那里获取所有的房屋信息列表 (客户端列表 Eureka Client List) ，而且租客为了获取最新的信息会定期向中介 (服务器 Eureka Server) 那里获取并更新本地列表。 服务下线 Cancel： 官方解释：Eureka客户端在程序关闭时向Eureka服务器发送取消请求。 发送请求后，该客户端实例信息将从服务器的实例注册表中删除。该下线请求不会自动完成，它需要调用以下内容：DiscoveryManager.getInstance().shutdownComponent(); 结合中介理解：房东 (提供者 Eureka Client Provider) 告诉中介 (服务器 Eureka Server) 我的房子不租了，中介之后就将注册的房屋信息从列表中剔除。 服务剔除 Eviction： 官方解释：在默认的情况下，当Eureka客户端连续90秒(3个续约周期)没有向Eureka服务器发送服务续约，即心跳，Eureka服务器会将该服务实例从服务注册列表删除，即服务剔除。 结合中介理解：房东(提供者 Eureka Client Provider) 会定期联系 中介 (服务器 Eureka Server) 告诉他我的房子还租(续约)，如果中介 (服务器 Eureka Server) 长时间没收到提供者的信息，那么中介会将他的房屋信息给下架(服务剔除)。 下面就是 Netflix 官方给出的 Eureka 架构图，你会发现和我们前面画的中介图别无二致。 当然，可以充当服务发现的组件有很多：Zookeeper ，Consul ， Eureka 等。 更多关于 Eureka 的知识(自我保护，初始注册策略等等)可以自己去官网查看，或者查看我的另一篇文章 深入理解 Eureka。 负载均衡之 Ribbon 什么是 RestTemplate? 不是讲 Ribbon 么？怎么扯到了 RestTemplate 了？你先别急，听我慢慢道来。 我不听我不听我不听:hear_no_evil::hear_no_evil::hear_no_evil:。 我就说一句！RestTemplate是Spring提供的一个访问Http服务的客户端类，怎么说呢？就是微服务之间的调用是使用的 RestTemplate 。比如这个时候我们 消费者B 需要调用 提供者A 所提供的服务我们就需要这么写。如我下面的伪代码。 @Autowired private RestTemplate restTemplate; // 这里是提供者A的ip地址，但是如果使用了 Eureka 那么就应该是提供者A的名称 private static final String SERVICE_PROVIDER_A = \"http://localhost:8081\"; @PostMapping(\"/judge\") public boolean judge(@RequestBody Request request) { String url = SERVICE_PROVIDER_A + \"/service1\"; return restTemplate.postForObject(url, request, Boolean.class); } 如果你对源码感兴趣的话，你会发现上面我们所讲的 Eureka 框架中的 注册、续约 等，底层都是使用的 RestTemplate 。 为什么需要 Ribbon？ Ribbon 是 Netflix 公司的一个开源的负载均衡 项目，是一个客户端/进程内负载均衡器，运行在消费者端。 我们再举个:chestnut:，比如我们设计了一个秒杀系统，但是为了整个系统的 高可用 ，我们需要将这个系统做一个集群，而这个时候我们消费者就可以拥有多个秒杀系统的调用途径了，如下图。 如果这个时候我们没有进行一些 均衡操作 ，如果我们对 秒杀系统1 进行大量的调用，而另外两个基本不请求，就会导致 秒杀系统1 崩溃，而另外两个就变成了傀儡，那么我们为什么还要做集群，我们高可用体现的意义又在哪呢？ 所以 Ribbon 出现了，注意我们上面加粗的几个字——运行在消费者端。指的是，Ribbon 是运行在消费者端的负载均衡器，如下图。 其工作原理就是 Consumer 端获取到了所有的服务列表之后，在其内部使用负载均衡算法，进行对多个系统的调用。 Nginx 和 Ribbon 的对比 提到 负载均衡 就不得不提到大名鼎鼎的 Nignx 了，而和 Ribbon 不同的是，它是一种集中式的负载均衡器。 何为集中式呢？简单理解就是 将所有请求都集中起来，然后再进行负载均衡。如下图。 我们可以看到 Nginx 是接收了所有的请求进行负载均衡的，而对于 Ribbon 来说它是在消费者端进行的负载均衡。如下图。 请注意 Request 的位置，在 Nginx 中请求是先进入负载均衡器，而在 Ribbon 中是先在客户端进行负载均衡才进行请求的。 Ribbon 的几种负载均衡算法 负载均衡，不管 Nginx 还是 Ribbon 都需要其算法的支持，如果我没记错的话 Nginx 使用的是 轮询和加权轮询算法。而在 Ribbon 中有更多的负载均衡调度算法，其默认是使用的 RoundRobinRule 轮询策略。 RoundRobinRule：轮询策略。Ribbon 默认采用的策略。若经过一轮轮询没有找到可用的 provider，其最多轮询 10 轮。若最终还没有找到，则返回 null。 RandomRule: 随机策略，从所有可用的 provider 中随机选择一个。 RetryRule: 重试策略。先按照 RoundRobinRule 策略获取 provider，若获取失败，则在指定的时限内重试。默认的时限为 500 毫秒。 🐦🐦🐦 还有很多，这里不一一举:chestnut:了，你最需要知道的是默认轮询算法，并且可以更换默认的负载均衡算法，只需要在配置文件中做出修改就行。 providerName: ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule 当然，在 Ribbon 中你还可以自定义负载均衡算法，你只需要实现 IRule 接口，然后修改配置文件或者自定义 Java Config 类。 什么是 Open Feign 有了 Eureka ，RestTemplate ，Ribbon， 我们就可以愉快地进行服务间的调用了，但是使用 RestTemplate 还是不方便，我们每次都要进行这样的调用。 @Autowired private RestTemplate restTemplate; // 这里是提供者A的ip地址，但是如果使用了 Eureka 那么就应该是提供者A的名称 private static final String SERVICE_PROVIDER_A = \"http://localhost:8081\"; @PostMapping(\"/judge\") public boolean judge(@RequestBody Request request) { String url = SERVICE_PROVIDER_A + \"/service1\"; // 是不是太麻烦了？？？每次都要 url、请求、返回类型的 return restTemplate.postForObject(url, request, Boolean.class); } 这样每次都调用 RestRemplate 的 API 是否太麻烦，我能不能像调用原来代码一样进行各个服务间的调用呢？ :bulb::bulb::bulb:聪明的小朋友肯定想到了，那就用 映射 呀，就像域名和IP地址的映射。我们可以将被调用的服务代码映射到消费者端，这样我们就可以 “无缝开发” 啦。 OpenFeign 也是运行在消费者端的，使用 Ribbon 进行负载均衡，所以 OpenFeign 直接内置了 Ribbon。 在导入了 Open Feign 之后我们就可以进行愉快编写 Consumer 端代码了。 // 使用 @FeignClient 注解来指定提供者的名字 @FeignClient(value = \"eureka-client-provider\") public interface TestClient { // 这里一定要注意需要使用的是提供者那端的请求相对路径，这里就相当于映射了 @RequestMapping(value = \"/provider/xxx\", method = RequestMethod.POST) CommonResponse> getPlans(@RequestBody planGetRequest request); } 然后我们在 Controller 就可以像原来调用 Service 层代码一样调用它了。 @RestController public class TestController { // 这里就相当于原来自动注入的 Service @Autowired private TestClient testClient; // controller 调用 service 层代码 @RequestMapping(value = \"/test\", method = RequestMethod.POST) public CommonResponse> get(@RequestBody planGetRequest request) { return testClient.getPlans(request); } } 必不可少的 Hystrix 什么是 Hystrix之熔断和降级 在分布式环境中，不可避免地会有许多服务依赖项中的某些失败。Hystrix是一个库，可通过添加等待时间容限和容错逻辑来帮助您控制这些分布式服务之间的交互。Hystrix通过隔离服务之间的访问点，停止服务之间的级联故障并提供后备选项来实现此目的，所有这些都可以提高系统的整体弹性。 总体来说 Hystrix 就是一个能进行 熔断 和 降级 的库，通过使用它能提高整个系统的弹性。 那么什么是 熔断和降级 呢？再举个:chestnut:，此时我们整个微服务系统是这样的。服务A调用了服务B，服务B再调用了服务C，但是因为某些原因，服务C顶不住了，这个时候大量请求会在服务C阻塞。 服务C阻塞了还好，毕竟只是一个系统崩溃了。但是请注意这个时候因为服务C不能返回响应，那么服务B调用服务C的的请求就会阻塞，同理服务B阻塞了，那么服务A也会阻塞崩溃。 请注意，为什么阻塞会崩溃。因为这些请求会消耗占用系统的线程、IO 等资源，消耗完你这个系统服务器不就崩了么。 这就叫 服务雪崩。妈耶，上面两个 熔断 和 降级 你都没给我解释清楚，你现在又给我扯什么 服务雪崩 ？:tired_face::tired_face::tired_face: 别急，听我慢慢道来。 不听我也得讲下去！ 所谓 熔断 就是服务雪崩的一种有效解决方案。当指定时间窗内的请求失败率达到设定阈值时，系统将通过 断路器 直接将此请求链路断开。 也就是我们上面服务B调用服务C在指定时间窗内，调用的失败率到达了一定的值，那么 Hystrix 则会自动将 服务B与C 之间的请求都断了，以免导致服务雪崩现象。 其实这里所讲的 熔断 就是指的 Hystrix 中的 断路器模式 ，你可以使用简单的 @HystrixCommand 注解来标注某个方法，这样 Hystrix 就会使用 断路器 来“包装”这个方法，每当调用时间超过指定时间时(默认为1000ms)，断路器将会中断对这个方法的调用。 当然你可以对这个注解的很多属性进行设置，比如设置超时时间，像这样。 @HystrixCommand( commandProperties = {@HystrixProperty(name = \"execution.isolation.thread.timeoutInMilliseconds\",value = \"1200\")} ) public List getXxxx() { // ...省略代码逻辑 } 但是，我查阅了一些博客，发现他们都将 熔断 和 降级 的概念混淆了，以我的理解，降级是为了更好的用户体验，当一个方法调用异常时，通过执行另一种代码逻辑来给用户友好的回复。这也就对应着 Hystrix 的 后备处理 模式。你可以通过设置 fallbackMethod 来给一个方法设置备用的代码逻辑。比如这个时候有一个热点新闻出现了，我们会推荐给用户查看详情，然后用户会通过id去查询新闻的详情，但是因为这条新闻太火了(比如最近什么易对吧)，大量用户同时访问可能会导致系统崩溃，那么我们就进行 *服务降级 ，一些请求会做一些降级处理比如当前人数太多请稍后查看等等。 // 指定了后备方法调用 @HystrixCommand(fallbackMethod = \"getHystrixNews\") @GetMapping(\"/get/news\") public News getNews(@PathVariable(\"id\") int id) { // 调用新闻系统的获取新闻api 代码逻辑省略 } // public News getHystrixNews(@PathVariable(\"id\") int id) { // 做服务降级 // 返回当前人数太多，请稍后查看 } 什么是Hystrix之其他 我在阅读 《Spring微服务实战》这本书的时候还接触到了一个 舱壁模式 的概念。在不使用舱壁模式的情况下，服务A调用服务B，这种调用默认的是 使用同一批线程来执行 的，而在一个服务出现性能问题的时候，就会出现所有线程被刷爆并等待处理工作，同时阻塞新请求，最终导致程序崩溃。而舱壁模式会将远程资源调用隔离在他们自己的线程池中，以便可以控制单个表现不佳的服务，而不会使该程序崩溃。 具体其原理我推荐大家自己去了解一下，本篇文章中对 舱壁模式 不做过多解释。当然还有 Hystrix 仪表盘，它是用来实时监控 Hystrix 的各项指标信息的，这里我将这个问题也抛出去，希望有不了解的可以自己去搜索一下。 微服务网关——Zuul ZUUL 是从设备和 web 站点到 Netflix 流应用后端的所有请求的前门。作为边界服务应用，ZUUL 是为了实现动态路由、监视、弹性和安全性而构建的。它还具有根据情况将请求路由到多个 Amazon Auto Scaling Groups（亚马逊自动缩放组，亚马逊的一种云计算方式） 的能力 在上面我们学习了 Eureka 之后我们知道了 服务提供者 是 消费者 通过 Eureka Server 进行访问的，即 Eureka Server 是 服务提供者 的统一入口。那么整个应用中存在那么多 消费者 需要用户进行调用，这个时候用户该怎样访问这些 消费者工程 呢？当然可以像之前那样直接访问这些工程。但这种方式没有统一的消费者工程调用入口，不便于访问与管理，而 Zuul 就是这样的一个对于 消费者 的统一入口。 如果学过前端的肯定都知道 Router 吧，比如 Flutter 中的路由，Vue，React中的路由，用了 Zuul 你会发现在路由功能方面和前端配置路由基本是一个理。:smile: 我偶尔撸撸 Flutter。 大家对网关应该很熟吧，简单来讲网关是系统唯一对外的入口，介于客户端与服务器端之间，用于对请求进行鉴权、限流、 路由、监控等功能。 没错，网关有的功能，Zuul 基本都有。而 Zuul 中最关键的就是 路由和过滤器 了，在官方文档中 Zuul 的标题就是 Router and Filter : Zuul Zuul 的路由功能 简单配置 本来想给你们复制一些代码，但是想了想，因为各个代码配置比较零散，看起来也比较零散，我决定还是给你们画个图来解释吧。 请不要因为我这么好就给我点赞 :thumbsup: 。 疯狂暗示。 比如这个时候我们已经向 Eureka Server 注册了两个 Consumer 、三个 Provicer ，这个时候我们再加个 Zuul 网关应该变成这样子了。 emmm，信息量有点大，我来解释一下。关于前面的知识我就不解释了:neutral_face:。 首先，Zuul 需要向 Eureka 进行注册，注册有啥好处呢？ 你傻呀，Consumer 都向 Eureka Server 进行注册了，我网关是不是只要注册就能拿到所有 Consumer 的信息了？ 拿到信息有什么好处呢？ 我拿到信息我是不是可以获取所有的 Consumer 的元数据(名称，ip，端口)？ 拿到这些元数据有什么好处呢？拿到了我们是不是直接可以做路由映射？比如原来用户调用 Consumer1 的接口 localhost:8001/studentInfo/update 这个请求，我们是不是可以这样进行调用了呢？localhost:9000/consumer1/studentInfo/update 呢？你这样是不是恍然大悟了？ 这里的url为了让更多人看懂所以没有使用 restful 风格。 上面的你理解了，那么就能理解关于 Zuul 最基本的配置了，看下面。 server: port: 9000 eureka: client: service-url: # 这里只要注册 Eureka 就行了 defaultZone: http://localhost:9997/eureka 然后在启动类上加入 @EnableZuulProxy 注解就行了。没错，就是那么简单:smiley:。 统一前缀 这个很简单，就是我们可以在前面加一个统一的前缀，比如我们刚刚调用的是 localhost:9000/consumer1/studentInfo/update，这个时候我们在 yaml 配置文件中添加如下。 zuul: prefix: /zuul 这样我们就需要通过 localhost:9000/zuul/consumer1/studentInfo/update 来进行访问了。 路由策略配置 你会发现前面的访问方式(直接使用服务名)，需要将微服务名称暴露给用户，会存在安全性问题。所以，可以自定义路径来替代微服务名称，即自定义路由策略。 zuul: routes: consumer1: /FrancisQ1/** consumer2: /FrancisQ2/** 这个时候你就可以使用 `localhost:9000/zuul/FrancisQ1/studentInfo/update 进行访问了。 服务名屏蔽 这个时候你别以为你好了，你可以试试，在你配置完路由策略之后使用微服务名称还是可以访问的，这个时候你需要将服务名屏蔽。 zuul: ignore-services: \"*\" 路径屏蔽 Zuul 还可以指定屏蔽掉的路径 URI，即只要用户请求中包含指定的 URI 路径，那么该请求将无法访问到指定的服务。通过该方式可以限制用户的权限。 zuul: ignore-patterns: **/auto/** 这样关于 auto 的请求我们就可以过滤掉了。 ** 代表匹配多级任意路径 *代表匹配一级任意路径 敏感请求头屏蔽 默认情况下，像 Cookie、Set-Cookie 等敏感请求头信息会被 zuul 屏蔽掉，我们可以将这些默认屏蔽去掉，当然，也可以添加要屏蔽的请求头。 Zuul 的过滤功能 如果说，路由功能是 Zuul 的基操的话，那么过滤器就是 Zuul的利器了。毕竟所有请求都经过网关(Zuul)，那么我们可以进行各种过滤，这样我们就能实现 限流，灰度发布，权限控制 等等。 简单实现一个请求时间日志打印 要实现自己定义的 Filter 我们只需要继承 ZuulFilter 然后将这个过滤器类以 @Component 注解加入 Spring 容器中就行了。 在给你们看代码之前我先给你们解释一下关于过滤器的一些注意点。 过滤器类型：Pre、Routing、Post。前置Pre就是在请求之前进行过滤，Routing路由过滤器就是我们上面所讲的路由策略，而Post后置过滤器就是在 Response 之前进行过滤的过滤器。你可以观察上图结合着理解，并且下面我会给出相应的注释。 // 加入Spring容器 @Component public class PreRequestFilter extends ZuulFilter { // 返回过滤器类型 这里是前置过滤器 @Override public String filterType() { return FilterConstants.PRE_TYPE; } // 指定过滤顺序 越小越先执行，这里第一个执行 // 当然不是只真正第一个 在Zuul内置中有其他过滤器会先执行 // 那是写死的 比如 SERVLET_DETECTION_FILTER_ORDER = -3 @Override public int filterOrder() { return 0; } // 什么时候该进行过滤 // 这里我们可以进行一些判断，这样我们就可以过滤掉一些不符合规定的请求等等 @Override public boolean shouldFilter() { return true; } // 如果过滤器允许通过则怎么进行处理 @Override public Object run() throws ZuulException { // 这里我设置了全局的RequestContext并记录了请求开始时间 RequestContext ctx = RequestContext.getCurrentContext(); ctx.set(\"startTime\", System.currentTimeMillis()); return null; } } // lombok的日志 @Slf4j // 加入 Spring 容器 @Component public class AccessLogFilter extends ZuulFilter { // 指定该过滤器的过滤类型 // 此时是后置过滤器 @Override public String filterType() { return FilterConstants.POST_TYPE; } // SEND_RESPONSE_FILTER_ORDER 是最后一个过滤器 // 我们此过滤器在它之前执行 @Override public int filterOrder() { return FilterConstants.SEND_RESPONSE_FILTER_ORDER - 1; } @Override public boolean shouldFilter() { return true; } // 过滤时执行的策略 @Override public Object run() throws ZuulException { RequestContext context = RequestContext.getCurrentContext(); HttpServletRequest request = context.getRequest(); // 从RequestContext获取原先的开始时间 并通过它计算整个时间间隔 Long startTime = (Long) context.get(\"startTime\"); // 这里我可以获取HttpServletRequest来获取URI并且打印出来 String uri = request.getRequestURI(); long duration = System.currentTimeMillis() - startTime; log.info(\"uri: \" + uri + \", duration: \" + duration / 100 + \"ms\"); return null; } } 上面就简单实现了请求时间日志打印功能，你有没有感受到 Zuul 过滤功能的强大了呢？ 没有？好的、那我们再来。 令牌桶限流 当然不仅仅是令牌桶限流方式，Zuul 只要是限流的活它都能干，这里我只是简单举个:chestnut:。 我先来解释一下什么是 令牌桶限流 吧。 首先我们会有个桶，如果里面没有满那么就会以一定 固定的速率 会往里面放令牌，一个请求过来首先要从桶中获取令牌，如果没有获取到，那么这个请求就拒绝，如果获取到那么就放行。很简单吧，啊哈哈、 下面我们就通过 Zuul 的前置过滤器来实现一下令牌桶限流。 package com.lgq.zuul.filter; import com.google.common.util.concurrent.RateLimiter; import com.netflix.zuul.ZuulFilter; import com.netflix.zuul.context.RequestContext; import com.netflix.zuul.exception.ZuulException; import lombok.extern.slf4j.Slf4j; import org.springframework.cloud.netflix.zuul.filters.support.FilterConstants; import org.springframework.stereotype.Component; @Component @Slf4j public class RouteFilter extends ZuulFilter { // 定义一个令牌桶，每秒产生2个令牌，即每秒最多处理2个请求 private static final RateLimiter RATE_LIMITER = RateLimiter.create(2); @Override public String filterType() { return FilterConstants.PRE_TYPE; } @Override public int filterOrder() { return -5; } @Override public Object run() throws ZuulException { log.info(\"放行\"); return null; } @Override public boolean shouldFilter() { RequestContext context = RequestContext.getCurrentContext(); if(!RATE_LIMITER.tryAcquire()) { log.warn(\"访问量超载\"); // 指定当前请求未通过过滤 context.setSendZuulResponse(false); // 向客户端返回响应码429，请求数量过多 context.setResponseStatusCode(429); return false; } return true; } } 这样我们就能将请求数量控制在一秒两个，有没有觉得很酷？ 关于 Zuul 的其他 Zuul 的过滤器的功能肯定不止上面我所实现的两种，它还可以实现 权限校验，包括我上面提到的 灰度发布 等等。 当然，Zuul 作为网关肯定也存在 单点问题 ，如果我们要保证 Zuul 的高可用，我们就需要进行 Zuul 的集群配置，这个时候可以借助额外的一些负载均衡器比如 Nginx 。 Spring Cloud配置管理——Config 为什么要使用进行配置管理？ 当我们的微服务系统开始慢慢地庞大起来，那么多 Consumer 、Provider 、Eureka Server 、Zuul 系统都会持有自己的配置，这个时候我们在项目运行的时候可能需要更改某些应用的配置，如果我们不进行配置的统一管理，我们只能去每个应用下一个一个寻找配置文件然后修改配置文件再重启应用。 首先对于分布式系统而言我们就不应该去每个应用下去分别修改配置文件，再者对于重启应用来说，服务无法访问所以直接抛弃了可用性，这是我们更不愿见到的。 那么有没有一种方法既能对配置文件统一地进行管理，又能在项目运行时动态修改配置文件呢？ 那就是我今天所要介绍的 Spring Cloud Config 。 能进行配置管理的框架不止 Spring Cloud Config 一种，大家可以根据需求自己选择（disconf，阿波罗等等）。而且对于 Config 来说有些地方实现的不是那么尽人意。 Config 是什么 Spring Cloud Config 为分布式系统中的外部化配置提供服务器和客户端支持。使用 Config 服务器，可以在中心位置管理所有环境中应用程序的外部属性。 简单来说，Spring Cloud Config 就是能将各个 应用/系统/模块 的配置文件存放到 统一的地方然后进行管理(Git 或者 SVN)。 你想一下，我们的应用是不是只有启动的时候才会进行配置文件的加载，那么我们的 Spring Cloud Config 就暴露出一个接口给启动应用来获取它所想要的配置文件，应用获取到配置文件然后再进行它的初始化工作。就如下图。 当然这里你肯定还会有一个疑问，如果我在应用运行时去更改远程配置仓库(Git)中的对应配置文件，那么依赖于这个配置文件的已启动的应用会不会进行其相应配置的更改呢？ 答案是不会的。 什么？那怎么进行动态修改配置文件呢？这不是出现了 配置漂移 吗？你个渣男:rage:，你又骗我！ 别急嘛，你可以使用 Webhooks ，这是 github 提供的功能，它能确保远程库的配置文件更新后客户端中的配置信息也得到更新。 噢噢，这还差不多。我去查查怎么用。 慢着，听我说完，Webhooks 虽然能解决，但是你了解一下会发现它根本不适合用于生产环境，所以基本不会使用它的。 而一般我们会使用 Bus 消息总线 + Spring Cloud Config 进行配置的动态刷新。 引出 Spring Cloud Bus 用于将服务和服务实例与分布式消息系统链接在一起的事件总线。在集群中传播状态更改很有用（例如配置更改事件）。 你可以简单理解为 Spring Cloud Bus 的作用就是管理和广播分布式系统中的消息，也就是消息引擎系统中的广播模式。当然作为 消息总线 的 Spring Cloud Bus 可以做很多事而不仅仅是客户端的配置刷新功能。 而拥有了 Spring Cloud Bus 之后，我们只需要创建一个简单的请求，并且加上 @ResfreshScope 注解就能进行配置的动态修改了，下面我画了张图供你理解。 总结 这篇文章中我带大家初步了解了 Spring Cloud 的各个组件，他们有 Eureka 服务发现框架 Ribbon 进程内负载均衡器 Open Feign 服务调用映射 Hystrix 服务降级熔断器 Zuul 微服务网关 Config 微服务统一配置中心 Bus 消息总线 如果你能这个时候能看懂文首那张图，也就说明了你已经对 Spring Cloud 微服务有了一定的架构认识。 "},"zother6-JavaGuide/system-design/micro-service/分布式id生成方案总结.html":{"url":"zother6-JavaGuide/system-design/micro-service/分布式id生成方案总结.html","title":"分布式id生成方案总结","keywords":"","body":" 点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。 本文授权转载自：https://juejin.im/post/5d6fc8eff265da03ef7a324b ，作者：1点25。 ID是数据的唯一标识，传统的做法是利用UUID和数据库的自增ID，在互联网企业中，大部分公司使用的都是Mysql，并且因为需要事务支持，所以通常会使用Innodb存储引擎，UUID太长以及无序，所以并不适合在Innodb中来作为主键，自增ID比较合适，但是随着公司的业务发展，数据量将越来越大，需要对数据进行分表，而分表后，每个表中的数据都会按自己的节奏进行自增，很有可能出现ID冲突。这时就需要一个单独的机制来负责生成唯一ID，生成出来的ID也可以叫做分布式ID，或全局ID。下面来分析各个生成分布式ID的机制。 这篇文章并不会分析的特别详细，主要是做一些总结，以后再出一些详细某个方案的文章。 数据库自增ID 第一种方案仍然还是基于数据库的自增ID，需要单独使用一个数据库实例，在这个实例中新建一个单独的表： 表结构如下： CREATE DATABASE `SEQID`; CREATE TABLE SEQID.SEQUENCE_ID ( id bigint(20) unsigned NOT NULL auto_increment, stub char(10) NOT NULL default '', PRIMARY KEY (id), UNIQUE KEY stub (stub) ) ENGINE=MyISAM; 可以使用下面的语句生成并获取到一个自增ID begin; replace into SEQUENCE_ID (stub) VALUES ('anyword'); select last_insert_id(); commit; stub字段在这里并没有什么特殊的意义，只是为了方便的去插入数据，只有能插入数据才能产生自增id。而对于插入我们用的是replace，replace会先看是否存在stub指定值一样的数据，如果存在则先delete再insert，如果不存在则直接insert。 这种生成分布式ID的机制，需要一个单独的Mysql实例，虽然可行，但是基于性能与可靠性来考虑的话都不够，业务系统每次需要一个ID时，都需要请求数据库获取，性能低，并且如果此数据库实例下线了，那么将影响所有的业务系统。 为了解决数据库可靠性问题，我们可以使用第二种分布式ID生成方案。 数据库多主模式 如果我们两个数据库组成一个主从模式集群，正常情况下可以解决数据库可靠性问题，但是如果主库挂掉后，数据没有及时同步到从库，这个时候会出现ID重复的现象。我们可以使用双主模式集群，也就是两个Mysql实例都能单独的生产自增ID，这样能够提高效率，但是如果不经过其他改造的话，这两个Mysql实例很可能会生成同样的ID。需要单独给每个Mysql实例配置不同的起始值和自增步长。 第一台Mysql实例配置： set @@auto_increment_offset = 1; -- 起始值 set @@auto_increment_increment = 2; -- 步长 第二台Mysql实例配置： set @@auto_increment_offset = 2; -- 起始值 set @@auto_increment_increment = 2; -- 步长 经过上面的配置后，这两个Mysql实例生成的id序列如下： mysql1,起始值为1,步长为2,ID生成的序列为：1,3,5,7,9,... mysql2,起始值为2,步长为2,ID生成的序列为：2,4,6,8,10,... 对于这种生成分布式ID的方案，需要单独新增一个生成分布式ID应用，比如DistributIdService，该应用提供一个接口供业务应用获取ID，业务应用需要一个ID时，通过rpc的方式请求DistributIdService，DistributIdService随机去上面的两个Mysql实例中去获取ID。 实行这种方案后，就算其中某一台Mysql实例下线了，也不会影响DistributIdService，DistributIdService仍然可以利用另外一台Mysql来生成ID。 但是这种方案的扩展性不太好，如果两台Mysql实例不够用，需要新增Mysql实例来提高性能时，这时就会比较麻烦。 现在如果要新增一个实例mysql3，要怎么操作呢？ 第一，mysql1、mysql2的步长肯定都要修改为3，而且只能是人工去修改，这是需要时间的。 第二，因为mysql1和mysql2是不停在自增的，对于mysql3的起始值我们可能要定得大一点，以给充分的时间去修改mysql1，mysql2的步长。 第三，在修改步长的时候很可能会出现重复ID，要解决这个问题，可能需要停机才行。 为了解决上面的问题，以及能够进一步提高DistributIdService的性能，如果使用第三种生成分布式ID机制。 号段模式 我们可以使用号段的方式来获取自增ID，号段可以理解成批量获取，比如DistributIdService从数据库获取ID时，如果能批量获取多个ID并缓存在本地的话，那样将大大提供业务应用获取ID的效率。 比如DistributIdService每次从数据库获取ID时，就获取一个号段，比如(1,1000]，这个范围表示了1000个ID，业务应用在请求DistributIdService提供ID时，DistributIdService只需要在本地从1开始自增并返回即可，而不需要每次都请求数据库，一直到本地自增到1000时，也就是当前号段已经被用完时，才去数据库重新获取下一号段。 所以，我们需要对数据库表进行改动，如下： CREATE TABLE id_generator ( id int(10) NOT NULL, current_max_id bigint(20) NOT NULL COMMENT '当前最大id', increment_step int(10) NOT NULL COMMENT '号段的长度', PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; 这个数据库表用来记录自增步长以及当前自增ID的最大值（也就是当前已经被申请的号段的最后一个值），因为自增逻辑被移到DistributIdService中去了，所以数据库不需要这部分逻辑了。 这种方案不再强依赖数据库，就算数据库不可用，那么DistributIdService也能继续支撑一段时间。但是如果DistributIdService重启，会丢失一段ID，导致ID空洞。 为了提高DistributIdService的高可用，需要做一个集群，业务在请求DistributIdService集群获取ID时，会随机的选择某一个DistributIdService节点进行获取，对每一个DistributIdService节点来说，数据库连接的是同一个数据库，那么可能会产生多个DistributIdService节点同时请求数据库获取号段，那么这个时候需要利用乐观锁来进行控制，比如在数据库表中增加一个version字段，在获取号段时使用如下SQL： update id_generator set current_max_id=#{newMaxId}, version=version+1 where version = #{version} 因为newMaxId是DistributIdService中根据oldMaxId+步长算出来的，只要上面的update更新成功了就表示号段获取成功了。 为了提供数据库层的高可用，需要对数据库使用多主模式进行部署，对于每个数据库来说要保证生成的号段不重复，这就需要利用最开始的思路，再在刚刚的数据库表中增加起始值和步长，比如如果现在是两台Mysql，那么 mysql1将生成号段（1,1001]，自增的时候序列为1，3，4，5，7.... mysql1将生成号段（2,1002]，自增的时候序列为2，4，6，8，10... 更详细的可以参考滴滴开源的TinyId：github.com/didi/tinyid… 在TinyId中还增加了一步来提高效率，在上面的实现中，ID自增的逻辑是在DistributIdService中实现的，而实际上可以把自增的逻辑转移到业务应用本地，这样对于业务应用来说只需要获取号段，每次自增时不再需要请求调用DistributIdService了。 雪花算法 上面的三种方法总的来说是基于自增思想的，而接下来就介绍比较著名的雪花算法-snowflake。 我们可以换个角度来对分布式ID进行思考，只要能让负责生成分布式ID的每台机器在每毫秒内生成不一样的ID就行了。 snowflake是twitter开源的分布式ID生成算法，是一种算法，所以它和上面的三种生成分布式ID机制不太一样，它不依赖数据库。 核心思想是：分布式ID固定是一个long型的数字，一个long型占8个字节，也就是64个bit，原始snowflake算法中对于bit的分配如下图： 第一个bit位是标识部分，在java中由于long的最高位是符号位，正数是0，负数是1，一般生成的ID为正数，所以固定为0。 时间戳部分占41bit，这个是毫秒级的时间，一般实现上不会存储当前的时间戳，而是时间戳的差值（当前时间-固定的开始时间），这样可以使产生的ID从更小值开始；41位的时间戳可以使用69年，(1L 60 60 24 365) = 69年 工作机器id占10bit，这里比较灵活，比如，可以使用前5位作为数据中心机房标识，后5位作为单机房机器标识，可以部署1024个节点。 序列号部分占12bit，支持同一毫秒内同一个节点可以生成4096个ID 根据这个算法的逻辑，只需要将这个算法用Java语言实现出来，封装为一个工具方法，那么各个业务应用可以直接使用该工具方法来获取分布式ID，只需保证每个业务应用有自己的工作机器id即可，而不需要单独去搭建一个获取分布式ID的应用。 snowflake算法实现起来并不难，提供一个github上用java实现的：github.com/beyondfengy… 在大厂里，其实并没有直接使用snowflake，而是进行了改造，因为snowflake算法中最难实践的就是工作机器id，原始的snowflake算法需要人工去为每台机器去指定一个机器id，并配置在某个地方从而让snowflake从此处获取机器id。 但是在大厂里，机器是很多的，人力成本太大且容易出错，所以大厂对snowflake进行了改造。 百度（uid-generator） github地址：uid-generator uid-generator使用的就是snowflake，只是在生产机器id，也叫做workId时有所不同。 uid-generator中的workId是由uid-generator自动生成的，并且考虑到了应用部署在docker上的情况，在uid-generator中用户可以自己去定义workId的生成策略，默认提供的策略是：应用启动时由数据库分配。说的简单一点就是：应用在启动时会往数据库表(uid-generator需要新增一个WORKER_NODE表)中去插入一条数据，数据插入成功后返回的该数据对应的自增唯一id就是该机器的workId，而数据由host，port组成。 对于uid-generator中的workId，占用了22个bit位，时间占用了28个bit位，序列化占用了13个bit位，需要注意的是，和原始的snowflake不太一样，时间的单位是秒，而不是毫秒，workId也不一样，同一个应用每重启一次就会消费一个workId。 具体可参考github.com/baidu/uid-g… 美团（Leaf） github地址：Leaf 美团的Leaf也是一个分布式ID生成框架。它非常全面，即支持号段模式，也支持snowflake模式。号段模式这里就不介绍了，和上面的分析类似。 Leaf中的snowflake模式和原始snowflake算法的不同点，也主要在workId的生成，Leaf中workId是基于ZooKeeper的顺序Id来生成的，每个应用在使用Leaf-snowflake时，在启动时都会都在Zookeeper中生成一个顺序Id，相当于一台机器对应一个顺序节点，也就是一个workId。 总结 总得来说，上面两种都是自动生成workId，以让系统更加稳定以及减少人工成功。 Redis 这里额外再介绍一下使用Redis来生成分布式ID，其实和利用Mysql自增ID类似，可以利用Redis中的incr命令来实现原子性的自增与返回，比如： 127.0.0.1:6379> set seq_id 1 // 初始化自增ID为1 OK 127.0.0.1:6379> incr seq_id // 增加1，并返回 (integer) 2 127.0.0.1:6379> incr seq_id // 增加1，并返回 (integer) 3 使用redis的效率是非常高的，但是要考虑持久化的问题。Redis支持RDB和AOF两种持久化的方式。 RDB持久化相当于定时打一个快照进行持久化，如果打完快照后，连续自增了几次，还没来得及做下一次快照持久化，这个时候Redis挂掉了，重启Redis后会出现ID重复。 AOF持久化相当于对每条写命令进行持久化，如果Redis挂掉了，不会出现ID重复的现象，但是会由于incr命令过得，导致重启恢复数据时间过长。 公众号 如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \"Java面试突击\" 即可免费领取！ Java工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。 "},"zother6-JavaGuide/system-design/website-architecture/8 张图读懂大型网站技术架构.html":{"url":"zother6-JavaGuide/system-design/website-architecture/8 张图读懂大型网站技术架构.html","title":"8 张图读懂大型网站技术架构","keywords":"","body":" 本文是作者读 《大型网站技术架构》所做的思维导图，在这里分享给各位，公众号(JavaGuide)后台回复：“架构”。即可获得下面图片的源文件以及思维导图源文件！ 1. 大型网站架构演化 2. 大型架构模式 3. 大型网站核心架构要素 4. 瞬时响应:网站的高性能架构 5. 万无一失:网站的高可用架构 6. 永无止境:网站的伸缩性架构 7. 随机应变:网站的可扩展性架构 8. 固若金汤:网站的安全机构 1. 大型网站架构演化 2. 大型架构模式 3. 大型网站核心架构要素 4. 瞬时响应:网站的高性能架构 5. 万无一失:网站的高可用架构 6. 永无止境:网站的伸缩性架构 7. 随机应变:网站的可扩展性架构 8. 固若金汤:网站的安全机构 "},"zother6-JavaGuide/system-design/website-architecture/关于大型网站系统架构你不得不懂的10个问题.html":{"url":"zother6-JavaGuide/system-design/website-architecture/关于大型网站系统架构你不得不懂的10个问题.html","title":"关于大型网站系统架构你不得不懂的10个问题","keywords":"","body":"下面这些问题都是一线大厂的真实面试问题，不论是对你面试还是说拓宽知识面都很有帮助。之前发过一篇8 张图读懂大型网站技术架构 可以作为不太了解大型网站系统技术架构朋友的入门文章。 1. 你使用过哪些组件或者方法来提升网站性能,可用性以及并发量 2. 设计高可用系统的常用手段 3. 现代互联网应用系统通常具有哪些特点? 4. 谈谈你对微服务领域的了解和认识 5. 谈谈你对 Dubbo 和 Spring Cloud 的认识(两者关系) 6. 性能测试了解吗?说说你知道的性能测试工具? 7. 对于一个单体应用系统,随着产品使用的用户越来越多,网站的流量会增加,最终单台服务器无法处理那么大的流量怎么办? 8. 大表优化的常见手段 9. 在系统中使用消息队列能带来什么好处? 1) 通过异步处理提高系统性能 2) 降低系统耦合性 10. 说说自己对 CAP 定理,BASE 理论的了解 CAP 定理 BASE 理论 参考 1. 你使用过哪些组件或者方法来提升网站性能,可用性以及并发量 提高硬件能力、增加系统服务器。（当服务器增加到某个程度的时候系统所能提供的并发访问量几乎不变，所以不能根本解决问题） 使用缓存（本地缓存：本地可以使用JDK自带的 Map、Guava Cache.分布式缓存：Redis、Memcache.本地缓存不适用于提高系统并发量，一般是用处用在程序中。比如Spring是如何实现单例的呢？大家如果看过源码的话，应该知道，S把已经初始过的变量放在一个Map中，下次再要使用这个变量的时候，先判断Map中有没有，这也就是系统中常见的单例模式的实现。） 消息队列 （解耦+削峰+异步） 采用分布式开发 （不同的服务部署在不同的机器节点上，并且一个服务也可以部署在多台机器上，然后利用 Nginx 负载均衡访问。这样就解决了单点部署(All In)的缺点，大大提高的系统并发量） 数据库分库（读写分离）、分表（水平分表、垂直分表） 采用集群 （多台机器提供相同的服务） CDN 加速 (将一些静态资源比如图片、视频等等缓存到离用户最近的网络节点) 浏览器缓存 使用合适的连接池（数据库连接池、线程池等等） 适当使用多线程进行开发。 2. 设计高可用系统的常用手段 降级： 服务降级是当服务器压力剧增的情况下，根据当前业务情况及流量对一些服务和页面有策略的降级，以此释放服务器资源以保证核心任务的正常运行。降级往往会指定不同的级别，面临不同的异常等级执行不同的处理。根据服务方式：可以拒接服务，可以延迟服务，也有时候可以随机服务。根据服务范围：可以砍掉某个功能，也可以砍掉某些模块。总之服务降级需要根据不同的业务需求采用不同的降级策略。主要的目的就是服务虽然有损但是总比没有好； 限流： 防止恶意请求流量、恶意攻击，或者防止流量超出系统峰值； 缓存： 避免大量请求直接落到数据库，将数据库击垮； 超时和重试机制： 避免请求堆积造成雪崩； 回滚机制： 快速修复错误版本。 3. 现代互联网应用系统通常具有哪些特点? 高并发，大流量； 高可用：系统7×24小时不间断服务； 海量数据：需要存储、管理海量数据，需要使用大量服务器； 用户分布广泛，网络情况复杂：许多大型互联网都是为全球用户提供服务的，用户分布范围广，各地网络情况千差万别； 安全环境恶劣：由于互联网的开放性，使得互联网更容易受到攻击，大型网站几乎每天都会被黑客攻击； 需求快速变更，发布频繁：和传统软件的版本发布频率不同，互联网产品为快速适应市场，满足用户需求，其产品发布频率是极高的； 渐进式发展：与传统软件产品或企业应用系统一开始就规划好全部的功能和非功能需求不同，几乎所有的大型互联网网站都是从一个小网站开始，渐进地发展起来。 4. 谈谈你对微服务领域的了解和认识 现在大公司都在用并且未来的趋势都是 Spring Cloud，而阿里开源的 Spring Cloud Alibaba 也是 Spring Cloud 规范的实现 。 我们通常把 Spring Cloud 理解为一系列开源组件的集合，但是 Spring Cloud并不是等同于 Spring Cloud Netflix 的 Ribbon、Feign、Eureka（停止更新）、Hystrix 这一套组件，而是抽象了一套通用的开发模式。它的目的是通过抽象出这套通用的模式，让开发者更快更好地开发业务。但是这套开发模式运行时的实际载体，还是依赖于 RPC、网关、服务发现、配置管理、限流熔断、分布式链路跟踪等组件的具体实现。 Spring Cloud Alibaba 是官方认证的新一套 Spring Cloud 规范的实现,Spring Cloud Alibaba 是一套国产开源产品集合，后续还会有中文 reference 和一些原理分析文章，所以，这对于国内的开发者是非常棒的一件事。阿里的这一举动势必会推动国内微服务技术的发展，因为在没有 Spring Cloud Alibaba 之前，我们的第一选择是 Spring Cloud Netflix，但是它们的文档都是英文的，出问题后排查也比较困难， 在国内并不是有特别多的人精通。Spring Cloud Alibaba 由阿里开源组件和阿里云产品组件两部分组成，其致力于提供微服务一站式解决方案，方便开发者通过 Spring Cloud 编程模型轻松开发微服务应用。 另外，Apache Dubbo Ecosystem 是围绕 Apache Dubbo 打造的微服务生态，是经过生产验证的微服务的最佳实践组合。在阿里巴巴的微服务解决方案中，Dubbo、Nacos 和 Sentinel，以及后续将开源的微服务组件，都是 Dubbo EcoSystem 的一部分。阿里后续也会将 Dubbo EcoSystem 集成到 Spring Cloud 的生态中。 5. 谈谈你对 Dubbo 和 Spring Cloud 的认识(两者关系) 具体可以看公众号-阿里巴巴中间件的这篇文章:独家解读：Dubbo Ecosystem - 从微服务框架到微服务生态 Dubbo 与 Spring Cloud 并不是竞争关系，Dubbo 作为成熟的 RPC 框架，其易用性、扩展性和健壮性已得到业界的认可。未来 Dubbo 将会作为 Spring Cloud Alibaba 的 RPC 组件，并与 Spring Cloud 原生的 Feign 以及 RestTemplate 进行无缝整合，实现“零”成本迁移。 在阿里巴巴的微服务解决方案中，Dubbo、Nacos 和 Sentinel，以及后续将开源的微服务组件，都是 Dubbo EcoSystem 的一部分。我们后续也会将 Dubbo EcoSystem 集成到 Spring Cloud 的生态中。 6. 性能测试了解吗?说说你知道的性能测试工具? 性能测试指通过自动化的测试工具模拟多种正常、峰值以及异常负载条件来对系统的各项性能指标进行测试。性能测试是总称，通常细分为： 基准测试： 在给系统施加较低压力时，查看系统的运行状况并记录相关数做为基础参考 负载测试： 是指对系统不断地增加压力或增加一定压力下的持续时间，直到系统的某项或多项性能指标达到安全临界值，例如某种资源已经达到饱和状态等 。此时继续加压，系统处理能力会下降。 压力测试： 超过安全负载情况下，不断施加压力（增加并发请求），直到系统崩溃或无法处理任何请求，依此获得系统最大压力承受能力。 稳定性测试： 被测试系统在特定硬件、软件、网络环境下，加载一定业务压力（模拟生产环境不同时间点、不均匀请求，呈波浪特性）运行一段较长时间，以此检测系统是否稳定。 后端程序员或者测试平常比较常用的测试工具是 JMeter（官网：https://jmeter.apache.org/）。Apache JMeter 是一款基于Java的压力测试工具(100％纯Java应用程序)，旨在加载测试功能行为和测量性能。它最初被设计用于 Web 应用测试但后来扩展到其他测试领域。 7. 对于一个单体应用系统,随着产品使用的用户越来越多,网站的流量会增加,最终单台服务器无法处理那么大的流量怎么办? 这个时候就要考虑扩容了。《亿级流量网站架构核心技术》这本书上面介绍到我们可以考虑下面几步来解决这个问题： 第一步，可以考虑简单的扩容来解决问题。比如增加系统的服务器，提高硬件能力等等。 第二步，如果简单扩容搞不定，就需要水平拆分和垂直拆分数据／应用来提升系统的伸缩性，即通过扩容提升系统负载能力。 第三步，如果通过水平拆分／垂直拆分还是搞不定，那就需要根据现有系统特性，架构层面进行重构甚至是重新设计，即推倒重来。 对于系统设计，理想的情况下应支持线性扩容和弹性扩容，即在系统瓶颈时，只需要增加机器就可以解决系统瓶颈，如降低延迟提升吞吐量，从而实现扩容需求。 如果你想扩容，则支持水平/垂直伸缩是前提。在进行拆分时，一定要清楚知道自己的目的是什么，拆分后带来的问题如何解决，拆分后如果没有得到任何收益就不要为了 拆而拆，即不要过度拆分，要适合自己的业务。 8. 大表优化的常见手段 当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下： 限定数据的范围： 务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内； 读/写分离： 经典的数据库拆分方案，主库负责写，从库负责读； 垂直分区： 根据数据库里面数据表的相关性进行拆分。 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表。 如下图所示，这样来说大家应该就更容易理解了。垂直拆分的优点： 可以使得行数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂； 水平分区： 保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。 水平拆分是指数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。水平拆分可以支持非常大的数据量。需要注意的一点是:分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以 水平拆分最好分库 。水平拆分能够 支持非常大的数据量存储，应用端改造也少，但 分片事务难以解决 ，跨界点Join性能较差，逻辑复杂。《Java工程师修炼之道》的作者推荐 尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维的各种复杂度 ，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，尽量选择客户端分片架构，这样可以减少一次和中间件的网络I/O。 下面补充一下数据库分片的两种常见方案： 客户端代理： 分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。 当当网的 Sharding-JDBC 、阿里的TDDL是两种比较常用的实现。 中间件代理： 在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。 我们现在谈的 Mycat 、360的Atlas、网易的DDB等等都是这种架构的实现。 9. 在系统中使用消息队列能带来什么好处? 《大型网站技术架构》第四章和第七章均有提到消息队列对应用性能及扩展性的提升。 1) 通过异步处理提高系统性能 如上图，在不使用消息队列服务器的时候，用户的请求数据直接写入数据库，在高并发的情况下数据库压力剧增，使得响应速度变慢。但是在使用消息队列之后，用户的请求数据发送给消息队列之后立即 返回，再由消息队列的消费者进程从消息队列中获取数据，异步写入数据库。由于消息队列服务器处理速度快于数据库（消息队列也比数据库有更好的伸缩性），因此响应速度得到大幅改善。 通过以上分析我们可以得出消息队列具有很好的削峰作用的功能——即通过异步处理，将短时间高并发产生的事务消息存储在消息队列中，从而削平高峰期的并发事务。 举例：在电子商务一些秒杀、促销活动中，合理使用消息队列可以有效抵御促销活动刚开始大量订单涌入对系统的冲击。如下图所示： 因为用户请求数据写入消息队列之后就立即返回给用户了，但是请求数据在后续的业务校验、写数据库等操作中可能失败。因此使用消息队列进行异步处理之后，需要适当修改业务流程进行配合，比如用户在提交订单之后，订单数据写入消息队列，不能立即返回用户订单提交成功，需要在消息队列的订单消费者进程真正处理完该订单之后，甚至出库后，再通过电子邮件或短信通知用户订单成功，以免交易纠纷。这就类似我们平时手机订火车票和电影票。 2) 降低系统耦合性 我们知道模块分布式部署以后聚合方式通常有两种：1.分布式消息队列和2.分布式服务。 先来简单说一下分布式服务： 目前使用比较多的用来构建SOA（Service Oriented Architecture面向服务体系结构）的分布式服务框架是阿里巴巴开源的Dubbo.如果想深入了解Dubbo的可以看我写的关于Dubbo的这一篇文章：《高性能优秀的服务框架-dubbo介绍》：https://juejin.im/post/5acadeb1f265da2375072f9c 再来谈我们的分布式消息队列： 我们知道如果模块之间不存在直接调用，那么新增模块或者修改模块就对其他模块影响较小，这样系统的可扩展性无疑更好一些。 我们最常见的事件驱动架构类似生产者消费者模式，在大型网站中通常用利用消息队列实现事件驱动结构。如下图所示： 消息队列使利用发布-订阅模式工作，消息发送者（生产者）发布消息，一个或多个消息接受者（消费者）订阅消息。 从上图可以看到消息发送者（生产者）和消息接受者（消费者）之间没有直接耦合，消息发送者将消息发送至分布式消息队列即结束对消息的处理，消息接受者从分布式消息队列获取该消息后进行后续处理，并不需要知道该消息从何而来。对新增业务，只要对该类消息感兴趣，即可订阅该消息，对原有系统和业务没有任何影响，从而实现网站业务的可扩展性设计。 消息接受者对消息进行过滤、处理、包装后，构造成一个新的消息类型，将消息继续发送出去，等待其他消息接受者订阅该消息。因此基于事件（消息对象）驱动的业务架构可以是一系列流程。 另外为了避免消息队列服务器宕机造成消息丢失，会将成功发送到消息队列的消息存储在消息生产者服务器上，等消息真正被消费者服务器处理后才删除消息。在消息队列服务器宕机后，生产者服务器会选择分布式消息队列服务器集群中的其他服务器发布消息。 备注： 不要认为消息队列只能利用发布-订阅模式工作，只不过在解耦这个特定业务环境下是使用发布-订阅模式的，比如在我们的ActiveMQ消息队列中还有点对点工作模式，具体的会在后面的文章给大家详细介绍，这一篇文章主要还是让大家对消息队列有一个更透彻的了解。 这个问题一般会在上一个问题问完之后，紧接着被问到。“使用消息队列会带来什么问题？”这个问题要引起重视，一般我们都会考虑使用消息队列会带来的好处而忽略它带来的问题！ 10. 说说自己对 CAP 定理,BASE 理论的了解 CAP 定理 在理论计算机科学中，CAP定理（CAP theorem），又被称作布鲁尔定理（Brewer's theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点： 一致性（Consistence） :所有节点访问同一份最新的数据副本 可用性（Availability）:每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据 分区容错性（Partition tolerance） : 分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。 CAP仅适用于原子读写的NOSQL场景中，并不适合数据库系统。现在的分布式系统具有更多特性比如扩展性、可用性等等，在进行系统设计和开发时，我们不应该仅仅局限在CAP问题上。 注意：不是所谓的3选2（不要被网上大多数文章误导了）: 大部分人解释这一定律时，常常简单的表述为：“一致性、可用性、分区容忍性三者你只能同时达到其中两个，不可能同时达到”。实际上这是一个非常具有误导性质的说法，而且在CAP理论诞生12年之后，CAP之父也在2012年重写了之前的论文。 当发生网络分区的时候，如果我们要继续服务，那么强一致性和可用性只能2选1。也就是说当网络分区之后P是前提，决定了P之后才有C和A的选择。也就是说分区容错性（Partition tolerance）我们是必须要实现的。 我在网上找了很多文章想看一下有没有文章提到这个不是所谓的3选2，用百度半天没找到了一篇，用谷歌搜索找到一篇比较不错的，如果想深入学习一下CAP就看这篇文章把，我这里就不多BB了：《分布式系统之CAP理论》 ： http://www.cnblogs.com/hxsyl/p/4381980.html BASE 理论 BASE 是 Basically Available（基本可用） 、Soft-state（软状态） 和 Eventually Consistent（最终一致性） 三个短语的缩写。BASE理论是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于CAP定理逐步演化而来的，它大大降低了我们对系统的要求。 BASE理论的核心思想： 即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。也就是牺牲数据的一致性来满足系统的高可用性，系统中一部分数据不可用或者不一致时，仍需要保持系统整体“主要可用”。 BASE理论三要素： 基本可用： 基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这绝不等价于系统不可用。 比如： ①响应时间上的损失:正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障，查询结果的响应时间增加了1~2秒；②系统功能上的损失：正常情况下，在一个电子商务网站上进行购物的时候，消费者几乎能够顺利完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面； 软状态： 软状态指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时； 最终一致性： 最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。 参考 《大型网站技术架构》 《亿级流量网站架构核心技术》 《Java工程师修炼之道》 https://www.cnblogs.com/puresoul/p/5456855.html "},"zother6-JavaGuide/system-design/website-architecture/分布式.html":{"url":"zother6-JavaGuide/system-design/website-architecture/分布式.html","title":"分布式","keywords":"","body":"一　分布式系统的经典基础理论 分布式系统的经典基础理论 本文主要是简单的介绍了三个常见的概念： 分布式系统设计理念 、 CAP定理 、 BASE理论 ，关于分布式系统的还有很多很多东西。 二　分布式事务 分布式事务就是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上。以上是百度百科的解释，简单的说，就是一次大的操作由不同的小操作组成，这些小的操作分布在不同的服务器上，且属于不同的应用，分布式事务需要保证这些小操作要么全部成功，要么全部失败。本质上来说，分布式事务就是为了保证不同数据库的数据一致性。 深入理解分布式事务 聊聊分布式事务，再说说解决方案 三　分布式系统一致性 分布式服务化系统一致性的“最佳实干” 四　一致性协议/算法 早在1900年就诞生了著名的 Paxos经典算法 （Zookeeper就采用了Paxos算法的近亲兄弟Zab算法），但由于Paxos算法非常难以理解、实现、排错。所以不断有人尝试简化这一算法，直到2013年才有了重大突破：斯坦福的Diego Ongaro、John Ousterhout以易懂性为目标设计了新的一致性算法—— Raft算法 ，并发布了对应的论文《In Search of an Understandable Consensus Algorithm》，到现在有十多种语言实现的Raft算法框架，较为出名的有以Go语言实现的Etcd，它的功能类似于Zookeeper，但采用了更为主流的Rest接口。 图解 Paxos 一致性协议 图解分布式协议-RAFT Zookeeper ZAB 协议分析 五　分布式存储 分布式存储系统将数据分散存储在多台独立的设备上。传统的网络存储系统采用集中的存储服务器存放所有数据，存储服务器成为系统性能的瓶颈，也是可靠性和安全性的焦点，不能满足大规模存储应用的需要。分布式网络存储系统采用可扩展的系统结构，利用多台存储服务器分担存储负荷，利用位置服务器定位存储信息，它不但提高了系统的可靠性、可用性和存取效率，还易于扩展。 分布式存储系统概要 六　分布式计算 所谓分布式计算是一门计算机科学，它研究如何把一个需要非常巨大的计算能力才能解决的问题分成许多小的部分，然后把这些部分分配给许多计算机进行处理，最后把这些计算结果综合起来得到最终的结果。 分布式网络存储技术是将数据分散的存储于多台独立的机器设备上。分布式网络存储系统采用可扩展的系统结构，利用多台存储服务器分担存储负荷，利用位置服务器定位存储信息，不但解决了传统集中式存储系统中单存储服务器的瓶颈问题，还提高了系统的可靠性、可用性和扩展性。 关于分布式计算的一些概念 "},"zother6-JavaGuide/system-design/website-architecture/如何设计一个高可用系统？要考虑哪些地方？.html":{"url":"zother6-JavaGuide/system-design/website-architecture/如何设计一个高可用系统？要考虑哪些地方？.html","title":"如何设计一个高可用系统？要考虑哪些地方？","keywords":"","body":"一篇短小的文章，面试经常遇到的这个问题。本文主要包括下面这些内容： 高可用的定义 哪些情况可能会导致系统不可用？ 有些提高系统可用性的方法？只是简单的提一嘴，更具体内容在后续的文章中介绍，就拿限流来说，你需要搞懂：何为限流？如何限流？为什么要限流？如何做呢？说一下原理？。 什么是高可用？可用性的判断标准是啥？ 高可用描述的是一个系统在大部分时间都是可用的，可以为我们提供服务的。高可用代表系统即使在发生硬件故障或者系统升级的时候，服务仍然是可用的。 一般情况下，我们使用多少个 9 来评判一个系统的可用性，比如 99.9999% 就是代表该系统在所有的运行时间中只有 0.0001% 的时间是不可用的，这样的系统就是非常非常高可用的了！当然，也会有系统如果可用性不太好的话，可能连 9 都上不了。 除此之外，系统的可用性还可以用某功能的失败次数与总的请求次数之比来衡量，比如对网站请求 1000 次，其中有 10 次请求失败，那么可用性就是 99%。 哪些情况会导致系统不可用？ 黑客攻击； 硬件故障，比如服务器坏掉。 并发量/用户请求量激增导致整个服务宕掉或者部分服务不可用。 代码中的坏味道导致内存泄漏或者其他问题导致程序挂掉。 网站架构某个重要的角色比如 Nginx 或者数据库突然不可用。 自然灾害或者人为破坏。 ...... 有哪些提高系统可用性的方法？ 1. 注重代码质量，测试严格把关 我觉得这个是最最最重要的，代码质量有问题比如比较常见的内存泄漏、循环依赖都是对系统可用性极大的损害。大家都喜欢谈限流、降级、熔断，但是我觉得从代码质量这个源头把关是首先要做好的一件很重要的事情。如何提高代码质量？比较实际可用的就是 CodeReview，不要在乎每天多花的那 1 个小时左右的时间，作用可大着呢！ 另外，安利这个对提高代码质量有实际效果的宝贝： sonarqube ：保证你写出更安全更干净的代码！（ps: 目前所在的项目基本都会用到这个插件）。 Alibaba 开源的 Java 诊断工具 Arthas 也是很不错的选择。 IDEA 自带的代码分析等工具进行代码扫描也是非常非常棒的。 2.使用集群，减少单点故障 先拿常用的 Redis 举个例子！我们如何保证我们的 Redis 缓存高可用呢？答案就是使用集群，避免单点故障。当我们使用一个 Redis 实例作为缓存的时候，这个 Redis 实例挂了之后，整个缓存服务可能就挂了。使用了集群之后，即使一台 Redis 实例，不到一秒就会有另外一台 Redis 实例顶上。 3.限流 流量控制（flow control），其原理是监控应用流量的 QPS 或并发线程数等指标，当达到指定的阈值时对流量进行控制，以避免被瞬时的流量高峰冲垮，从而保障应用的高可用性。——来自 alibaba-Sentinel 的 wiki。 4.超时和重试机制设置 一旦用户请求超过某个时间的得不到响应，就抛出异常。这个是非常重要的，很多线上系统故障都是因为没有进行超时设置或者超时设置的方式不对导致的。我们在读取第三方服务的时候，尤其适合设置超时和重试机制。一般我们使用一些 RPC 框架的时候，这些框架都自带的超时重试的配置。如果不进行超时设置可能会导致请求响应速度慢，甚至导致请求堆积进而让系统无法在处理请求。重试的次数一般设为 3 次，再多次的重试没有好处，反而会加重服务器压力（部分场景使用失败重试机制会不太适合）。 5.熔断机制 超时和重试机制设置之外，熔断机制也是很重要的。 熔断机制说的是系统自动收集所依赖服务的资源使用情况和性能指标，当所依赖的服务恶化或者调用失败次数达到某个阈值的时候就迅速失败，让当前系统立即切换依赖其他备用服务。 比较常用的是流量控制和熔断降级框架是 Netflix 的 Hystrix 和 alibaba 的 Sentinel。 6.异步调用 异步调用的话我们不需要关心最后的结果，这样我们就可以用户请求完成之后就立即返回结果，具体处理我们可以后续再做，秒杀场景用这个还是蛮多的。但是，使用异步之后我们可能需要 适当修改业务流程进行配合，比如用户在提交订单之后，不能立即返回用户订单提交成功，需要在消息队列的订单消费者进程真正处理完该订单之后，甚至出库后，再通过电子邮件或短信通知用户订单成功。除了可以在程序中实现异步之外，我们常常还使用消息队列，消息队列可以通过异步处理提高系统性能（削峰、减少响应所需时间）并且可以降低系统耦合性。 7.使用缓存 如果我们的系统属于并发量比较高的话，如果我们单纯使用数据库的话，当大量请求直接落到数据库可能数据库就会直接挂掉。使用缓存缓存热点数据，因为缓存存储在内存中，所以速度相当地快！ 8.其他 核心应用和服务优先使用更好的硬件 监控系统资源使用情况增加报警设置。 注意备份，必要时候回滚。 灰度发布： 将服务器集群分成若干部分，每天只发布一部分机器，观察运行稳定没有故障，第二天继续发布一部分机器，持续几天才把整个集群全部发布完毕，期间如果发现问题，只需要回滚已发布的一部分服务器即可 定期检查/更换硬件： 如果不是购买的云服务的话，定期还是需要对硬件进行一波检查的，对于一些需要更换或者升级的硬件，要及时更换或者升级。 .....(想起来再补充！也欢迎各位欢迎补充！) 总结 "},"zother6-JavaGuide/system-design/restful-api.html":{"url":"zother6-JavaGuide/system-design/restful-api.html","title":"Restful Api","keywords":"","body":" 大家好我是 Guide 哥！这是我的第 210 篇优质原创！这篇文章主要分享了后端程序员必备的 RestFul API 相关的知识。 RESTful API 是每个程序员都应该了解并掌握的基本知识，我们在开发过程中设计 API 的时候也应该至少要满足 RESTful API 的最基本的要求（比如接口中尽量使用名词，使用 POST 请求创建资源，DELETE 请求删除资源等等，示例：GET /notes/id：获取某个指定 id 的笔记的信息）。 如果你看 RESTful API 相关的文章的话一般都比较晦涩难懂，包括我下面的文章也会提到一些概念性的东西。但是，实际上我们平时开发用到的 RESTful API 的知识非常简单也很容易概括！举个例子,如果我给你下面两个 url 你是不是立马能知道它们是干什么的！这就是 RESTful API 的强大之处！ RESTful API 可以你看到 url + http method 就知道这个 url 是干什么的，让你看到了 http 状态码（status code）就知道请求结果如何。 GET /classes：列出所有班级 POST /classes：新建一个班级 下面的内容只是介绍了我觉得关于 RESTful API 比较重要的一些东西，欢迎补充。 一、重要概念 REST,即 REpresentational State Transfer 的缩写。这个词组的翻译过来就是\"表现层状态转化\"。这样理解起来甚是晦涩，实际上 REST 的全称是 Resource Representational State Transfe ，直白地翻译过来就是 “资源”在网络传输中以某种“表现形式”进行“状态转移” 。如果还是不能继续理解，请继续往下看，相信下面的讲解一定能让你理解到底啥是 REST 。 我们分别对上面涉及到的概念进行解读，以便加深理解，不过实际上你不需要搞懂下面这些概念，也能看懂我下一部分要介绍到的内容。不过，为了更好地能跟别人扯扯 “RESTful API”我建议你还是要好好理解一下！ 资源（Resource） ：我们可以把真实的对象数据称为资源。一个资源既可以是一个集合，也可以是单个个体。比如我们的班级 classes 是代表一个集合形式的资源，而特定的 class 代表单个个体资源。每一种资源都有特定的 URI（统一资源定位符）与之对应，如果我们需要获取这个资源，访问这个 URI 就可以了，比如获取特定的班级：/class/12。另外，资源也可以包含子资源，比如 /classes/classId/teachers：列出某个指定班级的所有老师的信息 表现形式（Representational）：\"资源\"是一种信息实体，它可以有多种外在表现形式。我们把\"资源\"具体呈现出来的形式比如 json，xml，image,txt 等等叫做它的\"表现层/表现形式\"。 状态转移（State Transfer） ：大家第一眼看到这个词语一定会很懵逼？内心 BB：这尼玛是啥啊？ 大白话来说 REST 中的状态转移更多地描述的服务器端资源的状态，比如你通过增删改查（通过 HTTP 动词实现）引起资源状态的改变。ps:互联网通信协议 HTTP 协议，是一个无状态协议，所有的资源状态都保存在服务器端。 综合上面的解释，我们总结一下什么是 RESTful 架构： 每一个 URI 代表一种资源； 客户端和服务器之间，传递这种资源的某种表现形式比如 json，xml，image,txt 等等； 客户端通过特定的 HTTP 动词，对服务器端资源进行操作，实现\"表现层状态转化\"。 二、REST 接口规范 1、动作 GET ：请求从服务器获取特定资源。举个例子：GET /classes（获取所有班级） POST ：在服务器上创建一个新的资源。举个例子：POST /classes（创建班级） PUT ：更新服务器上的资源（客户端提供更新后的整个资源）。举个例子：PUT /classes/12（更新编号为 12 的班级） DELETE ：从服务器删除特定的资源。举个例子：DELETE /classes/12（删除编号为 12 的班级） PATCH ：更新服务器上的资源（客户端提供更改的属性，可以看做作是部分更新），使用的比较少，这里就不举例子了。 2、路径（接口命名） 路径又称\"终点\"（endpoint），表示 API 的具体网址。实际开发中常见的规范如下： 网址中不能有动词，只能有名词，API 中的名词也应该使用复数。 因为 REST 中的资源往往和数据库中的表对应，而数据库中的表都是同种记录的\"集合\"（collection）。如果 API 调用并不涉及资源（如计算，翻译等操作）的话，可以用动词。 比如：GET /calculate?param1=11&param2=33 不用大写字母，建议用中杠 - 不用下杠 _ 比如邀请码写成 invitation-code而不是 invitation_code Talk is cheap！来举个实际的例子来说明一下吧！现在有这样一个 API 提供班级（class）的信息，还包括班级中的学生和教师的信息，则它的路径应该设计成下面这样。 接口尽量使用名词，禁止使用动词。 下面是一些例子： GET /classes：列出所有班级 POST /classes：新建一个班级 GET /classes/classId：获取某个指定班级的信息 PUT /classes/classId：更新某个指定班级的信息（一般倾向整体更新） PATCH /classes/classId：更新某个指定班级的信息（一般倾向部分更新） DELETE /classes/classId：删除某个班级 GET /classes/classId/teachers：列出某个指定班级的所有老师的信息 GET /classes/classId/students：列出某个指定班级的所有学生的信息 DELETE classes/classId/teachers/ID：删除某个指定班级下的指定的老师的信息 反例： /getAllclasses /createNewclass /deleteAllActiveclasses 理清资源的层次结构，比如业务针对的范围是学校，那么学校会是一级资源:/schools，老师: /schools/teachers，学生: /schools/students 就是二级资源。 3、过滤信息（Filtering） 如果我们在查询的时候需要添加特定条件的话，建议使用 url 参数的形式。比如我们要查询 state 状态为 active 并且 name 为 guidegege 的班级： GET /classes?state=active&name=guidegege 比如我们要实现分页查询： GET /classes?page=1&size=10 //指定第1页，每页10个数据 4、状态码（Status Codes） 状态码范围： 2xx：成功 3xx：重定向 4xx：客户端错误 5xx：服务器错误 200 成功 301 永久重定向 400 错误请求 500 服务器错误 201 创建 304 资源未修改 401 未授权 502 网关错误 403 禁止访问 504 网关超时 404 未找到 405 请求方法不对 三 HATEOAS RESTful 的极致是 hateoas ，但是这个基本不会在实际项目中用到。 上面是 RESTful API 最基本的东西，也是我们平时开发过程中最容易实践到的。实际上，RESTful API 最好做到 Hypermedia，即返回结果中提供链接，连向其他 API 方法，使得用户不查文档，也知道下一步应该做什么。 比如，当用户向 api.example.com 的根目录发出请求，会得到这样一个文档。 {\"link\": { \"rel\": \"collection https://www.example.com/classes\", \"href\": \"https://api.example.com/classes\", \"title\": \"List of classes\", \"type\": \"application/vnd.yourformat+json\" }} 上面代码表示，文档中有一个 link 属性，用户读取这个属性就知道下一步该调用什么 API 了。rel 表示这个 API 与当前网址的关系（collection 关系，并给出该 collection 的网址），href 表示 API 的路径，title 表示 API 的标题，type 表示返回类型 Hypermedia API 的设计被称为HATEOAS。 在 Spring 中有一个叫做 HATEOAS 的 API 库，通过它我们可以更轻松的创建除符合 HATEOAS 设计的 API。 文章推荐 RESTful API 介绍： RESTful API Tutorial RESTful API 最佳指南（阮一峰，这篇文章大部分内容来源） [译] RESTful API 设计最佳实践 那些年，我们一起误解过的 REST Testing RESTful Services in Java: Best Practices Spring 中使用 HATEOAS： 在 Spring Boot 中使用 HATEOAS Building REST services with Spring (Spring 官网 ) An Intro to Spring HATEOAS （by baeldung） spring-hateoas-examples Spring HATEOAS (Spring 官网 ) "},"zother6-JavaGuide/system-design/设计模式.html":{"url":"zother6-JavaGuide/system-design/设计模式.html","title":"设计模式","keywords":"","body":"Java 设计模式 下面是自己学习设计模式的时候做的总结，有些是自己的原创文章，有些是网上写的比较好的文章，保存下来细细消化吧！ 系列文章推荐：https://design-patterns.readthedocs.io/zh_CN/latest/index.html 创建型模式 创建型模式概述 创建型模式(Creational Pattern)对类的实例化过程进行了抽象，能够将软件模块中对象的创建和对象的使用分离。为了使软件的结构更加清晰，外界对于这些对象只需要知道它们共同的接口，而不清楚其具体的实现细节，使整个系统的设计更加符合单一职责原则。 创建型模式在创建什么(What)，由谁创建(Who)，何时创建(When)等方面都为软件设计者提供了尽可能大的灵活性。创建型模式隐藏了类的实例的创建细节，通过隐藏对象如何被创建和组合在一起达到使整个系统独立的目的。 常见创建型模式详解 单例模式： 深入理解单例模式——只有一个实例 工厂模式： 深入理解工厂模式——由对象工厂生成对象 建造者模式： 深入理解建造者模式 ——组装复杂的实例 原型模式： 深入理解原型模式 ——通过复制生成实例 结构型模式 结构型模式概述 结构型模式(Structural Pattern)： 描述如何将类或者对象结合在一起形成更大的结构，就像搭积木，可以通过简单积木的组合形成复杂的、功能更为强大的结构 结构型模式可以分为类结构型模式和对象结构型模式： 类结构型模式关心类的组合，由多个类可以组合成一个更大的系统，在类结构型模式中一般只存在继承关系和实现关系。 对象结构型模式关心类与对象的组合，通过关联关系使得在一个类中定义另一个类的实例对象，然后通过该对象调用其方法。根据“合成复用原则”，在系统中尽量使用关联关系来替代继承关系，因此大部分结构型模式都是对象结构型模式。 常见结构型模式详解 适配器模式： 深入理解适配器模式——加个“适配器”以便于复用 适配器模式原理及实例介绍-IBM 桥接模式： 设计模式笔记16：桥接模式(Bridge Pattern) 组合模式： 大话设计模式—组合模式 装饰模式： java模式—装饰者模式、Java设计模式-装饰者模式 外观模式： java设计模式之外观模式（门面模式） 享元模式： 享元模式 代理模式： 代理模式原理及实例讲解 （IBM出品，很不错） 轻松学，Java 中的代理模式及动态代理 Java代理模式及其应用 行为型模式 行为型模式概述 行为型模式(Behavioral Pattern)是对在不同的对象之间划分责任和算法的抽象化。 行为型模式不仅仅关注类和对象的结构，而且重点关注它们之间的相互作用。 通过行为型模式，可以更加清晰地划分类与对象的职责，并研究系统在运行时实例对象之间的交互。在系统运行时，对象并不是孤立的，它们可以通过相互通信与协作完成某些复杂功能，一个对象在运行时也将影响到其他对象的运行。 行为型模式分为类行为型模式和对象行为型模式两种： 类行为型模式： 类的行为型模式使用继承关系在几个类之间分配行为，类行为型模式主要通过多态等方式来分配父类与子类的职责。 对象行为型模式： 对象的行为型模式则使用对象的聚合关联关系来分配行为，对象行为型模式主要是通过对象关联等方式来分配两个或多个类的职责。根据“合成复用原则”，系统中要尽量使用关联关系来取代继承关系，因此大部分行为型设计模式都属于对象行为型设计模式。 职责链模式： Java设计模式之责任链模式、职责链模式 责任链模式实现的三种方式 命令模式： https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/command.html 在软件设计中，我们经常需要向某些对象发送请求，但是并不知道请求的接收者是谁，也不知道被请求的操作是哪个，我们只需在程序运行时指定具体的请求接收者即可，此时，可以使用命令模式来进行设计，使得请求发送者与请求接收者消除彼此之间的耦合，让对象之间的调用关系更加灵活。命令模式可以对发送者和接收者完全解耦，发送者与接收者之间没有直接引用关系，发送请求的对象只需要知道如何发送请求，而不必知道如何完成请求。这就是命令模式的模式动机。 解释器模式： 迭代器模式： 中介者模式： https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/mediator.html 备忘录模式： 观察者模式： https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/observer.html、https://juejin.im/post/5c712ab56fb9a049a7127114 状态模式：https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/state.html 策略模式：https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/strategy.html 策略模式作为设计原则中开闭原则最典型的体现，也是经常使用的。下面这篇博客介绍了策略模式一般的组成部分和概念，并用了一个小demo去说明了策略模式的应用。 java设计模式之策略模式 模板方法模式： 访问者模式： "},"zother6-JavaGuide/tools/github/github-star-ranking.html":{"url":"zother6-JavaGuide/tools/github/github-star-ranking.html","title":"Github Star Ranking","keywords":"","body":" 下面的 10 个项目还是很推荐的！JS 的项目占比挺大，其他基本都是文档/学习类型的仓库。 说明：数据统计于 2019-11-27。 1. freeCodeCamp Github地址：https://github.com/freeCodeCamp/freeCodeCamp star: 307 k 介绍: 开放源码代码库和课程。与数百万人一起免费学习编程。网站：https://www.freeCodeCamp.org （一个友好的社区，您可以在这里免费学习编码。它由捐助者支持、非营利组织运营，以帮助数百万忙碌的成年人学习编程技术。这个社区已经帮助10,000多人获得了第一份开发人员的工作。这里的全栈Web开发课程是完全免费的，并且可以自行调整进度。这里还有数以千计的交互式编码挑战，可帮助您扩展技能。） 比如我想学习 ES6 的语法，学习界面是下面这样的,你可以很方便地边练习边学习： 2. 996.ICU Github地址：https://github.com/996icu/996.ICU star: 248 k 介绍: 996.ICU 是指“工作 996， 生病 ICU” 。这是中国程序员之间的一种自嘲说法，意思是如果按照 996 的模式工作，那以后就得进 ICU 了。这个项目最早是某个中国程序员发起的，然后就火遍全网，甚至火到了全世界很多其他国家，其网站被翻译成了多种语言。网站地址：https://996.icu。 3. vue Github地址：https://github.com/vuejs/vue star: 153 k 介绍: 尤大的前端框架。国人用的最多（容易上手，文档比较丰富），所以 Star 数量比较多还是有道理的。Vue (读音 /vjuː/，类似于 view) 是一套用于构建用户界面的渐进式框架。与其它大型框架不同的是，Vue 被设计为可以自底向上逐层应用。Vue 的核心库只关注视图层，不仅易于上手，还便于与第三方库或既有项目整合。另一方面，当与现代化的工具链以及各种支持类库结合使用时，Vue 也完全能够为复杂的单页应用提供驱动。 4. React Github地址：https://gitstar-ranking.com/facebook/react star: 140 k 介绍: Facebook 开源的，大公司有保障。用于构建用户界面的声明式、基于组件开发，高效且灵活的JavaScript框架。我司大部分项目的前端都是 React ，我自己也用过一段时间，感觉还不错，但是也有一些小坑。 5. tensorflow Github地址：https://github.com/tensorflow/tensorflow star: 138 k 介绍: 适用于所有人的开源机器学习框架。TensorFlow是用于机器学习的端到端开源平台。TensorFlow最初是由Google机器智能研究组织内Google Brain团队的研究人员和工程师开发的，用于进行机器学习和深度神经网络研究。该系统具有足够的通用性，也可以适用于多种其他领域。TensorFlow提供了稳定的Python 和C ++ API，以及其他语言的非保证的向后兼容API 。 6. bootstrap Github地址：https://github.com/twbs/bootstrap star: 137 k 介绍: 相信初学前端的时候，大家一定或多或少地接触过这个框架。官网说它是最受欢迎的HTML，CSS和JavaScript框架，用于在网络上开发响应式，移动优先项目。 7. free-programming-books Github地址：https://github.com/EbookFoundation/free-programming-books star: 132 k 介绍: 免费提供的编程书籍。我自己没太搞懂为啥这个项目 Star 数这么多，知道的麻烦评论区吱一声。 8. Awesome Github地址 ： https://github.com/sindresorhus/awesome star: 120 k 介绍: github 上很多的各种 Awesome 系列合集。 下面是这个开源仓库的目录，可以看出其涵盖了很多方面的内容。 举个例子，这个仓库里面就有两个让你的电脑更好用的开源仓库，Mac 和 Windows都有： Awesome Mac:https://github.com/jaywcjlove/awesome-mac/blob/master/README-zh.m Awsome Windows: https://github.com/Awesome-Windows/Awesome/blob/master/README-cn.md 9. You-Dont-Know-JS Github地址：https://github.com/getify/You-Dont-Know-JS star: 112 k 介绍: 您还不认识JS（书籍系列）-第二版 10. oh-my-zsh Github地址：https://github.com/ohmyzsh/ohmyzsh star: 99.4 k 介绍: 一个令人愉快的社区驱动的框架（拥有近1500个贡献者），用于管理zsh配置。包括200多个可选插件（rails, git, OSX, hub, capistrano, brew, ant, php, python等），140多个主题，可为您的早晨增光添彩，以及一个自动更新工具，可让您轻松保持与来自社区的最新更新…… 下面就是 oh-my-zsh 提供的一个花里胡哨的主题： "},"zother6-JavaGuide/tools/Docker-Image.html":{"url":"zother6-JavaGuide/tools/Docker-Image.html","title":"Docker Image","keywords":"","body":"镜像作为 Docker 三大核心概念中，最重要的一个关键词，它有很多操作，是您想学习容器技术不得不掌握的。本文将带您一步一步，图文并重，上手操作来学习它。 目录 一 Docker 下载镜像 1.1 下载镜像 1.2 验证 1.3 下载镜像相关细节 1.4 PULL 子命令 二 Docker 查看镜像信息 2.1 images 命令列出镜像 2.2 使用 tag 命令为镜像添加标签 2.3 使用 inspect 命令查看镜像详细信息 2.4 使用 history 命令查看镜像历史 三 Docker 搜索镜像 3.1 search 命令 3.2 search 子命令 四 Docker 删除镜像 4.1 通过标签删除镜像 4.2 通过 ID 删除镜像 4.3 删除镜像的限制 4.4 清理镜像 五 Docker 创建镜像 5.1 基于已有的镜像创建 5.2 基于 Dockerfile 创建 六 Docker 导出&加载镜像 6.1 导出镜像 6.2 加载镜像 七 Docker 上传镜像 7.1 获取 Docker ID 7.2 创建镜像仓库 7.3 上传镜像 八 总结 一 Docker 下载镜像 如果我们想要在本地运行容器，就必须保证本地存在对应的镜像。所以，第一步，我们需要下载镜像。当我们尝试下载镜像时，Docker 会尝试先从默认的镜像仓库（默认使用 Docker Hub 公共仓库）去下载，当然了，用户也可以自定义配置想要下载的镜像仓库。 1.1 下载镜像 镜像是运行容器的前提，我们可以使用 docker pull[IMAGE_NAME]:[TAG]命令来下载镜像，其中 IMAGE_NAME 表示的是镜像的名称，而 TAG 是镜像的标签，也就是说我们需要通过 “镜像 + 标签” 的方式来下载镜像。 注意： 您也可以不显式地指定 TAG, 它会默认下载 latest 标签，也就是下载仓库中最新版本的镜像。这里并不推荐您下载 latest 标签，因为该镜像的内容会跟踪镜像的最新版本，并随之变化，所以它是不稳定的。在生产环境中，可能会出现莫名其妙的 bug, 推荐您最好还是显示的指定具体的 TAG。 举个例子，如我们想要下载一个 Mysql 5.7 镜像，可以通过命令来下载： docker pull mysql:5.7 会看到控制台输出内容如下： 注意： 由于官方 DockerHub 仓库服务器在国外，下载速度较慢，所以我将仓库的地址更改成了国内的 docker.io 的镜像仓库，所以在上图中，镜像前面会有 docker.io 出现。 当有 Downloaded 字符串输出的时候，说明下载成功了！！ 1.2 验证 让我们来验证一下，本地是否存在 Mysql5.7 的镜像，运行命令： docker images 可以看到本地的确存在该镜像，确实是下载成功了！ 1.3 下载镜像相关细节 再说说上面下载镜像的过程： 通过下载过程，可以看到，一个镜像一般是由多个层（ layer） 组成，类似 f7e2b70d04ae这样的串表示层的唯一 ID（实际上完整的 ID 包括了 256 个 bit, 64 个十六进制字符组成）。 您可能会想，如果多个不同的镜像中，同时包含了同一个层（ layer）,这样重复下载，岂不是导致了存储空间的浪费么? 实际上，Docker 并不会这么傻会去下载重复的层（ layer）,Docker 在下载之前，会去检测本地是否会有同样 ID 的层，如果本地已经存在了，就直接使用本地的就好了。 另一个问题，不同仓库中，可能也会存在镜像重名的情况发生, 这种情况咋办？ 严格意义上，我们在使用 docker pull 命令时，还需要在镜像前面指定仓库地址( Registry), 如果不指定，则 Docker 会使用您默认配置的仓库地址。例如上面，由于我配置的是国内 docker.io 的仓库地址，我在 pull 的时候，docker 会默认为我加上 docker.io/library 的前缀。 如：当我执行 docker pull mysql:5.7 命令时，实际上相当于 docker pull docker.io/mysql:5.7，如果您未自定义配置仓库，则默认在下载的时候，会在镜像前面加上 DockerHub 的地址。 Docker 通过前缀地址的不同，来保证不同仓库中，重名镜像的唯一性。 1.4 PULL 子命令 命令行中输入： docker pull --help 会得到如下信息： [root@iZbp1j8y1bab0djl9gdp33Z ~]# docker pull --help Usage: docker pull [OPTIONS] NAME[:TAG|@DIGEST] Pull an image or a repository from a registry Options: -a, --all-tags Download all tagged images in the repository --disable-content-trust Skip image verification (default true) --help Print usage 我们可以看到主要支持的子命令有： -a,--all-tags=true|false: 是否获取仓库中所有镜像，默认为否； --disable-content-trust: 跳过镜像内容的校验，默认为 true; 二 Docker 查看镜像信息 2.1 images 命令列出镜像 通过使用如下两个命令，列出本机已有的镜像： docker images 或： docker image ls 如下图所示： 对上述红色标注的字段做一下解释： REPOSITORY: 来自于哪个仓库； TAG: 镜像的标签信息，比如 5.7、latest 表示不同的版本信息； IMAGE ID: 镜像的 ID, 如果您看到两个 ID 完全相同，那么实际上，它们指向的是同一个镜像，只是标签名称不同罢了； CREATED: 镜像最后的更新时间； SIZE: 镜像的大小，优秀的镜像一般体积都比较小，这也是我更倾向于使用轻量级的 alpine 版本的原因； 注意：图中的镜像大小信息只是逻辑上的大小信息，因为一个镜像是由多个镜像层（ layer）组成的，而相同的镜像层本地只会存储一份，所以，真实情况下，占用的物理存储空间大小，可能会小于逻辑大小。 2.2 使用 tag 命令为镜像添加标签 通常情况下，为了方便在后续工作中，快速地找到某个镜像，我们可以使用 docker tag 命令，为本地镜像添加一个新的标签。如下图所示： 为 docker.io/mysql 镜像，添加新的镜像标签 allen_mysql:5.7。然后使用 docker images 命令，查看本地镜像： 可以看到，本地多了一个 allen_mysql:5.7 的镜像。细心的你一定还会发现， allen_mysql:5.7 和 docker.io/mysql:5.7 的镜像 ID 是一模一样的，说明它们是同一个镜像，只是别名不同而已。 docker tag 命令功能更像是, 为指定镜像添加快捷方式一样。 2.3 使用 inspect 命令查看镜像详细信息 通过 docker inspect 命令，我们可以获取镜像的详细信息，其中，包括创建者，各层的数字摘要等。 docker inspect docker.io/mysql:5.7 docker inspect 返回的是 JSON 格式的信息，如果您想获取其中指定的一项内容，可以通过 -f 来指定，如获取镜像大小： docker inspect -f .Size docker.io/mysql:5.7 2.4 使用 history 命令查看镜像历史 前面的小节中，我们知道了，一个镜像是由多个层（layer）组成的，那么，我们要如何知道各个层的具体内容呢？ 通过 docker history 命令，可以列出各个层（layer）的创建信息，如我们查看 docker.io/mysql:5.7 的各层信息： docker history docker.io/mysql:5.7 可以看到，上面过长的信息，为了方便展示，后面都省略了，如果您想要看具体信息，可以通过添加 --no-trunc 选项，如下面命令： docker history --no-trunc docker.io/mysql:5.7 三 Docker 搜索镜像 3.1 search 命令 您可以通过下面命令进行搜索： docker search [option] keyword 比如，您想搜索仓库中 mysql 相关的镜像，可以输入如下命令： docker search mysql 3.2 search 子命令 命令行输入 docker search--help, 输出如下： Usage: docker search [OPTIONS] TERM Search the Docker Hub for images Options: -f, --filter filter Filter output based on conditions provided --help Print usage --limit int Max number of search results (default 25) --no-index Don't truncate output --no-trunc Don't truncate output 可以看到 search 支持的子命令有： -f,--filter filter: 过滤输出的内容； --limitint：指定搜索内容展示个数; --no-index: 不截断输出内容； --no-trunc：不截断输出内容； 举个列子，比如我们想搜索官方提供的 mysql 镜像，命令如下： docker search --filter=is-official=true mysql 再比如，我们想搜索 Stars 数超过 100 的 mysql 镜像： docker search --filter=stars=100 mysql 四 Docker 删除镜像 4.1 通过标签删除镜像 通过如下两个都可以删除镜像： docker rmi [image] 或者： docker image rm [image] 支持的子命令如下： -f,-force: 强制删除镜像，即便有容器引用该镜像； -no-prune: 不要删除未带标签的父镜像； Docker 查看镜像信息 例如，我们想删除上章节创建的 allen_mysql:5.7 镜像，命令如下： docker rmi allen_mysql:5.7 从上面章节中，我们知道 allen_mysql:5.7 和 docker.io/mysql:5.7 实际上指向的是同一个镜像，那么，您可以能会有疑问，我删除了 allen_mysql:5.7, 会不会将 docker.io/mysql:5.7 镜像也给删除了？ 实际上，当同一个镜像拥有多个标签时，执行 docker rmi 命令，只是会删除了该镜像众多标签中，您指定的标签而已，并不会影响原始的那个镜像文件。 不信的话，我们可以执行 docker images 命令，来看下 docker.io/mysql:5.7 镜像还在不在： 可以看到， docker.io/mysql:5.7 镜像依然存在！ 那么，如果某个镜像不存在多个标签，当且仅当只有一个标签时，执行删除命令时，您就要小心了，这会彻底删除镜像。 例如，这个时候，我们再执行 docker rmi docker.io/mysql:5.7 命令： 从上图可以看到，我们已经删除了 docker.io/mysql:5.7 镜像的所有文件层。该镜像在本地已不复存在了！ 4.2 通过 ID 删除镜像 除了通过标签名称来删除镜像，我们还可以通过制定镜像 ID, 来删除镜像，如： docker rmi ee7cbd482336 一旦制定了通过 ID 来删除镜像，它会先尝试删除所有指向该镜像的标签，然后在删除镜像本身。 4.3 删除镜像的限制 删除镜像很简单，但也不是我们何时何地都能删除的，它存在一些限制条件。 当通过该镜像创建的容器未被销毁时，镜像是无法被删除的。为了验证这一点，我们来做个试验。首先，我们通过 docker pull alpine 命令，拉取一个最新的 alpine 镜像, 然后启动镜像，让其输出 hello,docker!: 接下来，我们来删除这个镜像试试： 可以看到提示信息，无法删除该镜像，因为有容器正在引用他！同时，这段信息还告诉我们，除非通过添加 -f 子命令，也就是强制删除，才能移除掉该镜像！ docker rmi -f docker.io/alpine 但是，我们一般不推荐这样暴力的做法，正确的做法应该是： 先删除引用这个镜像的容器； 再删除这个镜像； 也就是，根据上图中提示的，引用该镜像的容器 ID ( 9d59e2278553), 执行删除命令： docker rm 9d59e2278553 然后，再执行删除镜像的命令： docker rmi 5cb3aa00f899 Docker 删除镜像 这个时候，就能正常删除了！ 4.4 清理镜像 我们在使用 Docker 一段时间后，系统一般都会残存一些临时的、没有被使用的镜像文件，可以通过以下命令进行清理： docker image prune 它支持的子命令有： -a,--all: 删除所有没有用的镜像，而不仅仅是临时文件； -f,--force：强制删除镜像文件，无需弹出提示确认； 五 Docker 创建镜像 此小节中，您将学习 Docker 如何创建镜像？Docker 创建镜像主要有三种： 基于已有的镜像创建； 基于 Dockerfile 来创建； 基于本地模板来导入； 我们将主要介绍常用的 1，2 两种。 5.1 基于已有的镜像创建 通过如下命令来创建： docker container commit 支持的子命令如下： -a,--author=\"\": 作者信息； -c,--change=[]: 可以在提交的时候执行 Dockerfile 指令，如 CMD、ENTRYPOINT、ENV、EXPOSE、LABEL、ONBUILD、USER、VOLUME、WORIR 等； -m,--message=\"\": 提交信息； -p,--pause=true: 提交时，暂停容器运行。 接下来，基于本地已有的 Ubuntu 镜像，创建一个新的镜像： 首先，让我将它运行起来，并在其中创建一个 test.txt 文件： 命令如下： docker run -it docker.io/ubuntu:latest /bin/bashroot@a0a0c8cfec3a:/# touch test.txtroot@a0a0c8cfec3a:/# exit 创建完 test.txt 文件后，需要记住标注的容器 ID: a0a0c8cfec3a, 用它来提交一个新的镜像(PS: 你也可以通过名称来提交镜像，这里只演示通过 ID 的方式)。 执行命令： docker container commit -m \"Added test.txt file\" -a \"Allen\" a0a0c8cfec3a test:0.1 提交成功后，会返回新创建的镜像 ID 信息，如下图所示： 再次查看本地镜像信息，可以看到新创建的 test:0.1 镜像了： 5.2 基于 Dockerfile 创建 通过 Dockerfile 的方式来创建镜像，是最常见的一种方式了，也是比较推荐的方式。Dockerfile 是一个文本指令文件，它描述了是如何基于一个父镜像，来创建一个新镜像的过程。 下面让我们来编写一个简单的 Dockerfile 文件，它描述了基于 Ubuntu 父镜像，安装 Python3 环境的镜像： FROM docker.io/ubuntu:latest LABEL version=\"1.0\" maintainer=\"Allen \" RUN apt-get update && \\ apt-get install -y python3 && \\ apt-get clean && \\ rm -rf /var/lib/apt/lists/* 创建完成后，通过这个 Dockerfile 文件，来构建新的镜像，执行命令： docker image build -t python:3 . 注意： 命令的最后有个点，如果不加的话，会构建不成功 ！ 编译成功后，再次查看本地镜像信息，就可以看到新构建的 python:3 镜像了。 六 Docker 导出&加载镜像 此小节中，您将学习 Docker 如何导出&加载镜像。 通常我们会有下面这种需求，需要将镜像分享给别人，这个时候，我们可以将镜像导出成 tar 包，别人直接通过加载这个 tar 包，快速地将镜像引入到本地镜像库。 要想使用这两个功能，主要是通过如下两个命令： docker save docker load 6.1 导出镜像 查看本地镜像如下： 例如，我们想要将 python:3 镜像导出来，执行命令： docker save -o python_3.tar python:3 执行成功后，查看当前目录： Docker 导出文件 可以看到 python_3.tar 镜像文件已经生成。接下来，你可以将它通过复制的方式，分享给别人了！ 6.2 加载镜像 别人拿到了这个 tar 包后，要如何导入到本地的镜像库呢？ 通过执行如下命令： docker load -i python_3.tar 或者： docker load 导入成功后，查看本地镜像信息，你就可以获得别人分享的镜像了！怎么样，是不是很方便呢！ 七 Docker 上传镜像 我们将以上传到 Docker Hub 上为示例，演示 Docker 如何上传镜像。 7.1 获取 Docker ID 想要上传镜像到 Docker Hub 上，首先，我们需要注册 Docker Hub 账号。打开 Docker Hub 网址 https://hub.docker.com，开始注册： 填写您的 Docker ID (也就是账号)，以及密码，Email, 点击继续。 接下来，Docker Hub 会发送验证邮件，到您填写的邮箱当中： 点击验证即可，接下来，再次返回 Docker Hub 官网，用您刚刚注册的 Docker ID 和密码来登录账号！ 7.2 创建镜像仓库 登录成功后，会出现如下页面： 选择创建一个镜像仓库： 填写仓库名称、描述信息、是否公开后，点击创建。 仓库镜像展示页 我们看到，仓库已经创建成功了，但是里面还没有任何镜像，接下来开始上传镜像，到此新创建的仓库中。 7.3 上传镜像 进入命令行，用我们刚刚获取的 Docker ID 以及密码登录，执行命令： docker login 命令行登录 Docker ID 登录成功后，我们开始准备上传本地的 python:3 镜像： 首先，我们对其打一个新的标签，前缀与我们新创建的 Docker ID 、仓库名保持一致: docker tag python:3 weiwosuoai1991/python:3 查看本地信息，可以看到，标签打成功了。接下开，开始上传！执行命令： docker push weiwosuoai1991/python:3 上传成功！去 Docker Hub 官网，新创建的仓库的信息页面验证一下，是否真的成功了： 仓库镜像展示页 大工告成！！！ 八 总结 本文中，我们着重学习了 Docker 中下载镜像,、查看镜像信息、搜索镜像、删除镜像,、创建镜像、导出&加载镜像以及向 Docker Hub 上传镜像的相关操作。 "},"zother6-JavaGuide/tools/Docker.html":{"url":"zother6-JavaGuide/tools/Docker.html","title":"Docker","keywords":"","body":"本文只是对 Docker 的概念做了较为详细的介绍，并不涉及一些像 Docker 环境的安装以及 Docker 的一些常见操作和命令。 一 认识容器 Docker 是世界领先的软件容器平台，所以想要搞懂 Docker 的概念我们必须先从容器开始说起。 1.1 什么是容器? 先来看看容器较为官方的解释 一句话概括容器：容器就是将软件打包成标准化单元，以用于开发、交付和部署。 容器镜像是轻量的、可执行的独立软件包 ，包含软件运行所需的所有内容：代码、运行时环境、系统工具、系统库和设置。 容器化软件适用于基于 Linux 和 Windows 的应用，在任何环境中都能够始终如一地运行。 容器赋予了软件独立性　，使其免受外在环境差异（例如，开发和预演环境的差异）的影响，从而有助于减少团队间在相同基础设施上运行不同软件时的冲突。 再来看看容器较为通俗的解释 如果需要通俗的描述容器的话，我觉得容器就是一个存放东西的地方，就像书包可以装各种文具、衣柜可以放各种衣服、鞋架可以放各种鞋子一样。我们现在所说的容器存放的东西可能更偏向于应用比如网站、程序甚至是系统环境。 1.2 图解物理机,虚拟机与容器 关于虚拟机与容器的对比在后面会详细介绍到，这里只是通过网上的图片加深大家对于物理机、虚拟机与容器这三者的理解(下面的图片来源与网络)。 物理机 虚拟机： 容器： 通过上面这三张抽象图，我们可以大概可以通过类比概括出： 容器虚拟化的是操作系统而不是硬件，容器之间是共享同一套操作系统资源的。虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统。因此容器的隔离级别会稍低一些。 相信通过上面的解释大家对于容器这个既陌生又熟悉的概念有了一个初步的认识，下面我们就来谈谈 Docker 的一些概念。 二 再来谈谈 Docker 的一些概念 2.1 什么是 Docker? 说实话关于 Docker 是什么并太好说，下面我通过四点向你说明 Docker 到底是个什么东西。 Docker 是世界领先的软件容器平台。 Docker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核 提供的 CGroup 功能和 name space 来实现的，以及 AUFS 类的 UnionFS 等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术。 由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。 Docker 能够自动执行重复性任务，例如搭建和配置开发环境，从而解放了开发人员以便他们专注在真正重要的事情上：构建杰出的软件。 用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。 2.2 Docker 思想 集装箱 标准化： ① 运输方式 ② 存储方式 ③ API 接口 隔离 2.3 Docker 容器的特点 轻量 在一台机器上运行的多个 Docker 容器可以共享这台机器的操作系统内核；它们能够迅速启动，只需占用很少的计算和内存资源。镜像是通过文件系统层进行构造的，并共享一些公共文件。这样就能尽量降低磁盘用量，并能更快地下载镜像。 标准 Docker 容器基于开放式标准，能够在所有主流 Linux 版本、Microsoft Windows 以及包括 VM、裸机服务器和云在内的任何基础设施上运行。 安全 Docker 赋予应用的隔离性不仅限于彼此隔离，还独立于底层的基础设施。Docker 默认提供最强的隔离，因此应用出现问题，也只是单个容器的问题，而不会波及到整台机器。 2.4 为什么要用 Docker ? Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性，从而不会再出现 “这段代码在我机器上没问题啊” 这类问题；——一致的运行环境 可以做到秒级、甚至毫秒级的启动时间。大大的节约了开发、测试、部署的时间。——更快速的启动时间 避免公用的服务器，资源会容易受到其他用户的影响。——隔离性 善于处理集中爆发的服务器使用压力；——弹性伸缩，快速扩展 可以很轻易的将在一个平台上运行的应用，迁移到另一个平台上，而不用担心运行环境的变化导致应用无法正常运行的情况。——迁移方便 使用 Docker 可以通过定制应用镜像来实现持续集成、持续交付、部署。——持续交付和部署 三 容器 VS 虚拟机 每当说起容器，我们不得不将其与虚拟机做一个比较。就我而言，对于两者无所谓谁会取代谁，而是两者可以和谐共存。 简单来说： 容器和虚拟机具有相似的资源隔离和分配优势，但功能有所不同，因为容器虚拟化的是操作系统，而不是硬件，因此容器更容易移植，效率也更高。 3.1 两者对比图 传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程；而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便. 3.2 容器与虚拟机总结 容器是一个应用层抽象，用于将代码和依赖资源打包在一起。 多个容器可以在同一台机器上运行，共享操作系统内核，但各自作为独立的进程在用户空间中运行 。与虚拟机相比， 容器占用的空间较少（容器镜像大小通常只有几十兆），瞬间就能完成启动 。 虚拟机 (VM) 是一个物理硬件层抽象，用于将一台服务器变成多台服务器。 管理程序允许多个 VM 在一台机器上运行。每个 VM 都包含一整套操作系统、一个或多个应用、必要的二进制文件和库资源，因此 占用大量空间 。而且 VM 启动也十分缓慢 。 通过 Docker 官网，我们知道了这么多 Docker 的优势，但是大家也没有必要完全否定虚拟机技术，因为两者有不同的使用场景。虚拟机更擅长于彻底隔离整个运行环境。例如，云服务提供商通常采用虚拟机技术隔离不同的用户。而 Docker 通常用于隔离不同的应用 ，例如前端，后端以及数据库。 3.3 容器与虚拟机两者是可以共存的 就我而言，对于两者无所谓谁会取代谁，而是两者可以和谐共存。 四 Docker 基本概念 Docker 中有非常重要的三个基本概念，理解了这三个概念，就理解了 Docker 的整个生命周期。 镜像（Image） 容器（Container） 仓库（Repository） 理解了这三个概念，就理解了 Docker 的整个生命周期 4.1 镜像(Image):一个特殊的文件系统 操作系统分为内核和用户空间。对于 Linux 而言，内核启动后，会挂载 root 文件系统为其提供用户空间支持。而 Docker 镜像（Image），就相当于是一个 root 文件系统。 Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。 镜像不包含任何动态数据，其内容在构建之后也不会被改变。 Docker 设计时，就充分利用 Union FS的技术，将其设计为 分层存储的架构 。 镜像实际是由多层文件系统联合组成。 镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。　比如，删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。 分层存储的特征还使得镜像的复用、定制变的更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。 4.2 容器(Container):镜像运行时的实体 镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等 。 容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的 命名空间。前面讲过镜像使用的是分层存储，容器也是如此。 容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。 按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据 ，容器存储层要保持无状态化。所有的文件写入操作，都应该使用数据卷（Volume）、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主(或网络存储)发生读写，其性能和稳定性更高。数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此， 使用数据卷后，容器可以随意删除、重新 run ，数据却不会丢失。 4.3 仓库(Repository):集中存放镜像文件的地方 镜像构建完成后，可以很容易的在当前宿主上运行，但是， 如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry 就是这样的服务。 一个 Docker Registry 中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。所以说：镜像仓库是 Docker 用来集中存放镜像文件的地方类似于我们之前常用的代码仓库。 通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本 。我们可以通过:的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签.。 这里补充一下 Docker Registry 公开服务和私有 Docker Registry 的概念： Docker Registry 公开服务 是开放给用户使用、允许用户管理镜像的 Registry 服务。一般这类公开服务允许用户免费上传、下载公开的镜像，并可能提供收费服务供用户管理私有镜像。 最常使用的 Registry 公开服务是官方的 Docker Hub ，这也是默认的 Registry，并拥有大量的高质量的官方镜像，网址为：https://hub.docker.com/ 。官方是这样介绍 Docker Hub 的： Docker Hub 是 Docker 官方提供的一项服务，用于与您的团队查找和共享容器镜像。 比如我们想要搜索自己想要的镜像： 在 Docker Hub 的搜索结果中，有几项关键的信息有助于我们选择合适的镜像： OFFICIAL Image ：代表镜像为 Docker 官方提供和维护，相对来说稳定性和安全性较高。 Stars ：和点赞差不多的意思，类似 GitHub 的 Star。 Dowloads ：代表镜像被拉取的次数，基本上能够表示镜像被使用的频度。 当然，除了直接通过 Docker Hub 网站搜索镜像这种方式外，我们还可以通过 docker search 这个命令搜索 Docker Hub 中的镜像，搜索的结果是一致的。 ➜ ~ docker search mysql NAME DESCRIPTION STARS OFFICIAL AUTOMATED mysql MySQL is a widely used, open-source relation… 8763 [OK] mariadb MariaDB is a community-developed fork of MyS… 3073 [OK] mysql/mysql-server Optimized MySQL Server Docker images. Create… 650 [OK] 在国内访问Docker Hub 可能会比较慢国内也有一些云服务商提供类似于 Docker Hub 的公开服务。比如 时速云镜像库、网易云镜像服务、DaoCloud 镜像市场、阿里云镜像库等。 除了使用公开服务外，用户还可以在 本地搭建私有 Docker Registry 。Docker 官方提供了 Docker Registry 镜像，可以直接使用做为私有 Registry 服务。开源的 Docker Registry 镜像只提供了 Docker Registry API 的服务端实现，足以支持 docker 命令，不影响使用。但不包含图形界面，以及镜像维护、用户管理、访问控制等高级功能。 五 常见命令 5.1 基本命令 docker version # 查看docker版本 docker images # 查看所有已下载镜像，等价于：docker image ls 命令 docker container ls # 查看所有容器 docker ps #查看正在运行的容器 docker image prune # 清理临时的、没有被使用的镜像文件。-a, --all: 删除所有没有用的镜像，而不仅仅是临时文件； 5.2 拉取镜像 docker search mysql # 查看mysql相关镜像 docker pull mysql:5.7 # 拉取mysql镜像 docker image ls # 查看所有已下载镜像 5.3 删除镜像 比如我们要删除我们下载的 mysql 镜像。 通过 docker rmi [image] （等价于docker image rm [image]）删除镜像之前首先要确保这个镜像没有被容器引用（可以通过标签名称或者镜像 ID删除）。通过我们前面讲的docker ps命令即可查看。 ➜ ~ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c4cd691d9f80 mysql:5.7 \"docker-entrypoint.s…\" 7 weeks ago Up 12 days 0.0.0.0:3306->3306/tcp, 33060/tcp mysql 可以看到 mysql 正在被 id 为 c4cd691d9f80 的容器引用，我们需要首先通过 docker stop c4cd691d9f80 或者 docker stop mysql暂停这个容器。 然后查看 mysql 镜像的 id ➜ ~ docker images REPOSITORY TAG IMAGE ID CREATED SIZE mysql 5.7 f6509bac4980 3 months ago 373MB 通过 IMAGE ID 或者 REPOSITORY 名字即可删除 docker rmi f6509bac4980 # 或者 docker rmim mysql 六 Build Ship and Run Docker 的概念以及常见命令基本上已经讲完，我们再来谈谈：Build, Ship, and Run。 如果你搜索 Docker 官网，会发现如下的字样：“Docker - Build, Ship, and Run Any App, Anywhere”。那么 Build, Ship, and Run 到底是在干什么呢？ Build（构建镜像） ： 镜像就像是集装箱包括文件以及运行环境等等资源。 Ship（运输镜像） ：主机和仓库间运输，这里的仓库就像是超级码头一样。 Run （运行镜像） ：运行的镜像就是一个容器，容器就是运行程序的地方。 Docker 运行过程也就是去仓库把镜像拉到本地，然后用一条命令把镜像运行起来变成容器。所以，我们也常常将 Docker 称为码头工人或码头装卸工，这和 Docker 的中文翻译搬运工人如出一辙。 七 简单了解一下 Docker 底层原理 7.1 虚拟化技术 首先，Docker 容器虚拟化技术为基础的软件，那么什么是虚拟化技术呢？ 简单点来说，虚拟化技术可以这样定义： 虚拟化技术是一种资源管理技术，是将计算机的各种实体资源)（CPU、内存、磁盘空间、网络适配器等），予以抽象、转换后呈现出来并可供分割、组合为一个或多个电脑配置环境。由此，打破实体结构间的不可切割的障碍，使用户可以比原本的配置更好的方式来应用这些电脑硬件资源。这些资源的新虚拟部分是不受现有资源的架设方式，地域或物理配置所限制。一般所指的虚拟化资源包括计算能力和数据存储。 7.2 Docker 基于 LXC 虚拟容器技术 Docker 技术是基于 LXC（Linux container- Linux 容器）虚拟容器技术的。 LXC，其名称来自 Linux 软件容器（Linux Containers）的缩写，一种操作系统层虚拟化（Operating system–level virtualization）技术，为 Linux 内核容器功能的一个用户空间接口。它将应用软件系统打包成一个软件容器（Container），内含应用软件本身的代码，以及所需要的操作系统核心和库。通过统一的名字空间和共用 API 来分配不同软件容器的可用硬件资源，创造出应用程序的独立沙箱运行环境，使得 Linux 用户可以容易的创建和管理系统或应用容器。 LXC 技术主要是借助 Linux 内核中提供的 CGroup 功能和 name space 来实现的，通过 LXC 可以为软件提供一个独立的操作系统运行环境。 cgroup 和 namespace 介绍： namespace 是 Linux 内核用来隔离内核资源的方式。 通过 namespace 可以让一些进程只能看到与自己相关的一部分资源，而另外一些进程也只能看到与它们自己相关的资源，这两拨进程根本就感觉不到对方的存在。具体的实现方式是把一个或多个进程的相关资源指定在同一个 namespace 中。Linux namespaces 是对全局系统资源的一种封装隔离，使得处于不同 namespace 的进程拥有独立的全局系统资源，改变一个 namespace 中的系统资源只会影响当前 namespace 里的进程，对其他 namespace 中的进程没有影响。 （以上关于 namespace 介绍内容来自https://www.cnblogs.com/sparkdev/p/9365405.html ，更多关于 namespace 的呢内容可以查看这篇文章 ）。 CGroup 是 Control Groups 的缩写，是 Linux 内核提供的一种可以限制、记录、隔离进程组 (process groups) 所使用的物力资源 (如 cpu memory i/o 等等) 的机制。 （以上关于 CGroup 介绍内容来自 https://www.ibm.com/developerworks/cn/linux/1506_cgroup/index.html ，更多关于 CGroup 的呢内容可以查看这篇文章 ）。 cgroup 和 namespace 两者对比： 两者都是将进程进行分组，但是两者的作用还是有本质区别。namespace 是为了隔离进程组之间的资源，而 cgroup 是为了对一组进程进行统一的资源监控和限制。 八 总结 本文主要把 Docker 中的一些常见概念做了详细的阐述，但是并不涉及 Docker 的安装、镜像的使用、容器的操作等内容。这部分东西，希望读者自己可以通过阅读书籍与官方文档的形式掌握。如果觉得官方文档阅读起来很费力的话，这里推荐一本书籍《Docker 技术入门与实战第二版》。 九 推荐阅读 10 分钟看懂 Docker 和 K8S 从零开始入门 K8s：详解 K8s 容器基本概念 十 参考 Linux Namespace 和 Cgroup LXC vs Docker: Why Docker is Better CGroup 介绍、应用实例及原理描述 "},"zother6-JavaGuide/tools/Git.html":{"url":"zother6-JavaGuide/tools/Git.html","title":"Git","keywords":"","body":" 版本控制 什么是版本控制 为什么要版本控制 本地版本控制系统 集中化的版本控制系统 分布式版本控制系统 认识 Git Git 简史 Git 与其他版本管理系统的主要区别 Git 的三种状态 Git 使用快速入门 获取 Git 仓库 记录每次更新到仓库 一个好的 Git 提交消息 推送改动到远程仓库 远程仓库的移除与重命名 查看提交历史 撤销操作 分支 推荐阅读 版本控制 什么是版本控制 版本控制是一种记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统。 除了项目源代码，你可以对任何类型的文件进行版本控制。 为什么要版本控制 有了它你就可以将某个文件回溯到之前的状态，甚至将整个项目都回退到过去某个时间点的状态，你可以比较文件的变化细节，查出最后是谁修改了哪个地方，从而找出导致怪异问题出现的原因，又是谁在何时报告了某个功能缺陷等等。 本地版本控制系统 许多人习惯用复制整个项目目录的方式来保存不同的版本，或许还会改名加上备份时间以示区别。 这么做唯一的好处就是简单，但是特别容易犯错。 有时候会混淆所在的工作目录，一不小心会写错文件或者覆盖意想外的文件。 为了解决这个问题，人们很久以前就开发了许多种本地版本控制系统，大多都是采用某种简单的数据库来记录文件的历次更新差异。 集中化的版本控制系统 接下来人们又遇到一个问题，如何让在不同系统上的开发者协同工作？ 于是，集中化的版本控制系统（Centralized Version Control Systems，简称 CVCS）应运而生。 集中化的版本控制系统都有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。 这么做虽然解决了本地版本控制系统无法让在不同系统上的开发者协同工作的诟病，但也还是存在下面的问题： 单点故障： 中央服务器宕机，则其他人无法使用；如果中心数据库磁盘损坏有没有进行备份，你将丢失所有数据。本地版本控制系统也存在类似问题，只要整个项目的历史记录被保存在单一位置，就有丢失所有历史更新记录的风险。 必须联网才能工作： 受网络状况、带宽影响。 分布式版本控制系统 于是分布式版本控制系统（Distributed Version Control System，简称 DVCS）面世了。 Git 就是一个典型的分布式版本控制系统。 这类系统，客户端并不只提取最新版本的文件快照，而是把代码仓库完整地镜像下来。 这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。 因为每一次的克隆操作，实际上都是一次对代码仓库的完整备份。 分布式版本控制系统可以不用联网就可以工作，因为每个人的电脑上都是完整的版本库，当你修改了某个文件后，你只需要将自己的修改推送给别人就可以了。但是，在实际使用分布式版本控制系统的时候，很少会直接进行推送修改，而是使用一台充当“中央服务器”的东西。这个服务器的作用仅仅是用来方便“交换”大家的修改，没有它大家也一样干活，只是交换修改不方便而已。 分布式版本控制系统的优势不单是不必联网这么简单，后面我们还会看到 Git 极其强大的分支管理等功能。 认识 Git Git 简史 Linux 内核项目组当时使用分布式版本控制系统 BitKeeper 来管理和维护代码。但是，后来开发 BitKeeper 的商业公司同 Linux 内核开源社区的合作关系结束，他们收回了 Linux 内核社区免费使用 BitKeeper 的权力。 Linux 开源社区（特别是 Linux 的缔造者 Linus Torvalds）基于使用 BitKeeper 时的经验教训，开发出自己的版本系统，而且对新的版本控制系统做了很多改进。 Git 与其他版本管理系统的主要区别 Git 在保存和对待各种信息的时候与其它版本控制系统有很大差异，尽管操作起来的命令形式非常相近，理解这些差异将有助于防止你使用中的困惑。 下面我们主要说一个关于 Git 其他版本管理系统的主要差别：对待数据的方式。 Git采用的是直接记录快照的方式，而非差异比较。我后面会详细介绍这两种方式的差别。 大部分版本控制系统（CVS、Subversion、Perforce、Bazaar 等等）都是以文件变更列表的方式存储信息，这类系统将它们保存的信息看作是一组基本文件和每个文件随时间逐步累积的差异。 具体原理如下图所示，理解起来其实很简单，每个我们对提交更新一个文件之后，系统记录都会记录这个文件做了哪些更新，以增量符号Δ(Delta)表示。 我们怎样才能得到一个文件的最终版本呢？ 很简单，高中数学的基本知识，我们只需要将这些原文件和这些增加进行相加就行了。 这种方式有什么问题呢？ 比如我们的增量特别特别多的话，如果我们要得到最终的文件是不是会耗费时间和性能。 Git 不按照以上方式对待或保存数据。 反之，Git 更像是把数据看作是对小型文件系统的一组快照。 每次你提交更新，或在 Git 中保存项目状态时，它主要对当时的全部文件制作一个快照并保存这个快照的索引。 为了高效，如果文件没有修改，Git 不再重新存储该文件，而是只保留一个链接指向之前存储的文件。 Git 对待数据更像是一个 快照流。 Git 的三种状态 Git 有三种状态，你的文件可能处于其中之一： 已提交（committed）：数据已经安全的保存在本地数据库中。 已修改（modified）：已修改表示修改了文件，但还没保存到数据库中。 已暂存（staged）：表示对一个已修改文件的当前版本做了标记，使之包含在下次提交的快照中。 由此引入 Git 项目的三个工作区域的概念：Git 仓库(.git directoty)、工作目录(Working Directory) 以及 暂存区域(Staging Area) 。 基本的 Git 工作流程如下： 在工作目录中修改文件。 暂存文件，将文件的快照放入暂存区域。 提交更新，找到暂存区域的文件，将快照永久性存储到 Git 仓库目录。 Git 使用快速入门 获取 Git 仓库 有两种取得 Git 项目仓库的方法。 在现有目录中初始化仓库: 进入项目目录运行 git init 命令,该命令将创建一个名为 .git 的子目录。 从一个服务器克隆一个现有的 Git 仓库: git clone [url] 自定义本地仓库的名字: git clone [url] directoryname 记录每次更新到仓库 检测当前文件状态 : git status 提出更改（把它们添加到暂存区）：git add filename (针对特定文件)、git add *(所有文件)、git add *.txt（支持通配符，所有 .txt 文件） 忽略文件：.gitignore 文件 提交更新: git commit -m \"代码提交信息\" （每次准备提交前，先用 git status 看下，是不是都已暂存起来了， 然后再运行提交命令 git commit） 跳过使用暂存区域更新的方式 : git commit -a -m \"代码提交信息\"。 git commit 加上 -a 选项，Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 git add 步骤。 移除文件 ：git rm filename （从暂存区域移除，然后提交。） 对文件重命名 ：git mv README.md README(这个命令相当于mv README.md README、git rm README.md、git add README 这三条命令的集合) 一个好的 Git 提交消息 一个好的 Git 提交消息如下： 标题行：用这一行来描述和解释你的这次提交 主体部分可以是很少的几行，来加入更多的细节来解释提交，最好是能给出一些相关的背景或者解释这个提交能修复和解决什么问题。 主体部分当然也可以有几段，但是一定要注意换行和句子不要太长。因为这样在使用 \"git log\" 的时候会有缩进比较好看。 提交的标题行描述应该尽量的清晰和尽量的一句话概括。这样就方便相关的 Git 日志查看工具显示和其他人的阅读。 推送改动到远程仓库 如果你还没有克隆现有仓库，并欲将你的仓库连接到某个远程服务器，你可以使用如下命令添加：·git remote add origin ,比如我们要让本地的一个仓库和 Github 上创建的一个仓库关联可以这样git remote add origin https://github.com/Snailclimb/test.git 将这些改动提交到远端仓库：git push origin master (可以把 master 换成你想要推送的任何分支) 如此你就能够将你的改动推送到所添加的服务器上去了。 远程仓库的移除与重命名 将 test 重命名位 test1：git remote rename test test1 移除远程仓库 test1:git remote rm test1 查看提交历史 在提交了若干更新，又或者克隆了某个项目之后，你也许想回顾下提交历史。 完成这个任务最简单而又有效的工具是 git log 命令。git log 会按提交时间列出所有的更新，最近的更新排在最上面。 可以添加一些参数来查看自己希望看到的内容： 只看某个人的提交记录： git log --author=bob 撤销操作 有时候我们提交完了才发现漏掉了几个文件没有添加，或者提交信息写错了。 此时，可以运行带有 --amend 选项的提交命令尝试重新提交： git commit --amend 取消暂存的文件 git reset filename 撤消对文件的修改: git checkout -- filename 假如你想丢弃你在本地的所有改动与提交，可以到服务器上获取最新的版本历史，并将你本地主分支指向它： git fetch origin git reset --hard origin/master 分支 分支是用来将特性开发绝缘开来的。在你创建仓库的时候，master 是“默认的”分支。在其他分支上进行开发，完成后再将它们合并到主分支上。 我们通常在开发新功能、修复一个紧急 bug 等等时候会选择创建分支。单分支开发好还是多分支开发好，还是要看具体场景来说。 创建一个名字叫做 test 的分支 git branch test 切换当前分支到 test（当你切换分支的时候，Git 会重置你的工作目录，使其看起来像回到了你在那个分支上最后一次提交的样子。 Git 会自动添加、删除、修改文件以确保此时你的工作目录和这个分支最后一次提交时的样子一模一样） git checkout test 你也可以直接这样创建分支并切换过去(上面两条命令的合写) git checkout -b feature_x 切换到主分支 git checkout master 合并分支(可能会有冲突) git merge test 把新建的分支删掉 git branch -d feature_x 将分支推送到远端仓库（推送成功后其他人可见）： git push origin 推荐 在线演示学习工具： 「补充，来自issue729」Learn Git Branching https://oschina.gitee.io/learn-git-branching/ 。该网站可以方便的演示基本的git操作，讲解得明明白白。每一个基本命令的作用和结果。 推荐阅读： Git - 简明指南 图解Git 猴子都能懂得Git入门 https://git-scm.com/book/en/v2 Generating a new SSH key and adding it to the ssh-agent 一个好的 Git 提交消息，出自 Linus 之手 "},"zother6-JavaGuide/tools/阿里云服务器使用经验.html":{"url":"zother6-JavaGuide/tools/阿里云服务器使用经验.html","title":"阿里云服务器使用经验","keywords":"","body":"最近很多阿里云双 11 做活动，优惠力度还挺大的，很多朋友都买以最低的价格买到了自己的云服务器。不论是作为学习机还是部署自己的小型网站或者服务来说都是很不错的！ 但是，很多朋友都不知道如何正确去使用。下面我简单分享一下自己的使用经验。 总结一下，主要涉及下面几个部分，对于新手以及没有这么使用过云服务的朋友还是比较友好的： 善用阿里云镜像市场节省安装 Java 环境的时间，相关说明都在根目录下的 readme.txt. 文件里面； 本地通过 SSH 连接阿里云服务器很容易，配置好 Host地址，通过 root 用户加上实例密码直接连接即可。 本地连接 MySQL 数据库需要简单配置一下安全组和并且允许 root 用户在任何地方进行远程登录。 通过 Alibaba Cloud Toolkit 部署 Spring Boot 项目到阿里云服务器真的很方便。 活动地址 （仅限新人，老用户可以考虑使用家人或者朋友账号购买，推荐799/3年 2核4G 这个性价比和适用面更广） 善用阿里云镜像市场节省安装环境的时间 基本的购买流程这里就不多说了，另外这里需要注意的是：其实 Java 环境是不需要我们手动安装配置的，阿里云提供的镜像市场有一些常用的环境。 阿里云镜像市场是指阿里云建立的、由镜像服务商向用户提供其镜像及相关服务的网络平台。这些镜像在操作系统上整合了具体的软件环境和功能，比如Java、PHP运行环境、控制面板等，供有相关需求的用户开通实例时选用。 具体如何在购买云服务器的时候通过镜像创建实例或者已有ECS用户如何使用镜像可以查看官方详细的介绍，地址： https://help.aliyun.com/knowledge_detail/41987.html?spm=a2c4g.11186631.2.1.561e2098dIdCGZ 当我们成功购买服务器之后如何通过 SSH 连接呢？ 创建好 ECS 后，你绑定的手机会收到短信，会告知你初始密码的。你可以登录管理控制台对密码进行修改，修改密码需要在管理控制台重启服务器才能生效。 你也可以在阿里云 ECS 控制台重置实例密码，如下图所示。 第一种连接方式是直接在阿里云服务器管理的网页上连接。如上图所示， 点击远程连接，然后输入远程连接密码，这个并不是你重置实例密码得到的密码，如果忘记了直接修改远程连接密码即可。 第二种方式是在本地通过命令或者软件连接。 推荐使用这种方式，更加方便。 Windows 推荐使用 Xshell 连接，具体方式如下： Window电脑在家，这里直接用找到的一些图片给大家展示一个。 接着点开，输入账号：root,命名输入刚才设置的密码,点ok就可以了 Mac 或者 Linux 系统都可以直接使用 ssh 命令进行连接，非常方便。 成功连接之后，控制台会打印出如下消息。 ➜ ~ ssh root@47.107.159.12 -p 22 root@47.107.159.12's password: Last login: Wed Oct 30 09:31:31 2019 from 220.249.123.170 Welcome to Alibaba Cloud Elastic Compute Service ! 欢迎使用 Tomcat8 JDK8 Mysql5.7 环境 使用说明请参考 /root/readme.txt 文件 我当时选择是阿里云提供好的 Java 环境，自动就提供了 Tomcat、 JDK8 、Mysql5.7，所以不需要我们再进行安装配置了，节省了很多时间。另外，需要注意的是：一定要看 /readme.txt ，Tomcat、 JDK8 、Mysql5.7相关配置以及安装路径等说明都在里面。 如何连接数据库？ 如需外网远程访问mysql 请参考以上网址 设置mysql及阿里云安全组。 Mysql为了安全性，在默认情况下用户只允许在本地登录，但是可以使用 SSH 方式连接。如果我们不想通过 SSH 方式连接的话就需要对 MySQL 进行简单的配置。 #允许root用户在任何地方进行远程登录，并具有所有库任何操作权限： # *.*代表所有库表 “%”代表所有IP地址 mysql> GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY \"自定义密码\" WITH GRANT OPTION; Query OK, 0 rows affected, 1 warning (0.00 sec) #刷新权限。 mysql>flush privileges; #退出mysql mysql>exit #重启MySQL生效 [root@snailclimb]# systemctl restart mysql 这样的话，我们就能在本地进行连接了。Windows 推荐使用Navicat或者SQLyog。 Window电脑在家，这里用 Mac 上的MySQL可视化工具Sequel Pro给大家演示一下。 如何把一个Spring Boot 项目部署到服务器上呢？ 默认大家都是用 IDEA 进行开发。另外，你要有一个简单的 Spring Boot Web 项目。如果还不了解 Spring Boot 的话，一个简单的 Spring Boot 版 \"Hello World \"项目，地址如下： https://github.com/Snailclimb/springboot-guide/blob/master/docs/start/springboot-hello-world.md 。 1.下载一个叫做 Alibaba Cloud Toolkit 的插件。 2.进入 Preference 配置一个 Access Key ID 和 Access Key Secret。 3.部署项目到 ECS 上。 按照上面这样填写完基本配置之后，然后点击 run 运行即可。运行成功，控制台会打印出如下信息： [INFO] Deployment File is Uploading... [INFO] IDE Version:IntelliJ IDEA 2019.2 [INFO] Alibaba Cloud Toolkit Version:2019.9.1 [INFO] Start upload hello-world-0.0.1-SNAPSHOT.jar [INFO][##################################################] 100% (18609645/18609645) [INFO] Succeed to upload, 18609645 bytes have been uploaded. [INFO] Upload Deployment File to OSS Success [INFO] Target Deploy ECS: { 172.18.245.148 / 47.107.159.12 } [INFO] Command: { source /etc/profile; cd /springboot; } Tip: The deployment package will be temporarily stored in Alibaba Cloud Security OSS and will be deleted after the deployment is complete. Please be assured that no one can access it except you. [INFO] Create Deploy Directory Success. [INFO] Deployment File is Downloading... [INFO] Download Deployment File from OSS Success [INFO] File Upload Total time: 16.676 s 通过控制台答应出的信息可以看出：通过这个插件会自动把这个 Spring Boot 项目打包成一个 jar 包，然后上传到你的阿里云服务器中指定的文件夹中，你只需要登录你的阿里云服务器，然后通过 java -jar hello-world-0.0.1-SNAPSHOT.jar命令运行即可。 [root@snailclimb springboot]# ll total 18176 -rw-r--r-- 1 root root 18609645 Oct 30 08:25 hello-world-0.0.1-SNAPSHOT.jar [root@snailclimb springboot]# java -jar hello-world-0.0.1-SNAPSHOT.jar 然后你就可以在本地访问访问部署在你的阿里云 ECS 上的服务了。 推荐一下阿里云双11的活动：云服务器1折起，仅86元/年，限量抢购！ （仅限新人，老用户可以考虑使用家人或者朋友账号购买，推荐799/3年 2核4G 这个性价比和适用面更广） "},"zother6-JavaGuide/javaguide面试突击版.html":{"url":"zother6-JavaGuide/javaguide面试突击版.html","title":"javaguide面试突击版","keywords":"","body":"今天(2020-03-07)终于把PDF版本的《JavaGuide面试突击版》搞定！废话不多说，直接上成品： 如何获取 公众号后台回复：“面试突击”即可。 关于《JavaGuide面试突击版》 JavaGuide 目前已经 70k+ Star ，目前已经是所有 Java 类别项目中 Star 数量第二的开源项目了。Star虽然很多，但是价值远远比不上 Dubbo 这些开源项目，希望以后可以多出现一些这样的国产开源项目。国产开源项目！加油！奥利给！ 随着越来越多的人参与完善这个项目，这个专注 “Java知识总结+面试指南 ” 项目的知识体系和内容的不断完善。JavaGuide 目前包括下面这两部分内容： Java 核心知识总结； 面试方向：面试题、面试经验、备战面试系列文章以及面试真实体验系列文章 内容的庞大让JavaGuide 显的有一点臃肿。所以，我决定将专门为 Java 面试所写的文章以及来自读者投稿的文章整理成 《JavaGuide面试突击版》 系列，同时也为了更加方便大家阅读查阅。起这个名字也犹豫了很久，大家如果有更好的名字的话也可以向我建议。暂时的定位是将其作为 PDF 电子书，并不会像 JavaGuide 提供在线阅读版本。我之前也免费分享过PDF 版本的《Java面试突击》，期间一共更新了 3 个版本，但是由于后面难以同步和订正所以就没有再更新。《JavaGuide面试突击版》 pdf 版由于我工作流程的转变可以有效避免这个问题。 另外，这段时间，向我提这个建议的读者也不是一个两个，我自己当然也有这个感觉。只是自己一直没有抽出时间去做罢了！毕竟这算是一个比较耗费时间的工程。加油！奥利给！ 这件事情具体耗费时间的地方是内容的排版优化（为了方便导出PDF生成目录），导出 PDF 我是通过 Typora 来做的。 如何学习本项目 提供了非常详细的目录，建议可以从头看是看一遍，如果基础不错的话也可以挑自己需要的章节查看。看的过程中自己要多思考，碰到不懂的地方，自己记得要勤搜索，需要记忆的地方也不要吝啬自己的脑子。 关于更新 《JavaGuide面试突击版》 预计一个月左右会有一次内容更新和完善，大家在我的公众号 JavaGuide 后台回复“面试突击” 即可获取最新版！另外，为了保证自己的辛勤劳动不被恶意盗版滥用，所以我添加了水印并且在一些内容注明版权，希望大家理解。 如何贡献 大家阅读过程中如果遇到错误的地方可以通过微信与我交流（ps:加过我微信的就不要重复添加了，这是另外一个账号，前一个已经满了）。 希望大家给我提反馈的时候可以按照如下格式： 我觉得2.3节Java基础的 2.3.1 这部分的描述有问题，应该这样描述：～巴拉巴拉～ 会更好！具体可以参考Oracle 官方文档，地址：~~~~。 为了提高准确性已经不必要的时间花费，希望大家尽量确保自己想法的准确性。 如何赞赏 如果觉得本文档对你有帮助的话，欢迎加入我的知识星球。创建星球的目的主要是为了提高知识沉淀，微信群的弊端相比大家都了解。星球没有免费的原因是了设立门槛，提高进入读者的质量。我会在星球回答大家的问题，更新更多的大厂面试干货！ 我的知识星球的价格应该是我了解的圈子里面最低的，也就1顿饭钱吧！毕竟关注我的大部分还是学生，我打心底里希望自己分享的东西能对大家有帮助。 "},"zother6-JavaGuide/update-history.html":{"url":"zother6-JavaGuide/update-history.html","title":"Update History","keywords":"","body":"2020-03-07 refer 将面试指南部分的文章移除，避免JavaGuide因为内容的膨胀而过于臃肿。移除部分目前是通过 PDF 文档的形式来分享，详见JavaGuide面试突击版。 2020-02-29 refer 对整体知识体系结构进行了调整和完善。 "},"zother6-JavaGuide/公众号历史文章汇总.html":{"url":"zother6-JavaGuide/公众号历史文章汇总.html","title":"公众号历史文章汇总","keywords":"","body":"热文 盘点阿里巴巴 15 款开发者工具 蚂蚁金服2019实习生面经总结(已拿口头offer) 一千行 MySQL 学习笔记 可能是把Java内存区域讲的最清楚的一篇文章 搞定 JVM 垃圾回收就是这么简单 【原创】Java学习路线以及方法推荐 技术面试复习大纲 Java 必看书籍 Java学习必备书籍推荐终极版！ 基础 关于Java基础你不得不会的34个问题 剖析面试最常见问题之 Java 基础知识 Java8新特性 Java 8 新特性最佳指南 看完这篇文章，别说自己不会用Lambda表达式了！ JVM Java内存区域 垃圾回收 谈Java类文件结构 类加载过程 并发编程 并发编程面试必备：JUC 中的 Atomic 原子类总结 并发编程面试必备：AQS 原理以及 AQS 同步组件总结 BATJ都爱问的多线程面试题 通俗易懂，JDK 并发容器总结 代码质量 八点建议助您写出优雅的Java代码 十分钟搞懂Java效率工具Lombok使用与原理 如何写出让同事无法维护的代码？ Code Review最佳实践 后端开发必备的 RestFul API 知识 网络 搞定计算机网络面试，看这篇就够了（补充版） 系统设计 Spring Spring常见问题总结（补充版） 面试官:“谈谈Spring中都用到了那些设计模式?”。 可能是最漂亮的Spring事务管理详解 SpringMVC 工作原理详解 Spring编程式和声明式事务实例讲解 一文轻松搞懂Spring中bean的作用域与生命周期 SpringBoot 超详细，新手都能看懂 ！使用SpringBoot+Dubbo 搭建一个简单的分布式服务 基于 SpringBoot2.0+优雅整合 SpringBoot+Mybatis 新手也能实现，基于SpirngBoot2.0+ 的 SpringBoot+Mybatis 多数据源配置 SpringBoot 整合 阿里云OSS 存储服务，快来免费搭建一个自己的图床 Spring Boot 实现热部署的一种简单方式 SpringBoot 处理异常的几种常见姿势 5分钟搞懂如何在Spring Boot中Schedule Tasks MyBatis 面试官:“谈谈MyBatis中都用到了那些设计模式?”。 数据库 MySQL MySQL知识点总结[修订版] 【思维导图-索引篇】搞定数据库索引就是这么简单 一条SQL语句在MySQL中如何执行的 一文带你轻松搞懂事务隔离级别(图文详解) 详记一次MySQL千万级大表优化过程！ Redis 史上最全Redis高可用技术解决方案大全 redis 总结——重构版 面试相关 面试中常见的几道智力题 来看看你会做几道？ 面试中常见的几道智力题 来看看你会做几道（2）？ [算法总结] 搞定 BAT 面试——几道常见的子符串算法题 [BAT面试必备] ——几道常见的链表算法题 如何判断一个元素在亿级数据中是否存在？ 可能是一份最适合你的后端面试指南（部分内容前端同样适用） GitHub 上四万 Star 大佬的求职回忆 这7个问题，可能大部分Java程序员都比较关心吧！ 2018年BATJ面试题精选 一位大佬的亲身经历总结：简历和面试的技巧 包装严重的IT行业，作为面试官，我是如何甄别应聘者的包装程度 面试官：你是如何使用JDK来实现自己的缓存（支持高并发）？ 面经 5面阿里，终获offer 记一次蚂蚁金服的面试经历 2019年蚂蚁金服、头条、拼多多的面试总结 蚂蚁金服2019实习生面经总结(已拿口头offer) 2019年蚂蚁金服面经（已拿Offer）！附答案！！ 备战面试系列 【备战春招/秋招系列】程序员的简历就该这样写 【备战春招/秋招系列】初出茅庐的程序员该如何准备面试？ 【备战春招/秋招系列】Java程序员必备书单 【备战春招/秋招系列】面试官问你“有什么问题问我吗？”，你该如何回答？ 【备战春招/秋招系列】美团面经总结基础篇 （附详解答案） 【备战春招/秋招系列】美团面经总结进阶篇 （附详解答案） 【备战春招/秋招系列】美团Java面经总结终结篇 （附详解答案） 面试现场 【面试现场】如何实现可以获取最小值的栈？ 【面试现场】为什么要分稳定排序和非稳定排序？ 【面试现场】如何找到字符串中的最长回文子串？ 【面试现场】如何在10亿数中找出前1000大的数 算法 【算法技巧】位运算装逼指南 Github 热门Java项目推荐 近几个月Github上最热门的Java项目一览（2018-07-20） 推荐10个Java方向最热门的开源项目（8月）( 2018-08-28) Github上 Star 数相加超过 7w+ 的三个面试相关的仓库推荐（ 2018-11-17） 11月 Github Trending 榜最热门的 10 个 Java 项目（ 2018-12-01） 盘点一下Github上开源的Java面试/学习相关的仓库，看完弄懂薪资至少增加10k( 2018-12-24) 12月GithubTrending榜Java项目总结，多了几个新面孔（2019-01-02） 1月份Github上收获最多star的10个项目(2019-02-01) 2019年2月份Github上收获最多Star的10个Java项目（2019-03-05） 3月Github最热门的10个Java开源项目 五一假期充电指南：4月Github最热门的Java项目推荐 架构 8 张图读懂大型网站技术架构 【面试精选】关于大型网站系统架构你不得不懂的10个问题 分布式系统的经典基础理论 软件开发的七条原则 关于分布式计算的一些概念 工具 Git入门看这一篇就够了！ 团队开发中的 Git 实践 IDEA中的Git操作，看这一篇就够了！ 一文搞懂如何在Intellij IDEA中使用Debug，超级详细！ 十分钟搞懂Java效率工具Lombok使用与原理 效率 推荐几个可以提升工作效率的Chrome插件 思维开阔 不就是个短信登录API嘛，有这么复杂吗？ 进阶 可能是把Docker的概念讲的最清楚的一篇文章 后端必备——数据通信知识(RPC、消息队列)一站式总结 可能是全网把 ZooKeeper 概念讲的最清楚的一篇文章 外行人都能看懂的SpringCloud，错过了血亏！ 关于 Dubbo 的重要入门知识点总结 Java 工程师成神之路 | 2019正式版 聊一聊开发常用小工具 新手也能看懂，消息队列其实很简单 杂文闲记 一只准准程序员的唠叨(2018-06-09) 说几件近期的小事(2018-08-02) 选择技术方向都要考虑哪些因素（2018-08-04） 结束了我短暂的秋招，说点自己的感受（2018-10-22） 【周日闲谈】最近想说的几件小事(2018-11-18) 做公众号这一年的经历和一件“大事”(2019-03-10) 几经周折，公众号终于留言功能啦！(2019-03-15) 写在毕业季的大学总结！细数一下大学干过的“傻事”。 (2019-06-11) 入职一个月的职场小白,谈谈自己这段时间的感受 其他好文推荐 谈恋爱也要懂https 快速入门大厂后端面试必备的 Shell 编程 为什么阿里巴巴禁止工程师直接使用日志系统(Log4j、Logback)中的 API 一文搞懂 RabbitMQ 的重要概念以及安装 漫话：如何给女朋友解释什么是RPC Java人才市场年度盘点：转折与终局 Github 上日获 800多 star 的阿里微服务架构分布式事务解决方案 FESCAR开源啦 Cloud Toolkit新版本发布，开发效率 “biu” 起来了 我觉得技术人员该有的提问方式 "}}